{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be568cc0",
   "metadata": {},
   "source": [
    "# Batch & Streaming Load - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Opanowanie metod ładowania danych: COPY INTO, Auto Loader i Structured Streaming.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- COPY INTO: kiedy używać, parametry (FILEFORMAT, VALIDATION_MODE, PATTERN)\n",
    "- Auto Loader (CloudFiles): file notification, checkpointing, schema inference\n",
    "- Schema evolution w praktyce\n",
    "- Structured Streaming: micro-batch architecture\n",
    "- readStream() / writeStream()\n",
    "- Triggering: once vs processingTime\n",
    "- Zarządzanie checkpointami\n",
    "- MERGE na streamingu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98a575e",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 2 - Lakehouse & Delta Lake\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f5837",
   "metadata": {},
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie różnych metod ładowania danych do Delta Lake: batch vs streaming.\n",
    "\n",
    "**Podstawowe pojęcia:**\n",
    "- **COPY INTO**: SQL command dla batch loads z idempotency (incremental batch)\n",
    "- **Auto Loader**: Databricks-managed solution dla incremental file ingestion z automatycznym schema inference\n",
    "- **Structured Streaming**: Spark streaming API z micro-batch processing i exactly-once semantics\n",
    "- **Checkpoint**: Location przechowujący offset/progress dla fault tolerance\n",
    "\n",
    "**Dlaczego to ważne?**\n",
    "Wybór metody ingestion ma wpływ na latency, throughput, cost i operacyjną złożoność. COPY INTO dla batch (hourly/daily), Auto Loader dla near real-time z małymi plikami, Structured Streaming dla pure streaming sources (Kafka, Event Hub)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c590c92b",
   "metadata": {},
   "source": [
    "## Izolacja per użytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c974e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6425842c",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0819f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Wyświetl kontekst użytkownika\n",
    "print(\"=== Kontekst użytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Użytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog i schemat\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Ścieżki do danych i checkpointów\n",
    "ORDERS_BATCH_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "ORDERS_STREAMING_DIR = f\"{DATASET_BASE_PATH}/orders\"  # Folder dla plików streaming\n",
    "CHECKPOINT_PATH = f\"/tmp/{raw_user}/checkpoints\"\n",
    "\n",
    "print(f\"\\n=== Konfiguracja ===\")\n",
    "print(f\"Orders Batch JSON: {ORDERS_BATCH_JSON}\")\n",
    "print(f\"Orders Streaming Directory: {ORDERS_STREAMING_DIR}\")\n",
    "print(f\"Checkpoint path: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892fcbb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 1: COPY INTO - Batch Ingestion\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "COPY INTO to SQL command dla idempotent batch loads. Automatycznie śledzi załadowane pliki i pomija duplikaty. Idealny dla scheduled batch jobs (hourly, daily).\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **Idempotency**: Wielokrotne wykonanie COPY INTO z tymi samymi plikami nie powoduje duplikatów\n",
    "- **File tracking**: Delta Log przechowuje listę załadowanych plików\n",
    "- **Pattern matching**: Możliwość filtrowania plików po nazwie (PATTERN)\n",
    "- **Validation mode**: Kontrola zachowania przy błędach (PERMISSIVE, FAILFAST)\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Scheduled batch loads z cloud storage (S3, ADLS, GCS)\n",
    "- Incremental data ingestion bez ręcznego trackowania offsetów\n",
    "- ETL pipelines z retry logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cea57a",
   "metadata": {},
   "source": [
    "### Przykład 1.1: Podstawowy COPY INTO\n",
    "\n",
    "**Cel:** Załadowanie plików JSON za pomocą COPY INTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73df08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 1.1 - COPY INTO basic\n",
    "\n",
    "# Utwórz target table z poprawnymi typami danych\n",
    "copy_into_table = f\"{BRONZE_SCHEMA}.orders_copy_into\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {copy_into_table} (\n",
    "        order_id STRING,\n",
    "        customer_id STRING,\n",
    "        product_id STRING,\n",
    "        store_id STRING,\n",
    "        order_datetime TIMESTAMP,\n",
    "        quantity INT,\n",
    "        unit_price DOUBLE,\n",
    "        discount_percent INT,\n",
    "        total_amount DOUBLE,\n",
    "        payment_method STRING,\n",
    "        _metadata STRUCT<file_path: STRING, file_name: STRING, file_modification_time: TIMESTAMP>\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"✓ Utworzono target table: {copy_into_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50629aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wykonaj COPY INTO\n",
    "copy_result = spark.sql(f\"\"\"\n",
    "    COPY INTO {copy_into_table}\n",
    "    FROM (SELECT *, _metadata FROM '{ORDERS_BATCH_JSON}')\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('multiLine' = 'true')\n",
    "    COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== COPY INTO Result ===\")\n",
    "display(copy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d274a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź załadowane dane\n",
    "loaded_count = spark.table(copy_into_table).count()\n",
    "print(f\"Załadowano {loaded_count} rekordów\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7736aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyświetl przykładowe dane z metadata\n",
    "print(\"=== Dane z metadata ===\")\n",
    "display(\n",
    "    spark.table(copy_into_table)\n",
    "    .select(\"order_id\", \"customer_id\", \"total_amount\", \"payment_method\", \"_metadata.file_name\")\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f86d8",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "COPY INTO:\n",
    "- **_metadata column**: Automatycznie dodawana kolumna z file path, name, modification time\n",
    "- **Idempotency**: Ponowne wykonanie tego samego COPY INTO nie załaduje duplikatów\n",
    "- **mergeSchema**: Automatyczne dodawanie nowych kolumn (schema evolution)\n",
    "- **File tracking**: Delta Log przechowuje hash załadowanych plików"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450590f",
   "metadata": {},
   "source": [
    "### Przykład 1.2: COPY INTO z VALIDATION_MODE\n",
    "\n",
    "**Cel:** Kontrola zachowania przy błędach w danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c7f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 1.2 - COPY INTO z VALIDATION_MODE\n",
    "\n",
    "# VALIDATION_MODE pozwala testować ingestion bez zapisywania danych\n",
    "# Użyteczne dla weryfikacji schema i quality przed faktycznym załadowaniem\n",
    "\n",
    "validation_table = f\"{BRONZE_SCHEMA}.orders_validation\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {validation_table} (\n",
    "        order_id STRING,\n",
    "        customer_id STRING,\n",
    "        product_id STRING,\n",
    "        store_id STRING,\n",
    "        order_datetime TIMESTAMP,\n",
    "        quantity INT,\n",
    "        unit_price DOUBLE,\n",
    "        discount_percent INT,\n",
    "        total_amount DOUBLE,\n",
    "        payment_method STRING,\n",
    "        _metadata STRUCT<file_path: STRING, file_name: STRING, file_modification_time: TIMESTAMP>\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"✓ Utworzono validation table: {validation_table}\")\n",
    "\n",
    "# Re-run tego samego COPY INTO - demonstracja idempotency\n",
    "print(\"=== Ponowne wykonanie COPY INTO (idempotency test) ===\")\n",
    "copy_result_2 = spark.sql(f\"\"\"\n",
    "    COPY INTO {copy_into_table}\n",
    "    FROM (SELECT *, _metadata FROM '{ORDERS_BATCH_JSON}')\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('multiLine' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "display(copy_result_2)\n",
    "\n",
    "# Sprawdź czy count się nie zmienił (idempotency)\n",
    "new_count = spark.table(copy_into_table).count()\n",
    "print(f\"\\nLiczba rekordów (po ponownym COPY INTO): {new_count}\")\n",
    "print(f\"Czy idempotentny? {new_count == loaded_count}\")\n",
    "\n",
    "# Historia COPY INTO\n",
    "print(\"\\n=== Historia COPY INTO ===\")\n",
    "history = spark.sql(f\"DESCRIBE HISTORY {copy_into_table}\")\n",
    "display(\n",
    "    history\n",
    "    .filter(F.col(\"operation\") == \"COPY INTO\")\n",
    "    .select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46325459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VALIDATION_MODE = N - validate tylko N rekordów (dry run)\n",
    "print(\"=== VALIDATION_MODE - Test 10 rekordów ===\")\n",
    "validation_result = spark.sql(f\"\"\"\n",
    "    COPY INTO {validation_table}\n",
    "    FROM (SELECT *, _metadata FROM '{ORDERS_BATCH_JSON}')\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('multiLine' = 'true')\n",
    "    COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "    VALIDATION_MODE = 10\n",
    "\"\"\")\n",
    "\n",
    "display(validation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1565e199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Po pozytywnej walidacji - faktyczne załadowanie\n",
    "print(\"\\n=== Faktyczne załadowanie danych ===\")\n",
    "load_result = spark.sql(f\"\"\"\n",
    "    COPY INTO {validation_table}\n",
    "    FROM (SELECT *, _metadata FROM '{ORDERS_BATCH_JSON}')\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('multiLine' = 'true')\n",
    "    COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "display(load_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e05518e",
   "metadata": {},
   "source": [
    "### Przykład 1.3: COPY INTO z PATTERN (Selective Ingestion)\n",
    "\n",
    "**Cel:** Filtrowanie plików po nazwie - załaduj tylko pliki spełniające pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec5432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 1.3 - COPY INTO z PATTERN\n",
    "\n",
    "# PATTERN pozwala filtrować pliki po nazwie\n",
    "# Przydatne gdy mamy wiele plików, ale chcemy załadować tylko konkretne\n",
    "\n",
    "pattern_table = f\"{BRONZE_SCHEMA}.orders_pattern\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {pattern_table} (\n",
    "        order_id STRING,\n",
    "        customer_id STRING,\n",
    "        product_id STRING,\n",
    "        store_id STRING,\n",
    "        order_datetime TIMESTAMP,\n",
    "        quantity INT,\n",
    "        unit_price DOUBLE,\n",
    "        discount_percent INT,\n",
    "        total_amount DOUBLE,\n",
    "        payment_method STRING,\n",
    "        _metadata STRUCT<file_path: STRING, file_name: STRING, file_modification_time: TIMESTAMP>\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"✓ Utworzono pattern table: {pattern_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b3650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Załaduj tylko pliki streaming 001, 002, 003 (pattern)\n",
    "print(\"=== COPY INTO z PATTERN - tylko pliki 001-003 ===\")\n",
    "pattern_result = spark.sql(f\"\"\"\n",
    "    COPY INTO {pattern_table}\n",
    "    FROM (SELECT *, _metadata FROM '{ORDERS_STREAMING_DIR}')\n",
    "    FILEFORMAT = JSON\n",
    "    PATTERN = 'orders_stream_00[1-3].json'\n",
    "    FORMAT_OPTIONS ('multiLine' = 'true')\n",
    "    COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "display(pattern_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad72ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź które pliki zostały załadowane\n",
    "print(\"\\n=== Załadowane pliki (metadata) ===\")\n",
    "display(\n",
    "    spark.table(pattern_table)\n",
    "    .select(\"_metadata.file_name\")\n",
    "    .distinct()\n",
    "    .orderBy(\"file_name\")\n",
    ")\n",
    "\n",
    "print(f\"\\nŁącznie załadowano {spark.table(pattern_table).count()} rekordów z plików 001-003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e6a74c",
   "metadata": {},
   "source": [
    "### Przykład 1.4: COPY INTO - Test Idempotency\n",
    "\n",
    "**Cel:** Demonstracja że ponowne wykonanie COPY INTO nie powoduje duplikatów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e0118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 1.4 - Test idempotency\n",
    "\n",
    "# Zapisz obecny count\n",
    "count_before = spark.table(copy_into_table).count()\n",
    "print(f\"Liczba rekordów przed ponownym COPY INTO: {count_before}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ponowne wykonanie COPY INTO\n",
    "print(\"\\n=== Ponowne wykonanie COPY INTO (idempotency test) ===\")\n",
    "copy_result_2 = spark.sql(f\"\"\"\n",
    "    COPY INTO {copy_into_table}\n",
    "    FROM (SELECT *, _metadata FROM '{ORDERS_BATCH_JSON}')\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('multiLine' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "display(copy_result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650575f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź czy count się nie zmienił (idempotency)\n",
    "count_after = spark.table(copy_into_table).count()\n",
    "print(f\"\\nLiczba rekordów po ponownym COPY INTO: {count_after}\")\n",
    "print(f\"Czy idempotentny? {count_after == count_before} (żadne nowe rekordy nie zostały załadowane)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f8b9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historia COPY INTO\n",
    "print(\"\\n=== Historia COPY INTO ===\")\n",
    "history = spark.sql(f\"DESCRIBE HISTORY {copy_into_table}\")\n",
    "display(\n",
    "    history\n",
    "    .filter(F.col(\"operation\") == \"COPY INTO\")\n",
    "    .select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c25fb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 2: Auto Loader (CloudFiles)\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Auto Loader to Databricks-managed solution dla incremental file ingestion. Używa file notification (Event Grid/SQS) lub directory listing dla automatic discovery nowych plików. Idealny dla near real-time ingestion z małymi plikami.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **cloudFiles format**: Specjalny format Spark dla Auto Loader\n",
    "- **Schema inference**: Automatyczne wykrywanie i ewolucja schematu\n",
    "- **Checkpoint location**: Przechowuje progress i schema history\n",
    "- **File notification**: Event-driven approach dla cloud storage\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Near real-time ingestion (latency: sekundy-minuty)\n",
    "- Małe pliki arriving continuously\n",
    "- Schema evolution bez manual intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e3f5b7",
   "metadata": {},
   "source": [
    "### Przykład 2.1: Auto Loader - Basic Setup\n",
    "\n",
    "**Cel:** Konfiguracja Auto Loader z schema inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd4b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 2.1 - Auto Loader basic\n",
    "\n",
    "autoloader_table = f\"{BRONZE_SCHEMA}.orders_autoloader\"\n",
    "autoloader_checkpoint = f\"{CHECKPOINT_PATH}/autoloader_orders\"\n",
    "\n",
    "# Auto Loader z readStream - używa folderu (automatycznie znajdzie wszystkie pliki JSON)\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")  # Auto Loader format\n",
    "    .option(\"cloudFiles.format\", \"json\")  # Source format\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{autoloader_checkpoint}/schema\")  # Schema tracking\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")  # Infer types (not just strings)\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_STREAMING_DIR)  # Folder - Auto Loader znajdzie wszystkie pliki JSON\n",
    ")\n",
    "\n",
    "print(\"=== Auto Loader Stream Schema ===\")\n",
    "orders_stream.printSchema()\n",
    "\n",
    "# Zapis z trigger(once) dla demo (batch mode)\n",
    "query = (\n",
    "    orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", autoloader_checkpoint)\n",
    "    .trigger(once=True)  # Process all available data, then stop\n",
    "    .table(autoloader_table)\n",
    ")\n",
    "\n",
    "# Czekaj na zakończenie\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Auto Loader completed\")\n",
    "print(f\"Załadowano {spark.table(autoloader_table).count()} rekordów\")\n",
    "\n",
    "# Wyświetl dane\n",
    "print(\"\\n=== Załadowane dane ===\")\n",
    "display(spark.table(autoloader_table).limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188039b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapis z trigger(once) dla demo (batch mode)\n",
    "query = (\n",
    "    orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", autoloader_checkpoint)\n",
    "    .trigger(once=True)  # Process all available data, then stop\n",
    "    .table(autoloader_table)\n",
    ")\n",
    "\n",
    "# Czekaj na zakończenie\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Auto Loader completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0585704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź załadowane rekordy\n",
    "print(f\"Załadowano {spark.table(autoloader_table).count()} rekordów\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582bba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyświetl dane\n",
    "print(\"=== Załadowane dane ===\")\n",
    "display(spark.table(autoloader_table).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ac2228",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Auto Loader:\n",
    "- **cloudFiles format**: Specjalny format dla Structured Streaming\n",
    "- **trigger(once=True)**: Batch mode - process all files, then stop (użyteczne dla testing)\n",
    "- **checkpointLocation**: Obowiązkowe - przechowuje progress i schema\n",
    "- **Schema inference**: Automatyczne wykrywanie typów (inferColumnTypes=true)\n",
    "\n",
    "W produkcji: używamy `trigger(processingTime='5 minutes')` dla continuous processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f20e7a",
   "metadata": {},
   "source": [
    "### Przykład 2.2: Auto Loader - Schema Evolution\n",
    "\n",
    "**Cel:** Demonstracja automatycznej ewolucji schematu przy nowych plikach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 2.2 - Schema Evolution\n",
    "\n",
    "# Sprawdź schema location (gdzie Auto Loader przechowuje schema history)\n",
    "schema_location = f\"{autoloader_checkpoint}/schema\"\n",
    "print(f\"Schema location: {schema_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d97cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista plików w schema location\n",
    "print(\"=== Schema history files ===\")\n",
    "schema_files = dbutils.fs.ls(schema_location)\n",
    "for file in schema_files:\n",
    "    print(f\"  {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cce833",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odczytaj schema history\n",
    "print(\"=== Current Schema ===\")\n",
    "current_schema = spark.table(autoloader_table).schema\n",
    "for field in current_schema.fields:\n",
    "    print(f\"  {field.name}: {field.dataType}\")\n",
    "\n",
    "# W przypadku nowych plików z dodatkowymi kolumnami,\n",
    "# Auto Loader automatycznie zaktualizuje schemat\n",
    "print(\"\\n⚠️ Uwaga: Schema evolution działa automatycznie przy nowych plikach z dodatkowymi kolumnami\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79628954",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 3: Structured Streaming - Continuous Processing\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Structured Streaming to Spark API dla continuous data processing. Traktuje stream jako unbounded table z micro-batch execution. Zapewnia exactly-once semantics i fault tolerance.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **readStream / writeStream**: API dla streaming operations\n",
    "- **Trigger**: Processing interval (once, processingTime, availableNow)\n",
    "- **Output mode**: append, complete, update\n",
    "- **Watermark**: Time-based windowing dla late data handling\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Real-time ETL z Kafka, Event Hub, Kinesis\n",
    "- Continuous aggregations i windowing\n",
    "- Exactly-once processing semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470b6504",
   "metadata": {},
   "source": [
    "### Przykład 3.1: Structured Streaming - Basic Stream\n",
    "\n",
    "**Cel:** Utworzenie basic streaming pipeline z transformacjami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79512634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 3.1 - Structured Streaming basic\n",
    "\n",
    "streaming_table = f\"{BRONZE_SCHEMA}.orders_streaming\"\n",
    "streaming_checkpoint = f\"{CHECKPOINT_PATH}/streaming_orders\"\n",
    "\n",
    "# ReadStream z transformacjami - używa folderu\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{streaming_checkpoint}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_STREAMING_DIR)  # Folder - Auto Loader znajdzie wszystkie pliki JSON\n",
    ")\n",
    "\n",
    "# Transformacje na streamie (jak na batch DataFrame)\n",
    "orders_transformed = (\n",
    "    orders_stream\n",
    "    .withColumn(\"order_date\", F.to_date(F.col(\"order_datetime\")))  # Ekstrakcja daty z timestamp\n",
    "    .withColumn(\"payment_method_upper\", F.upper(F.col(\"payment_method\")))  # Normalizacja payment_method\n",
    "    .withColumn(\"stream_processed_ts\", F.current_timestamp())\n",
    "    .filter(F.col(\"total_amount\") > 0)  # Quality check - pozytywna kwota\n",
    ")\n",
    "\n",
    "print(\"=== Transformed Stream Schema ===\")\n",
    "orders_transformed.printSchema()\n",
    "\n",
    "# WriteStream z trigger(once) dla demo\n",
    "query = (\n",
    "    orders_transformed\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", streaming_checkpoint)\n",
    "    .trigger(once=True)\n",
    "    .table(streaming_table)\n",
    ")\n",
    "\n",
    "# Czekaj na zakończenie\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Streaming pipeline completed\")\n",
    "print(f\"Przetworzono {spark.table(streaming_table).count()} rekordów\")\n",
    "\n",
    "# Wyświetl dane z transformation\n",
    "print(\"\\n=== Przetworzone dane ===\")\n",
    "display(\n",
    "    spark.table(streaming_table)\n",
    "    .select(\"order_id\", \"order_date\", \"payment_method\", \"payment_method_upper\", \"total_amount\", \"stream_processed_ts\")\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c190abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WriteStream z trigger(once) dla demo\n",
    "query = (\n",
    "    orders_transformed\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", streaming_checkpoint)\n",
    "    .trigger(once=True)\n",
    "    .table(streaming_table)\n",
    ")\n",
    "\n",
    "# Czekaj na zakończenie\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Streaming pipeline completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5924077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź przetworzone rekordy\n",
    "print(f\"Przetworzono {spark.table(streaming_table).count()} rekordów\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57196c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyświetl dane z transformation\n",
    "print(\"=== Przetworzone dane ===\")\n",
    "display(\n",
    "    spark.table(streaming_table)\n",
    "    .select(\"order_id\", \"order_date\", \"payment_method\", \"payment_method_upper\", \"total_amount\", \"stream_processed_ts\")\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961206d2",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Structured Streaming:\n",
    "- **Transformacje**: Możemy używać standardowych DataFrame API (withColumn, filter, join)\n",
    "- **trigger(once=True)**: Batch mode - użyteczne dla testing i backfill\n",
    "- **outputMode=\"append\"**: Tylko nowe rekordy zapisywane (domyślne dla streaming)\n",
    "- **checkpointLocation**: Fault tolerance - możliwość recovery po failure\n",
    "- **Przykładowe transformacje**: ekstrakcja daty z timestamp, normalizacja payment_method, quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac4add",
   "metadata": {},
   "source": [
    "### Przykład 3.2: Streaming Aggregations\n",
    "\n",
    "**Cel:** Continuous aggregations na streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10052b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 3.2 - Streaming Aggregations\n",
    "\n",
    "streaming_agg_table = f\"{BRONZE_SCHEMA}.orders_streaming_agg\"\n",
    "agg_checkpoint = f\"{CHECKPOINT_PATH}/streaming_agg\"\n",
    "\n",
    "# ReadStream - używa folderu\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{agg_checkpoint}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_STREAMING_DIR)  # Folder - Auto Loader znajdzie wszystkie pliki JSON\n",
    ")\n",
    "\n",
    "# Agregacje: count i sum per payment_method\n",
    "orders_agg = (\n",
    "    orders_stream\n",
    "    .withColumn(\"payment_method_upper\", F.upper(F.col(\"payment_method\")))\n",
    "    .groupBy(\"payment_method_upper\")\n",
    "    .agg(\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_order_value\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# WriteStream z outputMode=\"complete\" dla agregacji\n",
    "query = (\n",
    "    orders_agg\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"complete\")  # Complete mode dla groupBy bez watermark\n",
    "    .option(\"checkpointLocation\", agg_checkpoint)\n",
    "    .trigger(once=True)\n",
    "    .table(streaming_agg_table)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Streaming aggregation completed\")\n",
    "print(\"\\n=== Wyniki agregacji per payment method ===\")\n",
    "display(spark.table(streaming_agg_table).orderBy(\"payment_method_upper\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85945717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WriteStream z outputMode=\"complete\" dla agregacji\n",
    "query = (\n",
    "    orders_agg\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"complete\")  # Complete mode dla groupBy bez watermark\n",
    "    .option(\"checkpointLocation\", agg_checkpoint)\n",
    "    .trigger(once=True)\n",
    "    .table(streaming_agg_table)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Streaming aggregation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71afaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyświetl wyniki agregacji per payment method\n",
    "print(\"=== Wyniki agregacji per payment method ===\")\n",
    "display(spark.table(streaming_agg_table).orderBy(\"payment_method_upper\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6546288",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Streaming Aggregations:\n",
    "- **outputMode=\"complete\"**: Cała tabela wynikowa zapisywana przy każdym micro-batch (wymagane dla groupBy bez watermark)\n",
    "- **outputMode=\"update\"**: Tylko zaktualizowane wiersze (użyteczne z watermark)\n",
    "- **Stateful operations**: GroupBy/Aggregations wymagają state management (przechowywany w checkpoint)\n",
    "- **Use case**: Agregacje per payment method (Cash, Credit Card, Debit Card, PayPal) w czasie rzeczywistym\n",
    "\n",
    "W produkcji: używamy watermark dla windowed aggregations i outputMode=\"update\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9bad4ad",
   "metadata": {},
   "source": [
    "### Przykład 3.3: Różne tryby Triggering\n",
    "\n",
    "**Cel:** Demonstracja różnych trybów triggering: once, availableNow, processingTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab8425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 3.3a - trigger(once=True)\n",
    "# Process all available data in single batch, then stop\n",
    "\n",
    "trigger_once_table = f\"{BRONZE_SCHEMA}.orders_trigger_once\"\n",
    "trigger_once_checkpoint = f\"{CHECKPOINT_PATH}/trigger_once\"\n",
    "\n",
    "print(\"=== Trigger: once=True ===\")\n",
    "print(\"Use case: Testing, backfill, one-time ingestion\")\n",
    "\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{trigger_once_checkpoint}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_STREAMING_DIR)\n",
    ")\n",
    "\n",
    "query = (\n",
    "    orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", trigger_once_checkpoint)\n",
    "    .trigger(once=True)  # Process all data once, then stop\n",
    "    .table(trigger_once_table)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(f\"✓ Trigger(once) completed: {spark.table(trigger_once_table).count()} rekordów\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f788e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 3.3b - trigger(availableNow=True)\n",
    "# Process all available data in multiple micro-batches, then stop\n",
    "\n",
    "trigger_available_table = f\"{BRONZE_SCHEMA}.orders_trigger_available\"\n",
    "trigger_available_checkpoint = f\"{CHECKPOINT_PATH}/trigger_available\"\n",
    "\n",
    "print(\"\\n=== Trigger: availableNow=True ===\")\n",
    "print(\"Use case: Backfill z zachowaniem micro-batch semantics\")\n",
    "\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{trigger_available_checkpoint}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_STREAMING_DIR)\n",
    ")\n",
    "\n",
    "query = (\n",
    "    orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", trigger_available_checkpoint)\n",
    "    .trigger(availableNow=True)  # Process all available data in micro-batches, then stop\n",
    "    .table(trigger_available_table)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(f\"✓ Trigger(availableNow) completed: {spark.table(trigger_available_table).count()} rekordów\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4532c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 3.3c - trigger(processingTime='10 seconds')\n",
    "# Continuous processing z micro-batch co 10 sekund\n",
    "\n",
    "trigger_continuous_table = f\"{BRONZE_SCHEMA}.orders_trigger_continuous\"\n",
    "trigger_continuous_checkpoint = f\"{CHECKPOINT_PATH}/trigger_continuous\"\n",
    "\n",
    "print(\"\\n=== Trigger: processingTime='10 seconds' ===\")\n",
    "print(\"Use case: Near real-time continuous processing\")\n",
    "print(\"⚠️ Ten przykład uruchomi streaming query na 30 sekund, potem zatrzyma\")\n",
    "\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{trigger_continuous_checkpoint}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_STREAMING_DIR)\n",
    ")\n",
    "\n",
    "query = (\n",
    "    orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", trigger_continuous_checkpoint)\n",
    "    .trigger(processingTime='10 seconds')  # Continuous: micro-batch co 10 sekund\n",
    "    .table(trigger_continuous_table)\n",
    ")\n",
    "\n",
    "# Start query (non-blocking)\n",
    "query_handle = query.start()\n",
    "\n",
    "print(\"Query running...\")\n",
    "print(\"Czekam 30 sekund (pozwoli na ~3 micro-batches)...\")\n",
    "time.sleep(30)\n",
    "\n",
    "# Stop query\n",
    "query_handle.stop()\n",
    "print(f\"\\n✓ Trigger(processingTime) stopped after 30s: {spark.table(trigger_continuous_table).count()} rekordów\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecc8af8",
   "metadata": {},
   "source": [
    "**Porównanie triggerów:**\n",
    "\n",
    "| Trigger | Use Case | Zatrzyma się? | Micro-batches |\n",
    "|---------|----------|---------------|---------------|\n",
    "| `once=True` | Testing, backfill | ✅ Tak | 1 batch |\n",
    "| `availableNow=True` | Backfill z micro-batch | ✅ Tak | Multiple |\n",
    "| `processingTime='X'` | Continuous production | ❌ Nie | Infinite |\n",
    "\n",
    "**Rekomendacje:**\n",
    "- **once**: Testing w notebookach, one-time data load\n",
    "- **availableNow**: Backfill historycznych danych z zachowaniem micro-batch semantics\n",
    "- **processingTime**: Production continuous streaming (Kafka, Event Hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6171b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 4: MERGE na Streamingu (Upsert)\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Structured Streaming może pisać do Delta z MERGE logic (upsert). Używamy foreachBatch dla custom write logic w każdym micro-batch.\n",
    "\n",
    "**Zastosowanie:**\n",
    "- CDC (Change Data Capture) streaming\n",
    "- Upsert streaming events do dimension tables\n",
    "- Deduplication w real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c815aaeb",
   "metadata": {},
   "source": [
    "### Przykład 4.1: Streaming MERGE (Upsert)\n",
    "\n",
    "**Cel:** Implementacja streaming upsert z MERGE INTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c4ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 4.1 - Streaming MERGE\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Target table dla upsert\n",
    "upsert_table = f\"{BRONZE_SCHEMA}.orders_upsert\"\n",
    "upsert_checkpoint = f\"{CHECKPOINT_PATH}/streaming_upsert\"\n",
    "\n",
    "# Utwórz target table jeśli nie istnieje - poprawne typy danych\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {upsert_table} (\n",
    "        order_id STRING,\n",
    "        customer_id STRING,\n",
    "        product_id STRING,\n",
    "        store_id STRING,\n",
    "        order_datetime TIMESTAMP,\n",
    "        quantity INT,\n",
    "        unit_price DOUBLE,\n",
    "        discount_percent INT,\n",
    "        total_amount DOUBLE,\n",
    "        payment_method STRING,\n",
    "        last_updated TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"✓ Utworzono upsert table: {upsert_table}\")\n",
    "\n",
    "# ForeachBatch function dla MERGE\n",
    "def upsert_to_delta(microBatchDF, batchId):\n",
    "    # Dodaj last_updated timestamp\n",
    "    microBatchDF = microBatchDF.withColumn(\"last_updated\", F.current_timestamp())\n",
    "    \n",
    "    # DeltaTable dla MERGE\n",
    "    deltaTable = DeltaTable.forName(spark, upsert_table)\n",
    "    \n",
    "    # MERGE INTO logic\n",
    "    (\n",
    "        deltaTable.alias(\"target\")\n",
    "        .merge(\n",
    "            microBatchDF.alias(\"source\"),\n",
    "            \"target.order_id = source.order_id\"\n",
    "        )\n",
    "        .whenMatchedUpdate(set = {\n",
    "            \"payment_method\": \"source.payment_method\",\n",
    "            \"total_amount\": \"source.total_amount\",\n",
    "            \"order_datetime\": \"source.order_datetime\",\n",
    "            \"quantity\": \"source.quantity\",\n",
    "            \"last_updated\": \"source.last_updated\"\n",
    "        })\n",
    "        .whenNotMatchedInsert(values = {\n",
    "            \"order_id\": \"source.order_id\",\n",
    "            \"customer_id\": \"source.customer_id\",\n",
    "            \"product_id\": \"source.product_id\",\n",
    "            \"store_id\": \"source.store_id\",\n",
    "            \"order_datetime\": \"source.order_datetime\",\n",
    "            \"quantity\": \"source.quantity\",\n",
    "            \"unit_price\": \"source.unit_price\",\n",
    "            \"discount_percent\": \"source.discount_percent\",\n",
    "            \"total_amount\": \"source.total_amount\",\n",
    "            \"payment_method\": \"source.payment_method\",\n",
    "            \"last_updated\": \"source.last_updated\"\n",
    "        })\n",
    "        .execute()\n",
    "    )\n",
    "    \n",
    "    print(f\"Batch {batchId}: Merged {microBatchDF.count()} records\")\n",
    "\n",
    "# ReadStream - używa wildcard dla plików streaming\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{upsert_checkpoint}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_STREAMING_JSON)  # Wildcard dla orders_stream_*.json\n",
    ")\n",
    "\n",
    "# WriteStream z foreachBatch\n",
    "query = (\n",
    "    orders_stream\n",
    "    .writeStream\n",
    "    .foreachBatch(upsert_to_delta)  # Custom MERGE logic\n",
    "    .option(\"checkpointLocation\", upsert_checkpoint)\n",
    "    .trigger(once=True)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Streaming MERGE completed\")\n",
    "print(f\"Final count: {spark.table(upsert_table).count()}\")\n",
    "\n",
    "print(\"\\n=== Upserted data ===\")\n",
    "display(spark.table(upsert_table).orderBy(\"order_id\").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ae6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ForeachBatch function dla MERGE\n",
    "def upsert_to_delta(microBatchDF, batchId):\n",
    "    \"\"\"\n",
    "    Custom MERGE logic dla każdego micro-batch.\n",
    "    Wykonuje UPSERT (UPDATE jeśli istnieje, INSERT jeśli nie).\n",
    "    \"\"\"\n",
    "    # Dodaj last_updated timestamp\n",
    "    microBatchDF = microBatchDF.withColumn(\"last_updated\", F.current_timestamp())\n",
    "    \n",
    "    # DeltaTable dla MERGE\n",
    "    deltaTable = DeltaTable.forName(spark, upsert_table)\n",
    "    \n",
    "    # MERGE INTO logic\n",
    "    (\n",
    "        deltaTable.alias(\"target\")\n",
    "        .merge(\n",
    "            microBatchDF.alias(\"source\"),\n",
    "            \"target.order_id = source.order_id\"\n",
    "        )\n",
    "        .whenMatchedUpdate(set = {\n",
    "            \"payment_method\": \"source.payment_method\",\n",
    "            \"total_amount\": \"source.total_amount\",\n",
    "            \"order_datetime\": \"source.order_datetime\",\n",
    "            \"quantity\": \"source.quantity\",\n",
    "            \"last_updated\": \"source.last_updated\"\n",
    "        })\n",
    "        .whenNotMatchedInsert(values = {\n",
    "            \"order_id\": \"source.order_id\",\n",
    "            \"customer_id\": \"source.customer_id\",\n",
    "            \"product_id\": \"source.product_id\",\n",
    "            \"store_id\": \"source.store_id\",\n",
    "            \"order_datetime\": \"source.order_datetime\",\n",
    "            \"quantity\": \"source.quantity\",\n",
    "            \"unit_price\": \"source.unit_price\",\n",
    "            \"discount_percent\": \"source.discount_percent\",\n",
    "            \"total_amount\": \"source.total_amount\",\n",
    "            \"payment_method\": \"source.payment_method\",\n",
    "            \"last_updated\": \"source.last_updated\"\n",
    "        })\n",
    "        .execute()\n",
    "    )\n",
    "    \n",
    "    print(f\"Batch {batchId}: Merged {microBatchDF.count()} records\")\n",
    "\n",
    "print(\"✓ Funkcja upsert_to_delta zdefiniowana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c52539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReadStream - używa folderu\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{upsert_checkpoint}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_STREAMING_DIR)  # Folder - Auto Loader znajdzie wszystkie pliki JSON\n",
    ")\n",
    "\n",
    "print(\"✓ Stream configured dla MERGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1442b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WriteStream z foreachBatch\n",
    "query = (\n",
    "    orders_stream\n",
    "    .writeStream\n",
    "    .foreachBatch(upsert_to_delta)  # Custom MERGE logic\n",
    "    .option(\"checkpointLocation\", upsert_checkpoint)\n",
    "    .trigger(once=True)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Streaming MERGE completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61d285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź final count\n",
    "print(f\"Final count: {spark.table(upsert_table).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6c78cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyświetl upserted data\n",
    "print(\"=== Upserted data ===\")\n",
    "display(spark.table(upsert_table).orderBy(\"order_id\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ae782c",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Streaming MERGE:\n",
    "- **foreachBatch**: Custom function wykonywana na każdym micro-batch\n",
    "- **MERGE INTO**: Upsert logic - UPDATE jeśli istnieje, INSERT jeśli nie\n",
    "- **Idempotency**: Ponowne procesowanie tego samego batch ID daje ten sam rezultat\n",
    "- **Use case**: CDC streaming, real-time dimension updates, deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e501fd",
   "metadata": {},
   "source": [
    "### Przykład 4.2: Zarządzanie Checkpointami\n",
    "\n",
    "**Cel:** Demonstracja jak działa checkpoint location i jak go zarządzać"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019eb53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 4.2 - Zarządzanie checkpointami\n",
    "\n",
    "# Checkpoint location przechowuje state streaming query\n",
    "# Jest krytyczny dla fault tolerance i exactly-once semantics\n",
    "\n",
    "print(\"=== Checkpoint Locations ===\")\n",
    "print(f\"Auto Loader: {autoloader_checkpoint}\")\n",
    "print(f\"Streaming: {streaming_checkpoint}\")\n",
    "print(f\"Upsert: {upsert_checkpoint}\")\n",
    "\n",
    "# Sprawdź strukturę checkpoint location\n",
    "print(f\"\\n=== Struktura checkpoint (autoloader) ===\")\n",
    "try:\n",
    "    checkpoint_files = dbutils.fs.ls(autoloader_checkpoint)\n",
    "    for file in checkpoint_files:\n",
    "        print(f\"  {file.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Checkpoint nie istnieje lub jest pusty: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16a1dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoint zawiera:\n",
    "# - offsets/: Offset każdego micro-batch (dla recovery)\n",
    "# - commits/: Commited batches (dla exactly-once semantics)\n",
    "# - metadata: Stream metadata (id, configuration)\n",
    "# - sources/: Source-specific tracking (np. file tracking dla Auto Loader)\n",
    "\n",
    "print(\"\\n=== Checkpoint Best Practices ===\")\n",
    "print(\"✅ Zawsze używaj checkpoint location dla production streams\")\n",
    "print(\"✅ Przechowuj w external location (S3, ADLS, DBFS)\")\n",
    "print(\"✅ Backup przed schema changes\")\n",
    "print(\"⚠️ Nie usuwaj checkpoint - loss of progress!\")\n",
    "print(\"⚠️ Nie współdziel checkpoint między różnymi queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e060929d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Porównanie metod ingestion\n",
    "\n",
    "| Feature | COPY INTO | Auto Loader | Structured Streaming |\n",
    "|---------|-----------|-------------|---------------------|\n",
    "| **Latency** | Minuty-godziny | Sekundy-minuty | Sub-sekundy |\n",
    "| **Use case** | Scheduled batch | Near real-time files | Pure streaming (Kafka) |\n",
    "| **Idempotency** | ✅ Built-in | ✅ Built-in | ⚠️ Requires checkpoint |\n",
    "| **Schema evolution** | ⚠️ Manual | ✅ Automatic | ⚠️ Manual |\n",
    "| **Complexity** | Low | Medium | High |\n",
    "| **Cost** | Lowest | Medium | Highest |\n",
    "| **File tracking** | Delta Log | Checkpoint | Checkpoint |\n",
    "\n",
    "**Rekomendacje:**\n",
    "- **COPY INTO**: Batch loads (hourly, daily), duże pliki, niskie koszty\n",
    "- **Auto Loader**: Near real-time, małe pliki, schema evolution\n",
    "- **Structured Streaming**: Pure streaming sources (Kafka), sub-second latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ec5a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "**COPY INTO:**\n",
    "- Używaj dla scheduled batch jobs (daily, hourly)\n",
    "- Zawsze dodawaj _metadata column dla audytu\n",
    "- Używaj PATTERN dla filtrowania plików\n",
    "- Monitor operationMetrics w DESCRIBE HISTORY\n",
    "\n",
    "**Auto Loader:**\n",
    "- Włącz schema inference (inferColumnTypes=true)\n",
    "- Używaj trigger(availableNow=True) dla backfill\n",
    "- Monitor schema evolution w schemaLocation\n",
    "- Rozważ file notification dla dużej liczby plików (>10k)\n",
    "\n",
    "**Structured Streaming:**\n",
    "- Zawsze ustawiaj checkpointLocation\n",
    "- Używaj trigger(processingTime) dla continuous streams\n",
    "- Implementuj watermark dla windowed aggregations\n",
    "- Monitor stream metrics (numInputRows, inputRowsPerSecond)\n",
    "\n",
    "**Checkpoints:**\n",
    "- Przechowuj w niezależnej lokalizacji (nie w table location)\n",
    "- Backup przed schema changes\n",
    "- Nie usuwaj checkpoint - loss of progress!\n",
    "- Używaj external location (S3, ADLS) dla durability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de02682",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Problem 1: \"Stream stopped unexpectedly\"**\n",
    "**Rozwiązanie:**\n",
    "- Sprawdź checkpoint location - czy istnieje i jest writable\n",
    "- Sprawdź logi streaming query: `query.lastProgress`\n",
    "- Monitor exceptions: `query.exception()`\n",
    "\n",
    "**Problem 2: \"Schema mismatch in Auto Loader\"**\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Włącz schema evolution\n",
    ".option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    ".option(\"mergeSchema\", \"true\")\n",
    "```\n",
    "\n",
    "**Problem 3: COPY INTO nie wykrywa nowych plików**\n",
    "**Rozwiązanie:**\n",
    "- COPY INTO śledzi tylko file path - zmiana zawartości nie jest wykrywana\n",
    "- Użyj COPY_OPTIONS ('force' = 'true') dla re-ingestion\n",
    "\n",
    "**Problem 4: Streaming aggregation state grows indefinitely**\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Dodaj watermark dla time-based cleanup\n",
    ".withWatermark(\"event_time\", \"1 hour\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2806e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "**W tym notebooku nauczyliśmy się:**\n",
    "\n",
    "✅ **COPY INTO:**\n",
    "- Idempotent batch loads z automatic file tracking\n",
    "- Pattern matching dla selective ingestion\n",
    "- _metadata column dla audytu\n",
    "\n",
    "✅ **Auto Loader:**\n",
    "- Near real-time file ingestion z cloudFiles format\n",
    "- Automatic schema inference i evolution\n",
    "- Checkpoint-based progress tracking\n",
    "\n",
    "✅ **Structured Streaming:**\n",
    "- Continuous processing z micro-batch architecture\n",
    "- Transformacje i agregacje na streaming data\n",
    "- foreachBatch dla custom write logic (MERGE)\n",
    "\n",
    "✅ **Triggering modes:**\n",
    "- trigger(once=True) - batch mode dla testing\n",
    "- trigger(processingTime) - continuous processing\n",
    "- trigger(availableNow=True) - backfill mode\n",
    "\n",
    "**Kluczowe wnioski:**\n",
    "1. Wybór metody ingestion zależy od latency requirements i source type\n",
    "2. COPY INTO dla scheduled batch, Auto Loader dla near real-time files\n",
    "3. Structured Streaming dla pure streaming sources (Kafka)\n",
    "4. Checkpoint location jest krytyczny dla fault tolerance\n",
    "\n",
    "**Następne kroki:**\n",
    "- **Kolejny notebook**: 04_bronze_silver_gold_pipeline.ipynb\n",
    "- **Warsztat praktyczny**: 02_ingestion_pipeline_workshop.ipynb\n",
    "- **Delta Live Tables**: Declarative pipelines z automatic orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5489ba0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Posprzątaj zasoby utworzone podczas notebooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28db334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcjonalne czyszczenie zasobów testowych\n",
    "# UWAGA: Uruchom tylko jeśli chcesz usunąć wszystkie utworzone dane\n",
    "\n",
    "# Usuń tabele\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {copy_into_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {validation_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {pattern_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {autoloader_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {streaming_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {streaming_agg_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {trigger_once_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {trigger_available_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {trigger_continuous_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {upsert_table}\")\n",
    "\n",
    "# Usuń checkpointy\n",
    "# dbutils.fs.rm(CHECKPOINT_PATH, recurse=True)\n",
    "\n",
    "# spark.catalog.clearCache()\n",
    "# print(\"Zasoby zostały wyczyszczone\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
