{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1590795a",
   "metadata": {},
   "source": [
    "# Spark SQL - Transformacje i Analiza Danych\n",
    "\n",
    "**Cel szkoleniowy:** Opanowanie Spark SQL jako alternatywy dla PySpark DataFrame API\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Podstawy Spark SQL i rejestracja widokow\n",
    "- Porownanie skladni SQL vs DataFrame API\n",
    "- Window Functions w SQL\n",
    "- CTE (Common Table Expressions) i subqueries\n",
    "- DDL operacje (CREATE TABLE AS SELECT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9ead66",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzien szkolenia**: Dzien 3 - Transformation, Governance, Integrations\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 14.3 LTS+\n",
    "  - Unity Catalog wlaczony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE VIEW, SELECT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54676243",
   "metadata": {},
   "source": [
    "## Wstep teoretyczny\n",
    "\n",
    "**Spark SQL vs DataFrame API**\n",
    "\n",
    "Spark oferuje dwa rownowazne podejscia do przetwarzania danych:\n",
    "\n",
    "| Aspekt | DataFrame API | Spark SQL |\n",
    "|--------|---------------|------------|\n",
    "| Skladnia | Python/Scala | Standard SQL |\n",
    "| Optymalizacja | Catalyst Optimizer | Catalyst Optimizer |\n",
    "| Wydajnosc | Identyczna | Identyczna |\n",
    "| Type Safety | Compile-time | Runtime |\n",
    "| Integracja | Programistyczna | BI Tools, Analitycy |\n",
    "\n",
    "**Kiedy uzywac Spark SQL:**\n",
    "- Analitycy znajacy SQL\n",
    "- Integracja z narzedzami BI\n",
    "- Szybkie eksploracje ad-hoc\n",
    "- Zlozone zapytania z CTE\n",
    "\n",
    "**Kiedy uzywac DataFrame API:**\n",
    "- Zlozona logika programistyczna\n",
    "- Dynamiczne generowanie zapytan\n",
    "- Reuzywalne komponenty\n",
    "- Unit testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad04eb4",
   "metadata": {},
   "source": [
    "## Izolacja per uzytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667750da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c87081a",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie kontekstu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f46a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec75c1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czesc 1: Podstawy Spark SQL\n",
    "\n",
    "### spark.sql() - wykonywanie zapytan SQL\n",
    "\n",
    "Funkcja `spark.sql()` wykonuje zapytanie SQL i zwraca DataFrame.\n",
    "\n",
    "**Kluczowe cechy:**\n",
    "- Zwraca DataFrame (mozna laczyc z DataFrame API)\n",
    "- Obsluguje wszystkie standardowe operacje SQL\n",
    "- Wykorzystuje Catalyst Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16aded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyklad: Proste zapytanie SQL\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        'Hello Spark SQL' as message,\n",
    "        current_date() as today,\n",
    "        current_timestamp() as now\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ee352",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7833e79",
   "metadata": {},
   "source": [
    "### Tworzenie danych testowych\n",
    "\n",
    "Przygotujemy dane do demonstracji Spark SQL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94757c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dane zamowien\n",
    "orders_data = [\n",
    "    (1, 101, \"2024-01-15\", 250.00, \"completed\"),\n",
    "    (2, 102, \"2024-01-16\", 150.00, \"completed\"),\n",
    "    (3, 101, \"2024-01-20\", 320.00, \"completed\"),\n",
    "    (4, 103, \"2024-02-01\", 180.00, \"pending\"),\n",
    "    (5, 101, \"2024-02-10\", 420.00, \"completed\"),\n",
    "    (6, 102, \"2024-02-15\", 90.00, \"cancelled\"),\n",
    "    (7, 103, \"2024-03-01\", 550.00, \"completed\"),\n",
    "    (8, 104, \"2024-03-05\", 280.00, \"completed\"),\n",
    "    (9, 101, \"2024-03-10\", 175.00, \"completed\"),\n",
    "    (10, 102, \"2024-03-15\", 340.00, \"completed\"),\n",
    "]\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"order_date\", StringType(), False),\n",
    "    StructField(\"amount\", DoubleType(), False),\n",
    "    StructField(\"status\", StringType(), False)\n",
    "])\n",
    "\n",
    "orders_df = spark.createDataFrame(orders_data, orders_schema) \\\n",
    "    .withColumn(\"order_date\", F.to_date(\"order_date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195d5321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dane klientow\n",
    "customers_data = [\n",
    "    (101, \"Jan\", \"Kowalski\", \"Premium\", \"Warszawa\"),\n",
    "    (102, \"Anna\", \"Nowak\", \"Standard\", \"Krakow\"),\n",
    "    (103, \"Piotr\", \"Wisniewski\", \"Premium\", \"Gdansk\"),\n",
    "    (104, \"Maria\", \"Wojcik\", \"Standard\", \"Poznan\"),\n",
    "]\n",
    "\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"first_name\", StringType(), False),\n",
    "    StructField(\"last_name\", StringType(), False),\n",
    "    StructField(\"tier\", StringType(), False),\n",
    "    StructField(\"city\", StringType(), False)\n",
    "])\n",
    "\n",
    "customers_df = spark.createDataFrame(customers_data, customers_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f221b35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(orders_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ad41b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(customers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea36a00",
   "metadata": {},
   "source": [
    "### Rejestracja Temp Views\n",
    "\n",
    "Aby uzywac DataFrame w zapytaniach SQL, nalezy zarejestrowac je jako widoki tymczasowe.\n",
    "\n",
    "**Typy widokow:**\n",
    "- `createOrReplaceTempView()` - widok lokalny dla sesji\n",
    "- `createOrReplaceGlobalTempView()` - widok globalny (dostepny z `global_temp.nazwa`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753d9f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rejestracja widokow tymczasowych\n",
    "orders_df.createOrReplaceTempView(\"orders\")\n",
    "customers_df.createOrReplaceTempView(\"customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ccf2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teraz mozemy uzywac SQL\n",
    "spark.sql(\"SELECT * FROM orders LIMIT 5\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e311b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czesc 2: Porownanie SQL vs DataFrame API\n",
    "\n",
    "### Przyklad: Filtrowanie i agregacja\n",
    "\n",
    "Wykonamy te sama operacje w obu podejsciach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fa6a93",
   "metadata": {},
   "source": [
    "**Zadanie:** Znajdz laczna wartosc zamowien completed per klient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9853f7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podejscie DataFrame API\n",
    "result_df = orders_df \\\n",
    "    .filter(F.col(\"status\") == \"completed\") \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"orders_count\"),\n",
    "        F.sum(\"amount\").alias(\"total_amount\"),\n",
    "        F.round(F.avg(\"amount\"), 2).alias(\"avg_amount\")\n",
    "    ) \\\n",
    "    .orderBy(F.col(\"total_amount\").desc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f49a45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5a2105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podejscie Spark SQL\n",
    "result_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        COUNT(*) as orders_count,\n",
    "        SUM(amount) as total_amount,\n",
    "        ROUND(AVG(amount), 2) as avg_amount\n",
    "    FROM orders\n",
    "    WHERE status = 'completed'\n",
    "    GROUP BY customer_id\n",
    "    ORDER BY total_amount DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ea60c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(result_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2676f48d",
   "metadata": {},
   "source": [
    "**Porownanie:** Oba podejscia daja identyczny wynik i plan wykonania."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094f63c2",
   "metadata": {},
   "source": [
    "### Przyklad: JOIN z wieloma tabelami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4ff57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame API - JOIN\n",
    "joined_df = orders_df \\\n",
    "    .join(customers_df, \"customer_id\", \"inner\") \\\n",
    "    .select(\n",
    "        \"order_id\",\n",
    "        F.concat_ws(\" \", \"first_name\", \"last_name\").alias(\"customer_name\"),\n",
    "        \"tier\",\n",
    "        \"order_date\",\n",
    "        \"amount\",\n",
    "        \"status\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3081b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(joined_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6299263b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark SQL - JOIN\n",
    "joined_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        CONCAT_WS(' ', c.first_name, c.last_name) as customer_name,\n",
    "        c.tier,\n",
    "        o.order_date,\n",
    "        o.amount,\n",
    "        o.status\n",
    "    FROM orders o\n",
    "    INNER JOIN customers c ON o.customer_id = c.customer_id\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb15b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(joined_sql.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db1628c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czesc 3: Window Functions w SQL\n",
    "\n",
    "### Skladnia Window Functions\n",
    "\n",
    "```sql\n",
    "funkcja() OVER (\n",
    "    PARTITION BY kolumna\n",
    "    ORDER BY kolumna\n",
    "    ROWS BETWEEN ... AND ...\n",
    ")\n",
    "```\n",
    "\n",
    "**Funkcje rankingowe:**\n",
    "- `ROW_NUMBER()` - unikalny numer wiersza\n",
    "- `RANK()` - rank z przerwami\n",
    "- `DENSE_RANK()` - rank bez przerw\n",
    "\n",
    "**Funkcje analityczne:**\n",
    "- `LAG()` - wartosc z poprzedniego wiersza\n",
    "- `LEAD()` - wartosc z nastepnego wiersza\n",
    "- `FIRST_VALUE()` / `LAST_VALUE()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0718dc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking zamowien per klient\n",
    "ranking_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        order_id,\n",
    "        customer_id,\n",
    "        order_date,\n",
    "        amount,\n",
    "        ROW_NUMBER() OVER (PARTITION BY customer_id ORDER BY order_date) as order_sequence,\n",
    "        RANK() OVER (PARTITION BY customer_id ORDER BY amount DESC) as amount_rank,\n",
    "        DENSE_RANK() OVER (PARTITION BY customer_id ORDER BY amount DESC) as amount_dense_rank\n",
    "    FROM orders\n",
    "    WHERE status = 'completed'\n",
    "    ORDER BY customer_id, order_date\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d65f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ranking_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3a03d3",
   "metadata": {},
   "source": [
    "### LAG i LEAD - analiza zmian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48aa5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porownanie z poprzednim zamowieniem\n",
    "lag_lead_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        order_id,\n",
    "        customer_id,\n",
    "        order_date,\n",
    "        amount,\n",
    "        LAG(amount, 1) OVER (PARTITION BY customer_id ORDER BY order_date) as prev_amount,\n",
    "        LEAD(amount, 1) OVER (PARTITION BY customer_id ORDER BY order_date) as next_amount,\n",
    "        amount - LAG(amount, 1) OVER (PARTITION BY customer_id ORDER BY order_date) as amount_change\n",
    "    FROM orders\n",
    "    WHERE status = 'completed'\n",
    "    ORDER BY customer_id, order_date\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e62ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(lag_lead_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2c7c1c",
   "metadata": {},
   "source": [
    "### Running Totals i Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c176e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suma narastajaca i srednia kroczaca\n",
    "running_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        order_id,\n",
    "        customer_id,\n",
    "        order_date,\n",
    "        amount,\n",
    "        SUM(amount) OVER (\n",
    "            PARTITION BY customer_id \n",
    "            ORDER BY order_date \n",
    "            ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "        ) as cumulative_amount,\n",
    "        ROUND(AVG(amount) OVER (\n",
    "            PARTITION BY customer_id \n",
    "            ORDER BY order_date \n",
    "            ROWS BETWEEN 2 PRECEDING AND CURRENT ROW\n",
    "        ), 2) as moving_avg_3\n",
    "    FROM orders\n",
    "    WHERE status = 'completed'\n",
    "    ORDER BY customer_id, order_date\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48aa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(running_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29402ef",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czesc 4: CTE (Common Table Expressions)\n",
    "\n",
    "### WITH clause\n",
    "\n",
    "CTE pozwalaja na tworzenie nazwanych podzapytan, ktore mozna wielokrotnie wykorzystywac.\n",
    "\n",
    "**Zalety CTE:**\n",
    "- Czytelnosc kodu\n",
    "- Reuzywanie logiki\n",
    "- Latwiejsze debugowanie\n",
    "- Rekurencyjne zapytania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76290f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTE - analiza klientow\n",
    "cte_analysis = spark.sql(\"\"\"\n",
    "    WITH customer_orders AS (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            COUNT(*) as orders_count,\n",
    "            SUM(amount) as total_spent,\n",
    "            AVG(amount) as avg_order_value\n",
    "        FROM orders\n",
    "        WHERE status = 'completed'\n",
    "        GROUP BY customer_id\n",
    "    ),\n",
    "    customer_ranking AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            RANK() OVER (ORDER BY total_spent DESC) as spending_rank,\n",
    "            CASE \n",
    "                WHEN total_spent >= 500 THEN 'High Value'\n",
    "                WHEN total_spent >= 300 THEN 'Medium Value'\n",
    "                ELSE 'Low Value'\n",
    "            END as value_segment\n",
    "        FROM customer_orders\n",
    "    )\n",
    "    SELECT \n",
    "        cr.*,\n",
    "        c.first_name,\n",
    "        c.last_name,\n",
    "        c.tier,\n",
    "        c.city\n",
    "    FROM customer_ranking cr\n",
    "    JOIN customers c ON cr.customer_id = c.customer_id\n",
    "    ORDER BY spending_rank\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0b8d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(cte_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cb2a80",
   "metadata": {},
   "source": [
    "### Wielokrotne uzycie CTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca899962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CTE uzyte wielokrotnie\n",
    "multi_cte = spark.sql(\"\"\"\n",
    "    WITH monthly_stats AS (\n",
    "        SELECT \n",
    "            DATE_TRUNC('month', order_date) as month,\n",
    "            customer_id,\n",
    "            SUM(amount) as monthly_spent\n",
    "        FROM orders\n",
    "        WHERE status = 'completed'\n",
    "        GROUP BY DATE_TRUNC('month', order_date), customer_id\n",
    "    )\n",
    "    SELECT \n",
    "        month,\n",
    "        COUNT(DISTINCT customer_id) as active_customers,\n",
    "        SUM(monthly_spent) as total_revenue,\n",
    "        ROUND(AVG(monthly_spent), 2) as avg_customer_spend,\n",
    "        MAX(monthly_spent) as max_customer_spend\n",
    "    FROM monthly_stats\n",
    "    GROUP BY month\n",
    "    ORDER BY month\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7633966",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(multi_cte)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdca2023",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czesc 5: Subqueries\n",
    "\n",
    "### Scalar Subqueries\n",
    "\n",
    "Podzapytania zwracajace pojedyncza wartosc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d20bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zamowienia powyzej sredniej\n",
    "scalar_subquery = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        order_id,\n",
    "        customer_id,\n",
    "        amount,\n",
    "        (SELECT ROUND(AVG(amount), 2) FROM orders WHERE status = 'completed') as avg_amount,\n",
    "        amount - (SELECT AVG(amount) FROM orders WHERE status = 'completed') as diff_from_avg\n",
    "    FROM orders\n",
    "    WHERE status = 'completed'\n",
    "      AND amount > (SELECT AVG(amount) FROM orders WHERE status = 'completed')\n",
    "    ORDER BY amount DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d609cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(scalar_subquery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdd2745",
   "metadata": {},
   "source": [
    "### Correlated Subqueries\n",
    "\n",
    "Podzapytania odnosace sie do zewnetrznego zapytania:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91aa414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klienci z zamowieniami powyzej ich sredniej\n",
    "correlated_subquery = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        o.customer_id,\n",
    "        o.amount,\n",
    "        (SELECT ROUND(AVG(o2.amount), 2) \n",
    "         FROM orders o2 \n",
    "         WHERE o2.customer_id = o.customer_id \n",
    "           AND o2.status = 'completed') as customer_avg\n",
    "    FROM orders o\n",
    "    WHERE o.status = 'completed'\n",
    "      AND o.amount > (\n",
    "          SELECT AVG(o2.amount) \n",
    "          FROM orders o2 \n",
    "          WHERE o2.customer_id = o.customer_id \n",
    "            AND o2.status = 'completed'\n",
    "      )\n",
    "    ORDER BY o.customer_id, o.amount DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0942f7b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(correlated_subquery)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3023749b",
   "metadata": {},
   "source": [
    "### EXISTS i IN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a3ce1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Klienci ktorzy maja zamowienia > 400\n",
    "exists_query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.first_name,\n",
    "        c.last_name,\n",
    "        c.tier\n",
    "    FROM customers c\n",
    "    WHERE EXISTS (\n",
    "        SELECT 1 \n",
    "        FROM orders o \n",
    "        WHERE o.customer_id = c.customer_id \n",
    "          AND o.amount > 400\n",
    "          AND o.status = 'completed'\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5868c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(exists_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cdd50a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czesc 6: CASE WHEN i zaawansowane wyrazenia\n",
    "\n",
    "### Warunkowa logika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5002610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segmentacja zamowien\n",
    "case_when_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        order_id,\n",
    "        customer_id,\n",
    "        amount,\n",
    "        CASE \n",
    "            WHEN amount >= 500 THEN 'Large'\n",
    "            WHEN amount >= 200 THEN 'Medium'\n",
    "            ELSE 'Small'\n",
    "        END as order_size,\n",
    "        CASE status\n",
    "            WHEN 'completed' THEN 1\n",
    "            WHEN 'pending' THEN 0\n",
    "            ELSE -1\n",
    "        END as status_code,\n",
    "        COALESCE(amount, 0) as amount_safe\n",
    "    FROM orders\n",
    "    ORDER BY amount DESC\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542d791f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(case_when_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d38adb",
   "metadata": {},
   "source": [
    "### NULLIF, COALESCE, NVL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ad4ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obsluga NULL\n",
    "null_handling = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        order_id,\n",
    "        amount,\n",
    "        status,\n",
    "        NULLIF(status, 'cancelled') as status_or_null,\n",
    "        COALESCE(NULLIF(status, 'cancelled'), 'N/A') as status_clean,\n",
    "        NVL(amount, 0) as amount_nvl\n",
    "    FROM orders\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15138b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(null_handling)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d57f81c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czesc 7: DDL w Spark SQL\n",
    "\n",
    "### CREATE TABLE AS SELECT (CTAS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5fbe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utworzenie tabeli z wynikami agregacji\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {CATALOG}.{GOLD_SCHEMA}.customer_summary AS\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.first_name,\n",
    "        c.last_name,\n",
    "        c.tier,\n",
    "        c.city,\n",
    "        COUNT(o.order_id) as total_orders,\n",
    "        COALESCE(SUM(CASE WHEN o.status = 'completed' THEN o.amount END), 0) as total_spent,\n",
    "        ROUND(COALESCE(AVG(CASE WHEN o.status = 'completed' THEN o.amount END), 0), 2) as avg_order_value,\n",
    "        MAX(o.order_date) as last_order_date\n",
    "    FROM customers c\n",
    "    LEFT JOIN orders o ON c.customer_id = o.customer_id\n",
    "    GROUP BY c.customer_id, c.first_name, c.last_name, c.tier, c.city\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e00e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja\n",
    "spark.sql(f\"SELECT * FROM {CATALOG}.{GOLD_SCHEMA}.customer_summary\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eab724a",
   "metadata": {},
   "source": [
    "### CREATE VIEW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a7b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utworzenie widoku\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {CATALOG}.{GOLD_SCHEMA}.v_monthly_revenue AS\n",
    "    SELECT \n",
    "        DATE_TRUNC('month', order_date) as month,\n",
    "        COUNT(*) as orders_count,\n",
    "        SUM(amount) as total_revenue,\n",
    "        ROUND(AVG(amount), 2) as avg_order_value\n",
    "    FROM orders\n",
    "    WHERE status = 'completed'\n",
    "    GROUP BY DATE_TRUNC('month', order_date)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec72df9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"SELECT * FROM {CATALOG}.{GOLD_SCHEMA}.v_monthly_revenue ORDER BY month\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab6b694",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czesc 8: Explain Plans\n",
    "\n",
    "### Analiza planu wykonania\n",
    "\n",
    "EXPLAIN pokazuje jak Spark wykona zapytanie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f0fa6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan wykonania dla zlozonego zapytania\n",
    "spark.sql(\"\"\"\n",
    "    EXPLAIN EXTENDED\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.first_name,\n",
    "        SUM(o.amount) as total\n",
    "    FROM customers c\n",
    "    JOIN orders o ON c.customer_id = o.customer_id\n",
    "    WHERE o.status = 'completed'\n",
    "    GROUP BY c.customer_id, c.first_name\n",
    "    ORDER BY total DESC\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3965e334",
   "metadata": {},
   "source": [
    "### Porownanie planow\n",
    "\n",
    "DataFrame API i SQL generuja identyczny plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbd71dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plan z DataFrame API\n",
    "df_plan = orders_df \\\n",
    "    .join(customers_df, \"customer_id\") \\\n",
    "    .filter(F.col(\"status\") == \"completed\") \\\n",
    "    .groupBy(\"customer_id\", \"first_name\") \\\n",
    "    .agg(F.sum(\"amount\").alias(\"total\")) \\\n",
    "    .orderBy(F.col(\"total\").desc())\n",
    "\n",
    "df_plan.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fae58a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "### Omowione zagadnienia\n",
    "\n",
    "1. **Podstawy Spark SQL**\n",
    "   - `spark.sql()` wykonanie zapytan\n",
    "   - `createOrReplaceTempView()` rejestracja widokow\n",
    "\n",
    "2. **Porownanie SQL vs DataFrame API**\n",
    "   - Identyczna wydajnosc (Catalyst Optimizer)\n",
    "   - Rozne przypadki uzycia\n",
    "\n",
    "3. **Window Functions w SQL**\n",
    "   - ROW_NUMBER, RANK, DENSE_RANK\n",
    "   - LAG, LEAD\n",
    "   - Running totals, moving averages\n",
    "\n",
    "4. **CTE i Subqueries**\n",
    "   - WITH clause dla czytelnosci\n",
    "   - Scalar i correlated subqueries\n",
    "   - EXISTS, IN\n",
    "\n",
    "5. **DDL operacje**\n",
    "   - CREATE TABLE AS SELECT\n",
    "   - CREATE VIEW\n",
    "\n",
    "---\n",
    "\n",
    "### Quick Reference\n",
    "\n",
    "| Operacja | Spark SQL | DataFrame API |\n",
    "|----------|-----------|---------------|\n",
    "| Filtrowanie | `WHERE col = 'x'` | `.filter(F.col(\"col\") == \"x\")` |\n",
    "| Agregacja | `GROUP BY col` | `.groupBy(\"col\").agg(...)` |\n",
    "| Ranking | `ROW_NUMBER() OVER (...)` | `row_number().over(window)` |\n",
    "| CTE | `WITH cte AS (...)` | Brak bezposredniego odpowiednika |\n",
    "| CASE WHEN | `CASE WHEN ... END` | `F.when(...).otherwise(...)` |\n",
    "\n",
    "### Nastepne kroki\n",
    "\n",
    "- **Kolejny notebook**: 03_lakeflow_jobs_orchestration.ipynb\n",
    "- **Warsztat**: 01_advanced_transformations_workshop.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3057229d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czyszczenie zasobow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e0fbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usuniecie temp views\n",
    "spark.catalog.dropTempView(\"orders\")\n",
    "spark.catalog.dropTempView(\"customers\")\n",
    "\n",
    "# Opcjonalnie: usuniecie utworzonych tabel\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{GOLD_SCHEMA}.customer_summary\")\n",
    "# spark.sql(f\"DROP VIEW IF EXISTS {CATALOG}.{GOLD_SCHEMA}.v_monthly_revenue\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
