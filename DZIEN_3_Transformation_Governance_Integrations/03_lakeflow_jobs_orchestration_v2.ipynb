{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82e80e01",
   "metadata": {},
   "source": [
    "# Lakeflow Jobs - Orkiestracja Workflows\n",
    "\n",
    "---\n",
    "\n",
    "## Kontekst biznesowy\n",
    "\n",
    "**KION** potrzebuje zautomatyzować codzienny proces ETL:\n",
    "- Ingestion danych zamówień o 2:00 w nocy\n",
    "- Transformacja do warstwy Silver\n",
    "- Agregacje do Gold\n",
    "- Powiadomienia przy błędach\n",
    "\n",
    "W tym module nauczysz się tworzyć i konfigurować Lakeflow Jobs przez UI.\n",
    "\n",
    "---\n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. Wprowadzenie do Lakeflow Jobs\n",
    "2. Przygotowanie notebooków do Job (taski)\n",
    "3. [DEMO UI] Tworzenie Multi-task Job\n",
    "4. [DEMO UI] Triggery i Schedule\n",
    "5. [DEMO UI] Opcje, Retry i Alerting\n",
    "6. Praktyka: Widgets i parametry\n",
    "7. Praktyka: Przekazywanie danych między taskami\n",
    "8. Monitoring przez System Tables\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b82da2",
   "metadata": {},
   "source": [
    "## 1. Wprowadzenie do Lakeflow Jobs\n",
    "\n",
    "**Lakeflow Jobs** (dawniej Databricks Jobs) to zarządzany serwis orkiestracji.\n",
    "\n",
    "### Kiedy używać Jobs?\n",
    "\n",
    "| Scenariusz | Rozwiązanie |\n",
    "|------------|-------------|\n",
    "| ETL pipeline z wieloma krokami | Multi-task Job |\n",
    "| Codzienny raport o stałej godzinie | Scheduled Job |\n",
    "| Reakcja na nowe pliki | File Arrival Trigger |\n",
    "| ML training pipeline | Job z notebook taskami |\n",
    "| Uruchomienie DLT pipeline | Job z DLT task |\n",
    "\n",
    "### Jobs vs DLT (Lakeflow Pipelines)\n",
    "\n",
    "| Cecha | Jobs | DLT |\n",
    "|-------|------|-----|\n",
    "| Orkiestracja | Ogólna | Tylko ETL |\n",
    "| Zależności | Ręczna konfiguracja | Automatyczne (DAG) |\n",
    "| Data Quality | Custom kod | Wbudowane expectations |\n",
    "| Elastyczność | Wysoka | Opinionated |\n",
    "\n",
    "**Best Practice**: Używaj DLT dla ETL pipelines, Jobs dla orkiestracji DLT + innych tasków.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ac85ec",
   "metadata": {},
   "source": [
    "## 2. Przygotowanie notebooków do Job\n",
    "\n",
    "Poniżej znajdują się 3 proste notebooki, które użyjemy w demo.\n",
    "\n",
    "**Instrukcja**: \n",
    "1. Utwórz folder `/Workspace/Users/<twoj-email>/jobs_demo/`\n",
    "2. Skopiuj każdy z poniższych kodów do osobnego notebooka\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdba6ec4",
   "metadata": {},
   "source": [
    "### Task 1: Validate Source (`task_01_validate.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc9ddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 1: Validate Source Data\n",
    "# Skopiuj ten kod do notebooka: task_01_validate\n",
    "# =============================================================================\n",
    "\n",
    "# Parametry z Job\n",
    "dbutils.widgets.text(\"source_table\", \"samples.nyctaxi.trips\")\n",
    "dbutils.widgets.text(\"min_rows\", \"100\")\n",
    "\n",
    "source_table = dbutils.widgets.get(\"source_table\")\n",
    "min_rows = int(dbutils.widgets.get(\"min_rows\"))\n",
    "\n",
    "print(f\"Validating: {source_table}\")\n",
    "print(f\"Minimum rows required: {min_rows}\")\n",
    "\n",
    "# Walidacja\n",
    "df = spark.table(source_table)\n",
    "row_count = df.count()\n",
    "\n",
    "if row_count < min_rows:\n",
    "    raise Exception(f\"Validation FAILED: {row_count} rows < {min_rows} minimum\")\n",
    "\n",
    "print(f\"Validation PASSED: {row_count} rows\")\n",
    "\n",
    "# Zwróć wynik do następnego taska\n",
    "import json\n",
    "dbutils.notebook.exit(json.dumps({\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"source_table\": source_table,\n",
    "    \"row_count\": row_count\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aef86da",
   "metadata": {},
   "source": [
    "### Task 2: Transform Data (`task_02_transform.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b351eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 2: Transform Data\n",
    "# Skopiuj ten kod do notebooka: task_02_transform\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "import json\n",
    "\n",
    "# Parametry\n",
    "dbutils.widgets.text(\"source_table\", \"samples.nyctaxi.trips\")\n",
    "dbutils.widgets.text(\"run_date\", \"\")\n",
    "\n",
    "source_table = dbutils.widgets.get(\"source_table\")\n",
    "run_date = dbutils.widgets.get(\"run_date\") or str(current_date())\n",
    "\n",
    "# Pobierz wynik z poprzedniego taska (opcjonalnie)\n",
    "try:\n",
    "    prev_result = dbutils.jobs.taskValues.get(\n",
    "        taskKey=\"validate\",\n",
    "        key=\"returnValue\",\n",
    "        default=\"{}\"\n",
    "    )\n",
    "    prev_data = json.loads(prev_result)\n",
    "    print(f\"Previous task result: {prev_data}\")\n",
    "except:\n",
    "    print(\"Running standalone (no previous task)\")\n",
    "\n",
    "# Transformacja\n",
    "print(f\"Transforming: {source_table}\")\n",
    "\n",
    "df = spark.table(source_table)\n",
    "\n",
    "df_transformed = (\n",
    "    df\n",
    "    .withColumn(\"trip_duration_minutes\", \n",
    "                round((col(\"tpep_dropoff_datetime\").cast(\"long\") - \n",
    "                       col(\"tpep_pickup_datetime\").cast(\"long\")) / 60, 2))\n",
    "    .withColumn(\"cost_per_mile\", \n",
    "                when(col(\"trip_distance\") > 0, \n",
    "                     round(col(\"fare_amount\") / col(\"trip_distance\"), 2))\n",
    "                .otherwise(0))\n",
    "    .withColumn(\"processing_date\", lit(run_date))\n",
    ")\n",
    "\n",
    "row_count = df_transformed.count()\n",
    "print(f\"Transformed {row_count} rows\")\n",
    "\n",
    "# Wyświetl sample\n",
    "df_transformed.select(\n",
    "    \"trip_distance\", \"fare_amount\", \"trip_duration_minutes\", \"cost_per_mile\"\n",
    ").show(5)\n",
    "\n",
    "# Zwróć wynik\n",
    "dbutils.notebook.exit(json.dumps({\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"rows_transformed\": row_count\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac0b4ed",
   "metadata": {},
   "source": [
    "### Task 3: Generate Report (`task_03_report.py`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4150ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 3: Generate Report\n",
    "# Skopiuj ten kod do notebooka: task_03_report\n",
    "# =============================================================================\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Parametry\n",
    "dbutils.widgets.text(\"source_table\", \"samples.nyctaxi.trips\")\n",
    "\n",
    "source_table = dbutils.widgets.get(\"source_table\")\n",
    "\n",
    "print(f\"Generating report from: {source_table}\")\n",
    "\n",
    "# Agregacje\n",
    "df = spark.table(source_table)\n",
    "\n",
    "report = df.agg(\n",
    "    count(\"*\").alias(\"total_trips\"),\n",
    "    round(sum(\"fare_amount\"), 2).alias(\"total_revenue\"),\n",
    "    round(avg(\"fare_amount\"), 2).alias(\"avg_fare\"),\n",
    "    round(avg(\"trip_distance\"), 2).alias(\"avg_distance\"),\n",
    "    round(max(\"fare_amount\"), 2).alias(\"max_fare\")\n",
    ").collect()[0]\n",
    "\n",
    "# Wyświetl raport\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DAILY REPORT\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Trips:    {report.total_trips:,}\")\n",
    "print(f\"Total Revenue:  ${report.total_revenue:,.2f}\")\n",
    "print(f\"Avg Fare:       ${report.avg_fare:.2f}\")\n",
    "print(f\"Avg Distance:   {report.avg_distance:.2f} miles\")\n",
    "print(f\"Max Fare:       ${report.max_fare:.2f}\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Generated at:   {datetime.now()}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# Zwróć wynik\n",
    "dbutils.notebook.exit(json.dumps({\n",
    "    \"status\": \"SUCCESS\",\n",
    "    \"total_trips\": report.total_trips,\n",
    "    \"total_revenue\": float(report.total_revenue)\n",
    "}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b65389",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. [DEMO UI] Tworzenie Multi-task Job\n",
    "\n",
    "### Checklist dla prowadzącego:\n",
    "\n",
    "**Krok 1: Utwórz nowy Job**\n",
    "- [ ] Workflows → Create Job\n",
    "- [ ] Nazwa: `KION_Demo_ETL_Pipeline`\n",
    "\n",
    "**Krok 2: Dodaj Task 1 (Validate)**\n",
    "- [ ] Task name: `validate`\n",
    "- [ ] Type: Notebook\n",
    "- [ ] Path: `/Workspace/.../task_01_validate`\n",
    "- [ ] Cluster: Serverless lub nowy Job Cluster\n",
    "- [ ] Parameters: `source_table` = `samples.nyctaxi.trips`\n",
    "\n",
    "**Krok 3: Dodaj Task 2 (Transform)**\n",
    "- [ ] Task name: `transform`\n",
    "- [ ] Depends on: `validate`\n",
    "- [ ] Path: `/Workspace/.../task_02_transform`\n",
    "- [ ] Parameters: `source_table` = `samples.nyctaxi.trips`\n",
    "\n",
    "**Krok 4: Dodaj Task 3 (Report)**\n",
    "- [ ] Task name: `report`\n",
    "- [ ] Depends on: `transform`\n",
    "- [ ] Path: `/Workspace/.../task_03_report`\n",
    "\n",
    "**Krok 5: Uruchom Job**\n",
    "- [ ] Run now\n",
    "- [ ] Pokaż: DAG visualization\n",
    "- [ ] Pokaż: Task logs i output\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436a470d",
   "metadata": {},
   "source": [
    "## 4. [DEMO UI] Triggery i Schedule\n",
    "\n",
    "### Checklist dla prowadzącego:\n",
    "\n",
    "**Opcje Triggerów** (zakładka Triggers):\n",
    "\n",
    "| Trigger Type | Użycie | Przykład |\n",
    "|--------------|--------|----------|\n",
    "| **Scheduled** | Stały harmonogram | Codziennie o 2:00 |\n",
    "| **File arrival** | Reakcja na nowe pliki | Nowy plik w `/landing/` |\n",
    "| **Continuous** | Ciągłe przetwarzanie | Streaming-like |\n",
    "| **Manual** | On-demand | Testowanie |\n",
    "\n",
    "**Demo: Scheduled Trigger**\n",
    "- [ ] Add trigger → Scheduled\n",
    "- [ ] Cron expression: `0 0 2 * * ?` (codziennie 2:00)\n",
    "- [ ] Timezone: `Europe/Warsaw`\n",
    "- [ ] Pokaż: Preview next runs\n",
    "\n",
    "**Demo: File Arrival Trigger** (opcjonalnie)\n",
    "- [ ] Add trigger → File arrival\n",
    "- [ ] URL: Unity Catalog Volume path\n",
    "- [ ] Min files: 1\n",
    "- [ ] Wait time: 5 minutes\n",
    "\n",
    "### Przydatne wyrażenia CRON:\n",
    "\n",
    "```\n",
    "0 0 2 * * ?        # Codziennie o 2:00\n",
    "0 0 * * * ?        # Co godzinę\n",
    "0 0 9 ? * MON-FRI  # Pon-Pt o 9:00\n",
    "0 0 0 1 * ?        # Pierwszy dzień miesiąca\n",
    "0 */15 * * * ?     # Co 15 minut\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f666c502",
   "metadata": {},
   "source": [
    "## 5. [DEMO UI] Opcje, Retry i Alerting\n",
    "\n",
    "### Checklist dla prowadzącego:\n",
    "\n",
    "**Task-level options** (per task):\n",
    "- [ ] Timeout: 30 minutes\n",
    "- [ ] Retries: 2\n",
    "- [ ] Retry delay: 60 seconds\n",
    "\n",
    "**Job-level options** (górny panel):\n",
    "- [ ] Max concurrent runs: 1 (zapobiega overlap)\n",
    "- [ ] Job timeout: 2 hours\n",
    "\n",
    "**Email Notifications**:\n",
    "- [ ] On failure: `team@company.com`\n",
    "- [ ] On success: (opcjonalnie)\n",
    "- [ ] On start: (opcjonalnie)\n",
    "\n",
    "**Webhook Integration** (Slack/Teams):\n",
    "- [ ] Admin Settings → Destinations\n",
    "- [ ] Add webhook URL\n",
    "- [ ] Przypisz do Job\n",
    "\n",
    "### Kiedy używać Retry?\n",
    "\n",
    "| Scenariusz | Retry? | Dlaczego |\n",
    "|------------|--------|----------|\n",
    "| Network timeout | Tak | Transient error |\n",
    "| API rate limit | Tak | Transient error |\n",
    "| Data quality issue | Nie | Retry nie naprawi danych |\n",
    "| Code bug | Nie | Retry nie naprawi kodu |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee69dc07",
   "metadata": {},
   "source": [
    "## 6. Praktyka: Widgets i Parametry\n",
    "\n",
    "Databricks Widgets pozwalają parametryzować notebooki.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535d13cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Typy widgetów\n",
    "\n",
    "# Text - dowolny tekst\n",
    "dbutils.widgets.text(\"environment\", \"dev\", \"Environment\")\n",
    "\n",
    "# Dropdown - wybór z listy\n",
    "dbutils.widgets.dropdown(\"region\", \"EU\", [\"EU\", \"US\", \"APAC\"], \"Region\")\n",
    "\n",
    "# Combobox - dropdown z możliwością wpisania\n",
    "dbutils.widgets.combobox(\"table\", \"orders\", [\"orders\", \"customers\", \"products\"], \"Table\")\n",
    "\n",
    "# Multiselect - wielokrotny wybór\n",
    "dbutils.widgets.multiselect(\"columns\", \"id\", [\"id\", \"name\", \"date\", \"amount\"], \"Columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aebc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobieranie wartości\n",
    "environment = dbutils.widgets.get(\"environment\")\n",
    "region = dbutils.widgets.get(\"region\")\n",
    "table = dbutils.widgets.get(\"table\")\n",
    "columns = dbutils.widgets.get(\"columns\")  # zwraca string z przecinkami\n",
    "\n",
    "print(f\"Environment: {environment}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Table: {table}\")\n",
    "print(f\"Columns: {columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35203e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamiczne parametry w Job\n",
    "# Te wartości są dostępne gdy notebook jest uruchomiony jako task w Job\n",
    "\n",
    "dynamic_params = {\n",
    "    \"{{job.start_time.iso_date}}\": \"Data uruchomienia (YYYY-MM-DD)\",\n",
    "    \"{{job.start_time}}\": \"Pełny timestamp\",\n",
    "    \"{{job.id}}\": \"ID Job\",\n",
    "    \"{{run.id}}\": \"ID bieżącego uruchomienia\",\n",
    "    \"{{task.name}}\": \"Nazwa bieżącego taska\"\n",
    "}\n",
    "\n",
    "for param, description in dynamic_params.items():\n",
    "    print(f\"{param:35} -> {description}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044bc98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup widgetów\n",
    "dbutils.widgets.removeAll()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2a7aa0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Praktyka: Przekazywanie danych między taskami\n",
    "\n",
    "Dwa sposoby przekazywania danych:\n",
    "\n",
    "1. **dbutils.notebook.exit()** - zwraca wartość z notebooka\n",
    "2. **dbutils.jobs.taskValues** - odczytuje wartość z poprzedniego taska\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7dce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task A - wysyła dane\n",
    "import json\n",
    "\n",
    "result = {\n",
    "    \"rows_processed\": 1500,\n",
    "    \"max_date\": \"2024-01-15\",\n",
    "    \"status\": \"SUCCESS\"\n",
    "}\n",
    "\n",
    "# Zwróć jako JSON string\n",
    "# dbutils.notebook.exit(json.dumps(result))\n",
    "print(f\"Task A would exit with: {json.dumps(result)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94c6f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task B - odbiera dane z Task A\n",
    "import json\n",
    "\n",
    "# W rzeczywistym Job:\n",
    "# task_a_output = dbutils.jobs.taskValues.get(\n",
    "#     taskKey=\"task_a\",           # nazwa poprzedniego taska\n",
    "#     key=\"returnValue\",          # klucz (domyślnie \"returnValue\")\n",
    "#     default=\"{}\",               # wartość domyślna\n",
    "#     debugValue=\"{}\"             # wartość do testowania lokalnie\n",
    "# )\n",
    "\n",
    "# Symulacja\n",
    "task_a_output = '{\"rows_processed\": 1500, \"max_date\": \"2024-01-15\", \"status\": \"SUCCESS\"}'\n",
    "\n",
    "data = json.loads(task_a_output)\n",
    "print(f\"Received from Task A:\")\n",
    "print(f\"  Rows: {data['rows_processed']}\")\n",
    "print(f\"  Max date: {data['max_date']}\")\n",
    "print(f\"  Status: {data['status']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8058a82",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Monitoring przez System Tables\n",
    "\n",
    "Databricks udostępnia system tables z historią Job runs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d73f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historia uruchomień Jobs (ostatnie 7 dni)\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        job_name,\n",
    "        run_id,\n",
    "        DATE(start_time) as run_date,\n",
    "        result_state,\n",
    "        ROUND((UNIX_TIMESTAMP(end_time) - UNIX_TIMESTAMP(start_time)) / 60, 1) as duration_min\n",
    "    FROM system.lakeflow.job_runs\n",
    "    WHERE start_time >= current_date() - INTERVAL 7 DAYS\n",
    "    ORDER BY start_time DESC\n",
    "    LIMIT 20\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed468260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Success rate per Job\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        job_name,\n",
    "        COUNT(*) as total_runs,\n",
    "        SUM(CASE WHEN result_state = 'SUCCESS' THEN 1 ELSE 0 END) as successful,\n",
    "        SUM(CASE WHEN result_state = 'FAILED' THEN 1 ELSE 0 END) as failed,\n",
    "        ROUND(SUM(CASE WHEN result_state = 'SUCCESS' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 1) as success_rate_pct\n",
    "    FROM system.lakeflow.job_runs\n",
    "    WHERE start_time >= current_date() - INTERVAL 30 DAYS\n",
    "    GROUP BY job_name\n",
    "    ORDER BY total_runs DESC\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91865700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trend czasu wykonania\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATE(start_time) as run_date,\n",
    "        job_name,\n",
    "        ROUND(AVG((UNIX_TIMESTAMP(end_time) - UNIX_TIMESTAMP(start_time)) / 60), 1) as avg_duration_min,\n",
    "        COUNT(*) as runs\n",
    "    FROM system.lakeflow.job_runs\n",
    "    WHERE start_time >= current_date() - INTERVAL 14 DAYS\n",
    "        AND result_state = 'SUCCESS'\n",
    "    GROUP BY run_date, job_name\n",
    "    ORDER BY run_date DESC\n",
    "\"\"\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248154e1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "### W tym module nauczyłeś się:\n",
    "\n",
    "| Temat | Kluczowe elementy |\n",
    "|-------|-------------------|\n",
    "| **Multi-task Jobs** | DAG workflow, zależności między taskami |\n",
    "| **Triggery** | Scheduled (CRON), File arrival, Continuous |\n",
    "| **Opcje** | Timeout, Retry, Max concurrent runs |\n",
    "| **Alerting** | Email, Webhooks (Slack/Teams) |\n",
    "| **Parametry** | Widgets, dynamic values, taskValues |\n",
    "| **Monitoring** | System tables, success rate, duration trends |\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. **Serverless** - używaj dla większości Jobs (szybki start, auto-scaling)\n",
    "2. **Idempotency** - Job powinien być bezpieczny do ponownego uruchomienia\n",
    "3. **Retry** - tylko dla transient errors (network, API)\n",
    "4. **Alerting** - zawsze konfiguruj powiadomienia o błędach\n",
    "5. **Monitoring** - regularnie sprawdzaj success rate i trendy\n",
    "\n",
    "### Następne kroki:\n",
    "\n",
    "- **Notebook 04**: Unity Catalog Governance\n",
    "- **Workshop**: Hands-on Job creation\n",
    "\n",
    "---\n",
    "\n",
    "## Materiały dodatkowe\n",
    "\n",
    "- [Databricks Jobs Documentation](https://docs.databricks.com/workflows/jobs/jobs.html)\n",
    "- [Serverless Jobs](https://docs.databricks.com/en/jobs/serverless.html)\n",
    "- [Jobs Best Practices](https://docs.databricks.com/workflows/jobs/jobs-best-practices.html)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
