{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "718d00c5",
   "metadata": {},
   "source": [
    "# Demo 1: Zaawansowane Transformacje PySpark\n",
    "\n",
    "**Temat:** Advanced PySpark Transformations  \n",
    "**Czas trwania:** 60 minut  \n",
    "**Typ:** Live coding demo + teoria\n",
    "\n",
    "---\n",
    "\n",
    "## üìã Cele szkoleniowe\n",
    "\n",
    "- Window Functions: partitionBy, orderBy, rowsBetween, rangeBetween\n",
    "- Funkcje okienkowe: lag, lead, row_number, dense_rank, rank\n",
    "- Rolling windows i agregacje ruchome\n",
    "- Struktury z≈Ço≈ºone: explode, posexplode, sequence\n",
    "- JSON processing: from_json, to_json, schema_of_json\n",
    "- Funkcje datowe: date_trunc, date_add, add_months, last_day\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea713080",
   "metadata": {},
   "source": [
    "## Inicjalizacja ≈õrodowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba7febe",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b813e4a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czƒô≈õƒá 1: Window Functions - podstawy\n",
    "\n",
    "### Koncepcja Window Functions\n",
    "\n",
    "Window Functions pozwalajƒÖ na wykonywanie oblicze≈Ñ na \"oknach\" (partycjach) danych bez aggregacji ca≈Çego DataFrame.\n",
    "\n",
    "**Kluczowe elementy:**\n",
    "- `partitionBy()`: Podzia≈Ç danych na grupy\n",
    "- `orderBy()`: Sortowanie w ramach partycji\n",
    "- `rowsBetween()` / `rangeBetween()`: Definicja zakresu okna\n",
    "\n",
    "**Zastosowania:**\n",
    "- Ranking (row_number, rank, dense_rank)\n",
    "- Por√≥wnania czasowe (lag, lead)\n",
    "- Agregacje ruchome (rolling sum, moving average)\n",
    "- Analiza trend√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260822ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, row_number, rank, dense_rank, lag, lead,\n",
    "    sum as _sum, avg, count, max as _max, min as _min,\n",
    "    to_date, date_trunc, date_add, add_months, last_day,\n",
    "    explode, posexplode, sequence, from_json, to_json, schema_of_json,\n",
    "    current_timestamp, round as _round, lit\n",
    ")\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType, DateType, ArrayType\n",
    "import datetime\n",
    "\n",
    "# Przygotowanie przyk≈Çadowych danych zam√≥wie≈Ñ\n",
    "orders_data = [\n",
    "    (1, 1, \"2024-01-15\", 150.0),\n",
    "    (2, 2, \"2024-01-16\", 200.0),\n",
    "    (3, 1, \"2024-02-10\", 300.0),\n",
    "    (4, 3, \"2024-02-12\", 100.0),\n",
    "    (5, 2, \"2024-03-05\", 450.0),\n",
    "    (6, 1, \"2024-03-15\", 250.0),\n",
    "    (7, 3, \"2024-03-20\", 180.0),\n",
    "    (8, 2, \"2024-04-01\", 320.0),\n",
    "    (9, 1, \"2024-04-10\", 400.0),\n",
    "    (10, 3, \"2024-04-15\", 220.0),\n",
    "]\n",
    "\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"order_date\", StringType(), False),\n",
    "    StructField(\"amount\", DoubleType(), False)\n",
    "])\n",
    "\n",
    "orders_df = spark.createDataFrame(orders_data, orders_schema)\n",
    "orders_df = orders_df.withColumn(\"order_date\", to_date(col(\"order_date\")))\n",
    "\n",
    "orders_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f44541",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 1: Ranking - row_number, rank, dense_rank\n",
    "\n",
    "**R√≥≈ºnice:**\n",
    "- `row_number()`: Unikalne numery (1, 2, 3, 4, ...)\n",
    "- `rank()`: Z przerwami przy r√≥wnych warto≈õciach (1, 2, 2, 4, ...)\n",
    "- `dense_rank()`: Bez przerw przy r√≥wnych warto≈õciach (1, 2, 2, 3, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c829d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking zam√≥wie≈Ñ dla ka≈ºdego klienta wed≈Çug kwoty (malejƒÖco)\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(col(\"amount\").desc())\n",
    "\n",
    "orders_ranked = orders_df.withColumn(\"row_num\", row_number().over(window_spec)) \\\n",
    "    .withColumn(\"rank\", rank().over(window_spec)) \\\n",
    "    .withColumn(\"dense_rank\", dense_rank().over(window_spec))\n",
    "\n",
    "orders_ranked.orderBy(\"customer_id\", \"row_num\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90393637",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 2: Funkcje lag i lead\n",
    "\n",
    "**lag()**: Warto≈õƒá z poprzedniego wiersza  \n",
    "**lead()**: Warto≈õƒá z nastƒôpnego wiersza\n",
    "\n",
    "**Zastosowania:**\n",
    "- Por√≥wnanie z poprzednim okresem\n",
    "- Obliczenie zmian (deltas)\n",
    "- Analiza sekwencji zdarze≈Ñ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f85b5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por√≥wnanie z poprzednim i nastƒôpnym zam√≥wieniem\n",
    "window_spec_time = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "\n",
    "orders_lag_lead = orders_df \\\n",
    "    .withColumn(\"prev_amount\", lag(\"amount\", 1).over(window_spec_time)) \\\n",
    "    .withColumn(\"next_amount\", lead(\"amount\", 1).over(window_spec_time)) \\\n",
    "    .withColumn(\"amount_change\", col(\"amount\") - col(\"prev_amount\")) \\\n",
    "    .withColumn(\"amount_change_pct\", \n",
    "                _round((col(\"amount\") - col(\"prev_amount\")) / col(\"prev_amount\") * 100, 2))\n",
    "\n",
    "orders_lag_lead.orderBy(\"customer_id\", \"order_date\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9036a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czƒô≈õƒá 2: Rolling Windows - agregacje ruchome\n",
    "\n",
    "### rowsBetween vs rangeBetween\n",
    "\n",
    "**rowsBetween(start, end)**: Zakres wierszy  \n",
    "- `Window.unboundedPreceding`: Od poczƒÖtku partycji  \n",
    "- `Window.unboundedFollowing`: Do ko≈Ñca partycji  \n",
    "- `-1, 0`: Poprzedni wiersz i bie≈ºƒÖcy  \n",
    "- `-2, 0`: Dwa poprzednie + bie≈ºƒÖcy (3-row window)\n",
    "\n",
    "**rangeBetween(start, end)**: Zakres warto≈õci (wymaga orderBy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cce1849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling sum - suma kroczƒÖca (wszystkie poprzednie + bie≈ºƒÖcy)\n",
    "window_cumulative = Window.partitionBy(\"customer_id\") \\\n",
    "    .orderBy(\"order_date\") \\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "orders_cumulative = orders_df \\\n",
    "    .withColumn(\"cumulative_sum\", _sum(\"amount\").over(window_cumulative)) \\\n",
    "    .withColumn(\"cumulative_count\", count(\"order_id\").over(window_cumulative)) \\\n",
    "    .withColumn(\"cumulative_avg\", _round(avg(\"amount\").over(window_cumulative), 2))\n",
    "\n",
    "orders_cumulative.orderBy(\"customer_id\", \"order_date\").display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d55a6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving average - ≈õrednia kroczƒÖca z 3 ostatnich zam√≥wie≈Ñ\n",
    "window_moving_3 = Window.partitionBy(\"customer_id\") \\\n",
    "    .orderBy(\"order_date\") \\\n",
    "    .rowsBetween(-2, 0)  # 2 poprzednie + bie≈ºƒÖcy = 3 wiersze\n",
    "\n",
    "orders_moving_avg = orders_df \\\n",
    "    .withColumn(\"moving_avg_3\", _round(avg(\"amount\").over(window_moving_3), 2)) \\\n",
    "    .withColumn(\"moving_sum_3\", _sum(\"amount\").over(window_moving_3)) \\\n",
    "    .withColumn(\"moving_max_3\", _max(\"amount\").over(window_moving_3)) \\\n",
    "    .withColumn(\"moving_min_3\", _min(\"amount\").over(window_moving_3))\n",
    "\n",
    "orders_moving_avg.orderBy(\"customer_id\", \"order_date\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0b89d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czƒô≈õƒá 3: Struktury z≈Ço≈ºone - Arrays\n",
    "\n",
    "### explode i posexplode\n",
    "\n",
    "**explode()**: Rozwija tablicƒô do osobnych wierszy  \n",
    "**posexplode()**: Jak explode, ale z numerem pozycji\n",
    "\n",
    "**Zastosowania:**\n",
    "- Normalizacja danych zagnie≈ºd≈ºonych\n",
    "- Analiza list (tagi, kategorie, produkty)\n",
    "- Event tracking (sekwencje akcji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820bd764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad: Produkty w zam√≥wieniach (array)\n",
    "orders_with_products_data = [\n",
    "    (1, 1, \"2024-01-15\", [\"Laptop\", \"Mouse\", \"Keyboard\"]),\n",
    "    (2, 2, \"2024-01-16\", [\"Monitor\", \"Cable\"]),\n",
    "    (3, 1, \"2024-02-10\", [\"Headphones\"]),\n",
    "    (4, 3, \"2024-02-12\", [\"Tablet\", \"Case\", \"Stylus\", \"Charger\"]),\n",
    "]\n",
    "\n",
    "orders_products_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), False),\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"order_date\", StringType(), False),\n",
    "    StructField(\"products\", ArrayType(StringType()), False)\n",
    "])\n",
    "\n",
    "orders_products_df = spark.createDataFrame(orders_with_products_data, orders_products_schema)\n",
    "\n",
    "print(\"Dane oryginalne (z arrayami):\")\n",
    "orders_products_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629f13e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode() - rozwija tablicƒô do osobnych wierszy\n",
    "orders_exploded = orders_products_df.select(\n",
    "    \"order_id\",\n",
    "    \"customer_id\",\n",
    "    \"order_date\",\n",
    "    explode(\"products\").alias(\"product\")\n",
    ")\n",
    "\n",
    "print(\"Dane po explode():\")\n",
    "orders_exploded.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea3c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# posexplode() - rozwija tablicƒô z numerem pozycji\n",
    "orders_posexploded = orders_products_df.select(\n",
    "    \"order_id\",\n",
    "    \"customer_id\",\n",
    "    \"order_date\",\n",
    "    posexplode(\"products\").alias(\"position\", \"product\")\n",
    ")\n",
    "\n",
    "print(\"Dane po posexplode() - z numerem pozycji:\")\n",
    "orders_posexploded.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bf6393",
   "metadata": {},
   "source": [
    "### sequence() - generowanie sekwencji\n",
    "\n",
    "**sequence(start, stop, step)**: Generuje tablicƒô warto≈õci\n",
    "\n",
    "**Zastosowania:**\n",
    "- Generowanie zakres√≥w dat\n",
    "- Tworzenie series czasowych\n",
    "- Wype≈Çnianie brak√≥w w danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f20298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad: Generowanie sekwencji dni miƒôdzy datami\n",
    "from pyspark.sql.functions import expr\n",
    "\n",
    "date_ranges_data = [\n",
    "    (\"2024-01-01\", \"2024-01-05\"),\n",
    "    (\"2024-02-01\", \"2024-02-03\"),\n",
    "]\n",
    "\n",
    "date_ranges_df = spark.createDataFrame(date_ranges_data, [\"start_date\", \"end_date\"]) \\\n",
    "    .withColumn(\"start_date\", to_date(col(\"start_date\"))) \\\n",
    "    .withColumn(\"end_date\", to_date(col(\"end_date\")))\n",
    "\n",
    "# Generuj sekwencjƒô dni\n",
    "date_sequence = date_ranges_df.withColumn(\n",
    "    \"date_array\",\n",
    "    expr(\"sequence(start_date, end_date, interval 1 day)\")\n",
    ")\n",
    "\n",
    "print(\"Sekwencja dat jako array:\")\n",
    "date_sequence.display()\n",
    "\n",
    "# Rozwij do osobnych wierszy\n",
    "print(\"\\nSekwencja dat po explode:\")\n",
    "date_sequence.select(\n",
    "    \"start_date\",\n",
    "    \"end_date\",\n",
    "    explode(\"date_array\").alias(\"date\")\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede36a7d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czƒô≈õƒá 4: JSON Processing\n",
    "\n",
    "### from_json, to_json, schema_of_json\n",
    "\n",
    "**from_json()**: Parsowanie JSON string ‚Üí struct/array  \n",
    "**to_json()**: Konwersja struct/array ‚Üí JSON string  \n",
    "**schema_of_json()**: Automatyczne wykrycie schematu JSON\n",
    "\n",
    "**Zastosowania:**\n",
    "- Parsowanie API responses\n",
    "- Event tracking (nested JSON events)\n",
    "- Log processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cacbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad: Parsowanie JSON payload\n",
    "json_data = [\n",
    "    (1, '{\"user_id\": 101, \"action\": \"click\", \"metadata\": {\"page\": \"home\", \"duration\": 30}}'),\n",
    "    (2, '{\"user_id\": 102, \"action\": \"purchase\", \"metadata\": {\"page\": \"checkout\", \"duration\": 120}}'),\n",
    "    (3, '{\"user_id\": 101, \"action\": \"view\", \"metadata\": {\"page\": \"product\", \"duration\": 45}}'),\n",
    "]\n",
    "\n",
    "json_df = spark.createDataFrame(json_data, [\"event_id\", \"json_payload\"])\n",
    "\n",
    "print(\"Dane oryginalne (JSON jako string):\")\n",
    "json_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3e333d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatyczne wykrycie schematu JSON\n",
    "json_schema = schema_of_json(lit(json_data[0][1]))\n",
    "print(f\"Wykryty schemat: {json_schema}\")\n",
    "\n",
    "# Parsowanie JSON\n",
    "json_parsed = json_df.withColumn(\"parsed_data\", from_json(col(\"json_payload\"), json_schema))\n",
    "\n",
    "print(\"\\nDane po parsowaniu JSON:\")\n",
    "json_parsed.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e49453d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WyciƒÖganie p√≥l z zagnie≈ºd≈ºonego JSON\n",
    "json_flattened = json_parsed.select(\n",
    "    \"event_id\",\n",
    "    col(\"parsed_data.user_id\").alias(\"user_id\"),\n",
    "    col(\"parsed_data.action\").alias(\"action\"),\n",
    "    col(\"parsed_data.metadata.page\").alias(\"page\"),\n",
    "    col(\"parsed_data.metadata.duration\").alias(\"duration\")\n",
    ")\n",
    "\n",
    "print(\"Dane po sp≈Çaszczeniu (flattening):\")\n",
    "json_flattened.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5a956f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czƒô≈õƒá 5: Funkcje datowe i czasowe\n",
    "\n",
    "### Kluczowe funkcje\n",
    "\n",
    "**date_trunc()**: Obciƒôcie do granicy (rok, miesiƒÖc, dzie≈Ñ, godzina)  \n",
    "**date_add()**: Dodawanie dni  \n",
    "**add_months()**: Dodawanie miesiƒôcy  \n",
    "**last_day()**: Ostatni dzie≈Ñ miesiƒÖca  \n",
    "**datediff()**: R√≥≈ºnica w dniach  \n",
    "**months_between()**: R√≥≈ºnica w miesiƒÖcach\n",
    "\n",
    "**Zastosowania:**\n",
    "- Agregacje temporalne (daily, monthly, yearly)\n",
    "- Analiza cohort\n",
    "- Retention analysis\n",
    "- Forecast horizons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb239309",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, months_between, year, month, dayofweek, quarter\n",
    "\n",
    "# Przyk≈Çad: Analiza temporalna zam√≥wie≈Ñ\n",
    "orders_temporal = orders_df \\\n",
    "    .withColumn(\"year\", year(\"order_date\")) \\\n",
    "    .withColumn(\"month\", month(\"order_date\")) \\\n",
    "    .withColumn(\"quarter\", quarter(\"order_date\")) \\\n",
    "    .withColumn(\"day_of_week\", dayofweek(\"order_date\")) \\\n",
    "    .withColumn(\"month_start\", date_trunc(\"month\", \"order_date\")) \\\n",
    "    .withColumn(\"month_end\", last_day(\"order_date\")) \\\n",
    "    .withColumn(\"next_month_start\", date_add(last_day(\"order_date\"), 1))\n",
    "\n",
    "orders_temporal.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13da278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad: Obliczanie okres√≥w miƒôdzy zam√≥wieniami\n",
    "window_date = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "\n",
    "orders_periods = orders_df \\\n",
    "    .withColumn(\"prev_order_date\", lag(\"order_date\", 1).over(window_date)) \\\n",
    "    .withColumn(\"days_since_last_order\", \n",
    "                datediff(col(\"order_date\"), col(\"prev_order_date\"))) \\\n",
    "    .withColumn(\"months_since_last_order\", \n",
    "                _round(months_between(col(\"order_date\"), col(\"prev_order_date\")), 2))\n",
    "\n",
    "orders_periods.orderBy(\"customer_id\", \"order_date\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2eda50d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czƒô≈õƒá 6: Praktyczny przyk≈Çad - Customer Behavior Analysis\n",
    "\n",
    "### Zadanie: Analiza zachowa≈Ñ klient√≥w\n",
    "\n",
    "Wykorzystamy wszystkie poznane techniki do kompleksowej analizy:\n",
    "1. Ranking zam√≥wie≈Ñ dla ka≈ºdego klienta\n",
    "2. Por√≥wnanie z poprzednim zam√≥wieniem (lag)\n",
    "3. ≈örednia ruchoma wydatk√≥w\n",
    "4. Segmentacja temporalna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e66cdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kompleksowa analiza zachowa≈Ñ klient√≥w\n",
    "window_customer_time = Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "window_customer_amount = Window.partitionBy(\"customer_id\").orderBy(col(\"amount\").desc())\n",
    "window_moving_avg = Window.partitionBy(\"customer_id\").orderBy(\"order_date\").rowsBetween(-2, 0)\n",
    "\n",
    "customer_behavior = orders_df \\\n",
    "    .withColumn(\"order_rank\", row_number().over(window_customer_amount)) \\\n",
    "    .withColumn(\"order_sequence\", row_number().over(window_customer_time)) \\\n",
    "    .withColumn(\"prev_amount\", lag(\"amount\", 1).over(window_customer_time)) \\\n",
    "    .withColumn(\"amount_change\", col(\"amount\") - col(\"prev_amount\")) \\\n",
    "    .withColumn(\"moving_avg_3\", _round(avg(\"amount\").over(window_moving_avg), 2)) \\\n",
    "    .withColumn(\"cumulative_spent\", _sum(\"amount\").over(\n",
    "        Window.partitionBy(\"customer_id\").orderBy(\"order_date\")\n",
    "        .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    )) \\\n",
    "    .withColumn(\"month\", date_trunc(\"month\", \"order_date\"))\n",
    "\n",
    "customer_behavior.orderBy(\"customer_id\", \"order_date\").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a63a84",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "### ‚úÖ Om√≥wione zagadnienia\n",
    "\n",
    "1. **Window Functions**\n",
    "   - partitionBy, orderBy\n",
    "   - row_number, rank, dense_rank\n",
    "   - lag, lead\n",
    "\n",
    "2. **Rolling Windows**\n",
    "   - rowsBetween / rangeBetween\n",
    "   - Cumulative aggregations\n",
    "   - Moving averages\n",
    "\n",
    "3. **Struktury z≈Ço≈ºone**\n",
    "   - explode / posexplode\n",
    "   - sequence()\n",
    "\n",
    "4. **JSON Processing**\n",
    "   - from_json, to_json\n",
    "   - schema_of_json\n",
    "   - Flattening nested JSON\n",
    "\n",
    "5. **Funkcje datowe**\n",
    "   - date_trunc, date_add, add_months, last_day\n",
    "   - datediff, months_between\n",
    "   - Temporal aggregations\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Best Practices\n",
    "\n",
    "1. **Window Functions**: U≈ºywaj partitionBy dla efektywno≈õci\n",
    "2. **Rolling Windows**: Wybierz odpowiedni zakres (rows vs range)\n",
    "3. **explode**: Uwa≈ºaj na performance przy du≈ºych arrayach\n",
    "4. **JSON**: Wykorzystuj schema_of_json dla automatycznego wykrycia struktury\n",
    "5. **Temporal**: Standaryzuj strefy czasowe przed analizƒÖ\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
