{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55f975a9",
   "metadata": {},
   "source": [
    "# Databricks Jobs Orchestration\n",
    "\n",
    "---\n",
    "\n",
    "## Agenda\n",
    "\n",
    "1. Wprowadzenie do Databricks Jobs\n",
    "2. Multi-task Jobs i zależności\n",
    "3. Task Types: Notebook, DLT, SQL, dbt\n",
    "4. Parametryzacja i Widgets\n",
    "5. Monitoring, Alerting i Retry Logic\n",
    "6. Best Practices dla produkcyjnych Jobs\n",
    "\n",
    "---\n",
    "\n",
    "## Cele szkolenia\n",
    "\n",
    "Po tym module będziesz potrafić:\n",
    "- Tworzyć multi-task Databricks Jobs\n",
    "- Konfigurować zależności między taskami\n",
    "- Parametryzować workflows\n",
    "- Monitorować i debugować Jobs\n",
    "- Implementować retry logic i alerting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b08a70",
   "metadata": {},
   "source": [
    "## 1. Wprowadzenie do Lakeflow Jobs\n",
    "\n",
    "**Lakeflow Jobs** (dawniej Databricks Jobs) to zarządzany service orkiestracji dla:\n",
    "- ETL/ELT pipelines\n",
    "- Machine Learning workflows\n",
    "- Scheduled reports\n",
    "- Data quality checks\n",
    "\n",
    "### Kluczowe cechy:\n",
    "- **Multi-task workflows**: DAG (Directed Acyclic Graph)\n",
    "- **Task types**: Notebook, Lakeflow SDP (dawniej DLT), SQL, dbt, Python wheel, JAR\n",
    "- **Scheduling**: cron, continuous, triggered\n",
    "- **Retry logic**: automatyczne retry przy błędach\n",
    "- **Alerting**: email, webhooks, integrations\n",
    "- **Cost optimization**: Serverless, spot instances, autoscaling\n",
    "\n",
    "### Serverless Jobs (zalecane od 2024)\n",
    "\n",
    "**Serverless compute dla Jobs** eliminuje potrzebę konfiguracji klastrów:\n",
    "- Near-zero startup time\n",
    "- Automatic optimization przez Databricks\n",
    "- Pay-per-use billing\n",
    "- Autoscaling i Photon włączone domyślnie\n",
    "- Performance Mode (od April 2025) - wybór między wydajnością a kosztami\n",
    "\n",
    "### Jobs vs Lakeflow SDP:\n",
    "\n",
    "| Feature | Lakeflow Jobs | Lakeflow SDP (DLT) |\n",
    "|---------|---------------|-------------------|\n",
    "| Use Case | General orchestration | ETL pipelines |\n",
    "| Task Types | Notebook, SQL, dbt, SDP | SDP only |\n",
    "| Dependencies | Manual configuration | Automatic (DAG) |\n",
    "| Data Quality | Custom code | Built-in expectations |\n",
    "| Flexibility | High | Opinionated |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342c985b",
   "metadata": {},
   "source": [
    "## 2. Multi-task Jobs i zależności\n",
    "\n",
    "### Podstawowa struktura Job:\n",
    "\n",
    "```\n",
    "Job: Daily_ETL_Pipeline\n",
    "├── Task 1: ingest_raw_data (Notebook)\n",
    "├── Task 2: validate_data (SQL) → depends_on: Task 1\n",
    "├── Task 3: transform_silver (Notebook) → depends_on: Task 2\n",
    "└── Task 4: aggregate_gold (DLT) → depends_on: Task 3\n",
    "```\n",
    "\n",
    "### Przykład konfiguracji Job (JSON):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11b3b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguracja Job przez Databricks REST API lub UI\n",
    "\n",
    "job_config = {\n",
    "    \"name\": \"KION_Daily_Orders_ETL\",\n",
    "    \"email_notifications\": {\n",
    "        \"on_failure\": [\"data-team@kion.com\"],\n",
    "        \"on_success\": [\"data-team@kion.com\"]\n",
    "    },\n",
    "    \"timeout_seconds\": 7200,  # 2 hours\n",
    "    \"max_concurrent_runs\": 1,\n",
    "    \"format\": \"MULTI_TASK\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_key\": \"ingest_bronze\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/KION/notebooks/01_ingest_orders\",\n",
    "                \"base_parameters\": {\n",
    "                    \"source_path\": \"/Volumes/main/default/kion_data/orders\",\n",
    "                    \"target_table\": \"bronze_orders\"\n",
    "                }\n",
    "            },\n",
    "            \"new_cluster\": {\n",
    "                \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                \"node_type_id\": \"Standard_DS3_v2\",\n",
    "                \"num_workers\": 2,\n",
    "                \"autoscale\": {\n",
    "                    \"min_workers\": 1,\n",
    "                    \"max_workers\": 4\n",
    "                }\n",
    "            },\n",
    "            \"timeout_seconds\": 3600\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"transform_silver\",\n",
    "            \"depends_on\": [{\"task_key\": \"ingest_bronze\"}],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/KION/notebooks/02_transform_silver\",\n",
    "                \"base_parameters\": {\n",
    "                    \"source_table\": \"bronze_orders\",\n",
    "                    \"target_table\": \"silver_orders\"\n",
    "                }\n",
    "            },\n",
    "            \"existing_cluster_id\": \"{{previous_task_cluster}}\"\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"aggregate_gold\",\n",
    "            \"depends_on\": [{\"task_key\": \"transform_silver\"}],\n",
    "            \"sql_task\": {\n",
    "                \"warehouse_id\": \"abc123def456\",\n",
    "                \"query\": {\n",
    "                    \"query_id\": \"gold_aggregations_query_id\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"schedule\": {\n",
    "        \"quartz_cron_expression\": \"0 0 2 * * ?\",  # Daily at 2 AM\n",
    "        \"timezone_id\": \"Europe/Warsaw\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Job configuration ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97359cf",
   "metadata": {},
   "source": [
    "### Dependency patterns:\n",
    "\n",
    "#### 1. Linear pipeline (sequential):\n",
    "```\n",
    "Task A → Task B → Task C → Task D\n",
    "```\n",
    "\n",
    "#### 2. Fan-out (parallel processing):\n",
    "```\n",
    " Task A\n",
    " / | \\\n",
    " B C D\n",
    "```\n",
    "\n",
    "#### 3. Fan-in (merge results):\n",
    "```\n",
    " A B C\n",
    " \\ | /\n",
    " Task D\n",
    "```\n",
    "\n",
    "#### 4. Diamond (complex):\n",
    "```\n",
    " Task A\n",
    " / \\\n",
    " B C\n",
    " \\ /\n",
    " Task D\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004dc98e",
   "metadata": {},
   "source": [
    "## 3. Task Types: Notebook, DLT, SQL, dbt\n",
    "\n",
    "### 1. Notebook Task\n",
    "\n",
    "**Use case**: Python/Scala/R processing, custom logic\n",
    "\n",
    "**Example notebook** (`01_ingest_orders.ipynb`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963551a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Task Example\n",
    "# File: 01_ingest_orders.ipynb\n",
    "\n",
    "# Get parameters from Job\n",
    "dbutils.widgets.text(\"source_path\", \"/Volumes/main/default/kion_data/orders\")\n",
    "dbutils.widgets.text(\"target_table\", \"bronze_orders\")\n",
    "dbutils.widgets.text(\"run_date\", \"\")\n",
    "\n",
    "source_path = dbutils.widgets.get(\"source_path\")\n",
    "target_table = dbutils.widgets.get(\"target_table\")\n",
    "run_date = dbutils.widgets.get(\"run_date\") or datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"Starting ingestion from {source_path} to {target_table}\")\n",
    "print(f\"Run date: {run_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "\n",
    "# Load data\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{source_path}/*.csv\")\n",
    ")\n",
    "\n",
    "# Add audit columns\n",
    "df_with_audit = (\n",
    "    df\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    .withColumn(\"run_date\", lit(run_date))\n",
    ")\n",
    "\n",
    "# Write to Delta\n",
    "df_with_audit.write.format(\"delta\").mode(\"append\").saveAsTable(target_table)\n",
    "\n",
    "# Return metrics to Job\n",
    "row_count = df_with_audit.count()\n",
    "dbutils.notebook.exit({\n",
    "    \"status\": \"success\",\n",
    "    \"rows_ingested\": row_count,\n",
    "    \"target_table\": target_table\n",
    "})\n",
    "\n",
    "print(f\"Ingested {row_count} rows to {target_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1085177c",
   "metadata": {},
   "source": [
    "### 2. SQL Task\n",
    "\n",
    "**Use case**: SQL-based transformations, aggregations\n",
    "\n",
    "**Example SQL query**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b21f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Task - może być zapisane jako SQL Query w Databricks SQL\n",
    "\n",
    "sql_query = \"\"\"\n",
    "-- Gold Layer Aggregation\n",
    "CREATE OR REPLACE TABLE gold_daily_sales AS\n",
    "SELECT \n",
    "    order_date,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers,\n",
    "    COUNT(order_id) as total_orders,\n",
    "    SUM(amount) as total_revenue,\n",
    "    AVG(amount) as avg_order_value,\n",
    "    MAX(amount) as max_order_value,\n",
    "    CURRENT_TIMESTAMP() as calculated_at\n",
    "FROM silver_orders\n",
    "WHERE status = 'completed'\n",
    "GROUP BY order_date\n",
    "ORDER BY order_date DESC\n",
    "\"\"\"\n",
    "\n",
    "# W Job configuration:\n",
    "# \"sql_task\": {\n",
    "#     \"warehouse_id\": \"abc123\",\n",
    "#     \"query\": {\"query_id\": \"saved_query_id\"}\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d172d1",
   "metadata": {},
   "source": [
    "### 3. DLT Task\n",
    "\n",
    "**Use case**: Delta Live Tables pipeline execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a6a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DLT Task configuration\n",
    "dlt_task = {\n",
    "    \"task_key\": \"run_dlt_pipeline\",\n",
    "    \"depends_on\": [{\"task_key\": \"validate_source_data\"}],\n",
    "    \"pipeline_task\": {\n",
    "        \"pipeline_id\": \"abc123-dlt-pipeline-id\",\n",
    "        \"full_refresh\": False  # Incremental by default\n",
    "    }\n",
    "}\n",
    "\n",
    "# DLT pipeline będzie uruchomiony jako task w większym Job workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5706f6c",
   "metadata": {},
   "source": [
    "### 4. dbt Task\n",
    "\n",
    "**Use case**: dbt models execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b392d99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbt Task configuration\n",
    "dbt_task = {\n",
    "    \"task_key\": \"run_dbt_models\",\n",
    "    \"dbt_task\": {\n",
    "        \"project_directory\": \"/Workspace/KION/dbt\",\n",
    "        \"commands\": [\n",
    "            \"dbt deps\",\n",
    "            \"dbt seed\",\n",
    "            \"dbt run --models tag:daily\",\n",
    "            \"dbt test\"\n",
    "        ],\n",
    "        \"profiles_directory\": \"/Workspace/KION/dbt\",\n",
    "        \"warehouse_id\": \"abc123\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# dbt models mogą być zintegrowane jako tasks w Job workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0f84c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Parametryzacja i Widgets\n",
    "\n",
    "### Passing parameters między taskami:\n",
    "\n",
    "#### Metoda 1: Job-level parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e11687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Job configuration z parametrami\n",
    "job_with_params = {\n",
    "    \"name\": \"Parameterized_ETL_Job\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_key\": \"task_1\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/KION/task1\",\n",
    "                \"base_parameters\": {\n",
    "                    \"environment\": \"production\",\n",
    "                    \"run_date\": \"{{job.start_time.iso_date}}\",  # Dynamic parameter\n",
    "                    \"source_path\": \"/Volumes/main/default/kion_data\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# W notebooku odbieramy parametry:\n",
    "# dbutils.widgets.text(\"environment\", \"dev\")\n",
    "# environment = dbutils.widgets.get(\"environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42434bb0",
   "metadata": {},
   "source": [
    "#### Metoda 2: Task output → Next task input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351b3d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1 - zwraca output\n",
    "# File: task1_ingest.ipynb\n",
    "\n",
    "# ... ingestion logic ...\n",
    "\n",
    "# Return output\n",
    "import json\n",
    "output = {\n",
    "    \"rows_ingested\": 1000,\n",
    "    \"target_table\": \"bronze_orders\",\n",
    "    \"max_order_date\": \"2024-01-15\"\n",
    "}\n",
    "dbutils.notebook.exit(json.dumps(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd4b643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2 - odbiera output z Task 1\n",
    "# File: task2_transform.ipynb\n",
    "\n",
    "import json\n",
    "\n",
    "# Get output from previous task\n",
    "task1_output = dbutils.jobs.taskValues.get(\n",
    "    taskKey=\"task_1\",\n",
    "    key=\"default\",\n",
    "    default=\"{}\",\n",
    "    debugValue=\"{}\"\n",
    ")\n",
    "\n",
    "output_dict = json.loads(task1_output)\n",
    "source_table = output_dict.get(\"target_table\")\n",
    "max_date = output_dict.get(\"max_order_date\")\n",
    "\n",
    "print(f\"Processing data from {source_table} up to {max_date}\")\n",
    "\n",
    "# Use in transformation\n",
    "df = spark.table(source_table).filter(f\"order_date <= '{max_date}'\")\n",
    "# ... rest of transformation ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186bfff6",
   "metadata": {},
   "source": [
    "#### Metoda 3: Dynamic values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a10eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks wspiera dynamic values w parametrach:\n",
    "\n",
    "dynamic_params = {\n",
    "    \"run_date\": \"{{job.start_time.iso_date}}\",  # YYYY-MM-DD\n",
    "    \"run_timestamp\": \"{{job.start_time}}\",      # Full timestamp\n",
    "    \"job_id\": \"{{job.id}}\",\n",
    "    \"run_id\": \"{{run.id}}\",\n",
    "    \"parent_run_id\": \"{{parent_run.id}}\"  # Dla nested jobs\n",
    "}\n",
    "\n",
    "# Przykład użycia:\n",
    "# base_parameters: {\n",
    "#     \"processing_date\": \"{{job.start_time.iso_date}}\",\n",
    "#     \"job_run_id\": \"{{run.id}}\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d94871",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Monitoring, Alerting i Retry Logic\n",
    "\n",
    "### Retry Logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de130849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task-level retry configuration\n",
    "task_with_retry = {\n",
    "    \"task_key\": \"data_ingestion\",\n",
    "    \"notebook_task\": {\n",
    "        \"notebook_path\": \"/Workspace/KION/ingest\"\n",
    "    },\n",
    "    \"max_retries\": 3,  # Retry up to 3 times\n",
    "    \"min_retry_interval_millis\": 60000,  # Wait 1 minute between retries\n",
    "    \"retry_on_timeout\": True\n",
    "}\n",
    "\n",
    "# Best practice: używaj retry dla transient errors (network, API rate limits)\n",
    "# NIE używaj retry dla data quality issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da0a874",
   "metadata": {},
   "source": [
    "### Email Alerting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1470a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Email notifications configuration\n",
    "email_config = {\n",
    "    \"email_notifications\": {\n",
    "        \"on_start\": [\"ops@kion.com\"],\n",
    "        \"on_success\": [\"data-team@kion.com\"],\n",
    "        \"on_failure\": [\"data-team@kion.com\", \"on-call@kion.com\"],\n",
    "        \"on_duration_warning_threshold_exceeded\": [\"ops@kion.com\"],\n",
    "        \"no_alert_for_skipped_runs\": True\n",
    "    },\n",
    "    \"health\": {\n",
    "        \"rules\": [\n",
    "            {\n",
    "                \"metric\": \"RUN_DURATION_SECONDS\",\n",
    "                \"op\": \"GREATER_THAN\",\n",
    "                \"value\": 3600  # Alert if job runs > 1 hour\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4f12b5",
   "metadata": {},
   "source": [
    "### Webhook Integration (Slack, Teams, PagerDuty):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f5e16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Webhook configuration dla Slack\n",
    "webhook_config = {\n",
    "    \"webhook_notifications\": {\n",
    "        \"on_failure\": [\n",
    "            {\n",
    "                \"id\": \"slack-data-alerts\"\n",
    "            }\n",
    "        ],\n",
    "        \"on_success\": [\n",
    "            {\n",
    "                \"id\": \"slack-data-alerts\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Webhook musi być wcześniej skonfigurowany w Databricks UI:\n",
    "# Admin Settings → Webhooks → Create Webhook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f84ad5",
   "metadata": {},
   "source": [
    "### Custom monitoring w notebooku:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2592673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom monitoring logic w task notebook\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Load data\n",
    "df = spark.table(\"bronze_orders\")\n",
    "\n",
    "# Data quality checks\n",
    "total_rows = df.count()\n",
    "null_order_ids = df.filter(col(\"order_id\").isNull()).count()\n",
    "null_percentage = (null_order_ids / total_rows) * 100\n",
    "\n",
    "# Alert if quality threshold exceeded\n",
    "if null_percentage > 5:\n",
    "    error_msg = f\"Data quality issue: {null_percentage:.2f}% null order_ids\"\n",
    "    print(error_msg)\n",
    "    \n",
    "    # Log to monitoring table\n",
    "    monitoring_df = spark.createDataFrame([\n",
    "        {\n",
    "            \"job_run_id\": dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"jobId\").get(),\n",
    "            \"check_type\": \"null_check\",\n",
    "            \"metric_name\": \"null_order_id_percentage\",\n",
    "            \"metric_value\": null_percentage,\n",
    "            \"threshold\": 5.0,\n",
    "            \"status\": \"FAILED\",\n",
    "            \"timestamp\": current_timestamp()\n",
    "        }\n",
    "    ])\n",
    "    monitoring_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"job_monitoring_metrics\")\n",
    "    \n",
    "    # Fail the job\n",
    "    raise Exception(error_msg)\n",
    "\n",
    "print(f\"Quality check passed: {null_percentage:.2f}% null order_ids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c6ccfb",
   "metadata": {},
   "source": [
    "### Monitoring Job runs przez API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b6b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query system tables dla job history\n",
    "job_history = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        job_id,\n",
    "        job_name,\n",
    "        run_id,\n",
    "        start_time,\n",
    "        end_time,\n",
    "        DATEDIFF(SECOND, start_time, end_time) as duration_seconds,\n",
    "        result_state,\n",
    "        task_runs\n",
    "    FROM system.lakeflow.job_runs\n",
    "    WHERE job_name = 'KION_Daily_Orders_ETL'\n",
    "        AND start_time >= current_date() - INTERVAL 7 DAYS\n",
    "    ORDER BY start_time DESC\n",
    "\"\"\")\n",
    "job_history.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99c9c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLA monitoring - track on-time completion\n",
    "sla_monitoring = spark.sql(\"\"\"\n",
    "    WITH daily_runs AS (\n",
    "        SELECT \n",
    "            DATE(start_time) as run_date,\n",
    "            COUNT(*) as total_runs,\n",
    "            SUM(CASE WHEN result_state = 'SUCCESS' THEN 1 ELSE 0 END) as successful_runs,\n",
    "            SUM(CASE WHEN result_state = 'FAILED' THEN 1 ELSE 0 END) as failed_runs,\n",
    "            AVG(DATEDIFF(SECOND, start_time, end_time)) as avg_duration_seconds\n",
    "        FROM system.lakeflow.job_runs\n",
    "        WHERE job_name = 'KION_Daily_Orders_ETL'\n",
    "            AND start_time >= current_date() - INTERVAL 30 DAYS\n",
    "        GROUP BY run_date\n",
    "    )\n",
    "    SELECT \n",
    "        run_date,\n",
    "        total_runs,\n",
    "        successful_runs,\n",
    "        failed_runs,\n",
    "        ROUND(successful_runs * 100.0 / total_runs, 2) as success_rate_pct,\n",
    "        ROUND(avg_duration_seconds / 60, 2) as avg_duration_minutes\n",
    "    FROM daily_runs\n",
    "    ORDER BY run_date DESC\n",
    "\"\"\")\n",
    "sla_monitoring.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b5213a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Best Practices dla produkcyjnych Jobs\n",
    "\n",
    "### 1. Cluster Strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e940d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practice: Job Clusters (new cluster per run)\n",
    "job_cluster_config = {\n",
    "    \"new_cluster\": {\n",
    "        \"spark_version\": \"13.3.x-scala2.12\",\n",
    "        \"node_type_id\": \"Standard_DS3_v2\",\n",
    "        \"autoscale\": {\n",
    "            \"min_workers\": 1,\n",
    "            \"max_workers\": 8\n",
    "        },\n",
    "        \"spark_conf\": {\n",
    "            \"spark.databricks.delta.optimizeWrite.enabled\": \"true\",\n",
    "            \"spark.databricks.delta.autoCompact.enabled\": \"true\"\n",
    "        },\n",
    "        \"aws_attributes\": {\n",
    "            \"availability\": \"SPOT_WITH_FALLBACK\",  # Cost optimization\n",
    "            \"spot_bid_price_percent\": 100\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Alternative: Cluster reuse dla linked tasks\n",
    "# \"existing_cluster_id\": \"{{previous_task_cluster}}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad520c81",
   "metadata": {},
   "source": [
    "### 2. Error Handling w Notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b39bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust error handling w notebook tasks\n",
    "\n",
    "try:\n",
    "    # Main processing logic\n",
    "    df = spark.table(\"bronze_orders\")\n",
    "    \n",
    "    # Validation\n",
    "    assert df.count() > 0, \"Source table is empty\"\n",
    "    \n",
    "    # Transformation\n",
    "    result_df = df.filter(col(\"order_id\").isNotNull())\n",
    "    \n",
    "    # Write result\n",
    "    result_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_orders\")\n",
    "    \n",
    "    # Return success\n",
    "    dbutils.notebook.exit(json.dumps({\n",
    "        \"status\": \"SUCCESS\",\n",
    "        \"rows_processed\": result_df.count()\n",
    "    }))\n",
    "    \n",
    "except AssertionError as e:\n",
    "    # Data validation failure\n",
    "    print(f\"Validation Error: {str(e)}\")\n",
    "    dbutils.notebook.exit(json.dumps({\n",
    "        \"status\": \"FAILED\",\n",
    "        \"error_type\": \"VALIDATION_ERROR\",\n",
    "        \"error_message\": str(e)\n",
    "    }))\n",
    "    \n",
    "except Exception as e:\n",
    "    # Unexpected error\n",
    "    print(f\"Unexpected Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    dbutils.notebook.exit(json.dumps({\n",
    "        \"status\": \"FAILED\",\n",
    "        \"error_type\": \"UNEXPECTED_ERROR\",\n",
    "        \"error_message\": str(e)\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9aa007",
   "metadata": {},
   "source": [
    "### 3. Idempotency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f8478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best Practice: Idempotent operations\n",
    "# Job powinien móc być uruchomiony wielokrotnie bez side effects\n",
    "\n",
    "# BAD: Non-idempotent\n",
    "# df.write.format(\"delta\").mode(\"append\").saveAsTable(\"target\")\n",
    "# ^ Re-running duplicates data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ceeac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOD: Partition-specific overwrite\n",
    "df.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .option(\"replaceWhere\", \"order_date = '2024-01-15'\") \\\n",
    "    .saveAsTable(\"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46a9081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOD: Idempotent with MERGE\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "target_table = DeltaTable.forName(spark, \"target\")\n",
    "target_table.alias(\"target\").merge(\n",
    "    df.alias(\"source\"),\n",
    "    \"target.order_id = source.order_id\"\n",
    ").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3678190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOOD: Idempotent with overwrite\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74afe606",
   "metadata": {},
   "source": [
    "### 4. Logging i Observability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bb4da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structured logging\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Log key events\n",
    "logger.info(f\"Job started at {datetime.now()}\")\n",
    "logger.info(f\"Processing partition: {run_date}\")\n",
    "\n",
    "# Log metrics\n",
    "start_time = datetime.now()\n",
    "df = spark.table(\"bronze_orders\")\n",
    "row_count = df.count()\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "logger.info(f\"Processed {row_count} rows in {duration}s\")\n",
    "\n",
    "# Persist metrics to monitoring table\n",
    "metrics_df = spark.createDataFrame([{\n",
    "    \"job_name\": \"KION_Daily_ETL\",\n",
    "    \"run_date\": run_date,\n",
    "    \"metric_name\": \"rows_processed\",\n",
    "    \"metric_value\": row_count,\n",
    "    \"execution_time_seconds\": duration,\n",
    "    \"timestamp\": datetime.now()\n",
    "}])\n",
    "metrics_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"job_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6d3b18",
   "metadata": {},
   "source": [
    "### 5. Cost Optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204e9f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost optimization strategies:\n",
    "\n",
    "cost_optimized_config = {\n",
    "    # 1. Use Spot instances\n",
    "    \"aws_attributes\": {\n",
    "        \"availability\": \"SPOT_WITH_FALLBACK\"\n",
    "    },\n",
    "    \n",
    "    # 2. Enable autoscaling\n",
    "    \"autoscale\": {\n",
    "        \"min_workers\": 1,\n",
    "        \"max_workers\": 8\n",
    "    },\n",
    "    \n",
    "    # 3. Set timeouts\n",
    "    \"timeout_seconds\": 3600,\n",
    "    \n",
    "    # 4. Cluster reuse dla linked tasks\n",
    "    # \"existing_cluster_id\": \"{{previous_task_cluster}}\",\n",
    "    \n",
    "    # 5. Photon acceleration\n",
    "    \"runtime_engine\": \"PHOTON\",\n",
    "    \n",
    "    # 6. Right-size clusters\n",
    "    \"node_type_id\": \"Standard_DS3_v2\",  # Choose appropriate size\n",
    "}\n",
    "\n",
    "# 7. Schedule during off-peak hours\n",
    "schedule_config = {\n",
    "    \"quartz_cron_expression\": \"0 0 2 * * ?\",  # 2 AM\n",
    "    \"timezone_id\": \"Europe/Warsaw\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33de779f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Kompletny przykład: Production-ready Multi-task Job\n",
    "\n",
    "### Scenario: Daily Orders ETL Pipeline\n",
    "```\n",
    "1. validate_source → 2. ingest_bronze → 3. transform_silver\n",
    " ↓\n",
    " 4a. aggregate_gold ← 5. data_quality_check\n",
    " ↓\n",
    " 4b. customer_metrics\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a188767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete Job configuration\n",
    "production_job = {\n",
    "    \"name\": \"KION_Daily_Orders_ETL_Production\",\n",
    "    \"email_notifications\": {\n",
    "        \"on_failure\": [\"data-team@kion.com\", \"on-call@kion.com\"],\n",
    "        \"on_success\": [\"data-team@kion.com\"]\n",
    "    },\n",
    "    \"webhook_notifications\": {\n",
    "        \"on_failure\": [{\"id\": \"slack-alerts\"}]\n",
    "    },\n",
    "    \"timeout_seconds\": 7200,\n",
    "    \"max_concurrent_runs\": 1,\n",
    "    \"format\": \"MULTI_TASK\",\n",
    "    \"tasks\": [\n",
    "        # Task 1: Validate source data availability\n",
    "        {\n",
    "            \"task_key\": \"validate_source\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/KION/jobs/00_validate_source\",\n",
    "                \"base_parameters\": {\n",
    "                    \"source_path\": \"/Volumes/main/default/kion_data/orders\",\n",
    "                    \"run_date\": \"{{job.start_time.iso_date}}\"\n",
    "                }\n",
    "            },\n",
    "            \"new_cluster\": {\n",
    "                \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                \"node_type_id\": \"Standard_DS3_v2\",\n",
    "                \"num_workers\": 1\n",
    "            },\n",
    "            \"timeout_seconds\": 600,\n",
    "            \"max_retries\": 2\n",
    "        },\n",
    "        # Task 2: Ingest to Bronze\n",
    "        {\n",
    "            \"task_key\": \"ingest_bronze\",\n",
    "            \"depends_on\": [{\"task_key\": \"validate_source\"}],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/KION/jobs/01_ingest_bronze\",\n",
    "                \"base_parameters\": {\n",
    "                    \"source_path\": \"/Volumes/main/default/kion_data/orders\",\n",
    "                    \"target_table\": \"bronze_orders\",\n",
    "                    \"run_date\": \"{{job.start_time.iso_date}}\"\n",
    "                }\n",
    "            },\n",
    "            \"new_cluster\": {\n",
    "                \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                \"node_type_id\": \"Standard_DS3_v2\",\n",
    "                \"autoscale\": {\"min_workers\": 2, \"max_workers\": 4},\n",
    "                \"aws_attributes\": {\"availability\": \"SPOT_WITH_FALLBACK\"}\n",
    "            },\n",
    "            \"timeout_seconds\": 1800,\n",
    "            \"max_retries\": 3,\n",
    "            \"min_retry_interval_millis\": 120000\n",
    "        },\n",
    "        # Task 3: Transform to Silver\n",
    "        {\n",
    "            \"task_key\": \"transform_silver\",\n",
    "            \"depends_on\": [{\"task_key\": \"ingest_bronze\"}],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/KION/jobs/02_transform_silver\",\n",
    "                \"base_parameters\": {\n",
    "                    \"source_table\": \"bronze_orders\",\n",
    "                    \"target_table\": \"silver_orders\",\n",
    "                    \"run_date\": \"{{job.start_time.iso_date}}\"\n",
    "                }\n",
    "            },\n",
    "            \"existing_cluster_id\": \"{{previous_task_cluster}}\",  # Reuse cluster\n",
    "            \"timeout_seconds\": 1800\n",
    "        },\n",
    "        # Task 4a: Aggregate to Gold (daily sales)\n",
    "        {\n",
    "            \"task_key\": \"aggregate_gold_daily\",\n",
    "            \"depends_on\": [{\"task_key\": \"transform_silver\"}],\n",
    "            \"sql_task\": {\n",
    "                \"warehouse_id\": \"abc123warehouse\",\n",
    "                \"query\": {\"query_id\": \"gold_daily_sales_query\"}\n",
    "            }\n",
    "        },\n",
    "        # Task 4b: Customer metrics (parallel with 4a)\n",
    "        {\n",
    "            \"task_key\": \"aggregate_gold_customers\",\n",
    "            \"depends_on\": [{\"task_key\": \"transform_silver\"}],\n",
    "            \"sql_task\": {\n",
    "                \"warehouse_id\": \"abc123warehouse\",\n",
    "                \"query\": {\"query_id\": \"gold_customer_ltv_query\"}\n",
    "            }\n",
    "        },\n",
    "        # Task 5: Data quality checks\n",
    "        {\n",
    "            \"task_key\": \"data_quality_check\",\n",
    "            \"depends_on\": [\n",
    "                {\"task_key\": \"aggregate_gold_daily\"},\n",
    "                {\"task_key\": \"aggregate_gold_customers\"}\n",
    "            ],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/KION/jobs/99_quality_checks\",\n",
    "                \"base_parameters\": {\n",
    "                    \"tables_to_check\": \"silver_orders,gold_daily_sales,gold_customer_ltv\",\n",
    "                    \"run_date\": \"{{job.start_time.iso_date}}\"\n",
    "                }\n",
    "            },\n",
    "            \"new_cluster\": {\n",
    "                \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                \"node_type_id\": \"Standard_DS3_v2\",\n",
    "                \"num_workers\": 1\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"schedule\": {\n",
    "        \"quartz_cron_expression\": \"0 0 2 * * ?\",  # Daily at 2 AM\n",
    "        \"timezone_id\": \"Europe/Warsaw\",\n",
    "        \"pause_status\": \"UNPAUSED\"\n",
    "    },\n",
    "    \"health\": {\n",
    "        \"rules\": [\n",
    "            {\n",
    "                \"metric\": \"RUN_DURATION_SECONDS\",\n",
    "                \"op\": \"GREATER_THAN\",\n",
    "                \"value\": 5400  # Alert if > 90 minutes\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Production Job configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9df810",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "### Nauczyłeś się:\n",
    "\n",
    "- **Multi-task Jobs**: DAG-based workflow orchestration \n",
    "- **Task Types**: Notebook, SQL, DLT, dbt tasks \n",
    "- **Dependencies**: Sequential, parallel, fan-out/fan-in patterns \n",
    "- **Parametryzacja**: Job parameters, task values, dynamic values \n",
    "- **Monitoring**: Email, webhooks, custom metrics \n",
    "- **Retry Logic**: Automatic retry dla transient errors \n",
    "- **Best Practices**: Idempotency, error handling, cost optimization \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Jobs = Orchestration**: Coordinate complex workflows\n",
    "2. **Flexibility**: Mix notebook, SQL, DLT, dbt tasks\n",
    "3. **Resilience**: Retry logic + alerting = robust pipelines\n",
    "4. **Cost-aware**: Spot instances + autoscaling + cluster reuse\n",
    "5. **Observability**: System tables + custom monitoring\n",
    "\n",
    "### Następne kroki:\n",
    "- **Notebook 04**: Unity Catalog Governance\n",
    "- **Workshop 02**: Hands-on Lakeflow + Jobs Orchestration\n",
    "\n",
    "---\n",
    "\n",
    "## Dodatkowe zasoby\n",
    "\n",
    "- [Databricks Jobs Documentation](https://docs.databricks.com/workflows/jobs/jobs.html)\n",
    "- [Jobs API Reference](https://docs.databricks.com/api/workspace/jobs)\n",
    "- [Orchestration Best Practices](https://docs.databricks.com/workflows/jobs/jobs-best-practices.html)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}