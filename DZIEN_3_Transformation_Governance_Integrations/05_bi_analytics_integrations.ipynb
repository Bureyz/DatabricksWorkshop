{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b7ed21a",
   "metadata": {},
   "source": [
    "# BI & Analytics Integrations\n",
    "\n",
    "**Cel szkoleniowy:** Integracja Databricks Lakehouse z narzedzniami BI i udostepnianie danych uzytkownikom biznesowym\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- SQL Warehouses: Serverless, Pro, Classic\n",
    "- Databricks Genie (AI/BI) - natural language queries\n",
    "- Integracje zewnetrzne: Power BI, Dremio\n",
    "- Przygotowanie danych dla warstwy BI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5e2ae2",
   "metadata": {},
   "source": [
    "## Setup i konfiguracja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1077636e",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 3 - Integracje i Governance\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    " - Uprawnienia do tworzenia SQL Warehouses (lub dostęp do istniejącego)\n",
    " - Unity Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9580c7ca",
   "metadata": {},
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie jak udostępniać dane z Lakehouse do świata zewnętrznego i użytkowników biznesowych.\n",
    "\n",
    "**Podstawowe pojęcia:**\n",
    "- **SQL Warehouse**: Zoptymalizowany silnik obliczeniowy dla zapytań SQL (nie dla kodu Python/Scala).\n",
    "- **Genie**: Inteligentny asystent danych, który rozumie strukturę tabel i odpowiada na pytania w języku naturalnym.\n",
    "- **Direct Lake**: Tryb połączenia Power BI, który czyta pliki Parquet bezpośrednio, pomijając warstwę SQL (najszybszy)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3424438",
   "metadata": {},
   "source": [
    "## Izolacja per użytkownik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8848c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b75e2",
   "metadata": {},
   "source": [
    "## Konfiguracja środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d0e5de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Ustawienie katalogu i schematu\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {GOLD_SCHEMA}\")\n",
    "\n",
    "display(spark.createDataFrame([\n",
    "    (\"Katalog\", CATALOG),\n",
    "    (\"Schemat\", GOLD_SCHEMA)\n",
    "], [\"Parametr\", \"Wartość\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3bbac0",
   "metadata": {},
   "source": [
    "## Sekcja 1: Databricks SQL Warehouses\n",
    "\n",
    "SQL Warehouses to \"serce\" warstwy BI w Databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cae7f2",
   "metadata": {},
   "source": [
    "### Typy Warehouse'ów\n",
    "\n",
    "1. **Serverless**: Startuje w sekundy, skaluje się automatycznie. Rekomendowany.\n",
    "2. **Pro**: Wykorzystuje silnik Photon, ale wymaga dłuższego startu (chyba że jest pula).\n",
    "3. **Classic**: Starsza architektura (VM-based)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbf7163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład definicji konfiguracji Warehouse (JSON)\n",
    "# Można to wykorzystać w API Databricks do automatyzacji tworzenia\n",
    "\n",
    "warehouse_config = {\n",
    "    \"name\": \"KION_BI_Warehouse_Demo\",\n",
    "    \"cluster_size\": \"2X-Small\",\n",
    "    \"min_num_clusters\": 1,\n",
    "    \"max_num_clusters\": 2,\n",
    "    \"auto_stop_mins\": 10,\n",
    "    \"enable_serverless_compute\": True,\n",
    "    \"warehouse_type\": \"PRO\",\n",
    "    \"tags\": {\n",
    "        \"department\": \"Sales\",\n",
    "        \"cost_center\": \"1234\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Wyświetlenie konfiguracji jako DataFrame\n",
    "config_df = spark.createDataFrame([\n",
    "    (\"name\", warehouse_config[\"name\"]),\n",
    "    (\"cluster_size\", warehouse_config[\"cluster_size\"]),\n",
    "    (\"min_num_clusters\", str(warehouse_config[\"min_num_clusters\"])),\n",
    "    (\"max_num_clusters\", str(warehouse_config[\"max_num_clusters\"])),\n",
    "    (\"auto_stop_mins\", str(warehouse_config[\"auto_stop_mins\"])),\n",
    "    (\"enable_serverless_compute\", str(warehouse_config[\"enable_serverless_compute\"])),\n",
    "    (\"warehouse_type\", warehouse_config[\"warehouse_type\"]),\n",
    "    (\"tags.department\", warehouse_config[\"tags\"][\"department\"]),\n",
    "    (\"tags.cost_center\", warehouse_config[\"tags\"][\"cost_center\"])\n",
    "], [\"Parametr\", \"Wartość\"])\n",
    "\n",
    "display(config_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1fcd91",
   "metadata": {},
   "source": [
    "## Sekcja 2: Databricks Genie (AI/BI)\n",
    "\n",
    "Genie pozwala na zadawanie pytań do danych bez znajomości SQL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7a8f1e",
   "metadata": {},
   "source": [
    "### Przygotowanie danych pod Genie\n",
    "\n",
    "Aby Genie działało dobrze, musimy zadbać o metadane (komentarze do tabel i kolumn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c10fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dodawanie komentarzy do tabeli Gold (kluczowe dla Genie!)\n",
    "spark.sql(f\"\"\"\n",
    "COMMENT ON TABLE {CATALOG}.{GOLD_SCHEMA}.fact_sales IS \n",
    "'Tabela faktów zawierająca transakcje sprzedaży. Zawiera kwoty, daty i klucze obce do wymiarów.'\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "COMMENT ON COLUMN {CATALOG}.{GOLD_SCHEMA}.fact_sales.total_amount IS \n",
    "'Całkowita wartość zamówienia w PLN (brutto).'\n",
    "\"\"\")\n",
    "\n",
    "display(spark.createDataFrame([\n",
    "    (\"Metadane\", \"Zaktualizowane\"),\n",
    "    (\"Cel\", \"Genie będzie teraz lepiej rozumieć te dane\")\n",
    "], [\"Status\", \"Wartość\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d89eb6",
   "metadata": {},
   "source": [
    "## Sekcja 3: Integracje Zewnętrzne (Power BI & Dremio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "010d7b3e",
   "metadata": {},
   "source": [
    "### Power BI - Direct Lake vs Direct Query\n",
    "\n",
    "- **Direct Lake**: Power BI Service -> OneLake/Storage (Parquet). Wymaga Fabric lub odpowiedniej konfiguracji.\n",
    "- **Direct Query**: Power BI -> SQL Warehouse -> Storage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb52ae4",
   "metadata": {},
   "source": [
    "### Dremio - Unity Catalog Iceberg Endpoint\n",
    "\n",
    "**Dremio** łączy się z Databricks poprzez **Unity Catalog Iceberg REST Catalog endpoint**. \n",
    "Wymaga to włączenia **UniForm (Iceberg reads)** na tabelach Delta.\n",
    "\n",
    "#### Jak to działa?\n",
    "\n",
    "```\n",
    "Dremio → Unity Catalog (Iceberg REST API) → Delta Table z UniForm → Parquet files\n",
    "```\n",
    "\n",
    "Delta Lake i Iceberg używają tych samych plików Parquet - UniForm generuje tylko dodatkowe metadane Iceberg, bez kopiowania danych.\n",
    "\n",
    "#### Wymagania:\n",
    "1. Tabela zarejestrowana w **Unity Catalog** (managed lub external)\n",
    "2. **Column mapping** włączony (`delta.columnMapping.mode = 'name'`)\n",
    "3. **Databricks Runtime 14.3 LTS+** do zapisu\n",
    "4. Tabela **bez deletion vectors** (lub użyj REORG do ich usunięcia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e4a9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Krok 1: Włączenie UniForm podczas tworzenia nowej tabeli\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {CATALOG}.{GOLD_SCHEMA}.fact_sales_iceberg\n",
    "TBLPROPERTIES (\n",
    "    'delta.columnMapping.mode' = 'name',\n",
    "    'delta.enableIcebergCompatV2' = 'true',\n",
    "    'delta.universalFormat.enabledFormats' = 'iceberg'\n",
    ")\n",
    "AS SELECT * FROM {CATALOG}.{GOLD_SCHEMA}.fact_sales\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be4764a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Krok 2: Włączenie UniForm na istniejącej tabeli (ALTER TABLE)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "ALTER TABLE {CATALOG}.{GOLD_SCHEMA}.dim_customer SET TBLPROPERTIES (\n",
    "    'delta.columnMapping.mode' = 'name',\n",
    "    'delta.enableIcebergCompatV2' = 'true',\n",
    "    'delta.universalFormat.enabledFormats' = 'iceberg'\n",
    ")\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2baeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Krok 3: Jeśli tabela ma Deletion Vectors - użyj REORG\n",
    "# REORG usuwa deletion vectors i włącza UniForm w jednym kroku\n",
    "\n",
    "# spark.sql(f\"\"\"\n",
    "# REORG TABLE {CATALOG}.{GOLD_SCHEMA}.fact_sales \n",
    "# APPLY (UPGRADE UNIFORM(ICEBERG_COMPAT_VERSION=2))\n",
    "# \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b3c383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Krok 4: Sprawdzenie statusu generowania metadanych Iceberg\n",
    "\n",
    "result = spark.sql(f\"DESCRIBE EXTENDED {CATALOG}.{GOLD_SCHEMA}.dim_customer\")\n",
    "display(result.filter(\"col_name LIKE '%iceberg%' OR col_name LIKE '%uniform%' OR col_name LIKE '%converted%'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afdaf26",
   "metadata": {},
   "source": [
    "#### Konfiguracja Dremio - Unity Catalog Iceberg REST\n",
    "\n",
    "Dremio łączy się przez **Iceberg REST Catalog API** - **NIE wymaga SQL Warehouse**.\n",
    "\n",
    "| Połączenie | SQL Warehouse? | Jak działa |\n",
    "|------------|----------------|------------|\n",
    "| Power BI / Tableau (JDBC) | TAK | Zapytania przez SQL Warehouse |\n",
    "| Dremio / Snowflake / Trino (Iceberg REST) | NIE | Bezpośredni dostęp do plików przez API |\n",
    "\n",
    "**Konfiguracja w Dremio:**\n",
    "\n",
    "1. **Dodaj źródło** → Iceberg / REST Catalog\n",
    "2. **Endpoint URI**: \n",
    "   ```\n",
    "   https://<workspace-url>/api/2.1/unity-catalog/iceberg-rest\n",
    "   ```\n",
    "3. **Warehouse** (= catalog name): `<uc-catalog-name>` np. `main`\n",
    "4. **Authentication**: Personal Access Token lub OAuth2 (Service Principal)\n",
    "\n",
    "Unity Catalog używa **credential vending** - przekazuje tymczasowe credentiale do storage (S3/ADLS), więc Dremio czyta pliki Parquet bezpośrednio.\n",
    "\n",
    "#### Ręczna synchronizacja metadanych\n",
    "\n",
    "Jeśli Dremio nie widzi najnowszych danych, wymuś synchronizację:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d9944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ręczna synchronizacja metadanych Iceberg (jeśli automatyczna nie zadziałała)\n",
    "# spark.sql(f\"MSCK REPAIR TABLE {CATALOG}.{GOLD_SCHEMA}.dim_customer SYNC METADATA\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d519313b",
   "metadata": {},
   "source": [
    "#### Ograniczenia UniForm dla Dremio\n",
    "\n",
    "| Ograniczenie | Opis |\n",
    "|--------------|------|\n",
    "| Read-only | Dremio może tylko czytać, nie pisać |\n",
    "| Deletion Vectors | Muszą być wyłączone (lub użyj REORG) |\n",
    "| Materialized Views | Nie wspierają UniForm |\n",
    "| Streaming Tables | Nie wspierają UniForm |\n",
    "| VOID type | Nie wspierany w Iceberg |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fecc694",
   "metadata": {},
   "source": [
    "### Przygotowanie dedykowanego widoku\n",
    "\n",
    "Dla narzędzi BI (Dremio, Power BI) dobrą praktyką jest tworzenie widoków, które ukrywają logikę złączeń."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d9bfa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tworzenie widoku raportowego\n",
    "view_name = \"v_sales_summary_bi\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW {CATALOG}.{GOLD_SCHEMA}.{view_name} AS\n",
    "SELECT \n",
    "    c.country,\n",
    "    year(f.order_date) as year,\n",
    "    month(f.order_date) as month,\n",
    "    count(distinct f.order_id) as orders_count,\n",
    "    sum(f.total_amount) as total_revenue\n",
    "FROM {CATALOG}.{GOLD_SCHEMA}.fact_sales f\n",
    "JOIN {CATALOG}.{GOLD_SCHEMA}.dim_customer c ON f.customer_id = c.customer_id\n",
    "GROUP BY 1, 2, 3\n",
    "\"\"\")\n",
    "\n",
    "display(spark.createDataFrame([\n",
    "    (\"Widok\", view_name),\n",
    "    (\"Lokalizacja\", f\"{CATALOG}.{GOLD_SCHEMA}.{view_name}\"),\n",
    "    (\"Status\", \"Gotowy do podłączenia w BI\")\n",
    "], [\"Parametr\", \"Wartość\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e5105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja widoku\n",
    "display(spark.table(f\"{CATALOG}.{GOLD_SCHEMA}.{view_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a1c197",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### Wydajność BI:\n",
    "- Używaj **Serverless SQL Warehouses** dla najlepszego UX (szybki start).\n",
    "- Włącz **Photon** (domyślne w Serverless/Pro).\n",
    "- Stosuj **Materialized Views** dla ciężkich agregacji, jeśli dashboardy działają wolno.\n",
    "\n",
    "### Governance:\n",
    "- Nie podłączaj BI bezpośrednio do tabel Silver/Bronze. Używaj tylko **Gold**.\n",
    "- Używaj dedykowanych **Service Principals** do połączeń z BI, a nie kont osobistych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1430c831",
   "metadata": {},
   "source": [
    "## Podsumowanie\n",
    "\n",
    "1. Skonfigurowaliśmy metadane dla **Genie**.\n",
    "2. Omówiliśmy typy **SQL Warehouses**.\n",
    "3. Przygotowaliśmy zoptymalizowany widok dla **Power BI / Dremio**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7482c46",
   "metadata": {},
   "source": [
    "## Czyszczenie zasobów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb11e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(f\"DROP VIEW IF EXISTS {CATALOG}.{GOLD_SCHEMA}.{view_name}\")\n",
    "display(spark.createDataFrame([(\"Status\", \"Zasoby zachowane do dalszych ćwiczeń\")], [\"Info\", \"Wartość\"]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
