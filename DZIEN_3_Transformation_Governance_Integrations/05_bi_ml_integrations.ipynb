{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "711d97bd",
   "metadata": {},
   "source": [
    "# BI & ML Integrations\n",
    "\n",
    "**KION Training - Dzie≈Ñ 3**\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Agenda\n",
    "\n",
    "1. Power BI Integration (Direct Lake vs Direct Query)\n",
    "2. Databricks SQL Warehouses\n",
    "3. MLflow Basics: Experiments, Tracking, Registry\n",
    "4. Feature Store Introduction\n",
    "5. Gold Layer ‚Üí ML Dataset Pipeline\n",
    "6. End-to-End Integration Architecture\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Cele szkolenia\n",
    "\n",
    "Po tym module bƒôdziesz potrafiƒá:\n",
    "- ≈ÅƒÖczyƒá Power BI z Databricks\n",
    "- U≈ºywaƒá SQL Warehouses do BI workloads\n",
    "- Trackowaƒá ML experiments z MLflow\n",
    "- Korzystaƒá z Feature Store\n",
    "- Przygotowywaƒá Gold layer dla ML i BI\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936c0036",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Power BI Integration\n",
    "\n",
    "Databricks oferuje dwa g≈Ç√≥wne tryby po≈ÇƒÖczenia z Power BI:\n",
    "\n",
    "### 1. Direct Lake (najnowsze, najbardziej wydajne)\n",
    "- **Native Delta Lake access**: Power BI czyta bezpo≈õrednio z Delta Lake\n",
    "- **Extreme performance**: brak po≈õrednik√≥w, cache w memory\n",
    "- **Automatic refresh**: changes w Delta ‚Üí automatic refresh\n",
    "- **Requirements**: Power BI Premium, Fabric integration\n",
    "\n",
    "### 2. Direct Query\n",
    "- **Live connection**: ka≈ºde query idzie do Databricks SQL Warehouse\n",
    "- **No data caching**: always fresh data\n",
    "- **Slower than import**: network latency per query\n",
    "- **Use case**: large datasets, real-time dashboards\n",
    "\n",
    "### 3. Import Mode (alternatywa)\n",
    "- **Data copied to Power BI**: snapshot of data\n",
    "- **Fast queries**: local cache\n",
    "- **Scheduled refresh needed**: manual/automatic refresh\n",
    "- **Size limits**: Power BI dataset size constraints\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f070fcb8",
   "metadata": {},
   "source": [
    "## üõ†Ô∏è Setup i konfiguracja\n",
    "\n",
    "### Izolacja per u≈ºytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalog√≥w i schemat√≥w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520cc976",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd311e3",
   "metadata": {},
   "source": [
    "### Import bibliotek i konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc9f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Wy≈õwietl kontekst u≈ºytkownika\n",
    "print(\"=== Kontekst u≈ºytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"U≈ºytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog jako domy≈õlny\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {SILVER_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68da1e0",
   "metadata": {},
   "source": [
    "## üîå Connection Setup\n",
    "\n",
    "### Step 1: Prepare data in Gold layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98515812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create fact table for Power BI\n",
    "fact_sales = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        o.order_date,\n",
    "        o.customer_id,\n",
    "        o.product_id,\n",
    "        o.quantity,\n",
    "        o.total_amount as amount,\n",
    "        o.payment_method as status,\n",
    "        YEAR(o.order_date) as year,\n",
    "        MONTH(o.order_date) as month,\n",
    "        QUARTER(o.order_date) as quarter,\n",
    "        DAYOFWEEK(o.order_date) as day_of_week\n",
    "    FROM {CATALOG}.{SILVER_SCHEMA}.orders_silver o\n",
    "    WHERE o.payment_method IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "fact_sales.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{GOLD_SCHEMA}.fact_sales\")\n",
    "\n",
    "print(\"‚úÖ Fact table created: fact_sales\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffa8341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dimension tables\n",
    "\n",
    "# Dim Customer\n",
    "dim_customer = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        email,\n",
    "        country,\n",
    "        registration_date\n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.customers_bronze\n",
    "\"\"\")\n",
    "\n",
    "dim_customer.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{GOLD_SCHEMA}.dim_customer\")\n",
    "\n",
    "# Dim Product  \n",
    "dim_product = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        product_id,\n",
    "        product_name,\n",
    "        category,\n",
    "        price\n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.products_bronze\n",
    "\"\"\")\n",
    "\n",
    "dim_product.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{GOLD_SCHEMA}.dim_product\")\n",
    "\n",
    "# Dim Date (date dimension)\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "start_date = datetime(2020, 1, 1)\n",
    "end_date = datetime(2025, 12, 31)\n",
    "date_list = [start_date + timedelta(days=x) for x in range((end_date - start_date).days + 1)]\n",
    "\n",
    "date_data = [(\n",
    "    d,\n",
    "    d.year,\n",
    "    d.month,\n",
    "    d.day,\n",
    "    (d.month - 1) // 3 + 1,  # quarter\n",
    "    d.strftime('%A'),  # day name\n",
    "    d.strftime('%B'),  # month name\n",
    "    d.isocalendar()[1]  # week number\n",
    ") for d in date_list]\n",
    "\n",
    "dim_date = spark.createDataFrame(\n",
    "    date_data,\n",
    "    [\"date\", \"year\", \"month\", \"day\", \"quarter\", \"day_name\", \"month_name\", \"week_number\"]\n",
    ")\n",
    "\n",
    "dim_date.write.format(\"delta\").mode(\"overwrite\").saveAsTable(f\"{CATALOG}.{GOLD_SCHEMA}.dim_date\")\n",
    "\n",
    "print(\"‚úÖ Dimension tables created: dim_customer, dim_product, dim_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ea11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize tables for BI performance\n",
    "spark.sql(f\"OPTIMIZE {CATALOG}.{GOLD_SCHEMA}.fact_sales ZORDER BY (order_date, customer_id)\")\n",
    "spark.sql(f\"OPTIMIZE {CATALOG}.{GOLD_SCHEMA}.dim_customer ZORDER BY (customer_id)\")\n",
    "spark.sql(f\"OPTIMIZE {CATALOG}.{GOLD_SCHEMA}.dim_product ZORDER BY (product_id)\")\n",
    "\n",
    "print(\"‚úÖ Tables optimized for query performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e55a129",
   "metadata": {},
   "source": [
    "### Step 2: Power BI Connection String\n",
    "\n",
    "**For Direct Query mode:**\n",
    "\n",
    "1. Open Power BI Desktop\n",
    "2. Get Data ‚Üí More ‚Üí Azure ‚Üí Azure Databricks\n",
    "3. Provide connection details:\n",
    "   - **Server hostname**: `your-workspace.cloud.databricks.com`\n",
    "   - **HTTP Path**: `/sql/1.0/warehouses/<warehouse-id>`\n",
    "   - **Authentication**: Azure AD / Personal Access Token\n",
    "\n",
    "4. Select tables from your Gold schema:\n",
    "   - Use catalog: `training_catalog`\n",
    "   - Use schema: `<your_user>_gold` (created by 00_setup.ipynb)\n",
    "   - Tables: `fact_sales`, `dim_customer`, `dim_product`, `dim_date`\n",
    "\n",
    "5. Choose: **Direct Query** (not Import)\n",
    "\n",
    "**Power BI will now query Databricks SQL Warehouse in real-time!**\n",
    "\n",
    "**Nota**: Twoje tabele znajdujƒÖ siƒô w schemacie `{CATALOG}.{GOLD_SCHEMA}`, gdzie `GOLD_SCHEMA` jest unikalny dla ka≈ºdego uczestnika (np. `trainer_gold`).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984d6311",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Databricks SQL Warehouses\n",
    "\n",
    "**SQL Warehouses** = Compute engine zoptymalizowany dla BI queries\n",
    "\n",
    "### Typy SQL Warehouses:\n",
    "\n",
    "| Type | Use Case | Performance | Cost |\n",
    "|------|----------|-------------|------|\n",
    "| **Serverless** | Ad-hoc queries, dashboards | Instant startup | Pay-per-query |\n",
    "| **Pro** | Production BI, high concurrency | Photon acceleration | Medium |\n",
    "| **Classic** | Development, testing | Standard Spark | Low |\n",
    "\n",
    "### Creating SQL Warehouse (via UI or API):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415c9dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL Warehouse configuration (example JSON for API)\n",
    "warehouse_config = {\n",
    "    \"name\": \"KION_BI_Warehouse\",\n",
    "    \"cluster_size\": \"Medium\",  # Small / Medium / Large / X-Large\n",
    "    \"min_num_clusters\": 1,\n",
    "    \"max_num_clusters\": 3,  # Auto-scaling\n",
    "    \"auto_stop_mins\": 10,  # Stop after 10 min idle\n",
    "    \"enable_photon\": True,  # Photon acceleration\n",
    "    \"enable_serverless_compute\": False,  # Pro warehouse\n",
    "    \"spot_instance_policy\": \"COST_OPTIMIZED\",\n",
    "    \"warehouse_type\": \"PRO\",\n",
    "    \"tags\": {\n",
    "        \"project\": \"kion_analytics\",\n",
    "        \"environment\": \"production\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"SQL Warehouse configuration for BI workloads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5c921a",
   "metadata": {},
   "source": [
    "### Querying through SQL Warehouse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93ca121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example BI query - aggregated sales by month\n",
    "bi_query = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        d.year,\n",
    "        d.month,\n",
    "        d.month_name,\n",
    "        COUNT(DISTINCT f.order_id) as total_orders,\n",
    "        COUNT(DISTINCT f.customer_id) as unique_customers,\n",
    "        SUM(f.amount) as total_revenue,\n",
    "        AVG(f.amount) as avg_order_value,\n",
    "        SUM(f.quantity) as total_quantity\n",
    "    FROM {CATALOG}.{GOLD_SCHEMA}.fact_sales f\n",
    "    JOIN {CATALOG}.{GOLD_SCHEMA}.dim_date d ON f.order_date = d.date\n",
    "    GROUP BY d.year, d.month, d.month_name\n",
    "    ORDER BY d.year DESC, d.month DESC\n",
    "\"\"\")\n",
    "\n",
    "bi_query.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c08003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example BI query - customer segmentation\n",
    "customer_analysis = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        c.country,\n",
    "        COUNT(DISTINCT f.customer_id) as customer_count,\n",
    "        SUM(f.amount) as total_revenue,\n",
    "        AVG(f.amount) as avg_transaction_value,\n",
    "        COUNT(f.order_id) / COUNT(DISTINCT f.customer_id) as avg_orders_per_customer\n",
    "    FROM {CATALOG}.{GOLD_SCHEMA}.fact_sales f\n",
    "    JOIN {CATALOG}.{GOLD_SCHEMA}.dim_customer c ON f.customer_id = c.customer_id\n",
    "    GROUP BY c.country\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "customer_analysis.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46960760",
   "metadata": {},
   "source": [
    "### Query result caching:\n",
    "\n",
    "SQL Warehouses automatically cache query results:\n",
    "- Identical queries return cached results (seconds)\n",
    "- Cache invalidated on data changes\n",
    "- Reduces cost and latency\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18aad82",
   "metadata": {},
   "source": [
    "## 3Ô∏è‚É£ MLflow Basics: Experiments, Tracking, Registry\n",
    "\n",
    "**MLflow** = Open-source platform dla ML lifecycle management\n",
    "\n",
    "### Komponenty MLflow:\n",
    "1. **Tracking**: Log parameters, metrics, artifacts\n",
    "2. **Models**: Package models for deployment\n",
    "3. **Registry**: Centralized model repository\n",
    "4. **Projects**: Reproducible runs\n",
    "\n",
    "### Basic MLflow workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d46027",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_experiment(\"/Users/your.email@company.com/KION_Customer_LTV_Prediction\")\n",
    "\n",
    "print(\"‚úÖ MLflow experiment set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83345f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training data from Gold layer\n",
    "training_data = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.country,\n",
    "        DATEDIFF(CURRENT_DATE(), c.registration_date) as days_since_registration,\n",
    "        COUNT(o.order_id) as total_orders,\n",
    "        SUM(o.total_amount) as lifetime_value,\n",
    "        AVG(o.total_amount) as avg_order_value,\n",
    "        MAX(o.order_date) as last_order_date,\n",
    "        DATEDIFF(CURRENT_DATE(), MAX(o.order_date)) as days_since_last_order\n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.customers_bronze c\n",
    "    LEFT JOIN {CATALOG}.{SILVER_SCHEMA}.orders_silver o ON c.customer_id = o.customer_id\n",
    "    GROUP BY c.customer_id, c.country, c.registration_date\n",
    "    HAVING total_orders >= 2  -- Only customers with at least 2 orders\n",
    "\"\"\")\n",
    "\n",
    "df = training_data.toPandas()\n",
    "print(f\"‚úÖ Training data loaded: {len(df)} customers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2380dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "df_encoded = pd.get_dummies(df, columns=['country'])\n",
    "\n",
    "# Split features and target\n",
    "X = df_encoded.drop(['customer_id', 'lifetime_value', 'last_order_date'], axis=1)\n",
    "y = df_encoded['lifetime_value']\n",
    "\n",
    "# Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {len(X_train)}, Test set: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb92ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with MLflow tracking\n",
    "with mlflow.start_run(run_name=\"RandomForest_v1\") as run:\n",
    "    \n",
    "    # Log parameters\n",
    "    n_estimators = 100\n",
    "    max_depth = 10\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    mlflow.log_param(\"model_type\", \"RandomForestRegressor\")\n",
    "    mlflow.log_param(\"train_size\", len(X_train))\n",
    "    mlflow.log_param(\"test_size\", len(X_test))\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=42\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = mse ** 0.5\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    mlflow.log_metric(\"rmse\", rmse)\n",
    "    mlflow.log_metric(\"r2_score\", r2)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(\n",
    "        model,\n",
    "        \"model\",\n",
    "        registered_model_name=\"kion_customer_ltv_model\"\n",
    "    )\n",
    "    \n",
    "    # Log feature importance\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': model.feature_importances_\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Save as artifact\n",
    "    feature_importance.to_csv('/tmp/feature_importance.csv', index=False)\n",
    "    mlflow.log_artifact('/tmp/feature_importance.csv')\n",
    "    \n",
    "    print(f\"‚úÖ Model trained and logged to MLflow\")\n",
    "    print(f\"   RMSE: {rmse:.2f}\")\n",
    "    print(f\"   R¬≤ Score: {r2:.4f}\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd52f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View experiment runs\n",
    "experiment = mlflow.get_experiment_by_name(\"/Users/your.email@company.com/KION_Customer_LTV_Prediction\")\n",
    "runs_df = mlflow.search_runs(experiment_ids=[experiment.experiment_id])\n",
    "\n",
    "runs_df[['run_id', 'params.n_estimators', 'metrics.rmse', 'metrics.r2_score']].display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec830a61",
   "metadata": {},
   "source": [
    "### Loading and using registered model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afaaef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model from registry\n",
    "model_name = \"kion_customer_ltv_model\"\n",
    "model_version = 1\n",
    "\n",
    "loaded_model = mlflow.sklearn.load_model(f\"models:/{model_name}/{model_version}\")\n",
    "\n",
    "# Make predictions\n",
    "predictions = loaded_model.predict(X_test)\n",
    "\n",
    "print(f\"‚úÖ Model loaded and predictions made\")\n",
    "print(f\"Sample predictions: {predictions[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7037796e",
   "metadata": {},
   "source": [
    "### Model Registry workflow:\n",
    "\n",
    "```\n",
    "Development ‚Üí Staging ‚Üí Production ‚Üí Archived\n",
    "```\n",
    "\n",
    "- **Development**: Experimental models\n",
    "- **Staging**: Models being tested/validated\n",
    "- **Production**: Active models serving predictions\n",
    "- **Archived**: Deprecated models\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1178dfc",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Feature Store Introduction\n",
    "\n",
    "**Feature Store** = Centralized repository dla ML features\n",
    "\n",
    "### Benefits:\n",
    "- **Reusability**: Share features across teams\n",
    "- **Consistency**: Same features in training & serving\n",
    "- **Discovery**: Browse available features\n",
    "- **Versioning**: Track feature evolution\n",
    "- **Online/Offline**: Batch training + real-time serving\n",
    "\n",
    "### Creating feature table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b238eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureStoreClient\n",
    "\n",
    "fs = FeatureStoreClient()\n",
    "\n",
    "# Create feature table\n",
    "feature_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        COUNT(order_id) as total_orders,\n",
    "        SUM(total_amount) as lifetime_value,\n",
    "        AVG(total_amount) as avg_order_value,\n",
    "        MAX(order_date) as last_order_date,\n",
    "        DATEDIFF(CURRENT_DATE(), MAX(order_date)) as days_since_last_order,\n",
    "        COUNT(DISTINCT YEAR(order_date)) as active_years,\n",
    "        CURRENT_TIMESTAMP() as feature_timestamp\n",
    "    FROM {CATALOG}.{SILVER_SCHEMA}.orders_silver\n",
    "    GROUP BY customer_id\n",
    "\"\"\")\n",
    "\n",
    "# Write to Feature Store\n",
    "fs.create_table(\n",
    "    name=f\"{CATALOG}.{GOLD_SCHEMA}.customer_features\",\n",
    "    primary_keys=[\"customer_id\"],\n",
    "    df=feature_df,\n",
    "    description=\"Customer behavioral features for LTV prediction\"\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Feature table created in Feature Store\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f143ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update features (incremental)\n",
    "# This would run daily to refresh features\n",
    "updated_features = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        COUNT(order_id) as total_orders,\n",
    "        SUM(total_amount) as lifetime_value,\n",
    "        AVG(total_amount) as avg_order_value,\n",
    "        MAX(order_date) as last_order_date,\n",
    "        DATEDIFF(CURRENT_DATE(), MAX(order_date)) as days_since_last_order,\n",
    "        COUNT(DISTINCT YEAR(order_date)) as active_years,\n",
    "        CURRENT_TIMESTAMP() as feature_timestamp\n",
    "    FROM {CATALOG}.{SILVER_SCHEMA}.orders_silver\n",
    "    WHERE order_date >= CURRENT_DATE() - INTERVAL 1 DAYS\n",
    "    GROUP BY customer_id\n",
    "\"\"\")\n",
    "\n",
    "fs.write_table(\n",
    "    name=f\"{CATALOG}.{GOLD_SCHEMA}.customer_features\",\n",
    "    df=updated_features,\n",
    "    mode=\"merge\"  # Merge updates\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Features updated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e7e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read features for training\n",
    "feature_df = fs.read_table(name=f\"{CATALOG}.{GOLD_SCHEMA}.customer_features\")\n",
    "feature_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa65219a",
   "metadata": {},
   "source": [
    "### Training with Feature Store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0f3e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from databricks.feature_store import FeatureLookup\n",
    "\n",
    "# Define feature lookups\n",
    "feature_lookups = [\n",
    "    FeatureLookup(\n",
    "        table_name=f\"{CATALOG}.{GOLD_SCHEMA}.customer_features\",\n",
    "        feature_names=[\n",
    "            \"total_orders\",\n",
    "            \"lifetime_value\",\n",
    "            \"avg_order_value\",\n",
    "            \"days_since_last_order\",\n",
    "            \"active_years\"\n",
    "        ],\n",
    "        lookup_key=\"customer_id\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Training set (just customer_ids + labels)\n",
    "# Note: You would need to create a churn labels table first\n",
    "# For demonstration purposes, we'll create a simple example\n",
    "training_set_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        CASE \n",
    "            WHEN DATEDIFF(CURRENT_DATE(), MAX(order_date)) > 90 THEN 1\n",
    "            ELSE 0\n",
    "        END as will_churn\n",
    "    FROM {CATALOG}.{SILVER_SCHEMA}.orders_silver\n",
    "    GROUP BY customer_id\n",
    "\"\"\")\n",
    "\n",
    "# Create training set with features\n",
    "training_set = fs.create_training_set(\n",
    "    df=training_set_df,\n",
    "    feature_lookups=feature_lookups,\n",
    "    label=\"will_churn\"\n",
    ")\n",
    "\n",
    "training_df = training_set.load_df()\n",
    "training_df.display()\n",
    "\n",
    "# Features are automatically joined from Feature Store!\n",
    "print(\"‚úÖ Training set created with Feature Store lookup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed00762e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Gold Layer ‚Üí ML Dataset Pipeline\n",
    "\n",
    "### Best practices dla ML-ready data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5010505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ML-optimized table\n",
    "ml_dataset = spark.sql(f\"\"\"\n",
    "    WITH customer_metrics AS (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            COUNT(order_id) as total_orders,\n",
    "            SUM(total_amount) as lifetime_value,\n",
    "            AVG(total_amount) as avg_order_value,\n",
    "            STDDEV(total_amount) as stddev_order_value,\n",
    "            MIN(order_date) as first_order_date,\n",
    "            MAX(order_date) as last_order_date,\n",
    "            DATEDIFF(MAX(order_date), MIN(order_date)) as customer_lifespan_days,\n",
    "            COUNT(DISTINCT DATE_TRUNC('month', order_date)) as active_months\n",
    "        FROM {CATALOG}.{SILVER_SCHEMA}.orders_silver\n",
    "        WHERE payment_method IS NOT NULL\n",
    "        GROUP BY customer_id\n",
    "    ),\n",
    "    customer_demographics AS (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            country,\n",
    "            DATEDIFF(CURRENT_DATE(), registration_date) as days_since_registration\n",
    "        FROM {CATALOG}.{BRONZE_SCHEMA}.customers_bronze\n",
    "    )\n",
    "    SELECT \n",
    "        cm.*,\n",
    "        cd.country,\n",
    "        cd.days_since_registration,\n",
    "        -- Derived features\n",
    "        cm.total_orders / NULLIF(cm.active_months, 0) as orders_per_month,\n",
    "        cm.lifetime_value / NULLIF(cm.total_orders, 0) as avg_order_value_calc,\n",
    "        DATEDIFF(CURRENT_DATE(), cm.last_order_date) as recency_days,\n",
    "        -- Target: Will customer order in next 90 days?\n",
    "        CASE \n",
    "            WHEN DATEDIFF(CURRENT_DATE(), cm.last_order_date) <= 90 THEN 1\n",
    "            ELSE 0\n",
    "        END as is_active_customer\n",
    "    FROM customer_metrics cm\n",
    "    JOIN customer_demographics cd ON cm.customer_id = cd.customer_id\n",
    "    WHERE cm.total_orders >= 2  -- Filter out one-time buyers\n",
    "\"\"\")\n",
    "\n",
    "ml_dataset.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{GOLD_SCHEMA}.customer_activity_prediction_dataset\")\n",
    "\n",
    "print(\"‚úÖ ML dataset created: customer_activity_prediction_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc34611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add metadata for ML team\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {CATALOG}.{GOLD_SCHEMA}.customer_activity_prediction_dataset\n",
    "    SET TBLPROPERTIES (\n",
    "        'ml_use_case' = 'customer_churn_prediction',\n",
    "        'target_variable' = 'is_active_customer',\n",
    "        'feature_count' = '15',\n",
    "        'last_updated' = current_timestamp(),\n",
    "        'refresh_frequency' = 'daily',\n",
    "        'owner' = 'ml-team@kion.com'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    COMMENT ON TABLE {CATALOG}.{GOLD_SCHEMA}.customer_activity_prediction_dataset IS\n",
    "    'ML-ready dataset for predicting customer activity in next 90 days.\n",
    "     Target: is_active_customer (1 = active, 0 = inactive).\n",
    "     Updated daily at 3 AM.'\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Metadata added to ML dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba01f59",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ End-to-End Integration Architecture\n",
    "\n",
    "### Complete data flow:\n",
    "\n",
    "```\n",
    "Source Systems (CSV, JSON, APIs)\n",
    "    ‚Üì\n",
    "Bronze Layer (Raw data + Audit)\n",
    "    ‚Üì\n",
    "Silver Layer (Cleaned + Validated)\n",
    "    ‚Üì\n",
    "    ‚îú‚Üí Gold Layer (BI) ‚Üí SQL Warehouse ‚Üí Power BI Dashboards\n",
    "    ‚îú‚Üí Gold Layer (ML) ‚Üí Feature Store ‚Üí MLflow Models\n",
    "    ‚îî‚Üí Delta Sharing ‚Üí External Partners\n",
    "```\n",
    "\n",
    "### Integration checklist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a675e027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integration health check\n",
    "integration_check = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        'BI Layer' as integration_type,\n",
    "        COUNT(*) as table_count,\n",
    "        SUM(size_in_bytes) / 1024 / 1024 / 1024 as size_gb\n",
    "    FROM system.information_schema.tables\n",
    "    WHERE table_catalog = '{CATALOG}'\n",
    "        AND table_schema = '{GOLD_SCHEMA}'\n",
    "        AND (table_name LIKE 'fact_%' OR table_name LIKE 'dim_%')\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'ML Layer' as integration_type,\n",
    "        COUNT(*) as table_count,\n",
    "        SUM(size_in_bytes) / 1024 / 1024 / 1024 as size_gb\n",
    "    FROM system.information_schema.tables\n",
    "    WHERE table_catalog = '{CATALOG}'\n",
    "        AND table_schema = '{GOLD_SCHEMA}'\n",
    "        AND table_name LIKE '%prediction%'\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'Feature Store' as integration_type,\n",
    "        COUNT(*) as table_count,\n",
    "        SUM(size_in_bytes) / 1024 / 1024 / 1024 as size_gb\n",
    "    FROM system.information_schema.tables\n",
    "    WHERE table_catalog = '{CATALOG}'\n",
    "        AND table_schema = '{GOLD_SCHEMA}'\n",
    "        AND table_name LIKE '%features%'\n",
    "\"\"\")\n",
    "\n",
    "integration_check.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fbb2a8",
   "metadata": {},
   "source": [
    "### Monitoring integration health:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509279d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query usage statistics\n",
    "usage_stats = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        DATE(event_time) as date,\n",
    "        request_params.full_name_arg as table_name,\n",
    "        COUNT(*) as query_count,\n",
    "        COUNT(DISTINCT user_identity.email) as unique_users\n",
    "    FROM system.access.audit\n",
    "    WHERE action_name = 'getTable'\n",
    "        AND request_params.full_name_arg LIKE '{CATALOG}.{GOLD_SCHEMA}%'\n",
    "        AND event_date >= CURRENT_DATE() - INTERVAL 7 DAYS\n",
    "    GROUP BY date, table_name\n",
    "    ORDER BY date DESC, query_count DESC\n",
    "\"\"\")\n",
    "\n",
    "usage_stats.display()\n",
    "\n",
    "print(\"üìä Gold layer usage by BI and ML teams\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9bd54d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Podsumowanie\n",
    "\n",
    "### Nauczy≈Çe≈õ siƒô:\n",
    "\n",
    "‚úÖ **Power BI Integration**: Direct Lake vs Direct Query  \n",
    "‚úÖ **SQL Warehouses**: Optimized compute dla BI workloads  \n",
    "‚úÖ **MLflow Tracking**: Log experiments, parameters, metrics  \n",
    "‚úÖ **Model Registry**: Centralized model management  \n",
    "‚úÖ **Feature Store**: Reusable ML features  \n",
    "‚úÖ **Gold ‚Üí ML Pipeline**: Preparing data for ML use cases  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Unified Platform**: BI, ML, Analytics on same Lakehouse\n",
    "2. **SQL Warehouses**: Separate compute dla BI = better performance\n",
    "3. **MLflow**: Track everything - experiments, models, artifacts\n",
    "4. **Feature Store**: Share features across teams, avoid duplication\n",
    "5. **Gold Layer**: Serve both BI dashboards AND ML models\n",
    "\n",
    "### Gratulacje! üéâ\n",
    "\n",
    "Uko≈Ñczy≈Çe≈õ **Dzie≈Ñ 3** szkolenia KION!\n",
    "\n",
    "Pozna≈Çe≈õ:\n",
    "- Advanced transformations (Day 3.1)\n",
    "- Delta Live Tables (Day 3.2)\n",
    "- Databricks Jobs (Day 3.3)\n",
    "- Unity Catalog (Day 3.4)\n",
    "- BI & ML Integrations (Day 3.5)\n",
    "\n",
    "**Nastƒôpny krok**: Workshop 03 - praktyczne ƒáwiczenia!\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Dodatkowe zasoby\n",
    "\n",
    "- [Power BI Integration Guide](https://docs.databricks.com/partners/bi/power-bi.html)\n",
    "- [SQL Warehouses Documentation](https://docs.databricks.com/sql/admin/sql-endpoints.html)\n",
    "- [MLflow Documentation](https://mlflow.org/docs/latest/index.html)\n",
    "- [Feature Store Guide](https://docs.databricks.com/machine-learning/feature-store/index.html)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
