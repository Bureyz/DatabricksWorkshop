{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7ea0a9e",
   "metadata": {},
   "source": [
    "## Setup i konfiguracja"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219481af",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 3 - Integracje i Governance\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Klaster ML (Databricks Runtime ML 13.0+)\n",
    "  - Dostęp do Unity Catalog (dla Feature Store i Vector Search)\n",
    "  - (Opcjonalnie) Skonfigurowany Vector Search Endpoint\n",
    "  - (Opcjonalnie) Dostęp do Model Serving (Foundation Models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94496786",
   "metadata": {},
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie jak Databricks wspiera pełny cykl życia AI/ML.\n",
    "\n",
    "**Podstawowe pojęcia:**\n",
    "- **MLflow**: Platforma open-source do zarządzania cyklem życia ML (eksperymenty, modele, wdrożenia).\n",
    "- **Feature Store**: Scentralizowane repozytorium cech, zapewniające spójność między treningiem a inferencją.\n",
    "- **Vector Search**: Baza danych wektorowych zintegrowana z Delta Lake, kluczowa dla RAG (Retrieval Augmented Generation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6158441",
   "metadata": {},
   "source": [
    "## Izolacja per użytkownik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d3c93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf91002a",
   "metadata": {},
   "source": [
    "## Konfiguracja środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6334b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import pandas as pd\n",
    "from pyspark.sql import functions as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from databricks import feature_store\n",
    "from databricks.feature_store import FeatureStoreClient\n",
    "\n",
    "# Ustawienie katalogu i schematu\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {GOLD_SCHEMA}\")\n",
    "\n",
    "print(f\"Working in: {CATALOG}.{GOLD_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07c01e4",
   "metadata": {},
   "source": [
    "## Sekcja 1: MLflow - Tracking & Registry\n",
    "\n",
    "Zbudujemy prosty model przewidujący wartość zamówienia i zarejestrujemy go w MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11894c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Konfiguracja eksperymentu\n",
    "username = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
    "experiment_path = f\"/Users/{username}/KION_ML_Experiment\"\n",
    "\n",
    "mlflow.set_experiment(experiment_path)\n",
    "print(f\"Eksperyment: {experiment_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8802c882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Przygotowanie danych\n",
    "df_sales = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        total_amount as label,\n",
    "        quantity,\n",
    "        month(order_date) as month,\n",
    "        year(order_date) as year\n",
    "    FROM {CATALOG}.{GOLD_SCHEMA}.fact_sales\n",
    "    WHERE total_amount IS NOT NULL\n",
    "\"\"\").toPandas()\n",
    "\n",
    "X = df_sales.drop(\"label\", axis=1)\n",
    "y = df_sales[\"label\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Train shape: {X_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be90ac0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Trening i logowanie\n",
    "with mlflow.start_run(run_name=\"RandomForest_v1\") as run:\n",
    "    # Parametry\n",
    "    n_estimators = 50\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    \n",
    "    # Model\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Metryki\n",
    "    mse = mean_squared_error(y_test, model.predict(X_test))\n",
    "    mlflow.log_metric(\"mse\", mse)\n",
    "    \n",
    "    # Logowanie modelu\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    print(f\"Run ID: {run.info.run_id}, MSE: {mse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0570ceed",
   "metadata": {},
   "source": [
    "## Sekcja 2: Feature Store\n",
    "\n",
    "Feature Store pozwala na definiowanie cech raz i używanie ich wszędzie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848c1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definicja tabeli cech (Feature Table)\n",
    "fs = FeatureStoreClient()\n",
    "feature_table_name = f\"{CATALOG}.{GOLD_SCHEMA}.customer_features_{username.split('@')[0].replace('.', '_')}\"\n",
    "\n",
    "# Logika obliczania cech\n",
    "features_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        count(*) as order_count,\n",
    "        sum(total_amount) as total_spend,\n",
    "        avg(total_amount) as avg_order_value\n",
    "    FROM {CATALOG}.{GOLD_SCHEMA}.fact_sales\n",
    "    GROUP BY customer_id\n",
    "\"\"\")\n",
    "\n",
    "# Zapis do Feature Store\n",
    "# Uwaga: create_table wymaga unikalnej nazwy i Primary Key\n",
    "try:\n",
    "    fs.create_table(\n",
    "        name=feature_table_name,\n",
    "        primary_keys=[\"customer_id\"],\n",
    "        df=features_df,\n",
    "        description=\"Podstawowe cechy klienta: liczba zamówień, wydatki.\"\n",
    "    )\n",
    "    print(f\"Tabela cech utworzona: {feature_table_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Tabela prawdopodobnie istnieje lub błąd uprawnień: {e}\")\n",
    "    # Jeśli istnieje, możemy ją zaktualizować:\n",
    "    # fs.write_table(name=feature_table_name, df=features_df, mode=\"merge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c92fc4a",
   "metadata": {},
   "source": [
    "## Sekcja 3: Vector Search (RAG Foundation)\n",
    "\n",
    "Vector Search to klucz do budowania systemów RAG. Pozwala wyszukiwać dokumenty podobne semantycznie do zapytania użytkownika.\n",
    "\n",
    "**Wymagania:**\n",
    "1.  **Vector Search Endpoint** (musi być utworzony w Compute -> Vector Search).\n",
    "2.  **Tabela źródłowa** z włączonym Change Data Feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a77666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Przygotowanie danych tekstowych (Symulacja bazy wiedzy)\n",
    "# Tworzymy tabelę z opisami produktów (w realnym scenariuszu to byłyby PDFy, dokumentacja itp.)\n",
    "\n",
    "docs_df = spark.createDataFrame([\n",
    "    (1, \"Wózek widłowy elektryczny, udźwig 2 tony, do pracy wewnątrz magazynu.\"),\n",
    "    (2, \"Wózek spalinowy Diesel, udźwig 5 ton, do pracy na zewnątrz, trudny teren.\"),\n",
    "    (3, \"Ręczny wózek paletowy, udźwig 500kg, do lekkich prac.\"),\n",
    "    (4, \"System automatyzacji magazynu, regały wysokiego składowania.\")\n",
    "], [\"id\", \"text\"])\n",
    "\n",
    "source_table = f\"{CATALOG}.{GOLD_SCHEMA}.product_docs_rag\"\n",
    "docs_df.write.format(\"delta\").mode(\"overwrite\").option(\"delta.enableChangeDataFeed\", \"true\").saveAsTable(source_table)\n",
    "print(f\"Tabela źródłowa utworzona: {source_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c45d9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Konfiguracja Vector Search (Kod właściwy)\n",
    "# Uwaga: To wymaga aktywnego Endpointu Vector Search. \n",
    "# Jeśli go nie ma, kod rzuci błąd, ale pokazuje poprawną ścieżkę implementacji.\n",
    "\n",
    "from databricks.vector_search.client import VectorSearchClient\n",
    "\n",
    "# Nazwa endpointu (musi istnieć w UI)\n",
    "VECTOR_SEARCH_ENDPOINT_NAME = \"kion_vector_endpoint_demo\" \n",
    "index_name = f\"{CATALOG}.{GOLD_SCHEMA}.product_docs_index\"\n",
    "\n",
    "try:\n",
    "    vsc = VectorSearchClient()\n",
    "    \n",
    "    # Sprawdź czy endpoint istnieje\n",
    "    endpoints = vsc.list_endpoints()\n",
    "    endpoint_exists = any(e['name'] == VECTOR_SEARCH_ENDPOINT_NAME for e in endpoints.get('endpoints', []))\n",
    "    \n",
    "    if endpoint_exists:\n",
    "        print(f\"Endpoint {VECTOR_SEARCH_ENDPOINT_NAME} znaleziony. Tworzenie indeksu...\")\n",
    "        \n",
    "        # Tworzenie indeksu (Delta Sync Index)\n",
    "        # To automatycznie zarządza embeddingami (używając modelu Databricks BGE lub OpenAI)\n",
    "        vsc.create_delta_sync_index(\n",
    "            endpoint_name=VECTOR_SEARCH_ENDPOINT_NAME,\n",
    "            source_table_name=source_table,\n",
    "            index_name=index_name,\n",
    "            pipeline_type=\"TRIGGERED\",\n",
    "            primary_key=\"id\",\n",
    "            embedding_source_column=\"text\",\n",
    "            embedding_model_endpoint_name=\"databricks-bge-large-en\" # Model wbudowany\n",
    "        )\n",
    "        print(\"Indeks jest tworzony w tle...\")\n",
    "    else:\n",
    "        print(f\"WARN: Endpoint '{VECTOR_SEARCH_ENDPOINT_NAME}' nie istnieje. Utwórz go w UI (Compute -> Vector Search).\")\n",
    "        print(\"Poniżej symulacja działania (dla celów demo).\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Błąd Vector Search (czy biblioteka jest zainstalowana?): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6028373f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Symulacja wyszukiwania (gdyby Vector Search działał)\n",
    "# query = \"Szukam wózka do ciężkich prac na dworze\"\n",
    "# results = vsc.get_index(endpoint_name, index_name).similarity_search(\n",
    "#     query_text=query,\n",
    "#     columns=[\"id\", \"text\"],\n",
    "#     num_results=1\n",
    "# )\n",
    "# print(results)\n",
    "\n",
    "# --- SYMULACJA LOKALNA (Dla celów edukacyjnych, gdy brak infrastruktury) ---\n",
    "print(\"\\n--- SYMULACJA WYNIKÓW ---\")\n",
    "print(\"Query: 'Szukam wózka do ciężkich prac na dworze'\")\n",
    "print(\"Najbardziej podobny dokument:\")\n",
    "print(\"ID: 2\")\n",
    "print(\"Text: Wózek spalinowy Diesel, udźwig 5 ton, do pracy na zewnątrz, trudny teren.\")\n",
    "print(\"Score: 0.89\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb4421",
   "metadata": {},
   "source": [
    "## Sekcja 4: GenAI & Model Serving\n",
    "\n",
    "Integracja z modelami LLM (Large Language Models) poprzez Databricks Model Serving."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c6fab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wywołanie modelu LLM (np. DBRX, Llama 3)\n",
    "import mlflow.deployments\n",
    "\n",
    "# Lista dostępnych endpointów (może wymagać uprawnień)\n",
    "# client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "# endpoints = client.list_endpoints()\n",
    "\n",
    "def query_llm(prompt):\n",
    "    try:\n",
    "        client = mlflow.deployments.get_deploy_client(\"databricks\")\n",
    "        \n",
    "        # Używamy modelu 'databricks-dbrx-instruct' lub 'databricks-meta-llama-3-70b-instruct'\n",
    "        # Te modele są często dostępne jako \"Pay-per-token\"\n",
    "        response = client.predict(\n",
    "            endpoint=\"databricks-dbrx-instruct\",\n",
    "            inputs={\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": \"Jesteś ekspertem od wózków widłowych KION.\"},\n",
    "                    {\"role\": \"user\", \"content\": prompt}\n",
    "                ],\n",
    "                \"max_tokens\": 100\n",
    "            }\n",
    "        )\n",
    "        return response['choices'][0]['message']['content']\n",
    "    except Exception as e:\n",
    "        return f\"Nie udało się połączyć z modelem: {e}\"\n",
    "\n",
    "# Test\n",
    "prompt = \"Jakie są zalety wózków elektrycznych?\"\n",
    "print(f\"Pytanie: {prompt}\")\n",
    "print(f\"Odpowiedź LLM: \\n{query_llm(prompt)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deddf32",
   "metadata": {},
   "source": [
    "## Podsumowanie\n",
    "\n",
    "1.  Zbudowaliśmy pipeline treningowy w **MLflow**.\n",
    "2.  Utworzyliśmy tabelę cech w **Feature Store**.\n",
    "3.  Pokazaliśmy jak skonfigurować **Vector Search** dla RAG.\n",
    "4.  Skorzystaliśmy z **GenAI** do generowania odpowiedzi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1331b8aa",
   "metadata": {},
   "source": [
    "## Czyszczenie zasobów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1952d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.sql(f\"DROP TABLE IF EXISTS {feature_table_name}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {source_table}\")\n",
    "print(\"Zasoby zachowane.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
