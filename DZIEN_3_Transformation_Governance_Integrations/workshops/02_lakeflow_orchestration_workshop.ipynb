{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1349b08",
   "metadata": {},
   "source": [
    "# Warsztat 2: Lakeflow Pipelines & Databricks Jobs Orchestration\n",
    "\n",
    "**Cel warsztatu:**\n",
    "- Poznanie Delta Live Tables (DLT) - deklaratywnych pipeline'Ã³w\n",
    "- Implementacja Data Quality z Expectations\n",
    "- Konfiguracja Multi-task Databricks Jobs\n",
    "- Parametryzacja i dependency management\n",
    "\n",
    "**Czas:** 90 minut\n",
    "\n",
    "**Uwaga:** Ten warsztat wymaga UI Databricks dla Delta Live Tables i Jobs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2460f360",
   "metadata": {},
   "source": [
    "## ðŸ“š Inicjalizacja Å›rodowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762a001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6da1f5",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ CzÄ™Å›Ä‡ 1: Delta Live Tables - Podstawy\n",
    "\n",
    "### Co to jest Delta Live Tables?\n",
    "\n",
    "**Delta Live Tables (DLT)** to deklaratywny framework do budowania reliable data pipelines.\n",
    "\n",
    "**Kluczowe koncepcje:**\n",
    "- **Deklaratywne** - opisujesz CO, nie JAK\n",
    "- **Streaming & Batch** - jedna skÅ‚adnia dla obu\n",
    "- **Data Quality** - wbudowane expectations\n",
    "- **Auto-scaling** - automatyczne zarzÄ…dzanie zasobami\n",
    "- **Lineage** - automatyczne Å›ledzenie zaleÅ¼noÅ›ci\n",
    "\n",
    "### Zadanie 1.1: Przygotowanie DLT Pipeline Definition\n",
    "\n",
    "**Instrukcje:**\n",
    "1. UtwÃ³rz nowy notebook dla DLT pipeline\n",
    "2. Zdefiniuj Bronze, Silver, Gold tables uÅ¼ywajÄ…c dekoratora `@dlt.table`\n",
    "3. Dodaj expectations dla data quality\n",
    "\n",
    "**PrzykÅ‚adowy kod DLT:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8bca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 1.1 - DLT Pipeline Definition\n",
    "# Ten kod musi byÄ‡ uruchomiony w DLT pipeline (nie w zwykÅ‚ym notebooku)\n",
    "\n",
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Bronze Table - raw data\n",
    "@dlt.table(\n",
    "    name=\"____\",  # bronze_customers\n",
    "    comment=\"Raw customer data from source\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\"\n",
    "    }\n",
    ")\n",
    "def bronze_customers():\n",
    "    # UÅ¼yj Å›cieÅ¼ki Volume jako ÅºrÃ³dÅ‚a danych\n",
    "    volume_path = \"/Volumes/main/default/kion_data\"\n",
    "    \n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"____\")  # cloudFiles\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .load(f\"____\")  # {volume_path}/customers/\n",
    "        .withColumn(\"_ingest_timestamp\", current_timestamp())\n",
    "    )\n",
    "\n",
    "# Silver Table - cleaned data z expectations\n",
    "@dlt.table(\n",
    "    name=\"silver_customers\",\n",
    "    comment=\"Cleaned customer data with quality checks\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"____\", \"____ IS NOT NULL\")  # valid_email, email\n",
    "@dlt.expect_or_drop(\"valid_customer_id\", \"customer_id > 0\")\n",
    "def silver_customers():\n",
    "    return (\n",
    "        dlt.read_stream(\"____\")  # bronze_customers\n",
    "        .select(\n",
    "            \"customer_id\",\n",
    "            \"name\",\n",
    "            \"email\",\n",
    "            \"city\",\n",
    "            upper(col(\"country\")).alias(\"country\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Gold Table - aggregated data\n",
    "@dlt.table(\n",
    "    name=\"____\",  # gold_customers_by_country\n",
    "    comment=\"Customer count per country\"\n",
    ")\n",
    "def gold_customers_by_country():\n",
    "    return (\n",
    "        dlt.read(\"____\")  # silver_customers\n",
    "        .groupBy(\"____\")  # country\n",
    "        .agg(count(\"____\").alias(\"total_customers\"))  # customer_id\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c19424",
   "metadata": {},
   "source": [
    "### Zadanie 1.2: DLT Expectations - Data Quality\n",
    "\n",
    "**Typy Expectations:**\n",
    "\n",
    "1. **`@dlt.expect()`** - Loguj naruszenia, ale kontynuuj\n",
    "2. **`@dlt.expect_or_drop()`** - OdrzuÄ‡ rekordy nie speÅ‚niajÄ…ce warunku\n",
    "3. **`@dlt.expect_or_fail()`** - Zatrzymaj pipeline przy naruszeniu\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Dodaj expectations do Silver table:\n",
    "   - Email musi zawieraÄ‡ '@'\n",
    "   - Customer_id > 0\n",
    "   - Country nie moÅ¼e byÄ‡ NULL\n",
    "2. UÅ¼yj rÃ³Å¼nych typÃ³w expectations (drop, fail, log)\n",
    "\n",
    "**PrzykÅ‚ad:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e038755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 1.2 - Expectations examples\n",
    "# (Ten kod jest przykÅ‚adem - musi byÄ‡ w DLT pipeline)\n",
    "\n",
    "@dlt.table(name=\"silver_orders\")\n",
    "@dlt.____(\"____\", \"____ IS NOT NULL\")  # expect, valid_order_id, order_id\n",
    "@dlt.expect_or_drop(\"____\", \"____ > 0\")  # positive_amount, total_amount\n",
    "@dlt.expect_or_fail(\"____\", \"status IN ('____', '____', '____')\")  # valid_status, pending, completed, cancelled\n",
    "def silver_orders():\n",
    "    # Å¹rÃ³dÅ‚o danych z Volume\n",
    "    volume_path = \"/Volumes/main/default/kion_data\"\n",
    "    \n",
    "    return (\n",
    "        dlt.read_stream(\"____\")  # bronze_orders - lub uÅ¼yj cloudFiles z volume_path/orders/\n",
    "        .select(\n",
    "            \"order_id\",\n",
    "            \"customer_id\",\n",
    "            to_date(col(\"order_date\")).alias(\"order_date\"),\n",
    "            col(\"total_amount\").cast(\"double\"),\n",
    "            \"status\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097444fe",
   "metadata": {},
   "source": [
    "### Zadanie 1.3: DLT Configuration & Deployment\n",
    "\n",
    "**Instrukcje (UI Databricks):**\n",
    "\n",
    "1. PrzejdÅº do **Workflows** â†’ **Delta Live Tables**\n",
    "2. Kliknij **Create Pipeline**\n",
    "3. Konfiguracja:\n",
    "   - **Pipeline name**: `workshop_dlt_pipeline`\n",
    "   - **Notebook libraries**: Dodaj notebook z definicjÄ… DLT\n",
    "   - **Target**: `{CATALOG}.{SCHEMA}`\n",
    "   - **Storage location**: `/mnt/dlt/{user}/`\n",
    "   - **Pipeline mode**: `Triggered` lub `Continuous`\n",
    "   - **Cluster mode**: `Fixed size` (1 worker dla testÃ³w)\n",
    "4. Kliknij **Create**\n",
    "5. Kliknij **Start** aby uruchomiÄ‡ pipeline\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Pipeline utworzony i uruchomiony\n",
    "- Tabele Bronze/Silver/Gold utworzone\n",
    "- Data quality metrics widoczne w UI\n",
    "\n",
    "**Zadanie do wykonania:**\n",
    "- StwÃ³rz DLT pipeline w UI\n",
    "- Zrzut ekranu z uruchomionego pipeline\n",
    "- SprawdÅº Event Log dla metryki quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f901ef06",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”— CzÄ™Å›Ä‡ 2: Databricks Jobs - Multi-task Orchestration\n",
    "\n",
    "### Co to sÄ… Databricks Jobs?\n",
    "\n",
    "**Databricks Jobs** - orkiestracja multi-task workflows.\n",
    "\n",
    "**Typy taskÃ³w:**\n",
    "- **Notebook task** - wykonaj notebook\n",
    "- **DLT task** - uruchom DLT pipeline\n",
    "- **SQL task** - wykonaj SQL query\n",
    "- **Python wheel task** - uruchom Python package\n",
    "- **JAR task** - uruchom Java/Scala\n",
    "\n",
    "### Zadanie 2.1: Widget Parameters - Parametryzacja notebookÃ³w\n",
    "\n",
    "**Instrukcje:**\n",
    "1. UÅ¼yj `dbutils.widgets` do zdefiniowania parametrÃ³w\n",
    "2. Odbierz parametry z Job execution\n",
    "3. UÅ¼yj parametrÃ³w w transformacjach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 2.1 - Widget parameters\n",
    "\n",
    "# Definicja widgets\n",
    "dbutils.widgets.text(\"____\", \"____\", \"Start Date (YYYY-MM-DD)\")  # start_date, 2024-01-01\n",
    "dbutils.widgets.text(\"____\", \"2024-12-31\", \"End Date (YYYY-MM-DD)\")  # end_date\n",
    "dbutils.widgets.dropdown(\"environment\", \"dev\", [\"dev\", \"____\", \"____\"], \"Environment\")  # staging, prod\n",
    "\n",
    "# Odczyt parametrÃ³w\n",
    "start_date = dbutils.widgets.get(\"____\")  # start_date\n",
    "end_date = dbutils.widgets.get(\"end_date\")\n",
    "environment = dbutils.widgets.get(\"____\")  # environment\n",
    "\n",
    "print(f\"=== Job Parameters ===\")\n",
    "print(f\"Start Date: {start_date}\")\n",
    "print(f\"End Date: {end_date}\")\n",
    "print(f\"Environment: {environment}\")\n",
    "\n",
    "# UÅ¼yj parametrÃ³w w query - wczytaj dane z Volume\n",
    "volume_path = \"/Volumes/main/default/kion_data\"\n",
    "orders_from_volume = spark.read.json(f\"{volume_path}/orders/orders_batch.json\")\n",
    "\n",
    "# Filtruj uÅ¼ywajÄ…c parametrÃ³w\n",
    "filtered_orders = orders_from_volume.filter(\n",
    "    (F.col(\"order_date\") >= start_date) & \n",
    "    (F.col(\"order_date\") <= end_date)\n",
    ")\n",
    "\n",
    "# Alternatywnie, jeÅ›li dane juÅ¼ sÄ… w tabelach:\n",
    "# filtered_orders = spark.sql(f\"\"\"\n",
    "#     SELECT * \n",
    "#     FROM {CATALOG}.{SCHEMA}.orders\n",
    "#     WHERE order_date BETWEEN '{start_date}' AND '{____}'  -- end_date\n",
    "# \"\"\")\n",
    "\n",
    "print(f\"Filtered orders: {filtered_orders.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c232a8",
   "metadata": {},
   "source": [
    "### Zadanie 2.2: Multi-task Job Configuration\n",
    "\n",
    "**Instrukcje (UI Databricks):**\n",
    "\n",
    "1. PrzejdÅº do **Workflows** â†’ **Jobs**\n",
    "2. Kliknij **Create Job**\n",
    "3. Dodaj pierwszy task:\n",
    "   - **Task name**: `ingest_bronze`\n",
    "   - **Type**: `Notebook`\n",
    "   - **Source**: Wybierz notebook dla Bronze ingestion\n",
    "   - **Cluster**: `New job cluster` (1 worker)\n",
    "   - **Parameters**: `{\"start_date\": \"2024-01-01\"}`\n",
    "4. Dodaj drugi task (depends on: `ingest_bronze`):\n",
    "   - **Task name**: `transform_silver`\n",
    "   - **Type**: `Notebook`\n",
    "   - **Source**: Notebook dla Silver transformations\n",
    "   - **Depends on**: `ingest_bronze`\n",
    "5. Dodaj trzeci task (depends on: `transform_silver`):\n",
    "   - **Task name**: `aggregate_gold`\n",
    "   - **Type**: `Notebook`\n",
    "   - **Depends on**: `transform_silver`\n",
    "6. Skonfiguruj **Schedule**: `Cron: 0 2 * * *` (2 AM daily)\n",
    "7. Dodaj **Email notifications** na failure\n",
    "8. Kliknij **Create**\n",
    "\n",
    "**DAG Structure:**\n",
    "```\n",
    "ingest_bronze\n",
    "      â†“\n",
    "transform_silver\n",
    "      â†“\n",
    "aggregate_gold\n",
    "```\n",
    "\n",
    "**Zadanie do wykonania:**\n",
    "- UtwÃ³rz Multi-task Job w UI\n",
    "- Zdefiniuj dependencies miÄ™dzy taskami\n",
    "- Uruchom Job rÄ™cznie (Run Now)\n",
    "- SprawdÅº Runs history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777227f6",
   "metadata": {},
   "source": [
    "### Zadanie 2.3: Job Dependencies & Conditional Execution\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Dodaj task z warunkiem (If/Else logic):\n",
    "   - JeÅ›li Bronze ma > 1000 rekordÃ³w â†’ uruchom Silver\n",
    "   - JeÅ›li nie â†’ wyÅ›lij alert\n",
    "2. UÅ¼yj task values do przekazania danych miÄ™dzy taskami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2393657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 2.3 - Task values (przekazywanie danych)\n",
    "\n",
    "# Task 1: SprawdÅº liczbÄ™ rekordÃ³w\n",
    "bronze_count = spark.table(f\"{CATALOG}.{SCHEMA}.bronze_customers\").count()\n",
    "\n",
    "print(f\"Bronze records: {bronze_count}\")\n",
    "\n",
    "# Ustaw task value (dostÄ™pne dla nastÄ™pnych taskÃ³w)\n",
    "dbutils.jobs.taskValues.set(\n",
    "    key=\"____\",  # bronze_count\n",
    "    value=bronze_count\n",
    ")\n",
    "\n",
    "# Task 2: Odbierz wartoÅ›Ä‡ z poprzedniego taska\n",
    "# received_count = dbutils.jobs.taskValues.get(\n",
    "#     taskKey=\"ingest_bronze\",\n",
    "#     key=\"bronze_count\",\n",
    "#     default=0\n",
    "# )\n",
    "\n",
    "# if received_count > 1000:\n",
    "#     print(\"âœ“ Sufficient data - proceeding to Silver\")\n",
    "# else:\n",
    "#     raise Exception(f\"Insufficient data: {received_count} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507072f0",
   "metadata": {},
   "source": [
    "### Zadanie 2.4: Job Monitoring & Alerting\n",
    "\n",
    "**Instrukcje (UI Databricks):**\n",
    "\n",
    "1. W Job configuration, przejdÅº do **Alerts**\n",
    "2. Dodaj email notification:\n",
    "   - **On failure**: `your-email@example.com`\n",
    "   - **On success**: (opcjonalnie)\n",
    "3. Skonfiguruj **Retries**:\n",
    "   - **Max retries**: 3\n",
    "   - **Timeout**: 3600 seconds (1 hour)\n",
    "4. Dodaj **SLA**:\n",
    "   - JeÅ›li Job trwa > 2 godziny â†’ alert\n",
    "\n",
    "**Monitoring w kodzie:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb195381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 2.4 - Job monitoring code\n",
    "\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # TwÃ³j kod transformacji\n",
    "    print(\"Processing data...\")\n",
    "    time.sleep(2)  # Symulacja przetwarzania\n",
    "    \n",
    "    # Zapisz metrics\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    dbutils.jobs.taskValues.set(\"____\", duration)  # execution_time\n",
    "    \n",
    "    print(f\"âœ“ Success! Duration: {duration:.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # Log error\n",
    "    print(f\"âœ— Error: {str(e)}\")\n",
    "    \n",
    "    # Ustaw failure reason\n",
    "    dbutils.jobs.taskValues.set(\"failure_reason\", str(e))\n",
    "    \n",
    "    # Re-raise dla Job failure\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53b2440",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Podsumowanie warsztatu\n",
    "\n",
    "**Zrealizowane cele:**\n",
    "- âœ… Delta Live Tables - deklaratywne pipeline definition\n",
    "- âœ… Data Quality z Expectations (drop, fail, log)\n",
    "- âœ… Multi-task Databricks Jobs z dependencies\n",
    "- âœ… Parametryzacja i task values\n",
    "- âœ… Monitoring i alerting\n",
    "\n",
    "**Kluczowe wnioski:**\n",
    "\n",
    "1. **DLT vs Traditional Spark:**\n",
    "   - DLT: Deklaratywne, auto-scaling, built-in quality\n",
    "   - Spark: Imperatywne, manual scaling, custom quality\n",
    "\n",
    "2. **Kiedy uÅ¼ywaÄ‡ DLT:**\n",
    "   - Production data pipelines\n",
    "   - Data quality critical\n",
    "   - Need auto-scaling\n",
    "   - Want automatic lineage\n",
    "\n",
    "3. **Databricks Jobs Best Practices:**\n",
    "   - MaÅ‚e, reusable notebooki (single responsibility)\n",
    "   - Parametryzacja przez widgets\n",
    "   - Task dependencies dla DAG\n",
    "   - Monitoring i alerting dla critical jobs\n",
    "\n",
    "**Quick Reference:**\n",
    "\n",
    "| Feature | DLT | Traditional Spark |\n",
    "|---------|-----|------------------|\n",
    "| SkÅ‚adnia | Deklaratywna | Imperatywna |\n",
    "| Data Quality | Built-in expectations | Custom code |\n",
    "| Lineage | Automatic | Manual |\n",
    "| Orchestration | Automatic | Manual (Jobs) |\n",
    "| Scaling | Auto | Manual |\n",
    "\n",
    "**NastÄ™pne kroki:**\n",
    "- **Kolejny warsztat**: 03_governance_integrations_workshop.ipynb\n",
    "- **Dokumentacja**: [DLT Guide](https://docs.databricks.com/delta-live-tables/)\n",
    "- **Best Practices**: [Databricks Jobs](https://docs.databricks.com/workflows/jobs/)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¹ Cleanup (opcjonalnie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e229745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UsuÅ„ widgets\n",
    "# dbutils.widgets.removeAll()\n",
    "\n",
    "# WyczyÅ›Ä‡ cache\n",
    "# spark.catalog.clearCache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
