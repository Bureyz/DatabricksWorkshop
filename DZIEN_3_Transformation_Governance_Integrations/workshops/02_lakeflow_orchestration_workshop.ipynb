{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1349b08",
   "metadata": {},
   "source": [
    "# Warsztat 2: Lakeflow Pipelines & Databricks Jobs Orchestration\n",
    "\n",
    "**Cel warsztatu:**\n",
    "- Poznanie **Lakeflow Pipelines** (nowa nazwa dla Delta Live Tables) - deklaratywnych pipeline'Ã³w\n",
    "- Implementacja Data Quality z Expectations (warn/drop/fail)\n",
    "- Konfiguracja Multi-task Databricks Jobs\n",
    "- Dependencies i parametryzacja miÄ™dzy taskami\n",
    "- Monitoring i alerting dla production workflows\n",
    "\n",
    "**Czas:** 90 minut\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Lakeflow pipeline creation - example code\n",
    "- Expectations setup (warn/drop/fail)\n",
    "- Multi-task Databricks Jobs\n",
    "- Dependencies i parametryzacja\n",
    "- Monitoring i alerting\n",
    "\n",
    "**Dataset:** Warsztat bazuje na plikach z `dataset/`: `orders_batch.json`, `customers.csv`\n",
    "\n",
    "**WAÅ»NE:** \n",
    "- **Lakeflow** to nowa nazwa marketingowa dla **Delta Live Tables (DLT)**\n",
    "- W kodzie uÅ¼ywamy moduÅ‚u `dlt` (API Databricks), ale koncepcyjnie mÃ³wimy o \"Lakeflow Pipelines\"\n",
    "- Lakeflow Pipelines sÄ… uruchamiane jako osobne job'y w UI Databricks, nie bezpoÅ›rednio w notebooku\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2460f360",
   "metadata": {},
   "source": [
    "## ðŸ“š Inicjalizacja Å›rodowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762a001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94578e8",
   "metadata": {},
   "source": [
    "## ðŸ“‚ Przygotowanie zmiennych pomocniczych\n",
    "\n",
    "Definiujemy Å›cieÅ¼ki do konkretnych plikÃ³w z folderu `dataset/`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiujemy Å›cieÅ¼ki do konkretnych plikÃ³w z dataset/ (bazujÄ…c na DATASET_BASE_PATH z 00_setup)\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "# ÅšcieÅ¼ki do plikÃ³w streaming (dla Lakeflow streaming tables)\n",
    "ORDERS_STREAM_DIR = f\"{DATASET_BASE_PATH}/orders/\"\n",
    "\n",
    "print(\"âœ“ Zmienne pomocnicze zdefiniowane:\")\n",
    "print(f\"  ORDERS_JSON: {ORDERS_JSON}\")\n",
    "print(f\"  CUSTOMERS_CSV: {CUSTOMERS_CSV}\")\n",
    "print(f\"  PRODUCTS_PARQUET: {PRODUCTS_PARQUET}\")\n",
    "print(f\"  ORDERS_STREAM_DIR: {ORDERS_STREAM_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6da1f5",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ CzÄ™Å›Ä‡ 1: Lakeflow Pipelines - Podstawy\n",
    "\n",
    "### Co to jest Lakeflow?\n",
    "\n",
    "**Lakeflow Pipelines** (poprzednio: Delta Live Tables) to deklaratywny framework Databricks do budowania reliable, production-ready data pipelines.\n",
    "\n",
    "**Kluczowe koncepcje:**\n",
    "- **Deklaratywne** - opisujesz CO chcesz osiÄ…gnÄ…Ä‡, nie JAK to zrobiÄ‡\n",
    "- **Streaming & Batch** - jedna skÅ‚adnia dla obu paradygmatÃ³w\n",
    "- **Data Quality** - wbudowane expectations (warn/drop/fail)\n",
    "- **Auto-scaling** - automatyczne zarzÄ…dzanie zasobami i klastrami\n",
    "- **Lineage** - automatyczne Å›ledzenie zaleÅ¼noÅ›ci miÄ™dzy tabelami\n",
    "- **Event Log** - szczegÃ³Å‚owy monitoring execution i quality metrics\n",
    "\n",
    "**Lakeflow vs Tradycyjne Notebooks:**\n",
    "\n",
    "| Aspekt | Tradycyjne Notebooks | Lakeflow Pipelines |\n",
    "|--------|---------------------|---------------------|\n",
    "| **Definicja** | Imperatywna (kroki) | Deklaratywna (rezultat) |\n",
    "| **ZaleÅ¼noÅ›ci** | RÄ™czne | Automatyczne (DAG) |\n",
    "| **Quality** | Custom kod | Wbudowane expectations |\n",
    "| **Monitoring** | Custom logging | Event log + lineage |\n",
    "| **Retry** | RÄ™czna logika | Automatyczne |\n",
    "\n",
    "### Zadanie 1.1: Lakeflow Pipeline Definition - Bronze Layer\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Zdefiniuj Bronze table uÅ¼ywajÄ…c `@dlt.table()` i prawdziwego pliku `orders_batch.json`\n",
    "2. Dodaj audit metadata (ingest_timestamp, source_file)\n",
    "3. UÅ¼yj streaming read dla incremental processing\n",
    "\n",
    "**ðŸ“ UWAGA:** Ten kod jest **definicjÄ…** pipeline - zostanie wykonany przez Lakeflow engine, nie bezpoÅ›rednio w tym notebooku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8bca94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 1.1 - Lakeflow Pipeline Definition (Bronze Layer)\n",
    "# Ten kod musi byÄ‡ wykonany w Lakeflow pipeline, nie w zwykÅ‚ym notebooku!\n",
    "\n",
    "import dlt  # Lakeflow uÅ¼ywa moduÅ‚u 'dlt' (Delta Live Tables API)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# ================================================================================\n",
    "# BRONZE LAYER - Raw Orders from JSON\n",
    "# ================================================================================\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"____\",  # TODO: lakeflow_bronze_orders\n",
    "    comment=\"Bronze layer: Raw orders from JSON - immutable landing zone\",\n",
    "    table_properties={\n",
    "        \"quality_layer\": \"bronze\",\n",
    "        \"source_format\": \"json\"\n",
    "    }\n",
    ")\n",
    "def lakeflow_bronze_orders():\n",
    "    \"\"\"\n",
    "    Bronze layer: Load raw orders from orders_batch.json.\n",
    "    \n",
    "    TODO:\n",
    "    1. UzupeÅ‚nij nazwÄ™ tabeli powyÅ¼ej\n",
    "    2. UÅ¼yj zmiennej ORDERS_JSON (zdefiniowanej wczeÅ›niej)\n",
    "    3. UzupeÅ‚nij format danych (json)\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.read\n",
    "        .format(\"____\")  # TODO: json\n",
    "        .option(\"multiLine\", \"true\")\n",
    "        .load(____)  # TODO: ORDERS_JSON\n",
    "        # Audit metadata\n",
    "        .withColumn(\"_bronze_ingest_timestamp\", F.current_timestamp())\n",
    "        .withColumn(\"_bronze_source_file\", F.lit(ORDERS_JSON))\n",
    "    )\n",
    "\n",
    "# ================================================================================\n",
    "# SILVER LAYER - Cleaned Orders with Expectations\n",
    "# ================================================================================\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"lakeflow_silver_orders\",\n",
    "    comment=\"Silver layer: Cleaned and validated orders with data quality checks\",\n",
    "    table_properties={\n",
    "        \"quality_layer\": \"____\"  # TODO: silver\n",
    "    }\n",
    ")\n",
    "# TODO: Dodaj expectations (przykÅ‚ady poniÅ¼ej - odkomentuj i uzupeÅ‚nij)\n",
    "# @dlt.expect_or_drop(\"____\", \"order_id IS NOT NULL\")  # TODO: valid_order_id\n",
    "# @dlt.expect_or_drop(\"valid_customer_id\", \"____ IS NOT NULL\")  # TODO: customer_id\n",
    "# @dlt.expect_or_drop(\"____\", \"total_amount > 0\")  # TODO: positive_amount\n",
    "def lakeflow_silver_orders():\n",
    "    \"\"\"\n",
    "    Silver layer: Apply data quality checks and business transformations.\n",
    "    \n",
    "    TODO:\n",
    "    1. Odczytaj dane z Bronze uÅ¼ywajÄ…c dlt.read()\n",
    "    2. Zastosuj deduplikacjÄ™\n",
    "    3. Dodaj transformacje biznesowe (order_date, order_status)\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dlt.read(\"____\")  # TODO: lakeflow_bronze_orders\n",
    "        \n",
    "        # Deduplikacja\n",
    "        .dropDuplicates([\"____\"])  # TODO: order_id\n",
    "        \n",
    "        # Transformacje dat\n",
    "        .withColumn(\"order_date\", F.to_date(F.col(\"____\")))  # TODO: order_datetime\n",
    "        .withColumn(\"order_year\", F.year(F.col(\"order_datetime\")))\n",
    "        \n",
    "        # Standaryzacja tekstÃ³w\n",
    "        .withColumn(\"payment_method\", F.upper(F.trim(F.col(\"____\"))))  # TODO: payment_method\n",
    "        \n",
    "        # Derived column\n",
    "        .withColumn(\"order_status\", \n",
    "                    F.when(F.col(\"total_amount\") > 0, \"____\")  # TODO: COMPLETED\n",
    "                     .otherwise(\"UNKNOWN\"))\n",
    "        \n",
    "        # Silver metadata\n",
    "        .withColumn(\"_silver_processed_timestamp\", F.current_timestamp())\n",
    "    )\n",
    "\n",
    "# ================================================================================\n",
    "# GOLD LAYER - Daily Aggregations\n",
    "# ================================================================================\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"____\",  # TODO: lakeflow_gold_daily_sales\n",
    "    comment=\"Gold layer: Daily sales aggregations for BI dashboards\"\n",
    ")\n",
    "def lakeflow_gold_daily_sales():\n",
    "    \"\"\"\n",
    "    Gold layer: Business-level aggregations.\n",
    "    \n",
    "    TODO:\n",
    "    1. UzupeÅ‚nij nazwÄ™ tabeli\n",
    "    2. Agreguj po order_date\n",
    "    3. Oblicz total_orders, total_revenue, avg_order_value\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dlt.read(\"____\")  # TODO: lakeflow_silver_orders\n",
    "        .groupBy(\"____\")  # TODO: order_date\n",
    "        .agg(\n",
    "            F.count(\"order_id\").alias(\"____\"),  # TODO: total_orders\n",
    "            F.sum(\"____\").alias(\"total_revenue\"),  # TODO: total_amount\n",
    "            F.avg(\"total_amount\").alias(\"____\")  # TODO: avg_order_value\n",
    "        )\n",
    "        .withColumn(\"_gold_created_timestamp\", F.current_timestamp())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c19424",
   "metadata": {},
   "source": [
    "### Zadanie 1.2: Lakeflow Expectations - Data Quality\n",
    "\n",
    "**Typy Expectations:**\n",
    "\n",
    "1. **`@dlt.expect()`** - Loguj naruszenia, ale kontynuuj\n",
    "2. **`@dlt.expect_or_drop()`** - OdrzuÄ‡ rekordy nie speÅ‚niajÄ…ce warunku\n",
    "3. **`@dlt.expect_or_fail()`** - Zatrzymaj pipeline przy naruszeniu\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Dodaj expectations do Silver table:\n",
    "   - Email musi zawieraÄ‡ '@'\n",
    "   - Customer_id > 0\n",
    "   - Country nie moÅ¼e byÄ‡ NULL\n",
    "2. UÅ¼yj rÃ³Å¼nych typÃ³w expectations (drop, fail, log)\n",
    "\n",
    "**PrzykÅ‚ad:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e038755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 1.2 - Expectations examples\n",
    "# (Ten kod jest przykÅ‚adem - musi byÄ‡ w Lakeflow pipeline)\n",
    "\n",
    "@dlt.table(name=\"silver_orders\")\n",
    "@dlt.____(\"____\", \"____ IS NOT NULL\")  # expect, valid_order_id, order_id\n",
    "@dlt.expect_or_drop(\"____\", \"____ > 0\")  # positive_amount, total_amount\n",
    "@dlt.expect_or_fail(\"____\", \"status IN ('____', '____', '____')\")  # valid_status, pending, completed, cancelled\n",
    "def silver_orders():\n",
    "    \"\"\"\n",
    "    PrzykÅ‚ad Silver table z expectations.\n",
    "    \n",
    "    TODO:\n",
    "    1. UzupeÅ‚nij expectations powyÅ¼ej (expect, expect_or_drop, expect_or_fail)\n",
    "    2. UÅ¼yj dlt.read_stream() do czytania z Bronze table\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dlt.read_stream(\"____\")  # TODO: lakeflow_bronze_orders (lub bronze_orders)\n",
    "        .select(\n",
    "            \"order_id\",\n",
    "            \"customer_id\",\n",
    "            F.to_date(F.col(\"order_date\")).alias(\"order_date\"),\n",
    "            F.col(\"total_amount\").cast(\"double\"),\n",
    "            \"status\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097444fe",
   "metadata": {},
   "source": [
    "### Zadanie 1.3: Lakeflow Configuration & Deployment\n",
    "\n",
    "**Instrukcje (UI Databricks):**\n",
    "\n",
    "1. PrzejdÅº do **Workflows** â†’ **Delta Live Tables** (nazwa w UI moÅ¼e byÄ‡ nadal DLT)\n",
    "2. Kliknij **Create Pipeline**\n",
    "3. Konfiguracja:\n",
    "   - **Pipeline name**: `workshop_lakeflow_pipeline`\n",
    "   - **Notebook libraries**: Dodaj notebook z definicjÄ… Lakeflow pipeline\n",
    "   - **Target**: `{CATALOG}.{SCHEMA}`\n",
    "   - **Storage location**: `/mnt/lakeflow/{user}/`\n",
    "   - **Pipeline mode**: `Triggered` lub `Continuous`\n",
    "   - **Cluster mode**: `Fixed size` (1 worker dla testÃ³w)\n",
    "4. Kliknij **Create**\n",
    "5. Kliknij **Start** aby uruchomiÄ‡ pipeline\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Pipeline utworzony i uruchomiony\n",
    "- Tabele Bronze/Silver/Gold utworzone\n",
    "- Data quality metrics widoczne w UI\n",
    "\n",
    "**Zadanie do wykonania:**\n",
    "- StwÃ³rz Lakeflow pipeline w UI\n",
    "- Zrzut ekranu z uruchomionego pipeline\n",
    "- SprawdÅº Event Log dla metryki quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f901ef06",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”— CzÄ™Å›Ä‡ 2: Databricks Jobs - Multi-task Orchestration\n",
    "\n",
    "### Co to sÄ… Databricks Jobs?\n",
    "\n",
    "**Databricks Jobs** - orkiestracja multi-task workflows.\n",
    "\n",
    "**Typy taskÃ³w:**\n",
    "- **Notebook task** - wykonaj notebook\n",
    "- **Lakeflow Pipeline task** - uruchom Lakeflow pipeline\n",
    "- **SQL task** - wykonaj SQL query\n",
    "- **Python wheel task** - uruchom Python package\n",
    "- **JAR task** - uruchom Java/Scala\n",
    "\n",
    "### Zadanie 2.1: Widget Parameters - Parametryzacja notebookÃ³w\n",
    "\n",
    "**Instrukcje:**\n",
    "1. UÅ¼yj `dbutils.widgets` do zdefiniowania parametrÃ³w\n",
    "2. Odbierz parametry z Job execution\n",
    "3. UÅ¼yj parametrÃ³w w transformacjach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034f4ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 2.1 - Widget parameters\n",
    "\n",
    "# Definicja widgets\n",
    "dbutils.widgets.text(\"____\", \"____\", \"Start Date (YYYY-MM-DD)\")  # start_date, 2024-01-01\n",
    "dbutils.widgets.text(\"____\", \"2024-12-31\", \"End Date (YYYY-MM-DD)\")  # end_date\n",
    "dbutils.widgets.dropdown(\"environment\", \"dev\", [\"dev\", \"____\", \"____\"], \"Environment\")  # staging, prod\n",
    "\n",
    "# Odczyt parametrÃ³w\n",
    "start_date = dbutils.widgets.get(\"____\")  # start_date\n",
    "end_date = dbutils.widgets.get(\"end_date\")\n",
    "environment = dbutils.widgets.get(\"____\")  # environment\n",
    "\n",
    "print(f\"=== Job Parameters ===\")\n",
    "print(f\"Start Date: {start_date}\")\n",
    "print(f\"End Date: {end_date}\")\n",
    "print(f\"Environment: {environment}\")\n",
    "\n",
    "# UÅ¼yj parametrÃ³w w query - wczytaj dane z dataset (uÅ¼ywajÄ…c zmiennej ORDERS_JSON)\n",
    "orders_df = spark.read.json(ORDERS_JSON)\n",
    "\n",
    "# Filtruj uÅ¼ywajÄ…c parametrÃ³w\n",
    "filtered_orders = orders_df.filter(\n",
    "    (F.col(\"order_date\") >= start_date) & \n",
    "    (F.col(\"order_date\") <= end_date)\n",
    ")\n",
    "\n",
    "# Alternatywnie, jeÅ›li dane juÅ¼ sÄ… w tabelach Delta:\n",
    "# filtered_orders = spark.sql(f\"\"\"\n",
    "#     SELECT * \n",
    "#     FROM {CATALOG}.{BRONZE_SCHEMA}.____  -- TODO: bronze_orders\n",
    "#     WHERE order_date BETWEEN '{start_date}' AND '{____}'  -- end_date\n",
    "# \"\"\")\n",
    "\n",
    "print(f\"Filtered orders: {filtered_orders.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c232a8",
   "metadata": {},
   "source": [
    "### Zadanie 2.2: Multi-task Job Configuration\n",
    "\n",
    "**Instrukcje (UI Databricks):**\n",
    "\n",
    "1. PrzejdÅº do **Workflows** â†’ **Jobs**\n",
    "2. Kliknij **Create Job**\n",
    "3. Dodaj pierwszy task:\n",
    "   - **Task name**: `ingest_bronze`\n",
    "   - **Type**: `Notebook`\n",
    "   - **Source**: Wybierz notebook dla Bronze ingestion\n",
    "   - **Cluster**: `New job cluster` (1 worker)\n",
    "   - **Parameters**: `{\"start_date\": \"2024-01-01\"}`\n",
    "4. Dodaj drugi task (depends on: `ingest_bronze`):\n",
    "   - **Task name**: `transform_silver`\n",
    "   - **Type**: `Notebook`\n",
    "   - **Source**: Notebook dla Silver transformations\n",
    "   - **Depends on**: `ingest_bronze`\n",
    "5. Dodaj trzeci task (depends on: `transform_silver`):\n",
    "   - **Task name**: `aggregate_gold`\n",
    "   - **Type**: `Notebook`\n",
    "   - **Depends on**: `transform_silver`\n",
    "6. Skonfiguruj **Schedule**: `Cron: 0 2 * * *` (2 AM daily)\n",
    "7. Dodaj **Email notifications** na failure\n",
    "8. Kliknij **Create**\n",
    "\n",
    "**DAG Structure:**\n",
    "```\n",
    "ingest_bronze\n",
    "      â†“\n",
    "transform_silver\n",
    "      â†“\n",
    "aggregate_gold\n",
    "```\n",
    "\n",
    "**Zadanie do wykonania:**\n",
    "- UtwÃ³rz Multi-task Job w UI\n",
    "- Zdefiniuj dependencies miÄ™dzy taskami\n",
    "- Uruchom Job rÄ™cznie (Run Now)\n",
    "- SprawdÅº Runs history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777227f6",
   "metadata": {},
   "source": [
    "### Zadanie 2.3: Job Dependencies & Conditional Execution\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Dodaj task z warunkiem (If/Else logic):\n",
    "   - JeÅ›li Bronze ma > 1000 rekordÃ³w â†’ uruchom Silver\n",
    "   - JeÅ›li nie â†’ wyÅ›lij alert\n",
    "2. UÅ¼yj task values do przekazania danych miÄ™dzy taskami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2393657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 2.3 - Task values (przekazywanie danych)\n",
    "\n",
    "# Task 1: SprawdÅº liczbÄ™ rekordÃ³w w Bronze table\n",
    "# UÅ¼yj zmiennej BRONZE_SCHEMA z 00_setup\n",
    "bronze_count = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.____\").count()  # TODO: lakeflow_bronze_orders\n",
    "\n",
    "print(f\"Bronze records: {bronze_count}\")\n",
    "\n",
    "# Ustaw task value (dostÄ™pne dla nastÄ™pnych taskÃ³w)\n",
    "dbutils.jobs.taskValues.set(\n",
    "    key=\"____\",  # TODO: bronze_count\n",
    "    value=bronze_count\n",
    ")\n",
    "\n",
    "# Task 2: Odbierz wartoÅ›Ä‡ z poprzedniego taska\n",
    "# (Ten kod jest przykÅ‚adem - wykonaj w osobnym tasku w Databricks Jobs)\n",
    "# received_count = dbutils.jobs.taskValues.get(\n",
    "#     taskKey=\"____\",  # TODO: ingest_bronze (nazwa poprzedniego taska)\n",
    "#     key=\"bronze_count\",\n",
    "#     default=0\n",
    "# )\n",
    "\n",
    "# if received_count > ____:  # TODO: 1000\n",
    "#     print(\"âœ“ Sufficient data - proceeding to Silver\")\n",
    "# else:\n",
    "#     raise Exception(f\"Insufficient data: {received_count} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507072f0",
   "metadata": {},
   "source": [
    "### Zadanie 2.4: Job Monitoring & Alerting\n",
    "\n",
    "**Instrukcje (UI Databricks):**\n",
    "\n",
    "1. W Job configuration, przejdÅº do **Alerts**\n",
    "2. Dodaj email notification:\n",
    "   - **On failure**: `your-email@example.com`\n",
    "   - **On success**: (opcjonalnie)\n",
    "3. Skonfiguruj **Retries**:\n",
    "   - **Max retries**: 3\n",
    "   - **Timeout**: 3600 seconds (1 hour)\n",
    "4. Dodaj **SLA**:\n",
    "   - JeÅ›li Job trwa > 2 godziny â†’ alert\n",
    "\n",
    "**Monitoring w kodzie:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb195381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 2.4 - Job monitoring code\n",
    "\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # TwÃ³j kod transformacji\n",
    "    print(\"Processing data...\")\n",
    "    time.sleep(2)  # Symulacja przetwarzania\n",
    "    \n",
    "    # Zapisz metrics\n",
    "    end_time = time.time()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    dbutils.jobs.taskValues.set(\"____\", duration)  # execution_time\n",
    "    \n",
    "    print(f\"âœ“ Success! Duration: {duration:.2f}s\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # Log error\n",
    "    print(f\"âœ— Error: {str(e)}\")\n",
    "    \n",
    "    # Ustaw failure reason\n",
    "    dbutils.jobs.taskValues.set(\"failure_reason\", str(e))\n",
    "    \n",
    "    # Re-raise dla Job failure\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53b2440",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Podsumowanie warsztatu\n",
    "\n",
    "**Zrealizowane cele:**\n",
    "- âœ… Lakeflow Pipelines - deklaratywne pipeline definition\n",
    "- âœ… Data Quality z Expectations (drop, fail, log)\n",
    "- âœ… Multi-task Databricks Jobs z dependencies\n",
    "- âœ… Parametryzacja i task values\n",
    "- âœ… Monitoring i alerting\n",
    "\n",
    "**Kluczowe wnioski:**\n",
    "\n",
    "1. **Lakeflow vs Traditional Spark:**\n",
    "   - Lakeflow: Deklaratywne, auto-scaling, built-in quality\n",
    "   - Spark: Imperatywne, manual scaling, custom quality\n",
    "\n",
    "2. **Kiedy uÅ¼ywaÄ‡ Lakeflow Pipelines:**\n",
    "   - Production data pipelines\n",
    "   - Data quality critical\n",
    "   - Need auto-scaling\n",
    "   - Want automatic lineage\n",
    "\n",
    "3. **Databricks Jobs Best Practices:**\n",
    "   - MaÅ‚e, reusable notebooki (single responsibility)\n",
    "   - Parametryzacja przez widgets\n",
    "   - Task dependencies dla DAG\n",
    "   - Monitoring i alerting dla critical jobs\n",
    "\n",
    "**Quick Reference:**\n",
    "\n",
    "| Feature | Lakeflow Pipelines | Traditional Spark |\n",
    "|---------|---------------------|------------------|\n",
    "| SkÅ‚adnia | Deklaratywna | Imperatywna |\n",
    "| Data Quality | Built-in expectations | Custom code |\n",
    "| Lineage | Automatic | Manual |\n",
    "| Orchestration | Automatic | Manual (Jobs) |\n",
    "| Scaling | Auto | Manual |\n",
    "\n",
    "**NastÄ™pne kroki:**\n",
    "- **Kolejny warsztat**: 03_governance_integrations_workshop.ipynb\n",
    "- **Dokumentacja**: [Lakeflow Pipelines Guide](https://docs.databricks.com/delta-live-tables/)\n",
    "- **Best Practices**: [Databricks Jobs](https://docs.databricks.com/workflows/jobs/)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¹ Cleanup (opcjonalnie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e229745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UsuÅ„ widgets\n",
    "# dbutils.widgets.removeAll()\n",
    "\n",
    "# WyczyÅ›Ä‡ cache\n",
    "# spark.catalog.clearCache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
