{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b79fe2f8",
   "metadata": {},
   "source": [
    "# Warsztat 1: Advanced PySpark Transformations\n",
    "\n",
    "**Cel warsztatu:**\n",
    "- Praktyczne zastosowanie Window Functions (lag, lead, rank, rolling aggregations)\n",
    "- Przetwarzanie zo偶onych struktur (JSON, arrays, structs)\n",
    "- Zaawansowane operacje na datach i czasie\n",
    "- Optymalizacja transformacji dla wydajnoci\n",
    "\n",
    "**Czas:** 90 minut\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049d992",
   "metadata": {},
   "source": [
    "##  Inicjalizacja rodowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06065d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7079deec",
   "metadata": {},
   "source": [
    "## Konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7cc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Wywietl kontekst u偶ytkownika\n",
    "print(\"=== Kontekst u偶ytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema: {BRONZE_SCHEMA}\")\n",
    "print(f\"U偶ytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog i schemat jako domylne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba81015",
   "metadata": {},
   "source": [
    "##  Przygotowanie danych z Databricks Volume\n",
    "\n",
    "Wczytaj dane z Databricks Volume dla warsztat贸w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28939c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cie偶ka do Volume\n",
    "volume_path = \"/Volumes/main/default/kion_data\"\n",
    "\n",
    "# Wczytanie danych klient贸w\n",
    "customers_df = spark.read.csv(f\"{volume_path}/customers/customers.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Wczytanie danych zam贸wie (batch)\n",
    "orders_df = spark.read.json(f\"{volume_path}/orders/orders_batch.json\")\n",
    "\n",
    "# Wczytanie danych produkt贸w\n",
    "products_df = spark.read.parquet(f\"{volume_path}/products/products.parquet\")\n",
    "\n",
    "# Przygotowanie zczonego widoku dla wicze\n",
    "test_orders = (\n",
    " orders_df\n",
    " .join(customers_df, \"customer_id\")\n",
    " .join(products_df, \"product_id\")\n",
    " .select(\n",
    " \"order_id\",\n",
    " \"customer_id\",\n",
    " F.col(\"order_date\").cast(\"date\").alias(\"order_date\"),\n",
    " F.col(\"total_amount\"),\n",
    " F.col(\"status\")\n",
    " )\n",
    ")\n",
    "\n",
    "test_orders.createOrReplaceTempView(\"orders\")\n",
    "display(test_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb614d00",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Cz 1: Window Functions\n",
    "\n",
    "### Zadanie 1.1: Ranking - ROW_NUMBER, RANK, DENSE_RANK\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Dla ka偶dego klienta, uszereguj zam贸wienia po dacie (od najnowszego)\n",
    "2. Dodaj kolumny:\n",
    " - `row_num`: u偶ywajc `row_number()`\n",
    " - `rank`: u偶ywajc `rank()`\n",
    " - `dense_rank`: u偶ywajc `dense_rank()`\n",
    "3. Window spec: `partitionBy(\"customer_id\").orderBy(F.desc(\"order_date\"))`\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Ka偶dy klient ma zam贸wienia ponumerowane od 1 (najnowsze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04376587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 1.1 - Ranking functions\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Definicja window spec\n",
    "window_spec = Window.____(\"____\").orderBy(F.____(\"____\")) # partitionBy customer_id, orderBy desc order_date\n",
    "\n",
    "# Dodaj kolumny ranking\n",
    "orders_ranked = (\n",
    " test_orders\n",
    " .withColumn(\"row_num\", F.____().____(window_spec)) # row_number, over\n",
    " .withColumn(\"rank\", F.____().over(____)) # rank, window_spec\n",
    " .withColumn(\"dense_rank\", F.____().over(window_spec)) # dense_rank\n",
    ")\n",
    "\n",
    "display(orders_ranked.orderBy(\"customer_id\", \"order_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8efa955",
   "metadata": {},
   "source": [
    "**Wyjanienie r贸偶nic:**\n",
    "\n",
    "- **ROW_NUMBER**: Unikalne numery sekwencyjne (1, 2, 3...)\n",
    "- **RANK**: Luki w numeracji przy r贸wnych wartociach (1, 2, 2, 4...)\n",
    "- **DENSE_RANK**: Brak luk przy r贸wnych wartociach (1, 2, 2, 3...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69daf8af",
   "metadata": {},
   "source": [
    "### Zadanie 1.2: LAG i LEAD - Por贸wnanie z poprzednimi/nastpnymi wartociami\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Dla ka偶dego klienta, oblicz:\n",
    " - `previous_order_amount`: warto poprzedniego zam贸wienia (u偶ywajc `lag`)\n",
    " - `next_order_amount`: warto nastpnego zam贸wienia (u偶ywajc `lead`)\n",
    " - `amount_diff_vs_previous`: r贸偶nica midzy aktualnym a poprzednim\n",
    "2. Window spec: `partitionBy(\"customer_id\").orderBy(\"order_date\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5cbec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 1.2 - LAG i LEAD\n",
    "\n",
    "# Window spec - porzdek chronologiczny\n",
    "window_chrono = Window.partitionBy(\"____\").orderBy(\"____\") # customer_id, order_date\n",
    "\n",
    "# U偶yj LAG i LEAD\n",
    "orders_lag_lead = (\n",
    " test_orders\n",
    " .withColumn(\"previous_order_amount\", F.____(____, ____).over(____)) # lag, total_amount, 1, window_chrono\n",
    " .withColumn(\"next_order_amount\", F.____(____, 1).over(window_chrono)) # lead, total_amount\n",
    " .withColumn(\n",
    " \"amount_diff_vs_previous\",\n",
    " F.col(\"____\") - F.col(\"____\") # total_amount, previous_order_amount\n",
    " )\n",
    ")\n",
    "\n",
    "display(orders_lag_lead.select(\n",
    " \"customer_id\", \"order_date\", \"total_amount\", \n",
    " \"previous_order_amount\", \"next_order_amount\", \"amount_diff_vs_previous\"\n",
    ").orderBy(\"customer_id\", \"order_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8ac7c9",
   "metadata": {},
   "source": [
    "### Zadanie 1.3: Rolling Aggregations - rednie ruchome\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Oblicz redni ruchom (rolling average) dla kwoty zam贸wienia:\n",
    " - Okno: 3 ostatnie zam贸wienia (current + 2 poprzednie)\n",
    "2. U偶yj `.rowsBetween(-2, 0)` dla window spec\n",
    "3. Dodaj kolumn `rolling_avg_3_orders`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db295511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 1.3 - Rolling aggregations\n",
    "\n",
    "# Window spec z rowsBetween\n",
    "window_rolling = (\n",
    " Window\n",
    " .partitionBy(\"customer_id\")\n",
    " .orderBy(\"order_date\")\n",
    " .____(____, ____) # rowsBetween, -2, 0 (3 ostatnie rekordy)\n",
    ")\n",
    "\n",
    "# Rolling average\n",
    "orders_rolling = (\n",
    " test_orders\n",
    " .withColumn(\n",
    " \"rolling_avg_3_orders\",\n",
    " F.____(\"____\").over(____) # avg, total_amount, window_rolling\n",
    " )\n",
    " .withColumn(\n",
    " \"rolling_sum_3_orders\",\n",
    " F.sum(\"total_amount\").over(window_rolling)\n",
    " )\n",
    ")\n",
    "\n",
    "display(orders_rolling.select(\n",
    " \"customer_id\", \"order_date\", \"total_amount\", \n",
    " \"rolling_avg_3_orders\", \"rolling_sum_3_orders\"\n",
    ").orderBy(\"customer_id\", \"order_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da347e5",
   "metadata": {},
   "source": [
    "### Zadanie 1.4: Cumulative Sum - Suma narastajca\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Oblicz sum narastajc (cumulative sum) kwot zam贸wie per klient\n",
    "2. U偶yj `.rowsBetween(Window.unboundedPreceding, Window.currentRow)`\n",
    "3. Dodaj kolumn `cumulative_amount`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc2f185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 1.4 - Cumulative sum\n",
    "\n",
    "# Window spec dla cumulative\n",
    "window_cumulative = (\n",
    " Window\n",
    " .partitionBy(\"____\")\n",
    " .orderBy(\"____\")\n",
    " .rowsBetween(Window.____, Window.____) # unboundedPreceding, currentRow\n",
    ")\n",
    "\n",
    "# Cumulative sum\n",
    "orders_cumulative = (\n",
    " test_orders\n",
    " .withColumn(\n",
    " \"cumulative_amount\",\n",
    " F.____(____(\"____\")).over(window_cumulative) # round, sum total_amount\n",
    " )\n",
    ")\n",
    "\n",
    "display(orders_cumulative.select(\n",
    " \"customer_id\", \"order_date\", \"total_amount\", \"cumulative_amount\"\n",
    ").orderBy(\"customer_id\", \"order_date\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b5e47e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cz 2: Przetwarzanie zo偶onych struktur\n",
    "\n",
    "### Zadanie 2.1: JSON Processing - from_json() i explode()\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane JSON z Volume (orders)\n",
    "2. U偶yj `from_json()` do sparsowania JSON jeli potrzeba\n",
    "3. U偶yj `explode()` do \"rozpakowania\" array\n",
    "4. Wycignij pola z nested struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26a6c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytaj dane JSON z Volume (zam贸wienia mog zawiera nested structures)\n",
    "# Volume zawiera ju偶 sparsowane JSON, ale mo偶emy stworzy przykad z zagnie偶d偶on struktur\n",
    "\n",
    "# Opcja 1: U偶yj danych z Volume i stw贸rz nested JSON\n",
    "json_orders = spark.read.json(f\"{volume_path}/orders/orders_batch.json\")\n",
    "\n",
    "# Opcja 2: Dla wicze stw贸rz testowe dane z zagnie偶d偶onym JSON string\n",
    "json_data = spark.createDataFrame([\n",
    " (1, '{\"items\": [{\"product\": \"laptop\", \"price\": 1200}, {\"product\": \"mouse\", \"price\": 25}], \"total\": 1225}'),\n",
    " (2, '{\"items\": [{\"product\": \"keyboard\", \"price\": 80}], \"total\": 80}'),\n",
    " (3, '{\"items\": [{\"product\": \"monitor\", \"price\": 350}, {\"product\": \"cable\", \"price\": 15}], \"total\": 365}')\n",
    "], [\"order_id\", \"order_json\"])\n",
    "\n",
    "display(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcd357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 2.1 - JSON processing\n",
    "\n",
    "# Definicja schematu JSON\n",
    "json_schema = StructType([\n",
    " StructField(\"items\", ArrayType(StructType([\n",
    " StructField(\"product\", StringType()),\n",
    " StructField(\"price\", IntegerType())\n",
    " ]))),\n",
    " StructField(\"total\", IntegerType())\n",
    "])\n",
    "\n",
    "# Parse JSON\n",
    "orders_parsed = (\n",
    " json_data\n",
    " .withColumn(\"parsed\", F.____(____(\"____\"), ____)) # from_json, order_json, json_schema\n",
    ")\n",
    "\n",
    "display(orders_parsed.select(\"order_id\", \"parsed\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa66a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Explode array i wycignij pola\n",
    "\n",
    "orders_exploded = (\n",
    " orders_parsed\n",
    " .withColumn(\"item\", F.____(\"____\")) # explode, parsed.items\n",
    " .select(\n",
    " \"order_id\",\n",
    " F.col(\"____\").alias(\"product_name\"), # item.product\n",
    " F.col(\"____\").alias(\"product_price\"), # item.price\n",
    " F.col(\"____\").alias(\"order_total\") # parsed.total\n",
    " )\n",
    ")\n",
    "\n",
    "display(orders_exploded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f6ca2",
   "metadata": {},
   "source": [
    "### Zadanie 2.2: Array Functions - collect_list, array_contains\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Zgrupuj zam贸wienia per klient\n",
    "2. U偶yj `collect_list()` do zebrania wszystkich kwot zam贸wie w array\n",
    "3. U偶yj `array_contains()` do sprawdzenia czy klient ma zam贸wienie > 500\n",
    "4. U偶yj `size()` do zliczenia liczby zam贸wie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec89493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 2.2 - Array functions\n",
    "\n",
    "customer_arrays = (\n",
    " test_orders\n",
    " .groupBy(\"____\") # customer_id\n",
    " .agg(\n",
    " F.____(____(\"____\")).alias(\"order_amounts\"), # collect_list, total_amount\n",
    " F.collect_list(\"order_date\").alias(\"order_dates\"),\n",
    " F.count(\"*\").alias(\"total_orders\")\n",
    " )\n",
    " .withColumn(\n",
    " \"num_orders\",\n",
    " F.____(\"____\") # size, order_amounts\n",
    " )\n",
    ")\n",
    "\n",
    "display(customer_arrays)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81cba817",
   "metadata": {},
   "source": [
    "### Zadanie 2.3: Struct - czenie kolumn w struktury\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Utw贸rz struct `customer_info` zawierajcy: customer_id, total_orders\n",
    "2. Utw贸rz struct `order_summary` zawierajcy: min/max/avg amount\n",
    "3. Wycignij pola ze struct u偶ywajc `.` notacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b221a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 2.3 - Struct operations\n",
    "\n",
    "customer_structs = (\n",
    " test_orders\n",
    " .groupBy(\"customer_id\")\n",
    " .agg(\n",
    " F.count(\"*\").alias(\"total_orders\"),\n",
    " F.min(\"total_amount\").alias(\"min_amount\"),\n",
    " F.max(\"total_amount\").alias(\"max_amount\"),\n",
    " F.avg(\"total_amount\").alias(\"avg_amount\")\n",
    " )\n",
    " .withColumn(\n",
    " \"customer_info\",\n",
    " F.____(\"____\", \"____\") # struct, customer_id, total_orders\n",
    " )\n",
    " .withColumn(\n",
    " \"order_summary\",\n",
    " F.struct(\"min_amount\", \"____\", \"____\") # max_amount, avg_amount\n",
    " )\n",
    ")\n",
    "\n",
    "display(customer_structs.select(\"customer_info\", \"order_summary\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5d4e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wycignij pola ze struct\n",
    "customer_flat = (\n",
    " customer_structs\n",
    " .select(\n",
    " F.col(\"customer_info.____\").alias(\"customer_id\"), # customer_id\n",
    " F.col(\"order_summary.____\").alias(\"avg_order_value\") # avg_amount\n",
    " )\n",
    ")\n",
    "\n",
    "display(customer_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fdf507d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Cz 3: Zaawansowane operacje na datach\n",
    "\n",
    "### Zadanie 3.1: Date truncation i extraction\n",
    "\n",
    "**Instrukcje:**\n",
    "1. U偶yj `date_trunc()` do zaokrglenia dat do: month, quarter, year\n",
    "2. U偶yj `year()`, `month()`, `dayofweek()` do ekstrakcji czci daty\n",
    "3. Oblicz `days_since_order` (r贸偶nica midzy dzisiaj a dat zam贸wienia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ae6bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 3.1 - Date functions\n",
    "\n",
    "orders_dates = (\n",
    " test_orders\n",
    " .withColumn(\"order_month\", F.____(____(\"____\"), \"____\")) # date_trunc, order_date, month\n",
    " .withColumn(\"order_quarter\", F.date_trunc(\"____\", \"order_date\")) # quarter\n",
    " .withColumn(\"order_year_num\", F.____(\"____\")) # year, order_date\n",
    " .withColumn(\"order_month_num\", F.____(____(\"____\"))) # month, order_date\n",
    " .withColumn(\"day_of_week\", F.____(____(\"order_date\"))) # dayofweek\n",
    " .withColumn(\n",
    " \"days_since_order\",\n",
    " F.datediff(F.____, \"____\") # current_date, order_date\n",
    " )\n",
    ")\n",
    "\n",
    "display(orders_dates.select(\n",
    " \"order_id\", \"order_date\", \"order_month\", \"order_quarter\",\n",
    " \"order_year_num\", \"order_month_num\", \"day_of_week\", \"days_since_order\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faf8565",
   "metadata": {},
   "source": [
    "### Zadanie 3.2: Date arithmetic - dodawanie/odejmowanie okres贸w\n",
    "\n",
    "**Instrukcje:**\n",
    "1. U偶yj `date_add()` do dodania 30 dni do daty zam贸wienia\n",
    "2. U偶yj `add_months()` do dodania 3 miesicy\n",
    "3. U偶yj `last_day()` do uzyskania ostatniego dnia miesica\n",
    "4. U偶yj `next_day()` do uzyskania najbli偶szego poniedziaku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c0b1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 3.2 - Date arithmetic\n",
    "\n",
    "orders_date_math = (\n",
    " test_orders\n",
    " .withColumn(\"delivery_date_estimate\", F.____(____(\"____\"), ____)) # date_add, order_date, 30\n",
    " .withColumn(\"renewal_date\", F.____(____(\"order_date\"), ____)) # add_months, 3\n",
    " .withColumn(\"month_end\", F.____(____(\"____\"))) # last_day, order_date\n",
    " .withColumn(\"next_monday\", F.next_day(\"____\", \"____\")) # order_date, Monday\n",
    ")\n",
    "\n",
    "display(orders_date_math.select(\n",
    " \"order_date\", \"delivery_date_estimate\", \"renewal_date\", \n",
    " \"month_end\", \"next_monday\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91643db",
   "metadata": {},
   "source": [
    "### Zadanie 3.3: Generowanie sekwencji dat\n",
    "\n",
    "**Instrukcje:**\n",
    "1. U偶yj `sequence()` do wygenerowania array dat midzy dwoma datami\n",
    "2. U偶yj `explode()` do utworzenia jednego wiersza per data\n",
    "3. Stw贸rz calendar table z wszystkimi dniami midzy min a max order_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10cdbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 3.3 - Date sequences\n",
    "\n",
    "# Znajd藕 min i max dates\n",
    "date_range = test_orders.select(\n",
    " F.min(\"order_date\").alias(\"min_date\"),\n",
    " F.max(\"order_date\").alias(\"max_date\")\n",
    ").first()\n",
    "\n",
    "# Generuj sekwencj dat\n",
    "calendar = (\n",
    " spark.range(1)\n",
    " .select(\n",
    " F.____( # explode\n",
    " F.____(\n",
    " F.lit(date_range[\"____\"]), # min_date\n",
    " F.lit(date_range[\"max_date\"]),\n",
    " F.expr(\"____\") # interval 1 day\n",
    " )\n",
    " ).alias(\"date\")\n",
    " )\n",
    " .withColumn(\"year\", F.year(\"date\"))\n",
    " .withColumn(\"month\", F.____(____(\"____\"))) # month, date\n",
    " .withColumn(\"day_of_week\", F.dayofweek(\"date\"))\n",
    ")\n",
    "\n",
    "print(f\"Calendar table: {calendar.count()} dni\")\n",
    "display(calendar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68a6a2b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie warsztatu\n",
    "\n",
    "**Zrealizowane cele:**\n",
    "- Window Functions (ranking, lag/lead, rolling aggregations, cumulative sum)\n",
    "- Przetwarzanie JSON (from_json, explode, struct)\n",
    "- Array operations (collect_list, array_contains, size)\n",
    "- Zaawansowane operacje na datach (truncation, arithmetic, sequences)\n",
    "\n",
    "**Kluczowe wnioski:**\n",
    "1. Window Functions pozwalaj na analizy per grupa bez GROUP BY\n",
    "2. JSON i struktury zo偶one s native w Spark\n",
    "3. Date functions umo偶liwiaj zaawansowane analizy temporalne\n",
    "4. Optymalizacja: u偶yj broadcast dla maych tabel w JOIN\n",
    "\n",
    "**Best Practices:**\n",
    "- Window Functions: zawsze definiuj explicit window spec\n",
    "- JSON: u偶ywaj schema inference tylko dla exploratation\n",
    "- Dates: u偶ywaj native date types (nie string)\n",
    "- Performance: cache() dla czsto u偶ywanych DataFrame\n",
    "\n",
    "---\n",
    "\n",
    "## Ч Cleanup (opcjonalnie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyczy temporary views\n",
    "# spark.catalog.dropTempView(\"orders\")\n",
    "# spark.catalog.clearCache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}