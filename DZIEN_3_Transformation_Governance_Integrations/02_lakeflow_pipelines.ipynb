{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78971093",
   "metadata": {},
   "source": [
    "# Delta Live Tables (Lakeflow) Pipelines\n",
    "\n",
    "**KION Training - Dzie≈Ñ 3**\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Agenda\n",
    "\n",
    "1. Wprowadzenie do Delta Live Tables (DLT)\n",
    "2. Deklaratywne definicje pipeline'√≥w\n",
    "3. Materialized Views vs Streaming Tables\n",
    "4. Data Quality Expectations\n",
    "5. Event Log i Lineage\n",
    "6. Automatic Orchestration\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Cele szkolenia\n",
    "\n",
    "Po tym module bƒôdziesz potrafiƒá:\n",
    "- Definiowaƒá deklaratywne pipeline'y w DLT\n",
    "- R√≥≈ºnicowaƒá Materialized Views i Streaming Tables\n",
    "- Implementowaƒá Data Quality Expectations\n",
    "- Monitorowaƒá pipeline'y przez Event Log\n",
    "- Konfigurowaƒá automatycznƒÖ orkiestracjƒô\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750fe993",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Wprowadzenie do Delta Live Tables (DLT)\n",
    "\n",
    "**Delta Live Tables (DLT)** to framework do deklaratywnego budowania ETL/ELT pipeline'√≥w w Databricks.\n",
    "\n",
    "### Kluczowe cechy:\n",
    "- **Deklaratywny**: definiujesz \"co\", nie \"jak\"\n",
    "- **Automatyczna orkiestracja**: DLT zarzƒÖdza zale≈ºno≈õciami\n",
    "- **Data Quality**: wbudowane expectations\n",
    "- **Monitoring**: event log, lineage, metrics\n",
    "- **SQL i Python API**: elastyczno≈õƒá jƒôzyka\n",
    "\n",
    "### R√≥≈ºnica DLT vs tradycyjne Notebooks:\n",
    "\n",
    "| Aspekt | Tradycyjne Notebooks | Delta Live Tables |\n",
    "|--------|---------------------|------------------|\n",
    "| Definicja | Imperatywna (kroki) | Deklaratywna (rezultat) |\n",
    "| Zale≈ºno≈õci | Rƒôczne | Automatyczne |\n",
    "| Quality | Custom kod | Wbudowane expectations |\n",
    "| Monitoring | Custom logging | Event log + lineage |\n",
    "| Orchestracja | Databricks Jobs | Automatyczna |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b169543",
   "metadata": {},
   "source": [
    "## 2Ô∏è‚É£ Deklaratywne definicje pipeline'√≥w\n",
    "\n",
    "### Python API - podstawowa sk≈Çadnia:\n",
    "\n",
    "W DLT definiujemy tabele za pomocƒÖ dekorator√≥w `@dlt.table()` lub `@dlt.view()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3657b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Przyk≈Çad 1: Prosta tabela DLT\n",
    "@dlt.table(\n",
    "    name=\"raw_orders\",\n",
    "    comment=\"Raw orders data from CSV source\"\n",
    ")\n",
    "def raw_orders():\n",
    "    return (\n",
    "        spark.read.format(\"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(\"/Volumes/main/default/kion_data/orders/*.csv\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6366e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 2: Tabela z transformacjami\n",
    "@dlt.table(\n",
    "    name=\"cleaned_orders\",\n",
    "    comment=\"Cleaned orders with data quality checks\"\n",
    ")\n",
    "def cleaned_orders():\n",
    "    return (\n",
    "        dlt.read(\"raw_orders\")  # Odczyt z innej tabeli DLT\n",
    "        .filter(col(\"order_id\").isNotNull())\n",
    "        .filter(col(\"amount\") > 0)\n",
    "        .withColumn(\"order_date\", to_date(col(\"order_date\")))\n",
    "        .withColumn(\"processing_time\", current_timestamp())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028fc256",
   "metadata": {},
   "source": [
    "### SQL API - alternatywna sk≈Çadnia:\n",
    "\n",
    "DLT wspiera r√≥wnie≈º czysty SQL - idealne dla zespo≈Ç√≥w analitycznych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b7a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL w DLT (w osobnym notebooku SQL):\n",
    "\n",
    "# CREATE OR REFRESH LIVE TABLE raw_orders\n",
    "# COMMENT \"Raw orders data from CSV source\"\n",
    "# AS\n",
    "# SELECT * FROM csv.`/Volumes/main/default/kion_data/orders/*.csv`\n",
    "\n",
    "# CREATE OR REFRESH LIVE TABLE cleaned_orders\n",
    "# COMMENT \"Cleaned orders with data quality checks\"\n",
    "# AS\n",
    "# SELECT \n",
    "#   *,\n",
    "#   CAST(order_date AS DATE) as order_date,\n",
    "#   CURRENT_TIMESTAMP() as processing_time\n",
    "# FROM LIVE.raw_orders\n",
    "# WHERE order_id IS NOT NULL AND amount > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8050ee5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Materialized Views vs Streaming Tables\n",
    "\n",
    "DLT oferuje dwa g≈Ç√≥wne typy tabel:\n",
    "\n",
    "### Materialized Views\n",
    "- **Batch processing**: przetwarzanie wsadowe\n",
    "- **Full refresh**: ka≈ºde uruchomienie przetwarza wszystkie dane\n",
    "- **Use case**: dane historyczne, agregacje, dimensionals\n",
    "\n",
    "### Streaming Tables\n",
    "- **Incremental processing**: tylko nowe dane\n",
    "- **Continuous updates**: append-only lub upsert\n",
    "- **Use case**: fact tables, real-time analytics, CDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f1e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad: Materialized View (batch)\n",
    "@dlt.table(\n",
    "    name=\"daily_sales_summary\",\n",
    "    comment=\"Daily aggregated sales - full refresh\"\n",
    ")\n",
    "def daily_sales_summary():\n",
    "    return (\n",
    "        dlt.read(\"cleaned_orders\")\n",
    "        .groupBy(\"order_date\")\n",
    "        .agg(\n",
    "            count(\"order_id\").alias(\"total_orders\"),\n",
    "            sum(\"amount\").alias(\"total_revenue\"),\n",
    "            avg(\"amount\").alias(\"avg_order_value\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad: Streaming Table (incremental)\n",
    "@dlt.table(\n",
    "    name=\"streaming_orders\",\n",
    "    comment=\"Streaming orders - incremental processing\"\n",
    ")\n",
    "def streaming_orders():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(\"/Volumes/main/default/kion_data/orders/\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9b24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming table z transformacjami\n",
    "@dlt.table(\n",
    "    name=\"silver_orders_stream\",\n",
    "    comment=\"Silver layer - streaming incremental\"\n",
    ")\n",
    "def silver_orders_stream():\n",
    "    return (\n",
    "        dlt.read_stream(\"streaming_orders\")  # read_stream dla streaming source\n",
    "        .filter(col(\"order_id\").isNotNull())\n",
    "        .withColumn(\"ingested_at\", current_timestamp())\n",
    "        .withColumn(\"year\", year(col(\"order_date\")))\n",
    "        .withColumn(\"month\", month(col(\"order_date\")))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5612d5",
   "metadata": {},
   "source": [
    "### Kiedy u≈ºywaƒá Materialized View vs Streaming Table?\n",
    "\n",
    "**Materialized View**:\n",
    "- Agregacje i raporty (Gold layer)\n",
    "- Dimensionale (np. produkty, klienci)\n",
    "- Ma≈Çe do ≈õrednich datasety\n",
    "- Potrzebujesz full refresh logiki\n",
    "\n",
    "**Streaming Table**:\n",
    "- Fact tables (transakcje, zdarzenia)\n",
    "- Real-time/near-real-time processing\n",
    "- Du≈ºe volumeny danych\n",
    "- CDC (Change Data Capture)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f83b9c",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Data Quality Expectations\n",
    "\n",
    "**Expectations** to deklaratywny spos√≥b definiowania regu≈Ç jako≈õci danych w DLT.\n",
    "\n",
    "### Trzy typy expectations:\n",
    "\n",
    "1. **WARN**: loguj naruszenia, ale zachowaj dane\n",
    "2. **DROP**: usu≈Ñ wiersze naruszajƒÖce regu≈Çƒô\n",
    "3. **FAIL**: zatrzymaj pipeline przy naruszeniu\n",
    "\n",
    "### Sk≈Çadnia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad667864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1: WARN - logowanie narusze≈Ñ\n",
    "@dlt.table(\n",
    "    name=\"orders_with_quality_checks\"\n",
    ")\n",
    "@dlt.expect(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@dlt.expect(\"positive_amount\", \"amount > 0\")\n",
    "def orders_with_quality_checks():\n",
    "    return dlt.read(\"raw_orders\")\n",
    "\n",
    "# Naruszenia sƒÖ logowane w Event Log, ale dane przep≈ÇywajƒÖ dalej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 2: DROP - usuwanie z≈Çych wierszy\n",
    "@dlt.table(\n",
    "    name=\"clean_orders\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"positive_amount\", \"amount > 0\")\n",
    "@dlt.expect_or_drop(\"valid_date\", \"order_date IS NOT NULL\")\n",
    "def clean_orders():\n",
    "    return dlt.read(\"raw_orders\")\n",
    "\n",
    "# Wiersze niespe≈ÇniajƒÖce expectations sƒÖ automatycznie usuwane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e768f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 3: FAIL - zatrzymanie pipeline\n",
    "@dlt.table(\n",
    "    name=\"critical_orders\"\n",
    ")\n",
    "@dlt.expect_or_fail(\"no_nulls_in_key\", \"order_id IS NOT NULL AND customer_id IS NOT NULL\")\n",
    "def critical_orders():\n",
    "    return dlt.read(\"raw_orders\")\n",
    "\n",
    "# Pipeline zatrzyma siƒô, je≈õli jakikolwiek wiersz naruszy regu≈Çƒô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962414c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 4: Z≈Ço≈ºone expectations\n",
    "@dlt.table(\n",
    "    name=\"validated_orders\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"realistic_amount\", \"amount BETWEEN 1 AND 1000000\")\n",
    "@dlt.expect_or_drop(\"valid_status\", \"status IN ('pending', 'completed', 'cancelled')\")\n",
    "@dlt.expect_or_drop(\"recent_date\", \"order_date >= '2020-01-01'\")\n",
    "@dlt.expect(\"preferred_customer\", \"customer_id IN (SELECT customer_id FROM LIVE.vip_customers)\")\n",
    "def validated_orders():\n",
    "    return (\n",
    "        dlt.read(\"raw_orders\")\n",
    "        .withColumn(\"validation_timestamp\", current_timestamp())\n",
    "    )\n",
    "\n",
    "# Kombinacja DROP (krytyczne) i WARN (informacyjne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b894c",
   "metadata": {},
   "source": [
    "### Best Practices dla Expectations:\n",
    "\n",
    "1. **U≈ºywaj FAIL tylko dla krytycznych warunk√≥w**: np. schema mismatch\n",
    "2. **DROP dla data quality issues**: np. nulls, invalid values\n",
    "3. **WARN dla business logic**: np. suspicious patterns\n",
    "4. **Monitoruj Event Log**: regularnie sprawdzaj metryki jako≈õci\n",
    "5. **Nazewnictwo expectations**: u≈ºywaj czytelnych nazw opisujƒÖcych regu≈Çƒô\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7247819",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Event Log i Lineage\n",
    "\n",
    "### Event Log\n",
    "\n",
    "Ka≈ºdy DLT pipeline generuje **Event Log** - szczeg√≥≈Çowy dziennik wszystkich operacji:\n",
    "- Czas wykonania ka≈ºdej tabeli\n",
    "- Liczba przetworzonych wierszy\n",
    "- Naruszenia expectations\n",
    "- Errors i warnings\n",
    "- Resource usage (CPU, memory)\n",
    "\n",
    "Event Log jest dostƒôpny przez:\n",
    "1. **DLT Pipeline UI**: graficzny interfejs\n",
    "2. **Event Log Table**: delta table z metadanymi\n",
    "\n",
    "### Zapytanie Event Log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7834039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event log jest zapisywany jako Delta Table\n",
    "# Lokalizacja: system.event_log.<pipeline_id>\n",
    "\n",
    "# Przyk≈Çadowe zapytanie:\n",
    "event_log_df = spark.read.table(\"system.event_log.kion_dlt_pipeline\")\n",
    "\n",
    "# Filtrowanie po typie eventu\n",
    "quality_events = event_log_df.filter(col(\"event_type\") == \"data_quality\")\n",
    "quality_events.display()\n",
    "\n",
    "# Statystyki jako≈õci danych\n",
    "quality_summary = (\n",
    "    quality_events\n",
    "    .groupBy(\"dataset\", \"expectation\")\n",
    "    .agg(\n",
    "        sum(\"passed_records\").alias(\"total_passed\"),\n",
    "        sum(\"failed_records\").alias(\"total_failed\")\n",
    "    )\n",
    ")\n",
    "quality_summary.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e093e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring flow metrics\n",
    "flow_progress = (\n",
    "    event_log_df\n",
    "    .filter(col(\"event_type\") == \"flow_progress\")\n",
    "    .select(\n",
    "        \"timestamp\",\n",
    "        \"dataset\",\n",
    "        \"num_output_rows\",\n",
    "        \"execution_duration\"\n",
    "    )\n",
    "    .orderBy(desc(\"timestamp\"))\n",
    ")\n",
    "flow_progress.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1858e255",
   "metadata": {},
   "source": [
    "### Data Lineage\n",
    "\n",
    "DLT automatycznie ≈õledzi **lineage** - relacje miƒôdzy tabelami:\n",
    "- Kt√≥re tabele sƒÖ ≈∫r√≥d≈Çami (upstream)\n",
    "- Kt√≥re tabele sƒÖ celami (downstream)\n",
    "- Jak dane przep≈ÇywajƒÖ przez pipeline\n",
    "\n",
    "**Lineage jest widoczny w**:\n",
    "1. **DLT Pipeline Graph**: wizualizacja zale≈ºno≈õci\n",
    "2. **Unity Catalog**: end-to-end lineage\n",
    "3. **System tables**: metadata queries\n",
    "\n",
    "### Przyk≈Çad lineage query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3b794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineage z Unity Catalog system tables\n",
    "lineage_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        source_table_full_name,\n",
    "        target_table_full_name,\n",
    "        source_type,\n",
    "        created_at\n",
    "    FROM system.access.table_lineage\n",
    "    WHERE target_table_full_name LIKE '%kion_dlt%'\n",
    "    ORDER BY created_at DESC\n",
    "\"\"\")\n",
    "lineage_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51498211",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Automatic Orchestration\n",
    "\n",
    "DLT automatycznie zarzƒÖdza:\n",
    "1. **Dependency resolution**: wykrywa kolejno≈õƒá wykonania\n",
    "2. **Parallelization**: wykonuje niezale≈ºne tabele r√≥wnolegle\n",
    "3. **Retry logic**: automatyczne retry przy b≈Çƒôdach\n",
    "4. **Checkpointing**: dla streaming tables\n",
    "\n",
    "### Konfiguracja Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguracja DLT Pipeline (JSON configuration)\n",
    "pipeline_config = {\n",
    "    \"name\": \"KION_Orders_DLT_Pipeline\",\n",
    "    \"storage\": \"/mnt/dlt/kion_orders\",\n",
    "    \"target\": \"kion_dlt_db\",\n",
    "    \"notebooks\": [\n",
    "        {\n",
    "            \"path\": \"/Workspace/KION/dlt_orders_bronze\"\n",
    "        },\n",
    "        {\n",
    "            \"path\": \"/Workspace/KION/dlt_orders_silver\"\n",
    "        },\n",
    "        {\n",
    "            \"path\": \"/Workspace/KION/dlt_orders_gold\"\n",
    "        }\n",
    "    ],\n",
    "    \"configuration\": {\n",
    "        \"source_path\": \"/Volumes/main/default/kion_data\",\n",
    "        \"pipeline.maxParallelTables\": \"4\"\n",
    "    },\n",
    "    \"clusters\": [\n",
    "        {\n",
    "            \"label\": \"default\",\n",
    "            \"num_workers\": 2,\n",
    "            \"node_type_id\": \"Standard_DS3_v2\"\n",
    "        }\n",
    "    ],\n",
    "    \"continuous\": False,  # False = triggered mode, True = continuous\n",
    "    \"development\": True   # True = development mode (full refresh ka≈ºde uruchomienie)\n",
    "}\n",
    "\n",
    "print(\"DLT Pipeline configuration ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb4241",
   "metadata": {},
   "source": [
    "### Modes of Execution:\n",
    "\n",
    "**Development Mode**:\n",
    "- Reuse cluster between runs\n",
    "- Automatic full refresh\n",
    "- Szybkie iteracje\n",
    "- U≈ºywaj podczas developmentu\n",
    "\n",
    "**Production Mode**:\n",
    "- New cluster per run\n",
    "- Incremental processing\n",
    "- Cost-optimized\n",
    "- U≈ºywaj w produkcji\n",
    "\n",
    "**Triggered vs Continuous**:\n",
    "- **Triggered**: on-demand lub scheduled\n",
    "- **Continuous**: always running, minimal latency\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adc13f",
   "metadata": {},
   "source": [
    "## üî® Kompletny przyk≈Çad: Bronze ‚Üí Silver ‚Üí Gold DLT Pipeline\n",
    "\n",
    "### Pipeline Architecture:\n",
    "```\n",
    "raw_orders (CSV) \n",
    "    ‚Üì\n",
    "bronze_orders (Raw + Audit)\n",
    "    ‚Üì\n",
    "silver_orders (Cleaned + Validated)\n",
    "    ‚Üì\n",
    "gold_daily_sales (Aggregated)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c00c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# BRONZE LAYER\n",
    "@dlt.table(\n",
    "    name=\"bronze_orders\",\n",
    "    comment=\"Bronze: Raw orders with audit columns\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\",\n",
    "        \"pipelines.autoOptimize.zOrderCols\": \"order_date\"\n",
    "    }\n",
    ")\n",
    "def bronze_orders():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(\"/Volumes/main/default/kion_data/orders/\")\n",
    "        .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "        .withColumn(\"source_file\", input_file_name())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b6783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SILVER LAYER\n",
    "@dlt.table(\n",
    "    name=\"silver_orders\",\n",
    "    comment=\"Silver: Cleaned and validated orders\",\n",
    "    table_properties={\n",
    "        \"quality\": \"silver\"\n",
    "    }\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_customer_id\", \"customer_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"positive_amount\", \"amount > 0\")\n",
    "@dlt.expect_or_drop(\"valid_date\", \"order_date IS NOT NULL AND order_date >= '2020-01-01'\")\n",
    "@dlt.expect(\"reasonable_amount\", \"amount < 1000000\")\n",
    "def silver_orders():\n",
    "    return (\n",
    "        dlt.read_stream(\"bronze_orders\")\n",
    "        .select(\n",
    "            col(\"order_id\").cast(\"int\"),\n",
    "            col(\"customer_id\").cast(\"int\"),\n",
    "            to_date(col(\"order_date\")).alias(\"order_date\"),\n",
    "            col(\"product_id\").cast(\"int\"),\n",
    "            col(\"quantity\").cast(\"int\"),\n",
    "            col(\"amount\").cast(\"double\"),\n",
    "            lower(trim(col(\"status\"))).alias(\"status\"),\n",
    "            col(\"ingestion_timestamp\")\n",
    "        )\n",
    "        .withColumn(\"year\", year(col(\"order_date\")))\n",
    "        .withColumn(\"month\", month(col(\"order_date\")))\n",
    "        .withColumn(\"quarter\", quarter(col(\"order_date\")))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facfe45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOLD LAYER - Aggregated daily sales\n",
    "@dlt.table(\n",
    "    name=\"gold_daily_sales\",\n",
    "    comment=\"Gold: Daily sales aggregations\",\n",
    "    table_properties={\n",
    "        \"quality\": \"gold\"\n",
    "    }\n",
    ")\n",
    "def gold_daily_sales():\n",
    "    return (\n",
    "        dlt.read(\"silver_orders\")\n",
    "        .groupBy(\"order_date\", \"year\", \"month\", \"quarter\")\n",
    "        .agg(\n",
    "            count(\"order_id\").alias(\"total_orders\"),\n",
    "            countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "            sum(\"amount\").alias(\"total_revenue\"),\n",
    "            avg(\"amount\").alias(\"avg_order_value\"),\n",
    "            max(\"amount\").alias(\"max_order_value\"),\n",
    "            sum(\"quantity\").alias(\"total_quantity\")\n",
    "        )\n",
    "        .withColumn(\"calculated_at\", current_timestamp())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447f53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOLD LAYER - Customer lifetime value\n",
    "@dlt.table(\n",
    "    name=\"gold_customer_ltv\",\n",
    "    comment=\"Gold: Customer lifetime value metrics\"\n",
    ")\n",
    "def gold_customer_ltv():\n",
    "    return (\n",
    "        dlt.read(\"silver_orders\")\n",
    "        .groupBy(\"customer_id\")\n",
    "        .agg(\n",
    "            count(\"order_id\").alias(\"total_orders\"),\n",
    "            sum(\"amount\").alias(\"lifetime_value\"),\n",
    "            avg(\"amount\").alias(\"avg_order_value\"),\n",
    "            min(\"order_date\").alias(\"first_order_date\"),\n",
    "            max(\"order_date\").alias(\"last_order_date\"),\n",
    "            datediff(max(\"order_date\"), min(\"order_date\")).alias(\"customer_age_days\")\n",
    "        )\n",
    "        .withColumn(\"customer_segment\",\n",
    "            when(col(\"lifetime_value\") > 10000, \"VIP\")\n",
    "            .when(col(\"lifetime_value\") > 5000, \"High Value\")\n",
    "            .when(col(\"lifetime_value\") > 1000, \"Medium Value\")\n",
    "            .otherwise(\"Low Value\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd152da8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Monitoring i Troubleshooting\n",
    "\n",
    "### Sprawdzanie statusu pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef14e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Event Log dla b≈Çƒôd√≥w\n",
    "errors_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        timestamp,\n",
    "        level,\n",
    "        dataset,\n",
    "        message\n",
    "    FROM event_log(system.event_log.kion_dlt_pipeline)\n",
    "    WHERE level = 'ERROR'\n",
    "    ORDER BY timestamp DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "errors_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e69577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality violations\n",
    "quality_violations = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        dataset,\n",
    "        expectation,\n",
    "        SUM(failed_records) as total_failures,\n",
    "        SUM(passed_records) as total_passed,\n",
    "        ROUND(SUM(failed_records) * 100.0 / (SUM(failed_records) + SUM(passed_records)), 2) as failure_rate_pct\n",
    "    FROM event_log(system.event_log.kion_dlt_pipeline)\n",
    "    WHERE event_type = 'data_quality'\n",
    "    GROUP BY dataset, expectation\n",
    "    HAVING SUM(failed_records) > 0\n",
    "    ORDER BY failure_rate_pct DESC\n",
    "\"\"\")\n",
    "quality_violations.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436117b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline execution time trends\n",
    "execution_trends = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        date_trunc('hour', timestamp) as execution_hour,\n",
    "        dataset,\n",
    "        AVG(execution_duration / 1000) as avg_execution_seconds,\n",
    "        SUM(num_output_rows) as total_rows_processed\n",
    "    FROM event_log(system.event_log.kion_dlt_pipeline)\n",
    "    WHERE event_type = 'flow_progress'\n",
    "    GROUP BY execution_hour, dataset\n",
    "    ORDER BY execution_hour DESC, dataset\n",
    "\"\"\")\n",
    "execution_trends.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf9a727",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Podsumowanie\n",
    "\n",
    "### Nauczy≈Çe≈õ siƒô:\n",
    "\n",
    "‚úÖ **Deklaratywne pipeline'y**: `@dlt.table()` API  \n",
    "‚úÖ **Materialized Views vs Streaming Tables**: batch vs incremental  \n",
    "‚úÖ **Data Quality Expectations**: warn / drop / fail  \n",
    "‚úÖ **Event Log**: monitoring i troubleshooting  \n",
    "‚úÖ **Lineage tracking**: automatyczne ≈õledzenie zale≈ºno≈õci  \n",
    "‚úÖ **Automatic Orchestration**: dependency resolution  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **DLT upraszcza ETL**: deklaratywna sk≈Çadnia, automatyczna orkiestracja\n",
    "2. **Quality first**: wbudowane expectations zapewniajƒÖ jako≈õƒá danych\n",
    "3. **Observability**: Event Log + Lineage = pe≈Çna widoczno≈õƒá\n",
    "4. **Streaming i Batch**: jeden framework dla obu paradygmat√≥w\n",
    "5. **Production-ready**: retry, checkpointing, monitoring out-of-the-box\n",
    "\n",
    "### Nastƒôpne kroki:\n",
    "- **Notebook 03**: Databricks Jobs Orchestration\n",
    "- **Workshop 02**: Hands-on DLT + Orchestration\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Dodatkowe zasoby\n",
    "\n",
    "- [Delta Live Tables Documentation](https://docs.databricks.com/delta-live-tables/index.html)\n",
    "- [DLT Best Practices](https://docs.databricks.com/delta-live-tables/best-practices.html)\n",
    "- [Event Log Reference](https://docs.databricks.com/delta-live-tables/observability.html)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
