{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1297fb2-7aa7-454e-884e-7d100f753890",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Lakeflow Spark Declarative Pipelines - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Zrozumienie deklaratywnego frameworku Lakeflow do budowy batch i streaming pipeline'ów oraz praktyczna implementacja Bronze→Silver→Gold z SQL API.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Koncepcje Lakeflow: deklaratywny sposób definicji pipeline'ów\n",
    "- SQL vs Python API (focus na SQL)\n",
    "- Materialized views / streaming tables\n",
    "- Expectations: warn / drop / fail (data quality)\n",
    "- Event log i lineage per tabela\n",
    "- Automatic orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b63fab-c778-461e-8e12-2bb40d6703e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 3 - Transformation & Governance\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    " - Databricks Runtime 16.4 LTS lub nowszy (zalecane: 17.3 LTS)\n",
    " - Unity Catalog włączony\n",
    " - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    " - Klaster: Standard lub Serverless Compute\n",
    " \n",
    "**Uwaga:** Ten notebook demonstruje **SQL API** dla Lakeflow SDP. Python API (`create_streaming_table()`, `table()`) jest alternatywą z tą samą funkcjonalnością.\n",
    "\n",
    "> **Aktualizacja (Czerwiec 2025):** Nazwa produktu zmieniła się z \"Delta Live Tables (DLT)\" na \"Lakeflow Spark Declarative Pipelines (SDP)\". Funkcjonalność pozostaje taka sama. Dodatkowo \"Databricks Jobs\" to teraz \"Lakeflow Jobs\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76ef371e-3db2-4942-b0e4-9690415b4d02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wstęp teoretyczny - Lakeflow Spark Declarative Pipelines\n",
    "\n",
    "**Cel sekcji:** Zrozumienie czym jest Lakeflow SDP i jak rewolucjonizuje budowę pipeline'ów ETL/ELT.\n",
    "\n",
    "---\n",
    "\n",
    "### Czym jest Lakeflow Spark Declarative Pipelines?\n",
    "\n",
    "**Lakeflow Spark Declarative Pipelines (SDP)** to deklaratywny framework do tworzenia batch i streaming data pipeline'ów w SQL i Python. Rozszerza Apache Spark Declarative Pipelines, działając na zoptymalizowanym Databricks Runtime.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│ TRADYCYJNY APPROACH (Procedural) │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│ 1. Napisz kod: df = spark.read.table(...) │\n",
    "│ 2. Napisz transformacje: df.filter().groupBy()... │\n",
    "│ 3. Napisz orchestration: if/else, try/catch, retry logic │\n",
    "│ 4. Napisz monitoring: log metrics, track failures │\n",
    "│ 5. Napisz quality checks: manual assertions │\n",
    "│ 6. Deploy: schedule w Jobs, zarządzaj dependencies │\n",
    "│ │\n",
    "│ = Setki linii kodu, manual orchestration, error handling │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "\n",
    " \n",
    "\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│ LAKEFLOW SDP (Declarative) │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│ 1. Zadeklaruj CO chcesz (WHAT): │\n",
    "│ CREATE OR REFRESH STREAMING TABLE bronze AS ... │\n",
    "│ CREATE OR REFRESH MATERIALIZED VIEW silver AS ... │\n",
    "│ CREATE OR REFRESH MATERIALIZED VIEW gold AS ... │\n",
    "│ │\n",
    "│ 2. Lakeflow automatycznie: │\n",
    "│ Orchestruje kolejność (dependency DAG) │\n",
    "│ Retry przy failures │\n",
    "│ Incremental processing │\n",
    "│ Monitoring (Event Log) │\n",
    "│ Data quality (expectations) │\n",
    "│ Lineage tracking │\n",
    "│ │\n",
    "│ = Kilkanaście linii SQL, zero orchestration code │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Kluczowe korzyści Lakeflow SDP\n",
    "\n",
    "**1. Automatic Orchestration**\n",
    "\n",
    "Lakeflow automatycznie:\n",
    "- Analizuje zależności między tabelami (kto czyta z kogo)\n",
    "- Buduje DAG (Directed Acyclic Graph)\n",
    "- Wykonuje w poprawnej kolejności z maksymalną paralelizacją\n",
    "- Retry na poziomie: task → flow → pipeline\n",
    "\n",
    "```sql\n",
    "-- Wystarczy zadeklarować:\n",
    "CREATE OR REFRESH MATERIALIZED VIEW silver AS \n",
    " SELECT * FROM bronze; -- Lakeflow wie: silver zależy od bronze\n",
    "\n",
    "CREATE OR REFRESH MATERIALIZED VIEW gold AS \n",
    " SELECT * FROM silver; -- Lakeflow wie: gold zależy od silver\n",
    "\n",
    "-- Execution order: bronze → silver → gold (automatic!)\n",
    "```\n",
    "\n",
    "**2. Declarative Processing**\n",
    "\n",
    "Deklaratywne API redukuje setki linii kodu do kilku:\n",
    "\n",
    "```sql\n",
    "-- Tradycyjnie (procedural):\n",
    "-- 1. Read source\n",
    "-- 2. Apply transformations\n",
    "-- 3. Handle schema evolution\n",
    "-- 4. Write to Delta\n",
    "-- 5. Error handling\n",
    "-- 6. Retry logic\n",
    "-- 7. Metrics logging\n",
    "-- = ~100+ lines of code\n",
    "\n",
    "-- Lakeflow (declarative):\n",
    "CREATE OR REFRESH STREAMING TABLE orders AS\n",
    " SELECT * FROM STREAM read_files('/path/to/orders');\n",
    "-- = 2 lines, wszystko powyższe automatyczne!\n",
    "```\n",
    "\n",
    "**3. Incremental Processing**\n",
    "\n",
    "Lakeflow przetwarza tylko nowe/zmienione dane:\n",
    "\n",
    "- **Streaming tables**: Append-only, każdy rekord raz\n",
    "- **Materialized views**: Incremental refresh (Databricks wykrywa zmiany w source)\n",
    "- **AUTO CDC**: Out-of-order events handling, SCD Type 1/2\n",
    "\n",
    "**4. Built-in Data Quality**\n",
    "\n",
    "Expectations = SQL constraints z flexible handling:\n",
    "\n",
    "```sql\n",
    "CREATE OR REFRESH STREAMING TABLE orders (\n",
    " CONSTRAINT valid_amount EXPECT (total_amount > 0) ON VIOLATION DROP ROW,\n",
    " CONSTRAINT valid_date EXPECT (order_date IS NOT NULL) ON VIOLATION FAIL UPDATE\n",
    ")\n",
    "AS SELECT * FROM ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Podstawowe pojęcia\n",
    "\n",
    "- **Lakeflow SDP**: Deklaratywny framework dla batch + streaming pipelines\n",
    "- **Flow**: Jednostka przetwarzania (Append, AUTO CDC, Materialized View)\n",
    "- **STREAMING TABLE**: Delta table dla streaming/incremental data (append-only, low-latency)\n",
    "- **MATERIALIZED VIEW**: Delta table z incremental refresh (batch, cache results)\n",
    "- **VIEW (temporary)**: Ephemeral, brak persist, zawsze recompute\n",
    "- **SINK**: Streaming target (Delta, Kafka, EventHub, custom Python)\n",
    "- **Pipeline**: Zbiór flows + tables + views + sinks (unit of deployment)\n",
    "- **Expectations**: Data quality constraints (warn/drop/fail)\n",
    "- **Event Log**: Delta table z metrykami, lineage, quality metrics\n",
    "\n",
    "**Dlaczego to ważne?**\n",
    "\n",
    "Lakeflow SDP eliminuje boilerplate code i pozwala skupić się na business logic zamiast orchestration. Deklaratywny model zapewnia:\n",
    "- **Separation of concerns**: CO (deklaracja) vs JAK (execution engine)\n",
    "- **Reusability**: Te same deklaracje w dev/test/prod\n",
    "- **Observability**: Event Log out-of-the-box\n",
    "- **Reliability**: Automatic retry i error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f44a05c2-05f0-4095-aa47-7994f88c0419",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Izolacja per użytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21b89975-086f-4205-ba28-b584620ec1de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba85972d-9ddb-4cc5-8a69-dde4ef27651e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b6bb78-bd75-4c53-b4be-25e2e1af8f2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# Ustaw katalog jako domyślny\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "\n",
    "# Ścieżki do danych źródłowych\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "# Wyświetl kontekst użytkownika\n",
    "display(spark.createDataFrame([\n",
    "    (\"Katalog\", CATALOG),\n",
    "    (\"Schema Bronze\", BRONZE_SCHEMA),\n",
    "    (\"Schema Silver\", SILVER_SCHEMA),\n",
    "    (\"Schema Gold\", GOLD_SCHEMA),\n",
    "    (\"Użytkownik\", raw_user),\n",
    "    (\"Orders path\", ORDERS_JSON),\n",
    "    (\"Customers path\", CUSTOMERS_CSV),\n",
    "    (\"Products path\", PRODUCTS_PARQUET)\n",
    "], [\"Parametr\", \"Wartość\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a93c5e5-5bac-45a3-802a-d3683fa75269",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 1: Koncepcje Lakeflow - Flows, Tables, Views\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Lakeflow SDP operuje na trzech kluczowych konceptach: **Flows** (jak dane przepływają), **Streaming Tables** (append-only targets), i **Materialized Views** (batch targets z incremental refresh).\n",
    "\n",
    "---\n",
    "\n",
    "### Flow Types (Typy przepływów danych)\n",
    "\n",
    "**Flow** to jednostka przetwarzania danych w Lakeflow - definiuje JAK dane przepływają od źródła do celu.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│ FLOW TYPES │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│ 1. APPEND FLOW │\n",
    "│ • Źródło: Append-only (files, Kafka, Kinesis, Delta) │\n",
    "│ • Semantyka: Streaming (continuous processing) │\n",
    "│ • Gwarancja: Exactly-once per rekord │\n",
    "│ • Latency: Low (seconds) │\n",
    "│ • Use case: Real-time ingest, log streaming │\n",
    "│ • Target: STREAMING TABLE │\n",
    "│ │\n",
    "│ Przykład SQL: │\n",
    "│ CREATE OR REFRESH STREAMING TABLE orders AS │\n",
    "│ SELECT * FROM STREAM read_files('/path'); │\n",
    "│ │\n",
    "│ 2. AUTO CDC FLOW │\n",
    "│ • Źródło: Change Data Capture (CDF-enabled Delta) │\n",
    "│ • Semantyka: Streaming z CDC operations │\n",
    "│ • Operations: INSERT, UPDATE, DELETE, TRUNCATE │\n",
    "│ • Sequencing: Out-of-order handling (automatic) │\n",
    "│ • SCD: Type 1 (update) lub Type 2 (history tracking) │\n",
    "│ • Use case: Sync z transactional DB, audit trail │\n",
    "│ • Target: STREAMING TABLE │\n",
    "│ │\n",
    "│ Przykład SQL: │\n",
    "│ AUTO CDC INTO target_table │\n",
    "│ FROM source_table │\n",
    "│ KEYS (user_id) │\n",
    "│ SEQUENCE BY timestamp │\n",
    "│ APPLY AS DELETE WHEN operation = 'DELETE'; │\n",
    "│ │\n",
    "│ 3. MATERIALIZED VIEW FLOW │\n",
    "│ • Źródło: Batch read (Delta tables, views) │\n",
    "│ • Semantyka: Batch (scheduled/triggered) │\n",
    "│ • Refresh: Incremental (tylko zmienione partitions) │\n",
    "│ • Cache: Wyniki persisted (performance) │\n",
    "│ • Recompute: Full przy schema changes lub explicit │\n",
    "│ • Use case: Aggregations, slow queries, BI dashboards │\n",
    "│ • Target: MATERIALIZED VIEW │\n",
    "│ │\n",
    "│ Przykład SQL: │\n",
    "│ CREATE OR REFRESH MATERIALIZED VIEW daily_summary AS │\n",
    "│ SELECT date, SUM(amount) FROM orders GROUP BY date; │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Kluczowe różnice:**\n",
    "\n",
    "| Flow Type | Processing | Source | Latency | Incremental | Use Case |\n",
    "|-----------|------------|--------|---------|-------------|----------|\n",
    "| **Append** | Streaming | Append-only | Seconds | (watermarks) | Real-time ingest |\n",
    "| **AUTO CDC** | Streaming | CDC events | Seconds | (sequencing) | DB sync, SCD |\n",
    "| **Materialized View** | Batch | Any Delta | Minutes | (smart refresh) | Aggregations, BI |\n",
    "\n",
    "---\n",
    "\n",
    "### STREAMING TABLE vs MATERIALIZED VIEW\n",
    "\n",
    "| Aspekt | STREAMING TABLE | MATERIALIZED VIEW |\n",
    "|--------|-----------------|-------------------|\n",
    "| **Semantyka** | Streaming (continuous) | Batch (scheduled/triggered) |\n",
    "| **Processing** | Exactly-once per rekord | Incremental refresh (zmienione dane) |\n",
    "| **Source** | `STREAM` keyword required | Batch read (no STREAM) |\n",
    "| **Latency** | Low (seconds) | Higher (minutes) |\n",
    "| **State** | Bounded (watermarks) | Stateless (recompute) |\n",
    "| **Joins** | Stream-snapshot (static dims) | Full recompute (always correct) |\n",
    "| **Use case** | Real-time ingest, CDC | Aggregations, slow queries |\n",
    "| **Schema evolution** | Limited (full refresh) | Flexible |\n",
    "\n",
    "**Kiedy używać:**\n",
    "- **STREAMING TABLE**: Ingest z files/Kafka, CDC, low-latency transformations\n",
    "- **MATERIALIZED VIEW**: Aggregations, joins z częstymi zmianami w dimensions, pre-compute slow queries\n",
    "\n",
    "---\n",
    "\n",
    "### VIEW (temporary)\n",
    "\n",
    "**VIEW** to ephemeral object - nie ma persist, zawsze recompute przy query.\n",
    "\n",
    "**Use cases:**\n",
    "- Intermediate transformations (reusable logic)\n",
    "- Data quality checks (nie publikuj do catalog)\n",
    "- Testing (nie zapisuj do Delta)\n",
    "\n",
    "```sql\n",
    "-- VIEW: nie zapisuje do Delta\n",
    "CREATE OR REFRESH VIEW temp_filtered AS\n",
    " SELECT * FROM bronze WHERE status = 'ACTIVE';\n",
    "\n",
    "-- Użyj w downstream table\n",
    "CREATE OR REFRESH MATERIALIZED VIEW silver AS\n",
    " SELECT * FROM temp_filtered;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Automatic Dependency Resolution (DAG)\n",
    "\n",
    "Lakeflow automatycznie buduje DAG z zależności:\n",
    "\n",
    "```sql\n",
    "-- Deklaracje (nie określasz kolejności):\n",
    "CREATE OR REFRESH STREAMING TABLE bronze AS ...;\n",
    "CREATE OR REFRESH MATERIALIZED VIEW silver AS SELECT * FROM bronze;\n",
    "CREATE OR REFRESH MATERIALIZED VIEW gold AS SELECT * FROM silver;\n",
    "\n",
    "-- Lakeflow execution order (automatic):\n",
    "-- 1. bronze (no dependencies)\n",
    "-- 2. silver (depends on bronze) [parallel if multiple silvers]\n",
    "-- 3. gold (depends on silver) [parallel if multiple golds]\n",
    "```\n",
    "\n",
    "**Key point:** Ty deklarujesz CO, Lakeflow decyduje JAK i KIEDY."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d221bda7-7e69-4ae1-9980-c98afff01f9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.1: STREAMING TABLE - Bronze Layer Ingest\n",
    "\n",
    "**Cel:** Demonstracja STREAMING TABLE dla real-time ingest z Auto Loader\n",
    "\n",
    "**Podejście:**\n",
    "1. Użyj `read_files()` dla Auto Loader (SQL API)\n",
    "2. `STREAM` keyword dla streaming semantics\n",
    "3. Zapis do STREAMING TABLE (append-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "594b11ff-a25f-4d82-957d-ad397f75102d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- Przykład 1.1 - STREAMING TABLE (Bronze Layer)\n",
    "-- UWAGA: Ten kod jest demonstracją składni. W production pipeline,\n",
    "-- uruchomiłbyś to jako część Lakeflow pipeline definition.\n",
    "\n",
    "-- Dla demonstracji w notebooku, użyjemy tradycyjnego podejścia\n",
    "-- z późniejszą konwersją na Lakeflow syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aabe110-6e8e-4cfc-9d4c-b3f1aba36489",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 1.1 - Bronze Layer (tradycyjne podejście dla demonstracji)\n",
    "# W production pipeline, użylibyśmy Lakeflow CREATE OR REFRESH STREAMING TABLE\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Bronze layer: wczytanie surowych danych z JSON (batch dla demo)\n",
    "orders_bronze_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    "    .withColumn(\"_bronze_ingest_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_bronze_source_file\", F.input_file_name())\n",
    "    .withColumn(\"_bronze_ingested_by\", F.lit(raw_user))\n",
    "    .withColumn(\"_bronze_version\", F.lit(1))\n",
    ")\n",
    "\n",
    "# Zapisz do Delta (Bronze table)\n",
    "bronze_table = \"orders_bronze\"\n",
    "(\n",
    "    orders_bronze_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(bronze_table)\n",
    ")\n",
    "\n",
    "# Podgląd\n",
    "display(spark.table(bronze_table).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "016f2855-b386-4fd5-8ea2-85a511dccde1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 2: Silver Layer - MATERIALIZED VIEW + Expectations\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Silver layer oczyszcza i waliduje dane z Bronze. MATERIALIZED VIEW zapewnia incremental refresh - przetwarza tylko zmienione dane. Expectations to wbudowane data quality constraints.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **MATERIALIZED VIEW**: Batch processing z incremental refresh\n",
    "- **Expectations**: SQL constraints z akcjami: EXPECT (warn), DROP ROW, FAIL UPDATE\n",
    "- **Data Quality Gates**: Walidacje między warstwami\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Deduplikacja po kluczu biznesowym\n",
    "- Walidacja: NOT NULL, ranges, business rules\n",
    "- Standardizacja: dates, text formats, type casting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a15b0a84-920c-4e59-8716-bdbc3e8913b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.1: MATERIALIZED VIEW z Expectations (Silver Layer)\n",
    "\n",
    "**Cel:** Demonstracja MATERIALIZED VIEW dla Silver z data quality constraints\n",
    "\n",
    "**Lakeflow SQL Syntax (Production):**\n",
    "\n",
    "```sql\n",
    "-- W production Lakeflow pipeline:\n",
    "CREATE OR REFRESH MATERIALIZED VIEW silver.orders_silver (\n",
    " -- Expectations: Data Quality Constraints\n",
    " CONSTRAINT valid_amount EXPECT (total_amount > 0) ON VIOLATION DROP ROW,\n",
    " CONSTRAINT valid_date EXPECT (order_datetime IS NOT NULL) ON VIOLATION DROP ROW,\n",
    " CONSTRAINT valid_ids EXPECT (order_id IS NOT NULL AND customer_id IS NOT NULL)\n",
    ")\n",
    "COMMENT 'Silver layer - oczyszczone zamówienia z quality checks'\n",
    "AS\n",
    "SELECT\n",
    " order_id,\n",
    " customer_id,\n",
    " product_id,\n",
    " store_id,\n",
    " to_date(order_datetime) AS order_date,\n",
    " to_timestamp(order_datetime) AS order_timestamp,\n",
    " quantity,\n",
    " unit_price,\n",
    " CAST(total_amount AS DECIMAL(10,2)) AS total_amount,\n",
    " UPPER(TRIM(payment_method)) AS payment_method,\n",
    " CASE \n",
    " WHEN total_amount > 0 THEN 'COMPLETED'\n",
    " ELSE 'UNKNOWN'\n",
    " END AS order_status,\n",
    " current_timestamp() AS _silver_processed_timestamp,\n",
    " 'VALID' AS _data_quality_flag\n",
    "FROM bronze.orders_bronze;\n",
    "```\n",
    "\n",
    "**Wyjaśnienie Expectations:**\n",
    "- **EXPECT (warn)**: Log violation, zachowaj rekord (domyślnie)\n",
    "- **ON VIOLATION DROP ROW**: Usuń invalid rekord\n",
    "- **ON VIOLATION FAIL UPDATE**: Przerwij pipeline przy violation (strict mode)\n",
    "\n",
    "**Implementacja tradycyjna (dla notebooka demo):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28c31e9d-dff0-45d2-8ab2-89e96d077364",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 2.1 - Silver Layer z data quality (tradycyjne podejście dla demo)\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {SILVER_SCHEMA}\")\n",
    "\n",
    "# Wczytaj dane z Bronze\n",
    "orders_bronze_df = spark.table(f\"{BRONZE_SCHEMA}.{bronze_table}\")\n",
    "\n",
    "# Silver transformations z walidacją (symulacja Expectations)\n",
    "orders_silver_df = (\n",
    "    orders_bronze_df\n",
    "    # Deduplikacja\n",
    "    .dropDuplicates([\"order_id\"])\n",
    "    # NOT NULL validation (DROP ROW equivalent)\n",
    "    .filter(F.col(\"order_id\").isNotNull())\n",
    "    .filter(F.col(\"customer_id\").isNotNull())\n",
    "    .filter(F.col(\"product_id\").isNotNull())\n",
    "    # Business rule validation\n",
    "    .filter(F.col(\"total_amount\") > 0)\n",
    "    .filter(F.col(\"order_datetime\").isNotNull())\n",
    "    # Standaryzacja\n",
    "    .withColumn(\"order_date\", F.to_date(F.col(\"order_datetime\")))\n",
    "    .withColumn(\"order_timestamp\", F.to_timestamp(F.col(\"order_datetime\")))\n",
    "    .withColumn(\"total_amount\", F.col(\"total_amount\").cast(\"decimal(10,2)\"))\n",
    "    .withColumn(\"payment_method\", F.upper(F.trim(F.col(\"payment_method\"))))\n",
    "    # Derived columns\n",
    "    .withColumn(\"order_status\", \n",
    "                F.when(F.col(\"total_amount\") > 0, \"COMPLETED\").otherwise(\"UNKNOWN\"))\n",
    "    # Silver metadata\n",
    "    .withColumn(\"_silver_processed_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_data_quality_flag\", F.lit(\"VALID\"))\n",
    ")\n",
    "\n",
    "# Quality metrics\n",
    "bronze_count = orders_bronze_df.count()\n",
    "silver_count = orders_silver_df.count()\n",
    "rejected_count = bronze_count - silver_count\n",
    "rejection_rate = (rejected_count / bronze_count * 100) if bronze_count > 0 else 0\n",
    "\n",
    "# Data Quality Metrics\n",
    "display(spark.createDataFrame([\n",
    "    (\"Bronze input\", bronze_count),\n",
    "    (\"Silver output\", silver_count),\n",
    "    (\"Rejected\", rejected_count),\n",
    "    (\"Rejection rate %\", round(rejection_rate, 2))\n",
    "], [\"Metryka\", \"Wartość\"]))\n",
    "\n",
    "# Zapisz do Silver schema\n",
    "silver_table = \"orders_silver\"\n",
    "(\n",
    "    orders_silver_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(silver_table)\n",
    ")\n",
    "\n",
    "# Sample Silver data\n",
    "display(spark.table(silver_table).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "100b5e45-bc21-43d1-8da6-cf4287a6106d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 3: Gold Layer - Business Aggregates\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Gold layer zawiera pre-aggregowane metryki biznesowe, denormalized tables i KPI. MATERIALIZED VIEW z incremental refresh zapewnia, że tylko zmienione partycje są przeliczane.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **Business-level aggregates**: Daily/Monthly summaries, KPIs\n",
    "- **Denormalization**: Pre-computed joins dla performance\n",
    "- **Incremental refresh**: Tylko affected partitions\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- BI dashboards (Power BI, Tableau)\n",
    "- Executive reporting\n",
    "- ML feature stores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddf6416f-11bc-4b6b-8a3b-7bd9c649a30f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.1: MATERIALIZED VIEW dla Gold (Daily Aggregates)\n",
    "\n",
    "**Cel:** Demonstracja Gold layer z business aggregates i KPI\n",
    "\n",
    "**Lakeflow SQL Syntax (Production):**\n",
    "\n",
    "```sql\n",
    "-- W production Lakeflow pipeline:\n",
    "CREATE OR REFRESH MATERIALIZED VIEW gold.daily_order_summary\n",
    "COMMENT 'Gold layer - dzienne podsumowanie zamówień (KPI)'\n",
    "AS\n",
    "SELECT\n",
    " order_date,\n",
    " order_status,\n",
    " -- Volume metrics\n",
    " COUNT(order_id) AS total_orders,\n",
    " COUNT(DISTINCT customer_id) AS unique_customers,\n",
    " -- Revenue metrics\n",
    " SUM(total_amount) AS total_revenue,\n",
    " AVG(total_amount) AS avg_order_value,\n",
    " MIN(total_amount) AS min_order_value,\n",
    " MAX(total_amount) AS max_order_value,\n",
    " -- Derived KPIs\n",
    " ROUND(SUM(total_amount) / COUNT(DISTINCT customer_id), 2) AS revenue_per_customer,\n",
    " -- Gold metadata\n",
    " current_timestamp() AS _gold_created_timestamp,\n",
    " 'DAILY' AS _gold_aggregation_level\n",
    "FROM silver.orders_silver\n",
    "GROUP BY order_date, order_status\n",
    "ORDER BY order_date DESC, order_status;\n",
    "```\n",
    "\n",
    "**Automatic Dependency:** Lakeflow wie, że `gold.daily_order_summary` depends on `silver.orders_silver` → automatic execution order!\n",
    "\n",
    "**Implementacja tradycyjna:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "431625e6-c7b6-4600-85d5-799bf7a4b9ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 3.1 - Gold Layer (Daily Aggregates)\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {GOLD_SCHEMA}\")\n",
    "\n",
    "# Wczytaj dane z Silver\n",
    "orders_silver_df = spark.table(f\"{SILVER_SCHEMA}.{silver_table}\")\n",
    "\n",
    "# Gold aggregation: Daily order summary z KPI\n",
    "daily_summary_df = (\n",
    "    orders_silver_df\n",
    "    .groupBy(\"order_date\", \"order_status\")\n",
    "    .agg(\n",
    "        # Volume metrics\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "        # Revenue metrics\n",
    "        F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "        F.min(\"total_amount\").alias(\"min_order_value\"),\n",
    "        F.max(\"total_amount\").alias(\"max_order_value\")\n",
    "    )\n",
    "    # Derived KPIs\n",
    "    .withColumn(\"revenue_per_customer\", \n",
    "                F.round(F.col(\"total_revenue\") / F.col(\"unique_customers\"), 2))\n",
    "    # Gold metadata\n",
    "    .withColumn(\"_gold_created_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_gold_aggregation_level\", F.lit(\"DAILY\"))\n",
    "    .orderBy(\"order_date\", \"order_status\")\n",
    ")\n",
    "\n",
    "# Summary statistics\n",
    "total_days = daily_summary_df.select(\"order_date\").distinct().count()\n",
    "total_orders_gold = daily_summary_df.agg(F.sum(\"total_orders\")).collect()[0][0]\n",
    "total_revenue_gold = daily_summary_df.agg(F.sum(\"total_revenue\")).collect()[0][0]\n",
    "\n",
    "# Gold layer summary\n",
    "display(spark.createDataFrame([\n",
    "    (\"Total days aggregated\", str(total_days)),\n",
    "    (\"Total orders\", f\"{total_orders_gold:,}\"),\n",
    "    (\"Total revenue\", f\"${total_revenue_gold:,.2f}\")\n",
    "], [\"Metryka\", \"Wartość\"]))\n",
    "\n",
    "# Zapisz do Gold schema\n",
    "gold_table = \"daily_order_summary\"\n",
    "(\n",
    "    daily_summary_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(gold_table)\n",
    ")\n",
    "\n",
    "# Sample Gold data (daily KPIs)\n",
    "display(spark.table(gold_table).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb5eaa33-42b4-42a5-849b-201efc47335e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 4: Event Log i Lineage\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Lakeflow automatycznie loguje wszystkie operacje do **Event Log** (Delta table). Event Log zawiera:\n",
    "- Flow progress (success/failure per tabela)\n",
    "- Data quality metrics (expectations violations)\n",
    "- Lineage tracking (source → target)\n",
    "- Performance metrics (duration, records processed)\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **Event Log**: Delta table w `system/events` (per pipeline)\n",
    "- **Flow types**: `flow_definition`, `flow_progress`, `expectation`, `user_action`\n",
    "- **Lineage**: Automatyczne śledzenie zależności Bronze → Silver → Gold\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Monitoring pipeline health\n",
    "- Debugging failures\n",
    "- Data quality reporting\n",
    "- Audit i compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c6f3181-889c-4ecf-97ad-58a61fee4bc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.1: Event Log - Monitoring i Lineage\n",
    "\n",
    "**Event Log Location:**\n",
    "```\n",
    "dbfs:/pipelines/<pipeline_id>/system/events\n",
    "```\n",
    "\n",
    "**Przykładowe queries Event Log (w production Lakeflow pipeline):**\n",
    "\n",
    "```python\n",
    "# 1. Query Event Log\n",
    "event_log_path = \"dbfs:/pipelines/<pipeline_id>/system/events\"\n",
    "event_log_df = spark.read.format(\"delta\").load(event_log_path)\n",
    "\n",
    "# 2. Flow progress per tabela\n",
    "flow_progress = (\n",
    " event_log_df\n",
    " .filter(\"event_type = 'flow_progress'\")\n",
    " .select(\"timestamp\", \"details.flow_name\", \"details.output_records\", \"details.status\")\n",
    ")\n",
    "\n",
    "# 3. Expectations violations (data quality metrics)\n",
    "expectations_df = (\n",
    " event_log_df\n",
    " .filter(\"event_type = 'expectation'\")\n",
    " .select(\n",
    " \"timestamp\",\n",
    " \"details.dataset\",\n",
    " \"details.name\",\n",
    " \"details.passed_records\",\n",
    " \"details.failed_records\"\n",
    " )\n",
    ")\n",
    "\n",
    "# 4. Lineage tracking\n",
    "lineage_df = (\n",
    " event_log_df\n",
    " .filter(\"event_type = 'flow_definition'\")\n",
    " .select(\"details.flow_name\", \"details.input_datasets\", \"details.output_dataset\")\n",
    ")\n",
    "```\n",
    "\n",
    "**Dla demo (bez production pipeline):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4464d23e-4b54-4c8d-9fb9-6c5ff7262691",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 4.1 - Lineage Tracking (symulacja dla demo)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LINEAGE TRACKING - Bronze → Silver → Gold\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Symulacja lineage metadata (w production: z Event Log)\n",
    "lineage_data = [\n",
    "    {\n",
    "        \"layer\": \"Bronze\",\n",
    "        \"table\": f\"{BRONZE_SCHEMA}.{bronze_table}\",\n",
    "        \"source\": ORDERS_JSON,\n",
    "        \"target_tables\": [f\"{SILVER_SCHEMA}.{silver_table}\"],\n",
    "        \"record_count\": spark.table(f\"{BRONZE_SCHEMA}.{bronze_table}\").count()\n",
    "    },\n",
    "    {\n",
    "        \"layer\": \"Silver\",\n",
    "        \"table\": f\"{SILVER_SCHEMA}.{silver_table}\",\n",
    "        \"source\": f\"{BRONZE_SCHEMA}.{bronze_table}\",\n",
    "        \"target_tables\": [f\"{GOLD_SCHEMA}.{gold_table}\"],\n",
    "        \"record_count\": spark.table(f\"{SILVER_SCHEMA}.{silver_table}\").count()\n",
    "    },\n",
    "    {\n",
    "        \"layer\": \"Gold\",\n",
    "        \"table\": f\"{GOLD_SCHEMA}.{gold_table}\",\n",
    "        \"source\": f\"{SILVER_SCHEMA}.{silver_table}\",\n",
    "        \"target_tables\": [\"BI Dashboards\", \"ML Models\"],\n",
    "        \"record_count\": spark.table(f\"{GOLD_SCHEMA}.{gold_table}\").count()\n",
    "    }\n",
    "]\n",
    "\n",
    "# Display lineage\n",
    "import pandas as pd\n",
    "lineage_df = pd.DataFrame(lineage_data)\n",
    "print(\"\\n[LINEAGE TABLE]\")\n",
    "print(lineage_df.to_string(index=False))\n",
    "\n",
    "# Data flow diagram\n",
    "print(\"\\n[DATA FLOW DIAGRAM]\")\n",
    "print(f\"\"\"\n",
    "Source Data (JSON)\n",
    "    │\n",
    "    ├─→ {BRONZE_SCHEMA}.{bronze_table} ({lineage_data[0]['record_count']} records)\n",
    "        │   [STREAMING TABLE - Append-only]\n",
    "        │\n",
    "        ├─→ {SILVER_SCHEMA}.{silver_table} ({lineage_data[1]['record_count']} records)\n",
    "            │   [MATERIALIZED VIEW - Validated + Cleaned]\n",
    "            │   [Quality: {rejection_rate:.2f}% rejection rate]\n",
    "            │\n",
    "            ├─→ {GOLD_SCHEMA}.{gold_table} ({lineage_data[2]['record_count']} aggregates)\n",
    "                    [MATERIALIZED VIEW - Business KPIs]\n",
    "                    ├─→ Power BI Dashboards\n",
    "                    └─→ ML Feature Store\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20b05740-734d-4a85-a826-2abb1a8004d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 5: SQL vs Python API\n",
    "\n",
    "**Wprowadzenie:**\n",
    "\n",
    "Lakeflow SDP oferuje dwa równoważne API: **SQL** i **Python**. Wybór zależy od preferencji zespołu i use case.\n",
    "\n",
    "### Porównanie składni\n",
    "\n",
    "| Aspekt | SQL | Python |\n",
    "|--------|-----|--------|\n",
    "| **STREAMING TABLE** | `CREATE OR REFRESH STREAMING TABLE` | `@dp.table()` |\n",
    "| **MATERIALIZED VIEW** | `CREATE OR REFRESH MATERIALIZED VIEW` | `@dp.materialized_view()` |\n",
    "| **VIEW** | `CREATE OR REFRESH VIEW` | `@dp.view()` / `@dp.temporary_view()` |\n",
    "| **Expectations** | `CONSTRAINT ... EXPECT ... ON VIOLATION` | `@dp.expect()`, `@dp.expect_or_drop()`, `@dp.expect_or_fail()` |\n",
    "| **Streaming read** | `FROM STREAM table` | `spark.readStream.table()` |\n",
    "\n",
    "---\n",
    "\n",
    "### Przykład: Ten sam pipeline w SQL i Python\n",
    "\n",
    "**SQL Approach:**\n",
    "\n",
    "```sql\n",
    "-- Bronze\n",
    "CREATE OR REFRESH STREAMING TABLE bronze.orders AS\n",
    "SELECT * FROM STREAM read_files('/path/orders', format => 'json');\n",
    "\n",
    "-- Silver\n",
    "CREATE OR REFRESH MATERIALIZED VIEW silver.orders (\n",
    " CONSTRAINT valid_amount EXPECT (total_amount > 0) ON VIOLATION DROP ROW\n",
    ")\n",
    "AS SELECT \n",
    " order_id, \n",
    " customer_id, \n",
    " CAST(total_amount AS DECIMAL(10,2)) AS total_amount\n",
    "FROM bronze.orders;\n",
    "\n",
    "-- Gold\n",
    "CREATE OR REFRESH MATERIALIZED VIEW gold.daily_summary AS\n",
    "SELECT \n",
    " DATE(order_date) AS date,\n",
    " SUM(total_amount) AS revenue\n",
    "FROM silver.orders\n",
    "GROUP BY DATE(order_date);\n",
    "```\n",
    "\n",
    "**Python Approach (equivalent):**\n",
    "\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Bronze\n",
    "@dp.table(comment=\"Bronze orders\")\n",
    "def orders_bronze():\n",
    " return (\n",
    " spark.readStream\n",
    " .format(\"cloudFiles\")\n",
    " .option(\"cloudFiles.format\", \"json\")\n",
    " .load(\"/path/orders\")\n",
    " )\n",
    "\n",
    "# Silver\n",
    "@dp.materialized_view(comment=\"Silver orders\")\n",
    "@dp.expect_or_drop(\"valid_amount\", \"total_amount > 0\")\n",
    "def orders_silver():\n",
    " return (\n",
    " spark.read.table(\"bronze.orders\")\n",
    " .select(\n",
    " \"order_id\",\n",
    " \"customer_id\",\n",
    " F.col(\"total_amount\").cast(\"decimal(10,2)\")\n",
    " )\n",
    " )\n",
    "\n",
    "# Gold\n",
    "@dp.materialized_view(comment=\"Gold daily summary\")\n",
    "def daily_summary():\n",
    " return (\n",
    " spark.read.table(\"silver.orders\")\n",
    " .groupBy(F.to_date(\"order_date\").alias(\"date\"))\n",
    " .agg(F.sum(\"total_amount\").alias(\"revenue\"))\n",
    " )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Kiedy używać SQL vs Python?\n",
    "\n",
    "**Użyj SQL jeśli:**\n",
    "- Zespół ma silne SQL skills\n",
    "- Proste transformacje (filters, aggregations)\n",
    "- Integracja z BI tools (SQL-native workflows)\n",
    "- Mniej metaprogramming\n",
    "\n",
    "**Użyj Python jeśli:**\n",
    "- Potrzebujesz loops / dynamic table creation\n",
    "- Complex transformacje (UDFs, window functions)\n",
    "- Integracja z ML workflows\n",
    "- Testing (unit tests dla transformacji)\n",
    "\n",
    "**Najlepiej:** Mieszaj! SQL dla prostych, Python dla złożonych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4147f5d-f780-4224-81ae-870895eff403",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### Projektowanie Pipeline'ów:\n",
    "\n",
    "**1. Separation of Concerns:**\n",
    "- Bronze: Tylko ingest + audit metadata (no business logic)\n",
    "- Silver: Data quality + standardization (no aggregations)\n",
    "- Gold: Business logic + aggregations (BI-ready)\n",
    "\n",
    "**2. Expectations Strategy:**\n",
    "- Bronze → Silver: NOT NULL, schema validation (DROP ROW)\n",
    "- Silver → Gold: Business rules, referential integrity (FAIL UPDATE dla critical)\n",
    "- Start z EXPECT (warn), potem tighten do DROP ROW po analizie\n",
    "\n",
    "**3. Incremental Processing:**\n",
    "- Używaj STREAMING TABLE dla append-only sources (files, Kafka)\n",
    "- Używaj MATERIALIZED VIEW dla batch z incremental refresh\n",
    "- Partition Silver/Gold po date dla incremental MERGE\n",
    "\n",
    "**4. Monitoring:**\n",
    "- Regularnie query Event Log dla expectations violations\n",
    "- Alert na spike w rejection rate (> 5%)\n",
    "- Dashboard per pipeline: throughput, latency, quality metrics\n",
    "\n",
    "### Wydajność:\n",
    "\n",
    "**5. Partitioning:**\n",
    "- Bronze: Minimal partitioning (bulk operations)\n",
    "- Silver: Partition po date lub region\n",
    "- Gold: Partition wg BI query patterns\n",
    "\n",
    "**6. Optimization:**\n",
    "- Włącz Auto Optimize dla Silver/Gold\n",
    "- ZORDER BY po często filtrowanych kolumnach\n",
    "- Vacuum regularnie (retention policy)\n",
    "\n",
    "### Governance:\n",
    "\n",
    "**7. Unity Catalog:**\n",
    "- Bronze/Silver/Gold jako osobne schemas\n",
    "- Różne access controls per warstwa\n",
    "- Service principals dla pipelines (nie user accounts)\n",
    "\n",
    "**8. Naming Conventions:**\n",
    "- `bronze_<entity>`, `silver_<entity>`, `gold_<metric>`\n",
    "- Comments per tabela (business context)\n",
    "- Expectations: opisowe nazwy (`valid_customer_email` zamiast `check1`)\n",
    "\n",
    "### Deployment:\n",
    "\n",
    "**9. CI/CD:**\n",
    "- Git repos dla pipeline code\n",
    "- Databricks Asset Bundles (DAB) dla deployment\n",
    "- Separate environments: dev → test → prod\n",
    "\n",
    "**10. Testing:**\n",
    "- Unit tests dla transformacji (pytest)\n",
    "- Integration tests end-to-end (sample data)\n",
    "- Data quality tests (Great Expectations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f10c6706-0824-4dd5-9b37-05c049561cb6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "### Co zostało osiągnięte:\n",
    "\n",
    " **Zrozumienie Lakeflow SDP:**\n",
    "- Deklaratywny framework dla batch + streaming pipelines\n",
    "- Automatic orchestration (DAG), retry, monitoring\n",
    "- Zero boilerplate code\n",
    "\n",
    " **Flow Types:**\n",
    "- STREAMING TABLE (append-only, low-latency)\n",
    "- MATERIALIZED VIEW (incremental refresh, batch)\n",
    "- VIEW (ephemeral, reusable logic)\n",
    "\n",
    " **Medallion Architecture z Lakeflow:**\n",
    "- Bronze: STREAMING TABLE z Auto Loader\n",
    "- Silver: MATERIALIZED VIEW z Expectations (data quality)\n",
    "- Gold: MATERIALIZED VIEW z business aggregates\n",
    "\n",
    " **Data Quality:**\n",
    "- Expectations: EXPECT (warn) / DROP ROW / FAIL UPDATE\n",
    "- Tracking metrics w Event Log\n",
    "- Automatic rejection rate calculation\n",
    "\n",
    " **Event Log & Lineage:**\n",
    "- Automatic logging do Delta table\n",
    "- Flow progress, expectations, lineage tracking\n",
    "- Query dla monitoring i debugging\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "\n",
    "1. **Deklaratywność eliminuje complexity**: `CREATE OR REFRESH` vs setki linii procedural code\n",
    "2. **Automatic orchestration**: Lakeflow buduje DAG i wykonuje w poprawnej kolejności\n",
    "3. **Incremental processing**: Tylko nowe/zmienione dane (performance!)\n",
    "4. **Built-in quality**: Expectations jako first-class citizens\n",
    "5. **Observability out-of-the-box**: Event Log dla monitoring\n",
    "\n",
    "### Quick Reference - Najważniejsze komendy:\n",
    "\n",
    "| Operacja | SQL | Python |\n",
    "|----------|-----|--------|\n",
    "| **Streaming table** | `CREATE OR REFRESH STREAMING TABLE` | `@dp.table()` |\n",
    "| **Materialized view** | `CREATE OR REFRESH MATERIALIZED VIEW` | `@dp.materialized_view()` |\n",
    "| **Expectations** | `CONSTRAINT ... EXPECT ... ON VIOLATION DROP ROW` | `@dp.expect_or_drop()` |\n",
    "| **Auto Loader** | `FROM STREAM read_files(...)` | `.format(\"cloudFiles\")` |\n",
    "| **Streaming read** | `FROM STREAM table` | `spark.readStream.table()` |\n",
    "\n",
    "### Następne kroki:\n",
    "\n",
    "- **Kolejny notebook**: 03_batch_streaming_load.ipynb - COPY INTO, Auto Loader deep dive\n",
    "- **Warsztat praktyczny**: 02_lakeflow_orchestration_workshop.ipynb\n",
    "- **Production deployment**: Databricks Jobs + Delta Live Tables UI\n",
    "- **Dokumentacja**: [Lakeflow Pipelines Docs](https://docs.databricks.com/aws/en/ldp/)\n",
    "\n",
    "### Zadanie domowe (opcjonalnie):\n",
    "\n",
    "Utwórz własny Lakeflow pipeline:\n",
    "1. Bronze: Wczytaj customers.csv z Auto Loader\n",
    "2. Silver: Waliduj email format, deduplikuj po customer_id\n",
    "3. Gold: Agreguj customers per region (daily counts)\n",
    "4. Dodaj Expectations dla email i phone number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1176f1b-d38e-4a0b-8b7b-1d2d9390606a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Czyszczenie zasobów\n",
    "\n",
    "Posprzątaj zasoby utworzone podczas notebooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9544be8f-4fdd-4b28-8705-db6415f13d2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Opcjonalne czyszczenie zasobów testowych\n",
    "# UWAGA: Uruchom tylko jeśli chcesz usunąć wszystkie utworzone dane\n",
    "\n",
    "# Usuń tabele Demo\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {BRONZE_SCHEMA}.{bronze_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {SILVER_SCHEMA}.{silver_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {GOLD_SCHEMA}.{gold_table}\")\n",
    "\n",
    "# Wyczyść cache\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "displayHTML(\"<p> Aby usunąć tabele, odkomentuj kod powyżej i uruchom komórkę</p>\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_lakeflow_pipelines",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
