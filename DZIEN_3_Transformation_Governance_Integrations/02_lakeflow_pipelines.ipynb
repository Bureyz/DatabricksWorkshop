{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a9b2e2",
   "metadata": {},
   "source": [
    "# Lakeflow Spark Declarative Pipelines - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Zrozumienie deklaratywnego frameworku Lakeflow do budowy batch i streaming pipeline'Ã³w oraz praktyczna implementacja Bronzeâ†’Silverâ†’Gold z SQL API.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Koncepcje Lakeflow: deklaratywny sposÃ³b definicji pipeline'Ã³w\n",
    "- SQL vs Python API (focus na SQL)\n",
    "- Materialized views / streaming tables\n",
    "- Expectations: warn / drop / fail (data quality)\n",
    "- Event log i lineage per tabela\n",
    "- Automatic orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cb9bb0",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **DzieÅ„ szkolenia**: DzieÅ„ 2 - Lakehouse & Delta Lake\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog wÅ‚Ä…czony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers\n",
    "  \n",
    "**Uwaga:** Ten notebook demonstruje **SQL API** dla Lakeflow pipelines. Python API (@dp.table, @dp.materialized_view) jest alternatywÄ… z tÄ… samÄ… funkcjonalnoÅ›ciÄ…."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e58772",
   "metadata": {},
   "source": [
    "## WstÄ™p teoretyczny - Lakeflow Spark Declarative Pipelines\n",
    "\n",
    "**Cel sekcji:** Zrozumienie czym jest Lakeflow SDP i jak rewolucjonizuje budowÄ™ pipeline'Ã³w ETL/ELT.\n",
    "\n",
    "---\n",
    "\n",
    "### Czym jest Lakeflow Spark Declarative Pipelines?\n",
    "\n",
    "**Lakeflow Spark Declarative Pipelines (SDP)** to deklaratywny framework do tworzenia batch i streaming data pipeline'Ã³w w SQL i Python. Rozszerza Apache Spark Declarative Pipelines, dziaÅ‚ajÄ…c na zoptymalizowanym Databricks Runtime.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         TRADYCYJNY APPROACH (Procedural)                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1. Napisz kod: df = spark.read.table(...)                  â”‚\n",
    "â”‚  2. Napisz transformacje: df.filter().groupBy()...          â”‚\n",
    "â”‚  3. Napisz orchestration: if/else, try/catch, retry logic  â”‚\n",
    "â”‚  4. Napisz monitoring: log metrics, track failures          â”‚\n",
    "â”‚  5. Napisz quality checks: manual assertions                â”‚\n",
    "â”‚  6. Deploy: schedule w Jobs, zarzÄ…dzaj dependencies         â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  = Setki linii kodu, manual orchestration, error handling   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "                            â¬‡ï¸\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚         LAKEFLOW SDP (Declarative)                           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1. Zadeklaruj CO chcesz (WHAT):                            â”‚\n",
    "â”‚     CREATE OR REFRESH STREAMING TABLE bronze AS ...         â”‚\n",
    "â”‚     CREATE OR REFRESH MATERIALIZED VIEW silver AS ...       â”‚\n",
    "â”‚     CREATE OR REFRESH MATERIALIZED VIEW gold AS ...         â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  2. Lakeflow automatycznie:                                  â”‚\n",
    "â”‚     âœ… Orchestruje kolejnoÅ›Ä‡ (dependency DAG)               â”‚\n",
    "â”‚     âœ… Retry przy failures                                   â”‚\n",
    "â”‚     âœ… Incremental processing                                â”‚\n",
    "â”‚     âœ… Monitoring (Event Log)                                â”‚\n",
    "â”‚     âœ… Data quality (expectations)                           â”‚\n",
    "â”‚     âœ… Lineage tracking                                      â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  = KilkanaÅ›cie linii SQL, zero orchestration code           â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Kluczowe korzyÅ›ci Lakeflow SDP\n",
    "\n",
    "**1. Automatic Orchestration**\n",
    "\n",
    "Lakeflow automatycznie:\n",
    "- Analizuje zaleÅ¼noÅ›ci miÄ™dzy tabelami (kto czyta z kogo)\n",
    "- Buduje DAG (Directed Acyclic Graph)\n",
    "- Wykonuje w poprawnej kolejnoÅ›ci z maksymalnÄ… paralelizacjÄ…\n",
    "- Retry na poziomie: task â†’ flow â†’ pipeline\n",
    "\n",
    "```sql\n",
    "-- Wystarczy zadeklarowaÄ‡:\n",
    "CREATE OR REFRESH MATERIALIZED VIEW silver AS \n",
    "  SELECT * FROM bronze;  -- Lakeflow wie: silver zaleÅ¼y od bronze\n",
    "\n",
    "CREATE OR REFRESH MATERIALIZED VIEW gold AS \n",
    "  SELECT * FROM silver;  -- Lakeflow wie: gold zaleÅ¼y od silver\n",
    "\n",
    "-- Execution order: bronze â†’ silver â†’ gold (automatic!)\n",
    "```\n",
    "\n",
    "**2. Declarative Processing**\n",
    "\n",
    "Deklaratywne API redukuje setki linii kodu do kilku:\n",
    "\n",
    "```sql\n",
    "-- Tradycyjnie (procedural):\n",
    "-- 1. Read source\n",
    "-- 2. Apply transformations\n",
    "-- 3. Handle schema evolution\n",
    "-- 4. Write to Delta\n",
    "-- 5. Error handling\n",
    "-- 6. Retry logic\n",
    "-- 7. Metrics logging\n",
    "-- = ~100+ lines of code\n",
    "\n",
    "-- Lakeflow (declarative):\n",
    "CREATE OR REFRESH STREAMING TABLE orders AS\n",
    "  SELECT * FROM STREAM read_files('/path/to/orders');\n",
    "-- = 2 lines, wszystko powyÅ¼sze automatyczne!\n",
    "```\n",
    "\n",
    "**3. Incremental Processing**\n",
    "\n",
    "Lakeflow przetwarza tylko nowe/zmienione dane:\n",
    "\n",
    "- **Streaming tables**: Append-only, kaÅ¼dy rekord raz\n",
    "- **Materialized views**: Incremental refresh (Databricks wykrywa zmiany w source)\n",
    "- **AUTO CDC**: Out-of-order events handling, SCD Type 1/2\n",
    "\n",
    "**4. Built-in Data Quality**\n",
    "\n",
    "Expectations = SQL constraints z flexible handling:\n",
    "\n",
    "```sql\n",
    "CREATE OR REFRESH STREAMING TABLE orders (\n",
    "  CONSTRAINT valid_amount EXPECT (total_amount > 0) ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_date EXPECT (order_date IS NOT NULL) ON VIOLATION FAIL UPDATE\n",
    ")\n",
    "AS SELECT * FROM ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Podstawowe pojÄ™cia\n",
    "\n",
    "- **Lakeflow SDP**: Deklaratywny framework dla batch + streaming pipelines\n",
    "- **Flow**: Jednostka przetwarzania (Append, AUTO CDC, Materialized View)\n",
    "- **STREAMING TABLE**: Delta table dla streaming/incremental data (append-only, low-latency)\n",
    "- **MATERIALIZED VIEW**: Delta table z incremental refresh (batch, cache results)\n",
    "- **VIEW (temporary)**: Ephemeral, brak persist, zawsze recompute\n",
    "- **SINK**: Streaming target (Delta, Kafka, EventHub, custom Python)\n",
    "- **Pipeline**: ZbiÃ³r flows + tables + views + sinks (unit of deployment)\n",
    "- **Expectations**: Data quality constraints (warn/drop/fail)\n",
    "- **Event Log**: Delta table z metrykami, lineage, quality metrics\n",
    "\n",
    "**Dlaczego to waÅ¼ne?**\n",
    "\n",
    "Lakeflow SDP eliminuje boilerplate code i pozwala skupiÄ‡ siÄ™ na business logic zamiast orchestration. Deklaratywny model zapewnia:\n",
    "- **Separation of concerns**: CO (deklaracja) vs JAK (execution engine)\n",
    "- **Reusability**: Te same deklaracje w dev/test/prod\n",
    "- **Observability**: Event Log out-of-the-box\n",
    "- **Reliability**: Automatic retry i error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab02b8",
   "metadata": {},
   "source": [
    "## Izolacja per uÅ¼ytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogÃ³w i schematÃ³w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d722c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea6b4e",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych Å›rodowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fca3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "\n",
    "# WyÅ›wietl kontekst uÅ¼ytkownika\n",
    "print(\"=== Kontekst uÅ¼ytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"UÅ¼ytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog jako domyÅ›lny\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "\n",
    "# ÅšcieÅ¼ki do danych ÅºrÃ³dÅ‚owych\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "print(f\"\\n=== ÅšcieÅ¼ki do danych ===\")\n",
    "print(f\"Orders: {ORDERS_JSON}\")\n",
    "print(f\"Customers: {CUSTOMERS_CSV}\")\n",
    "print(f\"Products: {PRODUCTS_PARQUET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5287c6a2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 1: Koncepcje Lakeflow - Flows, Tables, Views\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Lakeflow SDP operuje na trzech kluczowych konceptach: **Flows** (jak dane przepÅ‚ywajÄ…), **Streaming Tables** (append-only targets), i **Materialized Views** (batch targets z incremental refresh).\n",
    "\n",
    "---\n",
    "\n",
    "### Flow Types (Typy przepÅ‚ywÃ³w danych)\n",
    "\n",
    "**Flow** to jednostka przetwarzania danych w Lakeflow - definiuje JAK dane przepÅ‚ywajÄ… od ÅºrÃ³dÅ‚a do celu.\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    FLOW TYPES                                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  1. APPEND FLOW                                              â”‚\n",
    "â”‚     â€¢ Å¹rÃ³dÅ‚o: Append-only (files, Kafka, Kinesis, Delta)    â”‚\n",
    "â”‚     â€¢ Semantyka: Streaming (continuous processing)          â”‚\n",
    "â”‚     â€¢ Gwarancja: Exactly-once per rekord                     â”‚\n",
    "â”‚     â€¢ Latency: Low (seconds)                                 â”‚\n",
    "â”‚     â€¢ Use case: Real-time ingest, log streaming             â”‚\n",
    "â”‚     â€¢ Target: STREAMING TABLE                                â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚     PrzykÅ‚ad SQL:                                            â”‚\n",
    "â”‚     CREATE OR REFRESH STREAMING TABLE orders AS             â”‚\n",
    "â”‚       SELECT * FROM STREAM read_files('/path');             â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  2. AUTO CDC FLOW                                            â”‚\n",
    "â”‚     â€¢ Å¹rÃ³dÅ‚o: Change Data Capture (CDF-enabled Delta)       â”‚\n",
    "â”‚     â€¢ Semantyka: Streaming z CDC operations                 â”‚\n",
    "â”‚     â€¢ Operations: INSERT, UPDATE, DELETE, TRUNCATE          â”‚\n",
    "â”‚     â€¢ Sequencing: Out-of-order handling (automatic)         â”‚\n",
    "â”‚     â€¢ SCD: Type 1 (update) lub Type 2 (history tracking)    â”‚\n",
    "â”‚     â€¢ Use case: Sync z transactional DB, audit trail        â”‚\n",
    "â”‚     â€¢ Target: STREAMING TABLE                                â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚     PrzykÅ‚ad SQL:                                            â”‚\n",
    "â”‚     AUTO CDC INTO target_table                               â”‚\n",
    "â”‚       FROM source_table                                      â”‚\n",
    "â”‚       KEYS (user_id)                                         â”‚\n",
    "â”‚       SEQUENCE BY timestamp                                  â”‚\n",
    "â”‚       APPLY AS DELETE WHEN operation = 'DELETE';            â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚  3. MATERIALIZED VIEW FLOW                                   â”‚\n",
    "â”‚     â€¢ Å¹rÃ³dÅ‚o: Batch read (Delta tables, views)              â”‚\n",
    "â”‚     â€¢ Semantyka: Batch (scheduled/triggered)                â”‚\n",
    "â”‚     â€¢ Refresh: Incremental (tylko zmienione partitions)     â”‚\n",
    "â”‚     â€¢ Cache: Wyniki persisted (performance)                 â”‚\n",
    "â”‚     â€¢ Recompute: Full przy schema changes lub explicit      â”‚\n",
    "â”‚     â€¢ Use case: Aggregations, slow queries, BI dashboards   â”‚\n",
    "â”‚     â€¢ Target: MATERIALIZED VIEW                              â”‚\n",
    "â”‚                                                              â”‚\n",
    "â”‚     PrzykÅ‚ad SQL:                                            â”‚\n",
    "â”‚     CREATE OR REFRESH MATERIALIZED VIEW daily_summary AS    â”‚\n",
    "â”‚       SELECT date, SUM(amount) FROM orders GROUP BY date;   â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "**Kluczowe rÃ³Å¼nice:**\n",
    "\n",
    "| Flow Type | Processing | Source | Latency | Incremental | Use Case |\n",
    "|-----------|------------|--------|---------|-------------|----------|\n",
    "| **Append** | Streaming | Append-only | Seconds | âœ… (watermarks) | Real-time ingest |\n",
    "| **AUTO CDC** | Streaming | CDC events | Seconds | âœ… (sequencing) | DB sync, SCD |\n",
    "| **Materialized View** | Batch | Any Delta | Minutes | âœ… (smart refresh) | Aggregations, BI |\n",
    "\n",
    "---\n",
    "\n",
    "### STREAMING TABLE vs MATERIALIZED VIEW\n",
    "\n",
    "| Aspekt | STREAMING TABLE | MATERIALIZED VIEW |\n",
    "|--------|-----------------|-------------------|\n",
    "| **Semantyka** | Streaming (continuous) | Batch (scheduled/triggered) |\n",
    "| **Processing** | Exactly-once per rekord | Incremental refresh (zmienione dane) |\n",
    "| **Source** | `STREAM` keyword required | Batch read (no STREAM) |\n",
    "| **Latency** | Low (seconds) | Higher (minutes) |\n",
    "| **State** | Bounded (watermarks) | Stateless (recompute) |\n",
    "| **Joins** | Stream-snapshot (static dims) | Full recompute (always correct) |\n",
    "| **Use case** | Real-time ingest, CDC | Aggregations, slow queries |\n",
    "| **Schema evolution** | Limited (full refresh) | Flexible |\n",
    "\n",
    "**Kiedy uÅ¼ywaÄ‡:**\n",
    "- **STREAMING TABLE**: Ingest z files/Kafka, CDC, low-latency transformations\n",
    "- **MATERIALIZED VIEW**: Aggregations, joins z czÄ™stymi zmianami w dimensions, pre-compute slow queries\n",
    "\n",
    "---\n",
    "\n",
    "### VIEW (temporary)\n",
    "\n",
    "**VIEW** to ephemeral object - nie ma persist, zawsze recompute przy query.\n",
    "\n",
    "**Use cases:**\n",
    "- Intermediate transformations (reusable logic)\n",
    "- Data quality checks (nie publikuj do catalog)\n",
    "- Testing (nie zapisuj do Delta)\n",
    "\n",
    "```sql\n",
    "-- VIEW: nie zapisuje do Delta\n",
    "CREATE OR REFRESH VIEW temp_filtered AS\n",
    "  SELECT * FROM bronze WHERE status = 'ACTIVE';\n",
    "\n",
    "-- UÅ¼yj w downstream table\n",
    "CREATE OR REFRESH MATERIALIZED VIEW silver AS\n",
    "  SELECT * FROM temp_filtered;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Automatic Dependency Resolution (DAG)\n",
    "\n",
    "Lakeflow automatycznie buduje DAG z zaleÅ¼noÅ›ci:\n",
    "\n",
    "```sql\n",
    "-- Deklaracje (nie okreÅ›lasz kolejnoÅ›ci):\n",
    "CREATE OR REFRESH STREAMING TABLE bronze AS ...;\n",
    "CREATE OR REFRESH MATERIALIZED VIEW silver AS SELECT * FROM bronze;\n",
    "CREATE OR REFRESH MATERIALIZED VIEW gold AS SELECT * FROM silver;\n",
    "\n",
    "-- Lakeflow execution order (automatic):\n",
    "-- 1. bronze (no dependencies)\n",
    "-- 2. silver (depends on bronze) [parallel if multiple silvers]\n",
    "-- 3. gold (depends on silver) [parallel if multiple golds]\n",
    "```\n",
    "\n",
    "**Key point:** Ty deklarujesz CO, Lakeflow decyduje JAK i KIEDY."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595cb59b",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 1.1: STREAMING TABLE - Bronze Layer Ingest\n",
    "\n",
    "**Cel:** Demonstracja STREAMING TABLE dla real-time ingest z Auto Loader\n",
    "\n",
    "**PodejÅ›cie:**\n",
    "1. UÅ¼yj `read_files()` dla Auto Loader (SQL API)\n",
    "2. `STREAM` keyword dla streaming semantics\n",
    "3. Zapis do STREAMING TABLE (append-only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6017b787",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- PrzykÅ‚ad 1.1 - STREAMING TABLE (Bronze Layer)\n",
    "-- UWAGA: Ten kod jest demonstracjÄ… skÅ‚adni. W production pipeline,\n",
    "-- uruchomiÅ‚byÅ› to jako czÄ™Å›Ä‡ Lakeflow pipeline definition.\n",
    "\n",
    "-- Dla demonstracji w notebooku, uÅ¼yjemy tradycyjnego podejÅ›cia\n",
    "-- z pÃ³ÅºniejszÄ… konwersjÄ… na Lakeflow syntax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2acaaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrzykÅ‚ad 1.1 - Bronze Layer (tradycyjne podejÅ›cie dla demonstracji)\n",
    "# W production pipeline, uÅ¼ylibyÅ›my Lakeflow CREATE OR REFRESH STREAMING TABLE\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Bronze layer: wczytanie surowych danych z JSON (batch dla demo)\n",
    "orders_bronze_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    "    .withColumn(\"_bronze_ingest_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_bronze_source_file\", F.input_file_name())\n",
    "    .withColumn(\"_bronze_ingested_by\", F.lit(raw_user))\n",
    "    .withColumn(\"_bronze_version\", F.lit(1))\n",
    ")\n",
    "\n",
    "# Zapisz do Delta (Bronze table)\n",
    "bronze_table = \"orders_bronze\"\n",
    "(\n",
    "    orders_bronze_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(bronze_table)\n",
    ")\n",
    "\n",
    "print(f\"âœ… Bronze table utworzony: {BRONZE_SCHEMA}.{bronze_table}\")\n",
    "print(f\"   Records: {spark.table(bronze_table).count()}\")\n",
    "\n",
    "# PodglÄ…d\n",
    "display(spark.table(bronze_table).limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75496475",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 2: Silver Layer - MATERIALIZED VIEW + Expectations\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Silver layer oczyszcza i waliduje dane z Bronze. MATERIALIZED VIEW zapewnia incremental refresh - przetwarza tylko zmienione dane. Expectations to wbudowane data quality constraints.\n",
    "\n",
    "**Kluczowe pojÄ™cia:**\n",
    "- **MATERIALIZED VIEW**: Batch processing z incremental refresh\n",
    "- **Expectations**: SQL constraints z akcjami: EXPECT (warn), DROP ROW, FAIL UPDATE\n",
    "- **Data Quality Gates**: Walidacje miÄ™dzy warstwami\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Deduplikacja po kluczu biznesowym\n",
    "- Walidacja: NOT NULL, ranges, business rules\n",
    "- Standardizacja: dates, text formats, type casting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deef978",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 2.1: MATERIALIZED VIEW z Expectations (Silver Layer)\n",
    "\n",
    "**Cel:** Demonstracja MATERIALIZED VIEW dla Silver z data quality constraints\n",
    "\n",
    "**Lakeflow SQL Syntax (Production):**\n",
    "\n",
    "```sql\n",
    "-- W production Lakeflow pipeline:\n",
    "CREATE OR REFRESH MATERIALIZED VIEW silver.orders_silver (\n",
    "  -- Expectations: Data Quality Constraints\n",
    "  CONSTRAINT valid_amount EXPECT (total_amount > 0) ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_date EXPECT (order_datetime IS NOT NULL) ON VIOLATION DROP ROW,\n",
    "  CONSTRAINT valid_ids EXPECT (order_id IS NOT NULL AND customer_id IS NOT NULL)\n",
    ")\n",
    "COMMENT 'Silver layer - oczyszczone zamÃ³wienia z quality checks'\n",
    "AS\n",
    "SELECT\n",
    "  order_id,\n",
    "  customer_id,\n",
    "  product_id,\n",
    "  store_id,\n",
    "  to_date(order_datetime) AS order_date,\n",
    "  to_timestamp(order_datetime) AS order_timestamp,\n",
    "  quantity,\n",
    "  unit_price,\n",
    "  CAST(total_amount AS DECIMAL(10,2)) AS total_amount,\n",
    "  UPPER(TRIM(payment_method)) AS payment_method,\n",
    "  CASE \n",
    "    WHEN total_amount > 0 THEN 'COMPLETED'\n",
    "    ELSE 'UNKNOWN'\n",
    "  END AS order_status,\n",
    "  current_timestamp() AS _silver_processed_timestamp,\n",
    "  'VALID' AS _data_quality_flag\n",
    "FROM bronze.orders_bronze;\n",
    "```\n",
    "\n",
    "**WyjaÅ›nienie Expectations:**\n",
    "- **EXPECT (warn)**: Log violation, zachowaj rekord (domyÅ›lnie)\n",
    "- **ON VIOLATION DROP ROW**: UsuÅ„ invalid rekord\n",
    "- **ON VIOLATION FAIL UPDATE**: Przerwij pipeline przy violation (strict mode)\n",
    "\n",
    "**Implementacja tradycyjna (dla notebooka demo):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de267dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrzykÅ‚ad 2.1 - Silver Layer z data quality (tradycyjne podejÅ›cie dla demo)\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {SILVER_SCHEMA}\")\n",
    "\n",
    "# Wczytaj dane z Bronze\n",
    "orders_bronze_df = spark.table(f\"{BRONZE_SCHEMA}.{bronze_table}\")\n",
    "\n",
    "print(f\"=== Bronze input ===\")\n",
    "print(f\"Records: {orders_bronze_df.count()}\")\n",
    "\n",
    "# Silver transformations z walidacjÄ… (symulacja Expectations)\n",
    "orders_silver_df = (\n",
    "    orders_bronze_df\n",
    "    # Deduplikacja\n",
    "    .dropDuplicates([\"order_id\"])\n",
    "    # NOT NULL validation (DROP ROW equivalent)\n",
    "    .filter(F.col(\"order_id\").isNotNull())\n",
    "    .filter(F.col(\"customer_id\").isNotNull())\n",
    "    .filter(F.col(\"product_id\").isNotNull())\n",
    "    # Business rule validation\n",
    "    .filter(F.col(\"total_amount\") > 0)\n",
    "    .filter(F.col(\"order_datetime\").isNotNull())\n",
    "    # Standaryzacja\n",
    "    .withColumn(\"order_date\", F.to_date(F.col(\"order_datetime\")))\n",
    "    .withColumn(\"order_timestamp\", F.to_timestamp(F.col(\"order_datetime\")))\n",
    "    .withColumn(\"total_amount\", F.col(\"total_amount\").cast(\"decimal(10,2)\"))\n",
    "    .withColumn(\"payment_method\", F.upper(F.trim(F.col(\"payment_method\"))))\n",
    "    # Derived columns\n",
    "    .withColumn(\"order_status\", \n",
    "                F.when(F.col(\"total_amount\") > 0, \"COMPLETED\").otherwise(\"UNKNOWN\"))\n",
    "    # Silver metadata\n",
    "    .withColumn(\"_silver_processed_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_data_quality_flag\", F.lit(\"VALID\"))\n",
    ")\n",
    "\n",
    "# Quality metrics\n",
    "bronze_count = orders_bronze_df.count()\n",
    "silver_count = orders_silver_df.count()\n",
    "rejected_count = bronze_count - silver_count\n",
    "rejection_rate = (rejected_count / bronze_count * 100) if bronze_count > 0 else 0\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATA QUALITY METRICS (Silver Layer)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Bronze input:  {bronze_count:,} records\")\n",
    "print(f\"Silver output: {silver_count:,} records\")\n",
    "print(f\"Rejected:      {rejected_count:,} records ({rejection_rate:.2f}%)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Zapisz do Silver schema\n",
    "silver_table = \"orders_silver\"\n",
    "(\n",
    "    orders_silver_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(silver_table)\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Silver table utworzony: {SILVER_SCHEMA}.{silver_table}\")\n",
    "print(f\"\\n=== Sample Silver data ===\")\n",
    "display(spark.table(silver_table).limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05f3ab1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 3: Gold Layer - Business Aggregates\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Gold layer zawiera pre-aggregowane metryki biznesowe, denormalized tables i KPI. MATERIALIZED VIEW z incremental refresh zapewnia, Å¼e tylko zmienione partycje sÄ… przeliczane.\n",
    "\n",
    "**Kluczowe pojÄ™cia:**\n",
    "- **Business-level aggregates**: Daily/Monthly summaries, KPIs\n",
    "- **Denormalization**: Pre-computed joins dla performance\n",
    "- **Incremental refresh**: Tylko affected partitions\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- BI dashboards (Power BI, Tableau)\n",
    "- Executive reporting\n",
    "- ML feature stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948c880a",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 3.1: MATERIALIZED VIEW dla Gold (Daily Aggregates)\n",
    "\n",
    "**Cel:** Demonstracja Gold layer z business aggregates i KPI\n",
    "\n",
    "**Lakeflow SQL Syntax (Production):**\n",
    "\n",
    "```sql\n",
    "-- W production Lakeflow pipeline:\n",
    "CREATE OR REFRESH MATERIALIZED VIEW gold.daily_order_summary\n",
    "COMMENT 'Gold layer - dzienne podsumowanie zamÃ³wieÅ„ (KPI)'\n",
    "AS\n",
    "SELECT\n",
    "  order_date,\n",
    "  order_status,\n",
    "  -- Volume metrics\n",
    "  COUNT(order_id) AS total_orders,\n",
    "  COUNT(DISTINCT customer_id) AS unique_customers,\n",
    "  -- Revenue metrics\n",
    "  SUM(total_amount) AS total_revenue,\n",
    "  AVG(total_amount) AS avg_order_value,\n",
    "  MIN(total_amount) AS min_order_value,\n",
    "  MAX(total_amount) AS max_order_value,\n",
    "  -- Derived KPIs\n",
    "  ROUND(SUM(total_amount) / COUNT(DISTINCT customer_id), 2) AS revenue_per_customer,\n",
    "  -- Gold metadata\n",
    "  current_timestamp() AS _gold_created_timestamp,\n",
    "  'DAILY' AS _gold_aggregation_level\n",
    "FROM silver.orders_silver\n",
    "GROUP BY order_date, order_status\n",
    "ORDER BY order_date DESC, order_status;\n",
    "```\n",
    "\n",
    "**Automatic Dependency:** Lakeflow wie, Å¼e `gold.daily_order_summary` depends on `silver.orders_silver` â†’ automatic execution order!\n",
    "\n",
    "**Implementacja tradycyjna:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1237813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrzykÅ‚ad 3.1 - Gold Layer (Daily Aggregates)\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {GOLD_SCHEMA}\")\n",
    "\n",
    "# Wczytaj dane z Silver\n",
    "orders_silver_df = spark.table(f\"{SILVER_SCHEMA}.{silver_table}\")\n",
    "\n",
    "print(\"=== Gold Layer - Business Aggregation ===\")\n",
    "\n",
    "# Gold aggregation: Daily order summary z KPI\n",
    "daily_summary_df = (\n",
    "    orders_silver_df\n",
    "    .groupBy(\"order_date\", \"order_status\")\n",
    "    .agg(\n",
    "        # Volume metrics\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "        # Revenue metrics\n",
    "        F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "        F.min(\"total_amount\").alias(\"min_order_value\"),\n",
    "        F.max(\"total_amount\").alias(\"max_order_value\")\n",
    "    )\n",
    "    # Derived KPIs\n",
    "    .withColumn(\"revenue_per_customer\", \n",
    "                F.round(F.col(\"total_revenue\") / F.col(\"unique_customers\"), 2))\n",
    "    # Gold metadata\n",
    "    .withColumn(\"_gold_created_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_gold_aggregation_level\", F.lit(\"DAILY\"))\n",
    "    .orderBy(\"order_date\", \"order_status\")\n",
    ")\n",
    "\n",
    "# Summary statistics\n",
    "total_days = daily_summary_df.select(\"order_date\").distinct().count()\n",
    "total_orders_gold = daily_summary_df.agg(F.sum(\"total_orders\")).collect()[0][0]\n",
    "total_revenue_gold = daily_summary_df.agg(F.sum(\"total_revenue\")).collect()[0][0]\n",
    "\n",
    "print(f\"Total days aggregated: {total_days}\")\n",
    "print(f\"Total orders: {total_orders_gold:,}\")\n",
    "print(f\"Total revenue: ${total_revenue_gold:,.2f}\")\n",
    "\n",
    "# Zapisz do Gold schema\n",
    "gold_table = \"daily_order_summary\"\n",
    "(\n",
    "    daily_summary_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(gold_table)\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Gold table utworzony: {GOLD_SCHEMA}.{gold_table}\")\n",
    "print(f\"\\n=== Sample Gold data (daily KPIs) ===\")\n",
    "display(spark.table(gold_table).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b306ee51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 4: Event Log i Lineage\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Lakeflow automatycznie loguje wszystkie operacje do **Event Log** (Delta table). Event Log zawiera:\n",
    "- Flow progress (success/failure per tabela)\n",
    "- Data quality metrics (expectations violations)\n",
    "- Lineage tracking (source â†’ target)\n",
    "- Performance metrics (duration, records processed)\n",
    "\n",
    "**Kluczowe pojÄ™cia:**\n",
    "- **Event Log**: Delta table w `system/events` (per pipeline)\n",
    "- **Flow types**: `flow_definition`, `flow_progress`, `expectation`, `user_action`\n",
    "- **Lineage**: Automatyczne Å›ledzenie zaleÅ¼noÅ›ci Bronze â†’ Silver â†’ Gold\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Monitoring pipeline health\n",
    "- Debugging failures\n",
    "- Data quality reporting\n",
    "- Audit i compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeebaae1",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 4.1: Event Log - Monitoring i Lineage\n",
    "\n",
    "**Event Log Location:**\n",
    "```\n",
    "dbfs:/pipelines/<pipeline_id>/system/events\n",
    "```\n",
    "\n",
    "**PrzykÅ‚adowe queries Event Log (w production Lakeflow pipeline):**\n",
    "\n",
    "```python\n",
    "# 1. Query Event Log\n",
    "event_log_path = \"dbfs:/pipelines/<pipeline_id>/system/events\"\n",
    "event_log_df = spark.read.format(\"delta\").load(event_log_path)\n",
    "\n",
    "# 2. Flow progress per tabela\n",
    "flow_progress = (\n",
    "    event_log_df\n",
    "    .filter(\"event_type = 'flow_progress'\")\n",
    "    .select(\"timestamp\", \"details.flow_name\", \"details.output_records\", \"details.status\")\n",
    ")\n",
    "\n",
    "# 3. Expectations violations (data quality metrics)\n",
    "expectations_df = (\n",
    "    event_log_df\n",
    "    .filter(\"event_type = 'expectation'\")\n",
    "    .select(\n",
    "        \"timestamp\",\n",
    "        \"details.dataset\",\n",
    "        \"details.name\",\n",
    "        \"details.passed_records\",\n",
    "        \"details.failed_records\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# 4. Lineage tracking\n",
    "lineage_df = (\n",
    "    event_log_df\n",
    "    .filter(\"event_type = 'flow_definition'\")\n",
    "    .select(\"details.flow_name\", \"details.input_datasets\", \"details.output_dataset\")\n",
    ")\n",
    "```\n",
    "\n",
    "**Dla demo (bez production pipeline):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d4be42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrzykÅ‚ad 4.1 - Lineage Tracking (symulacja dla demo)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"LINEAGE TRACKING - Bronze â†’ Silver â†’ Gold\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Symulacja lineage metadata (w production: z Event Log)\n",
    "lineage_data = [\n",
    "    {\n",
    "        \"layer\": \"Bronze\",\n",
    "        \"table\": f\"{BRONZE_SCHEMA}.{bronze_table}\",\n",
    "        \"source\": ORDERS_JSON,\n",
    "        \"target_tables\": [f\"{SILVER_SCHEMA}.{silver_table}\"],\n",
    "        \"record_count\": spark.table(f\"{BRONZE_SCHEMA}.{bronze_table}\").count()\n",
    "    },\n",
    "    {\n",
    "        \"layer\": \"Silver\",\n",
    "        \"table\": f\"{SILVER_SCHEMA}.{silver_table}\",\n",
    "        \"source\": f\"{BRONZE_SCHEMA}.{bronze_table}\",\n",
    "        \"target_tables\": [f\"{GOLD_SCHEMA}.{gold_table}\"],\n",
    "        \"record_count\": spark.table(f\"{SILVER_SCHEMA}.{silver_table}\").count()\n",
    "    },\n",
    "    {\n",
    "        \"layer\": \"Gold\",\n",
    "        \"table\": f\"{GOLD_SCHEMA}.{gold_table}\",\n",
    "        \"source\": f\"{SILVER_SCHEMA}.{silver_table}\",\n",
    "        \"target_tables\": [\"BI Dashboards\", \"ML Models\"],\n",
    "        \"record_count\": spark.table(f\"{GOLD_SCHEMA}.{gold_table}\").count()\n",
    "    }\n",
    "]\n",
    "\n",
    "# Display lineage\n",
    "import pandas as pd\n",
    "lineage_df = pd.DataFrame(lineage_data)\n",
    "print(\"\\n[LINEAGE TABLE]\")\n",
    "print(lineage_df.to_string(index=False))\n",
    "\n",
    "# Data flow diagram\n",
    "print(\"\\n[DATA FLOW DIAGRAM]\")\n",
    "print(f\"\"\"\n",
    "Source Data (JSON)\n",
    "    â”‚\n",
    "    â”œâ”€â†’ {BRONZE_SCHEMA}.{bronze_table} ({lineage_data[0]['record_count']} records)\n",
    "        â”‚   [STREAMING TABLE - Append-only]\n",
    "        â”‚\n",
    "        â”œâ”€â†’ {SILVER_SCHEMA}.{silver_table} ({lineage_data[1]['record_count']} records)\n",
    "            â”‚   [MATERIALIZED VIEW - Validated + Cleaned]\n",
    "            â”‚   [Quality: {rejection_rate:.2f}% rejection rate]\n",
    "            â”‚\n",
    "            â”œâ”€â†’ {GOLD_SCHEMA}.{gold_table} ({lineage_data[2]['record_count']} aggregates)\n",
    "                    [MATERIALIZED VIEW - Business KPIs]\n",
    "                    â”œâ”€â†’ Power BI Dashboards\n",
    "                    â””â”€â†’ ML Feature Store\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a614d99d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 5: SQL vs Python API\n",
    "\n",
    "**Wprowadzenie:**\n",
    "\n",
    "Lakeflow SDP oferuje dwa rÃ³wnowaÅ¼ne API: **SQL** i **Python**. WybÃ³r zaleÅ¼y od preferencji zespoÅ‚u i use case.\n",
    "\n",
    "### PorÃ³wnanie skÅ‚adni\n",
    "\n",
    "| Aspekt | SQL | Python |\n",
    "|--------|-----|--------|\n",
    "| **STREAMING TABLE** | `CREATE OR REFRESH STREAMING TABLE` | `@dp.table()` |\n",
    "| **MATERIALIZED VIEW** | `CREATE OR REFRESH MATERIALIZED VIEW` | `@dp.materialized_view()` |\n",
    "| **VIEW** | `CREATE OR REFRESH VIEW` | `@dp.view()` / `@dp.temporary_view()` |\n",
    "| **Expectations** | `CONSTRAINT ... EXPECT ... ON VIOLATION` | `@dp.expect()`, `@dp.expect_or_drop()`, `@dp.expect_or_fail()` |\n",
    "| **Streaming read** | `FROM STREAM table` | `spark.readStream.table()` |\n",
    "\n",
    "---\n",
    "\n",
    "### PrzykÅ‚ad: Ten sam pipeline w SQL i Python\n",
    "\n",
    "**SQL Approach:**\n",
    "\n",
    "```sql\n",
    "-- Bronze\n",
    "CREATE OR REFRESH STREAMING TABLE bronze.orders AS\n",
    "SELECT * FROM STREAM read_files('/path/orders', format => 'json');\n",
    "\n",
    "-- Silver\n",
    "CREATE OR REFRESH MATERIALIZED VIEW silver.orders (\n",
    "  CONSTRAINT valid_amount EXPECT (total_amount > 0) ON VIOLATION DROP ROW\n",
    ")\n",
    "AS SELECT \n",
    "  order_id, \n",
    "  customer_id, \n",
    "  CAST(total_amount AS DECIMAL(10,2)) AS total_amount\n",
    "FROM bronze.orders;\n",
    "\n",
    "-- Gold\n",
    "CREATE OR REFRESH MATERIALIZED VIEW gold.daily_summary AS\n",
    "SELECT \n",
    "  DATE(order_date) AS date,\n",
    "  SUM(total_amount) AS revenue\n",
    "FROM silver.orders\n",
    "GROUP BY DATE(order_date);\n",
    "```\n",
    "\n",
    "**Python Approach (equivalent):**\n",
    "\n",
    "```python\n",
    "from pyspark import pipelines as dp\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Bronze\n",
    "@dp.table(comment=\"Bronze orders\")\n",
    "def orders_bronze():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"json\")\n",
    "        .load(\"/path/orders\")\n",
    "    )\n",
    "\n",
    "# Silver\n",
    "@dp.materialized_view(comment=\"Silver orders\")\n",
    "@dp.expect_or_drop(\"valid_amount\", \"total_amount > 0\")\n",
    "def orders_silver():\n",
    "    return (\n",
    "        spark.read.table(\"bronze.orders\")\n",
    "        .select(\n",
    "            \"order_id\",\n",
    "            \"customer_id\",\n",
    "            F.col(\"total_amount\").cast(\"decimal(10,2)\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Gold\n",
    "@dp.materialized_view(comment=\"Gold daily summary\")\n",
    "def daily_summary():\n",
    "    return (\n",
    "        spark.read.table(\"silver.orders\")\n",
    "        .groupBy(F.to_date(\"order_date\").alias(\"date\"))\n",
    "        .agg(F.sum(\"total_amount\").alias(\"revenue\"))\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Kiedy uÅ¼ywaÄ‡ SQL vs Python?\n",
    "\n",
    "**UÅ¼yj SQL jeÅ›li:**\n",
    "- ZespÃ³Å‚ ma silne SQL skills\n",
    "- Proste transformacje (filters, aggregations)\n",
    "- Integracja z BI tools (SQL-native workflows)\n",
    "- Mniej metaprogramming\n",
    "\n",
    "**UÅ¼yj Python jeÅ›li:**\n",
    "- Potrzebujesz loops / dynamic table creation\n",
    "- Complex transformacje (UDFs, window functions)\n",
    "- Integracja z ML workflows\n",
    "- Testing (unit tests dla transformacji)\n",
    "\n",
    "**Najlepiej:** Mieszaj! SQL dla prostych, Python dla zÅ‚oÅ¼onych."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40d6491",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### Projektowanie Pipeline'Ã³w:\n",
    "\n",
    "**1. Separation of Concerns:**\n",
    "- Bronze: Tylko ingest + audit metadata (no business logic)\n",
    "- Silver: Data quality + standardization (no aggregations)\n",
    "- Gold: Business logic + aggregations (BI-ready)\n",
    "\n",
    "**2. Expectations Strategy:**\n",
    "- Bronze â†’ Silver: NOT NULL, schema validation (DROP ROW)\n",
    "- Silver â†’ Gold: Business rules, referential integrity (FAIL UPDATE dla critical)\n",
    "- Start z EXPECT (warn), potem tighten do DROP ROW po analizie\n",
    "\n",
    "**3. Incremental Processing:**\n",
    "- UÅ¼ywaj STREAMING TABLE dla append-only sources (files, Kafka)\n",
    "- UÅ¼ywaj MATERIALIZED VIEW dla batch z incremental refresh\n",
    "- Partition Silver/Gold po date dla incremental MERGE\n",
    "\n",
    "**4. Monitoring:**\n",
    "- Regularnie query Event Log dla expectations violations\n",
    "- Alert na spike w rejection rate (> 5%)\n",
    "- Dashboard per pipeline: throughput, latency, quality metrics\n",
    "\n",
    "### WydajnoÅ›Ä‡:\n",
    "\n",
    "**5. Partitioning:**\n",
    "- Bronze: Minimal partitioning (bulk operations)\n",
    "- Silver: Partition po date lub region\n",
    "- Gold: Partition wg BI query patterns\n",
    "\n",
    "**6. Optimization:**\n",
    "- WÅ‚Ä…cz Auto Optimize dla Silver/Gold\n",
    "- ZORDER BY po czÄ™sto filtrowanych kolumnach\n",
    "- Vacuum regularnie (retention policy)\n",
    "\n",
    "### Governance:\n",
    "\n",
    "**7. Unity Catalog:**\n",
    "- Bronze/Silver/Gold jako osobne schemas\n",
    "- RÃ³Å¼ne access controls per warstwa\n",
    "- Service principals dla pipelines (nie user accounts)\n",
    "\n",
    "**8. Naming Conventions:**\n",
    "- `bronze_<entity>`, `silver_<entity>`, `gold_<metric>`\n",
    "- Comments per tabela (business context)\n",
    "- Expectations: opisowe nazwy (`valid_customer_email` zamiast `check1`)\n",
    "\n",
    "### Deployment:\n",
    "\n",
    "**9. CI/CD:**\n",
    "- Git repos dla pipeline code\n",
    "- Databricks Asset Bundles (DAB) dla deployment\n",
    "- Separate environments: dev â†’ test â†’ prod\n",
    "\n",
    "**10. Testing:**\n",
    "- Unit tests dla transformacji (pytest)\n",
    "- Integration tests end-to-end (sample data)\n",
    "- Data quality tests (Great Expectations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50fe636",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "### Co zostaÅ‚o osiÄ…gniÄ™te:\n",
    "\n",
    "âœ… **Zrozumienie Lakeflow SDP:**\n",
    "- Deklaratywny framework dla batch + streaming pipelines\n",
    "- Automatic orchestration (DAG), retry, monitoring\n",
    "- Zero boilerplate code\n",
    "\n",
    "âœ… **Flow Types:**\n",
    "- STREAMING TABLE (append-only, low-latency)\n",
    "- MATERIALIZED VIEW (incremental refresh, batch)\n",
    "- VIEW (ephemeral, reusable logic)\n",
    "\n",
    "âœ… **Medallion Architecture z Lakeflow:**\n",
    "- Bronze: STREAMING TABLE z Auto Loader\n",
    "- Silver: MATERIALIZED VIEW z Expectations (data quality)\n",
    "- Gold: MATERIALIZED VIEW z business aggregates\n",
    "\n",
    "âœ… **Data Quality:**\n",
    "- Expectations: EXPECT (warn) / DROP ROW / FAIL UPDATE\n",
    "- Tracking metrics w Event Log\n",
    "- Automatic rejection rate calculation\n",
    "\n",
    "âœ… **Event Log & Lineage:**\n",
    "- Automatic logging do Delta table\n",
    "- Flow progress, expectations, lineage tracking\n",
    "- Query dla monitoring i debugging\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "\n",
    "1. **DeklaratywnoÅ›Ä‡ eliminuje complexity**: `CREATE OR REFRESH` vs setki linii procedural code\n",
    "2. **Automatic orchestration**: Lakeflow buduje DAG i wykonuje w poprawnej kolejnoÅ›ci\n",
    "3. **Incremental processing**: Tylko nowe/zmienione dane (performance!)\n",
    "4. **Built-in quality**: Expectations jako first-class citizens\n",
    "5. **Observability out-of-the-box**: Event Log dla monitoring\n",
    "\n",
    "### Quick Reference - NajwaÅ¼niejsze komendy:\n",
    "\n",
    "| Operacja | SQL | Python |\n",
    "|----------|-----|--------|\n",
    "| **Streaming table** | `CREATE OR REFRESH STREAMING TABLE` | `@dp.table()` |\n",
    "| **Materialized view** | `CREATE OR REFRESH MATERIALIZED VIEW` | `@dp.materialized_view()` |\n",
    "| **Expectations** | `CONSTRAINT ... EXPECT ... ON VIOLATION DROP ROW` | `@dp.expect_or_drop()` |\n",
    "| **Auto Loader** | `FROM STREAM read_files(...)` | `.format(\"cloudFiles\")` |\n",
    "| **Streaming read** | `FROM STREAM table` | `spark.readStream.table()` |\n",
    "\n",
    "### NastÄ™pne kroki:\n",
    "\n",
    "- **Kolejny notebook**: 03_batch_streaming_load.ipynb - COPY INTO, Auto Loader deep dive\n",
    "- **Warsztat praktyczny**: 02_lakeflow_orchestration_workshop.ipynb\n",
    "- **Production deployment**: Databricks Jobs + Delta Live Tables UI\n",
    "- **Dokumentacja**: [Lakeflow Pipelines Docs](https://docs.databricks.com/aws/en/ldp/)\n",
    "\n",
    "### Zadanie domowe (opcjonalnie):\n",
    "\n",
    "UtwÃ³rz wÅ‚asny Lakeflow pipeline:\n",
    "1. Bronze: Wczytaj customers.csv z Auto Loader\n",
    "2. Silver: Waliduj email format, deduplikuj po customer_id\n",
    "3. Gold: Agreguj customers per region (daily counts)\n",
    "4. Dodaj Expectations dla email i phone number"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601fcdbf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czyszczenie zasobÃ³w\n",
    "\n",
    "PosprzÄ…taj zasoby utworzone podczas notebooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee95ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcjonalne czyszczenie zasobÃ³w testowych\n",
    "# UWAGA: Uruchom tylko jeÅ›li chcesz usunÄ…Ä‡ wszystkie utworzone dane\n",
    "\n",
    "# UsuÅ„ tabele Demo\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {BRONZE_SCHEMA}.{bronze_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {SILVER_SCHEMA}.{silver_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {GOLD_SCHEMA}.{gold_table}\")\n",
    "\n",
    "# WyczyÅ›Ä‡ cache\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "print(\"ðŸ’¡ Aby usunÄ…Ä‡ tabele, odkomentuj kod powyÅ¼ej i uruchom komÃ³rkÄ™\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
