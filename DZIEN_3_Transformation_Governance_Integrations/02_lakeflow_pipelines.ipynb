{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78971093",
   "metadata": {},
   "source": [
    "# Lakeflow Pipelines - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Zrozumienie deklaratywnego podej≈õcia do budowania data pipelines w Databricks Lakeflow.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Koncepcje Lakeflow: deklaratywny spos√≥b definicji pipeline'√≥w\n",
    "- SQL vs Python API\n",
    "- Materialized views / streaming tables\n",
    "- Expectations: warn / drop / fail\n",
    "- Event log i lineage per tabela\n",
    "- Automatic orchestration\n",
    "\n",
    "**WA≈ªNE:** Lakeflow to nowa nazwa marketingowa dla **Delta Live Tables (DLT)**. W kodzie u≈ºywamy modu≈Çu `dlt`, ale koncepcyjnie m√≥wimy o \"Lakeflow Pipelines\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750fe993",
   "metadata": {},
   "source": [
    "## 1Ô∏è‚É£ Wprowadzenie do Lakeflow\n",
    "\n",
    "**Lakeflow** to framework do deklaratywnego budowania ETL/ELT pipeline'√≥w w Databricks.\n",
    "\n",
    "### üìå Nota: Lakeflow = Delta Live Tables\n",
    "**Lakeflow** to nowa nazwa dla **Delta Live Tables (DLT)**. \n",
    "- API i sk≈Çadnia pozostajƒÖ te same (`@dlt.table()`)\n",
    "- Dokumentacja mo≈ºe u≈ºywaƒá obu nazw\n",
    "- W kodzie u≈ºywamy modu≈Çu `dlt`\n",
    "\n",
    "### Kluczowe cechy:\n",
    "- **Deklaratywny**: definiujesz \"co chcesz osiƒÖgnƒÖƒá\", nie \"jak to zrobiƒá\"\n",
    "- **Automatyczna orkiestracja**: Lakeflow sam zarzƒÖdza zale≈ºno≈õciami miƒôdzy tabelami\n",
    "- **Data Quality**: wbudowane expectations (warn/drop/fail)\n",
    "- **Monitoring**: event log, lineage, quality metrics out-of-the-box\n",
    "- **SQL i Python API**: elastyczno≈õƒá wyboru jƒôzyka\n",
    "\n",
    "### R√≥≈ºnica Lakeflow vs tradycyjne Notebooks:\n",
    "\n",
    "| Aspekt | Tradycyjne Notebooks | Lakeflow Pipelines |\n",
    "|--------|---------------------|---------------------|\n",
    "| **Definicja** | Imperatywna (kroki) | Deklaratywna (rezultat) |\n",
    "| **Zale≈ºno≈õci** | Rƒôczne (musisz okre≈õliƒá kolejno≈õƒá) | Automatyczne (DAG inference) |\n",
    "| **Quality** | Custom kod walidacji | Wbudowane expectations |\n",
    "| **Monitoring** | Custom logging | Event log + lineage |\n",
    "| **Orchestracja** | Databricks Jobs (manual) | Automatyczna |\n",
    "| **Incremental** | Rƒôczna implementacja | Built-in streaming tables |\n",
    "| **Retry logic** | Custom error handling | Automatyczne retry |\n",
    "\n",
    "### Dlaczego Lakeflow?\n",
    "1. **Mniej kodu**: deklaratywna sk≈Çadnia = mniej boilerplate\n",
    "2. **Lepsza jako≈õƒá**: expectations enforcement\n",
    "3. **≈Åatwiejszy debugging**: event log pokazuje dok≈Çadnie gdzie problem\n",
    "4. **Production-ready**: retry, checkpointing, monitoring out-of-the-box\n",
    "5. **Team collaboration**: SQL API dla analityk√≥w, Python dla engineers\n",
    "\n",
    "---\n",
    "\n",
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzie≈Ñ szkolenia**: Dzie≈Ñ 3 - Transformation, Governance & Integrations\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.3 LTS+ z Lakeflow support\n",
    "  - Unity Catalog w≈ÇƒÖczony\n",
    "  - Uprawnienia: CREATE CATALOG, CREATE SCHEMA, CREATE TABLE\n",
    "  - Klaster: Standard z minimum 2 workers\n",
    "\n",
    "---\n",
    "\n",
    "## Wstƒôp teoretyczny - Lakeflow Pipelines\n",
    "\n",
    "**Lakeflow Pipelines** (poprzednio: Delta Live Tables) to framework Databricks do deklaratywnego budowania ETL/ELT pipeline'√≥w.\n",
    "\n",
    "### Kluczowe cechy Lakeflow:\n",
    "- **Deklaratywny**: definiujesz \"co chcesz osiƒÖgnƒÖƒá\", nie \"jak to zrobiƒá\"\n",
    "- **Automatyczna orkiestracja**: Lakeflow sam zarzƒÖdza zale≈ºno≈õciami miƒôdzy tabelami\n",
    "- **Data Quality**: wbudowane expectations (warn/drop/fail)\n",
    "- **Monitoring**: event log, lineage, quality metrics out-of-the-box\n",
    "- **SQL i Python API**: elastyczno≈õƒá wyboru jƒôzyka\n",
    "\n",
    "### Lakeflow vs Tradycyjne Notebooks:\n",
    "\n",
    "| Aspekt | Tradycyjne Notebooks | Lakeflow Pipelines |\n",
    "|--------|---------------------|---------------------|\n",
    "| **Definicja** | Imperatywna (kroki) | Deklaratywna (rezultat) |\n",
    "| **Zale≈ºno≈õci** | Rƒôczne (musisz okre≈õliƒá kolejno≈õƒá) | Automatyczne (DAG inference) |\n",
    "| **Quality** | Custom kod walidacji | Wbudowane expectations |\n",
    "| **Monitoring** | Custom logging | Event log + lineage |\n",
    "| **Orchestracja** | Databricks Jobs (manual) | Automatyczna |\n",
    "| **Incremental** | Rƒôczna implementacja | Built-in streaming tables |\n",
    "| **Retry logic** | Custom error handling | Automatyczne retry |\n",
    "\n",
    "### Dlaczego Lakeflow?\n",
    "1. **Mniej kodu**: deklaratywna sk≈Çadnia = mniej boilerplate\n",
    "2. **Lepsza jako≈õƒá**: expectations enforcement\n",
    "3. **≈Åatwiejszy debugging**: event log pokazuje dok≈Çadnie gdzie problem\n",
    "4. **Production-ready**: retry, checkpointing, monitoring out-of-the-box\n",
    "5. **Team collaboration**: SQL API dla analityk√≥w, Python dla engineers\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe152279",
   "metadata": {},
   "source": [
    "## Izolacja per u≈ºytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalog√≥w i schemat√≥w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c21cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b63bf1",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych ≈õrodowiskowych:\n",
    "\n",
    "**UWAGA:** Lakeflow Pipelines sƒÖ uruchamiane jako osobne job'y, nie bezpo≈õrednio w notebooku. Ten notebook zawiera **definicje** tabel Lakeflow, kt√≥re zostanƒÖ wykonane przez Lakeflow engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c723fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt  # Lakeflow u≈ºywa modu≈Çu 'dlt' (Delta Live Tables API)\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Wy≈õwietl kontekst u≈ºytkownika\n",
    "print(\"=== Kontekst u≈ºytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema (Lakeflow target): {BRONZE_SCHEMA}\")  # Lakeflow bƒôdzie tworzyƒá tabele tutaj\n",
    "print(f\"U≈ºytkownik: {raw_user}\")\n",
    "\n",
    "# ≈öcie≈ºki do danych ≈∫r√≥d≈Çowych (z dataset/)\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "print(f\"\\n=== ≈öcie≈ºki do danych ≈∫r√≥d≈Çowych ===\")\n",
    "print(f\"Orders (JSON): {ORDERS_JSON}\")\n",
    "print(f\"Customers (CSV): {CUSTOMERS_CSV}\")\n",
    "print(f\"Products (Parquet): {PRODUCTS_PARQUET}\")\n",
    "\n",
    "print(\"\\n‚úÖ Konfiguracja Lakeflow Pipeline gotowa!\")\n",
    "print(\"üìù Poni≈ºsze definicje tabel bƒôdƒÖ wykonane przez Lakeflow engine, nie bezpo≈õrednio w notebooku.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b169543",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 1: Deklaratywne definicje pipeline'√≥w\n",
    "\n",
    "**Cel:** Zrozumienie podstawowej sk≈Çadni Lakeflow - deklaratywne defin icje tabel.\n",
    "\n",
    "### Python API - podstawowa sk≈Çadnia:\n",
    "\n",
    "W Lakeflow definiujemy tabele za pomocƒÖ dekorator√≥w `@dlt.table()` lub `@dlt.view()`.\n",
    "\n",
    "**Klucz do zrozumienia:** \n",
    "- Funkcja z `@dlt.table()` **deklaruje**, co tabela powinna zawieraƒá\n",
    "- Lakeflow engine automatycznie wykonuje funkcjƒô i materializuje wynik jako Delta table\n",
    "- Nie musisz rƒôcznie wo≈Ç aƒá `.write.saveAsTable()` - Lakeflow robi to za Ciebie\n",
    "\n",
    "**UWAGA:** W kodzie u≈ºywamy modu≈Çu `dlt` (Delta Live Tables API), ale koncepcyjnie m√≥wimy o \"Lakeflow Pipelines\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3657b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlt  # Lakeflow u≈ºywa modu≈Çu 'dlt' (Delta Live Tables)\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Przyk≈Çad 1.1: Prosta tabela Lakeflow - Bronze Layer Orders\n",
    "\n",
    "# Ta funkcja definiuje, JAK dane powinny byƒá wczytane do tabeli bronze_orders\n",
    "# Lakeflow engine automatycznie:\n",
    "# 1. Wykona tƒô funkcjƒô\n",
    "# 2. Zmaterializuje wynik jako Delta table\n",
    "# 3. Umie≈õci tabelƒô w target schema (z konfiguracji pipeline)\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"bronze_orders\",\n",
    "    comment=\"Bronze layer: Raw orders from JSON source - immutable landing zone\"\n",
    ")\n",
    "def bronze_orders():\n",
    "    # Wczytaj surowe dane z orders_batch.json\n",
    "    # UWAGA: W Lakeflow u≈ºywamy zmiennych przekazanych przez pipeline configuration\n",
    "    return (\n",
    "        spark.read\n",
    "        .format(\"json\")\n",
    "        .option(\"multiLine\", \"true\")\n",
    "        .load(ORDERS_JSON)  # ≈öcie≈ºka z konfiguracji\n",
    "        .withColumn(\"_bronze_ingest_timestamp\", F.current_timestamp())\n",
    "        .withColumn(\"_bronze_source_file\", F.input_file_name())\n",
    "    )\n",
    "\n",
    "# Lakeflow automatycznie:\n",
    "# - Materializuje ten DataFrame jako Delta table `bronze_orders`\n",
    "# - Umieszcza tabelƒô w schema okre≈õlonym w pipeline config\n",
    "# - ZarzƒÖdza life cycle (create, refresh, update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6366e8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.2: Tabela z transformacjami - Silver Layer\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"silver_orders\",\n",
    "    comment=\"Silver layer: Cleaned and validated orders with business logic\"\n",
    ")\n",
    "def silver_orders():\n",
    "    # dlt.read() - odczyt z innej tabeli Lakeflow (batch mode)\n",
    "    # Lakeflow automatycznie wykryje zale≈ºno≈õƒá: silver_orders depends on bronze_orders\n",
    "    return (\n",
    "        dlt.read(\"bronze_orders\")\n",
    "        \n",
    "        # Data quality: filtruj tylko valid records\n",
    "        .filter(F.col(\"order_id\").isNotNull())\n",
    "        .filter(F.col(\"customer_id\").isNotNull())\n",
    "        .filter(F.col(\"total_amount\") > 0)\n",
    "        \n",
    "        # Transformacje biznesowe\n",
    "        .withColumn(\"order_date\", F.to_date(F.col(\"order_datetime\")))\n",
    "        .withColumn(\"order_year\", F.year(F.col(\"order_datetime\")))\n",
    "        .withColumn(\"order_month\", F.month(F.col(\"order_datetime\")))\n",
    "        .withColumn(\"payment_method\", F.upper(F.trim(F.col(\"payment_method\"))))\n",
    "        \n",
    "        # Derived columns\n",
    "        .withColumn(\"order_status\", \n",
    "                    F.when(F.col(\"total_amount\") > 0, \"COMPLETED\")\n",
    "                     .otherwise(\"UNKNOWN\"))\n",
    "        \n",
    "        # Silver metadata\n",
    "        .withColumn(\"_silver_processed_timestamp\", F.current_timestamp())\n",
    "    )\n",
    "\n",
    "# Lakeflow automatycznie:\n",
    "# 1. Wykrywa dependency: silver_orders ‚Üí bronze_orders\n",
    "# 2. Zapewnia, ≈ºe bronze_orders jest przetworzony PRZED silver_orders\n",
    "# 3. Materializuje wynik jako Delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028fc256",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### SQL vs Python API\n",
    "\n",
    "Lakeflow wspiera **dwa API**:\n",
    "1. **Python API**: `@dlt.table()` - dla data engineers, elastyczne transformacje PySpark\n",
    "2. **SQL API**: `CREATE OR REFRESH LIVE TABLE` - dla analytics engineers, czytelna sk≈Çadnia SQL\n",
    "\n",
    "**Klucz:** Mo≈ºesz mieszaƒá oba API w jednym pipeline! Bronze w Python, Silver/Gold w SQL.\n",
    "\n",
    "### SQL API - przyk≈Çady:\n",
    "\n",
    "SQL Lakeflow u≈ºywa specjalnej sk≈Çadni `CREATE OR REFRESH LIVE TABLE` zamiast zwyk≈Çego `CREATE TABLE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b7a57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL w Lakeflow (wykonywany w osobnym notebooku SQL):\n",
    "# Te przyk≈Çady pokazujƒÖ sk≈Çadniƒô SQL dla Lakeflow\n",
    "\n",
    "# ==============================================================================\n",
    "# PRZYK≈ÅAD SQL 1: Bronze Layer w SQL\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "CREATE OR REFRESH LIVE TABLE bronze_customers\n",
    "COMMENT \"Bronze layer: Raw customers from CSV source\"\n",
    "AS\n",
    "SELECT \n",
    "  *,\n",
    "  current_timestamp() as _bronze_ingest_timestamp,\n",
    "  input_file_name() as _bronze_source_file\n",
    "FROM read_files(\n",
    "  '/Volumes/.../dataset/customers/customers.csv',\n",
    "  format => 'csv',\n",
    "  header => true\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# PRZYK≈ÅAD SQL 2: Silver Layer w SQL\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "CREATE OR REFRESH LIVE TABLE silver_customers\n",
    "COMMENT \"Silver layer: Cleaned and validated customers\"\n",
    "AS\n",
    "SELECT \n",
    "  customer_id,\n",
    "  UPPER(TRIM(customer_name)) as customer_name,\n",
    "  LOWER(TRIM(customer_email)) as customer_email,\n",
    "  customer_segment,\n",
    "  current_timestamp() as _silver_processed_timestamp\n",
    "FROM LIVE.bronze_customers  -- LIVE. prefix dla odczytu z innej tabeli Lakeflow\n",
    "WHERE customer_id IS NOT NULL\n",
    "  AND customer_email IS NOT NULL\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# PRZYK≈ÅAD SQL 3: Gold Layer agregacje w SQL\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "CREATE OR REFRESH LIVE TABLE gold_customer_summary\n",
    "COMMENT \"Gold layer: Customer aggregations for BI\"\n",
    "AS\n",
    "SELECT \n",
    "  customer_id,\n",
    "  customer_segment,\n",
    "  COUNT(*) as total_orders,\n",
    "  SUM(total_amount) as lifetime_value,\n",
    "  AVG(total_amount) as avg_order_value,\n",
    "  MIN(order_date) as first_order_date,\n",
    "  MAX(order_date) as last_order_date\n",
    "FROM LIVE.silver_orders\n",
    "GROUP BY customer_id, customer_segment\n",
    "\"\"\"\n",
    "\n",
    "print(\"üìù Powy≈ºsze przyk≈Çady SQL pokazujƒÖ sk≈Çadniƒô Lakeflow dla SQL notebooks\")\n",
    "print(\"üí° W praktyce: Python API dla ingest/complex transforms, SQL API dla analytics/aggregations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8050ee5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Materialized Views vs Streaming Tables\n",
    "\n",
    "Lakeflow oferuje dwa g≈Ç√≥wne typy tabel:\n",
    "\n",
    "### Materialized Views\n",
    "- **Batch processing**: przetwarzanie wsadowe\n",
    "- **Full refresh**: ka≈ºde uruchomienie przetwarza wszystkie dane\n",
    "- **Use case**: dane historyczne, agregacje, dimensionals\n",
    "\n",
    "### Streaming Tables\n",
    "- **Incremental processing**: tylko nowe dane\n",
    "- **Continuous updates**: append-only lub upsert\n",
    "- **Use case**: fact tables, real-time analytics, CDC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f1e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad: Materialized View (batch)\n",
    "@dlt.table(\n",
    "    name=\"daily_sales_summary\",\n",
    "    comment=\"Daily aggregated sales - full refresh\"\n",
    ")\n",
    "def daily_sales_summary():\n",
    "    return (\n",
    "        dlt.read(\"cleaned_orders\")\n",
    "        .groupBy(\"order_date\")\n",
    "        .agg(\n",
    "            count(\"order_id\").alias(\"total_orders\"),\n",
    "            sum(\"amount\").alias(\"total_revenue\"),\n",
    "            avg(\"amount\").alias(\"avg_order_value\")\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbb7f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad: Streaming Table (incremental)\n",
    "@dlt.table(\n",
    "    name=\"streaming_orders\",\n",
    "    comment=\"Streaming orders - incremental processing\"\n",
    ")\n",
    "def streaming_orders():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        .option(\"header\", \"true\")\n",
    "        .option(\"inferSchema\", \"true\")\n",
    "        .load(\"/Volumes/main/default/kion_data/orders/\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9b24da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming table z transformacjami\n",
    "@dlt.table(\n",
    "    name=\"silver_orders_stream\",\n",
    "    comment=\"Silver layer - streaming incremental\"\n",
    ")\n",
    "def silver_orders_stream():\n",
    "    return (\n",
    "        dlt.read_stream(\"streaming_orders\")  # read_stream dla streaming source\n",
    "        .filter(col(\"order_id\").isNotNull())\n",
    "        .withColumn(\"ingested_at\", current_timestamp())\n",
    "        .withColumn(\"year\", year(col(\"order_date\")))\n",
    "        .withColumn(\"month\", month(col(\"order_date\")))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5612d5",
   "metadata": {},
   "source": [
    "### Kiedy u≈ºywaƒá Materialized View vs Streaming Table?\n",
    "\n",
    "**Materialized View**:\n",
    "- Agregacje i raporty (Gold layer)\n",
    "- Dimensionale (np. produkty, klienci)\n",
    "- Ma≈Çe do ≈õrednich datasety\n",
    "- Potrzebujesz full refresh logiki\n",
    "\n",
    "**Streaming Table**:\n",
    "- Fact tables (transakcje, zdarzenia)\n",
    "- Real-time/near-real-time processing\n",
    "- Du≈ºe volumeny danych\n",
    "- CDC (Change Data Capture)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f83b9c",
   "metadata": {},
   "source": [
    "## 4Ô∏è‚É£ Data Quality Expectations\n",
    "\n",
    "**Expectations** to deklaratywny spos√≥b definiowania regu≈Ç jako≈õci danych w Lakeflow.\n",
    "\n",
    "### Trzy typy expectations:\n",
    "\n",
    "1. **WARN**: loguj naruszenia, ale zachowaj dane\n",
    "2. **DROP**: usu≈Ñ wiersze naruszajƒÖce regu≈Çƒô\n",
    "3. **FAIL**: zatrzymaj pipeline przy naruszeniu\n",
    "\n",
    "### Sk≈Çadnia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad667864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1: WARN - logowanie narusze≈Ñ\n",
    "@dlt.table(\n",
    "    name=\"orders_with_quality_checks\"\n",
    ")\n",
    "@dlt.expect(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@dlt.expect(\"positive_amount\", \"amount > 0\")\n",
    "def orders_with_quality_checks():\n",
    "    return dlt.read(\"raw_orders\")\n",
    "\n",
    "# Naruszenia sƒÖ logowane w Event Log, ale dane przep≈ÇywajƒÖ dalej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52237f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 2: DROP - usuwanie z≈Çych wierszy\n",
    "@dlt.table(\n",
    "    name=\"clean_orders\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"positive_amount\", \"amount > 0\")\n",
    "@dlt.expect_or_drop(\"valid_date\", \"order_date IS NOT NULL\")\n",
    "def clean_orders():\n",
    "    return dlt.read(\"raw_orders\")\n",
    "\n",
    "# Wiersze niespe≈ÇniajƒÖce expectations sƒÖ automatycznie usuwane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e768f153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 3: FAIL - zatrzymanie pipeline\n",
    "@dlt.table(\n",
    "    name=\"critical_orders\"\n",
    ")\n",
    "@dlt.expect_or_fail(\"no_nulls_in_key\", \"order_id IS NOT NULL AND customer_id IS NOT NULL\")\n",
    "def critical_orders():\n",
    "    return dlt.read(\"raw_orders\")\n",
    "\n",
    "# Pipeline zatrzyma siƒô, je≈õli jakikolwiek wiersz naruszy regu≈Çƒô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962414c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 4: Z≈Ço≈ºone expectations\n",
    "@dlt.table(\n",
    "    name=\"validated_orders\"\n",
    ")\n",
    "@dlt.expect_or_drop(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"realistic_amount\", \"amount BETWEEN 1 AND 1000000\")\n",
    "@dlt.expect_or_drop(\"valid_status\", \"status IN ('pending', 'completed', 'cancelled')\")\n",
    "@dlt.expect_or_drop(\"recent_date\", \"order_date >= '2020-01-01'\")\n",
    "@dlt.expect(\"preferred_customer\", \"customer_id IN (SELECT customer_id FROM LIVE.vip_customers)\")\n",
    "def validated_orders():\n",
    "    return (\n",
    "        dlt.read(\"raw_orders\")\n",
    "        .withColumn(\"validation_timestamp\", current_timestamp())\n",
    "    )\n",
    "\n",
    "# Kombinacja DROP (krytyczne) i WARN (informacyjne)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96b894c",
   "metadata": {},
   "source": [
    "### Best Practices dla Expectations:\n",
    "\n",
    "1. **U≈ºywaj FAIL tylko dla krytycznych warunk√≥w**: np. schema mismatch\n",
    "2. **DROP dla data quality issues**: np. nulls, invalid values\n",
    "3. **WARN dla business logic**: np. suspicious patterns\n",
    "4. **Monitoruj Event Log**: regularnie sprawdzaj metryki jako≈õci\n",
    "5. **Nazewnictwo expectations**: u≈ºywaj czytelnych nazw opisujƒÖcych regu≈Çƒô\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7247819",
   "metadata": {},
   "source": [
    "## 5Ô∏è‚É£ Event Log i Lineage\n",
    "\n",
    "### Event Log\n",
    "\n",
    "Ka≈ºdy Lakeflow pipeline generuje **Event Log** - szczeg√≥≈Çowy dziennik wszystkich operacji:\n",
    "- Czas wykonania ka≈ºdej tabeli\n",
    "- Liczba przetworzonych wierszy\n",
    "- Naruszenia expectations\n",
    "- Errors i warnings\n",
    "- Resource usage (CPU, memory)\n",
    "\n",
    "Event Log jest dostƒôpny przez:\n",
    "1. **Lakeflow Pipeline UI**: graficzny interfejs\n",
    "2. **Event Log Table**: delta table z metadanymi\n",
    "\n",
    "### Zapytanie Event Log:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7834039",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Event log jest zapisywany jako Delta Table\n",
    "# Lokalizacja: system.event_log.<pipeline_id>\n",
    "\n",
    "# Przyk≈Çadowe zapytanie:\n",
    "event_log_df = spark.read.table(\"system.event_log.kion_lakeflow_pipeline\")\n",
    "\n",
    "# Filtrowanie po typie eventu\n",
    "quality_events = event_log_df.filter(col(\"event_type\") == \"data_quality\")\n",
    "quality_events.display()\n",
    "\n",
    "# Statystyki jako≈õci danych\n",
    "quality_summary = (\n",
    "    quality_events\n",
    "    .groupBy(\"dataset\", \"expectation\")\n",
    "    .agg(\n",
    "        sum(\"passed_records\").alias(\"total_passed\"),\n",
    "        sum(\"failed_records\").alias(\"total_failed\")\n",
    "    )\n",
    ")\n",
    "quality_summary.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e093e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitoring flow metrics\n",
    "flow_progress = (\n",
    "    event_log_df\n",
    "    .filter(col(\"event_type\") == \"flow_progress\")\n",
    "    .select(\n",
    "        \"timestamp\",\n",
    "        \"dataset\",\n",
    "        \"num_output_rows\",\n",
    "        \"execution_duration\"\n",
    "    )\n",
    "    .orderBy(desc(\"timestamp\"))\n",
    ")\n",
    "flow_progress.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1858e255",
   "metadata": {},
   "source": [
    "### Data Lineage\n",
    "\n",
    "Lakeflow automatycznie ≈õledzi **lineage** - relacje miƒôdzy tabelami:\n",
    "- Kt√≥re tabele sƒÖ ≈∫r√≥d≈Çami (upstream)\n",
    "- Kt√≥re tabele sƒÖ celami (downstream)\n",
    "- Jak dane przep≈ÇywajƒÖ przez pipeline\n",
    "\n",
    "**Lineage jest widoczny w**:\n",
    "1. **Lakeflow Pipeline Graph**: wizualizacja zale≈ºno≈õci\n",
    "2. **Unity Catalog**: end-to-end lineage\n",
    "3. **System tables**: metadata queries\n",
    "\n",
    "### Przyk≈Çad lineage query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3b794a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lineage z Unity Catalog system tables\n",
    "lineage_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        source_table_full_name,\n",
    "        target_table_full_name,\n",
    "        source_type,\n",
    "        created_at\n",
    "    FROM system.access.table_lineage\n",
    "    WHERE target_table_full_name LIKE '%kion_lakeflow%'\n",
    "    ORDER BY created_at DESC\n",
    "\"\"\")\n",
    "lineage_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51498211",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Automatic Orchestration\n",
    "\n",
    "Lakeflow automatycznie zarzƒÖdza:\n",
    "1. **Dependency resolution**: wykrywa kolejno≈õƒá wykonania\n",
    "2. **Parallelization**: wykonuje niezale≈ºne tabele r√≥wnolegle\n",
    "3. **Retry logic**: automatyczne retry przy b≈Çƒôdach\n",
    "4. **Checkpointing**: dla streaming tables\n",
    "\n",
    "### Konfiguracja Pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b56c06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguracja Lakeflow Pipeline (JSON configuration)\n",
    "pipeline_config = {\n",
    "    \"name\": \"KION_Orders_Lakeflow_Pipeline\",\n",
    "    \"storage\": \"/mnt/lakeflow/kion_orders\",\n",
    "    \"target\": \"kion_lakeflow_db\",\n",
    "    \"notebooks\": [\n",
    "        {\n",
    "            \"path\": \"/Workspace/KION/lakeflow_orders_bronze\"\n",
    "        },\n",
    "        {\n",
    "            \"path\": \"/Workspace/KION/lakeflow_orders_silver\"\n",
    "        },\n",
    "        {\n",
    "            \"path\": \"/Workspace/KION/lakeflow_orders_gold\"\n",
    "        }\n",
    "    ],\n",
    "    \"configuration\": {\n",
    "        \"source_path\": \"/Volumes/main/default/kion_data\",\n",
    "        \"pipeline.maxParallelTables\": \"4\"\n",
    "    },\n",
    "    \"clusters\": [\n",
    "        {\n",
    "            \"label\": \"default\",\n",
    "            \"num_workers\": 2,\n",
    "            \"node_type_id\": \"Standard_DS3_v2\"\n",
    "        }\n",
    "    ],\n",
    "    \"continuous\": False,  # False = triggered mode, True = continuous\n",
    "    \"development\": True   # True = development mode (full refresh ka≈ºde uruchomienie)\n",
    "}\n",
    "\n",
    "print(\"Lakeflow Pipeline configuration ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eb4241",
   "metadata": {},
   "source": [
    "### Modes of Execution:\n",
    "\n",
    "**Development Mode**:\n",
    "- Reuse cluster between runs\n",
    "- Automatic full refresh\n",
    "- Szybkie iteracje\n",
    "- U≈ºywaj podczas developmentu\n",
    "\n",
    "**Production Mode**:\n",
    "- New cluster per run\n",
    "- Incremental processing\n",
    "- Cost-optimized\n",
    "- U≈ºywaj w produkcji\n",
    "\n",
    "**Triggered vs Continuous**:\n",
    "- **Triggered**: on-demand lub scheduled\n",
    "- **Continuous**: always running, minimal latency\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adc13f",
   "metadata": {},
   "source": [
    "## üî® Kompletny przyk≈Çad: Bronze ‚Üí Silver ‚Üí Gold Lakeflow Pipeline\n",
    "\n",
    "### Pipeline Architecture:\n",
    "```\n",
    "raw_orders (CSV) \n",
    "    ‚Üì\n",
    "bronze_orders (Raw + Audit)\n",
    "    ‚Üì\n",
    "silver_orders (Cleaned + Validated)\n",
    "    ‚Üì\n",
    "gold_daily_sales (Aggregated)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5322fe89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# PRZYK≈ÅAD KOMPLETNY: Bronze ‚Üí Silver ‚Üí Gold Lakeflow Pipeline\n",
    "# Bazuje na plikach z dataset/: orders_batch.json, customers.csv\n",
    "# ================================================================================\n",
    "\n",
    "# BRONZE LAYER - Raw Data Landing\n",
    "@dlt.table(\n",
    "    name=\"lakeflow_bronze_orders\",\n",
    "    comment=\"Bronze: Raw orders from JSON - immutable landing zone\",\n",
    "    table_properties={\n",
    "        \"quality_layer\": \"bronze\",\n",
    "        \"source_format\": \"json\",\n",
    "        \"pipelines.autoOptimize.zOrderCols\": \"order_datetime\"\n",
    "    }\n",
    ")\n",
    "def lakeflow_bronze_orders():\n",
    "    \"\"\"\n",
    "    Bronze layer: Load raw orders from JSON without transformations.\n",
    "    \n",
    "    Data quality: NONE (raw = raw)\n",
    "    Processing: Full load (batch mode)\n",
    "    \"\"\"\n",
    "    return (\n",
    "        spark.read\n",
    "        .format(\"json\")\n",
    "        .option(\"multiLine\", \"true\")\n",
    "        .load(ORDERS_JSON)\n",
    "        # Audit metadata\n",
    "        .withColumn(\"_bronze_ingest_timestamp\", F.current_timestamp())\n",
    "        .withColumn(\"_bronze_source_file\", F.lit(ORDERS_JSON))\n",
    "        .withColumn(\"_bronze_ingested_by\", F.lit(\"lakeflow_pipeline\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b6783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SILVER LAYER - Cleaned & Validated\n",
    "@dlt.table(\n",
    "    name=\"lakeflow_silver_orders\",\n",
    "    comment=\"Silver: Cleaned, validated, and standardized orders with business logic\",\n",
    "    table_properties={\n",
    "        \"quality_layer\": \"silver\",\n",
    "        \"pipelines.autoOptimize.zOrderCols\": \"order_date\"\n",
    "    }\n",
    ")\n",
    "# Data Quality Expectations (Sekcja 3)\n",
    "@dlt.expect_or_drop(\"valid_order_id\", \"order_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"valid_customer_id\", \"customer_id IS NOT NULL\")\n",
    "@dlt.expect_or_drop(\"positive_amount\", \"total_amount > 0\")\n",
    "@dlt.expect_or_drop(\"valid_datetime\", \"order_datetime IS NOT NULL\")\n",
    "@dlt.expect(\"reasonable_amount\", \"total_amount < 100000\")  # WARN tylko\n",
    "def lakeflow_silver_orders():\n",
    "    \"\"\"\n",
    "    Silver layer: Apply data quality checks and business transformations.\n",
    "    \n",
    "    Data quality: NOT NULL validation, business rules (amount > 0)\n",
    "    Processing: Batch mode (full refresh from Bronze)\n",
    "    Expectations: DROP invalid records, WARN for suspicious values\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dlt.read(\"lakeflow_bronze_orders\")  # Dependency: Bronze ‚Üí Silver\n",
    "        \n",
    "        # Deduplikacja (je≈õli Bronze ma duplikaty)\n",
    "        .dropDuplicates([\"order_id\"])\n",
    "        \n",
    "        # Transformacje dat\n",
    "        .withColumn(\"order_date\", F.to_date(F.col(\"order_datetime\")))\n",
    "        .withColumn(\"order_timestamp\", F.to_timestamp(F.col(\"order_datetime\")))\n",
    "        .withColumn(\"order_year\", F.year(F.col(\"order_datetime\")))\n",
    "        .withColumn(\"order_month\", F.month(F.col(\"order_datetime\")))\n",
    "        .withColumn(\"order_quarter\", F.quarter(F.col(\"order_datetime\")))\n",
    "        \n",
    "        # Standaryzacja tekst√≥w\n",
    "        .withColumn(\"payment_method\", F.upper(F.trim(F.col(\"payment_method\"))))\n",
    "        \n",
    "        # Type casting\n",
    "        .withColumn(\"total_amount\", F.col(\"total_amount\").cast(\"decimal(10,2)\"))\n",
    "        .withColumn(\"quantity\", F.col(\"quantity\").cast(\"int\"))\n",
    "        \n",
    "        # Derived business columns\n",
    "        .withColumn(\"order_status\", \n",
    "                    F.when(F.col(\"total_amount\") > 0, \"COMPLETED\")\n",
    "                     .otherwise(\"UNKNOWN\"))\n",
    "        \n",
    "        # Silver audit metadata\n",
    "        .withColumn(\"_silver_processed_timestamp\", F.current_timestamp())\n",
    "        .withColumn(\"_data_quality_flag\", F.lit(\"VALID\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facfe45e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOLD LAYER - Business Aggregates (Daily)\n",
    "@dlt.table(\n",
    "    name=\"lakeflow_gold_daily_sales\",\n",
    "    comment=\"Gold: Daily sales aggregations for BI dashboards and reporting\",\n",
    "    table_properties={\n",
    "        \"quality_layer\": \"gold\",\n",
    "        \"aggregation_level\": \"daily\"\n",
    "    }\n",
    ")\n",
    "def lakeflow_gold_daily_sales():\n",
    "    \"\"\"\n",
    "    Gold layer: Business-level aggregations for BI consumption.\n",
    "    \n",
    "    Granularity: Daily (order_date)\n",
    "    Use case: Sales dashboards, trend analysis, executive reporting\n",
    "    Processing: Batch aggregation from Silver\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dlt.read(\"lakeflow_silver_orders\")  # Dependency: Silver ‚Üí Gold\n",
    "        .groupBy(\"order_date\", \"order_year\", \"order_month\", \"order_quarter\", \"order_status\")\n",
    "        .agg(\n",
    "            # Volume metrics\n",
    "            F.count(\"order_id\").alias(\"total_orders\"),\n",
    "            F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "            F.sum(\"quantity\").alias(\"total_quantity\"),\n",
    "            \n",
    "            # Revenue metrics\n",
    "            F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "            F.avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "            F.min(\"total_amount\").alias(\"min_order_value\"),\n",
    "            F.max(\"total_amount\").alias(\"max_order_value\"),\n",
    "            \n",
    "            # Payment method distribution\n",
    "            F.count(F.when(F.col(\"payment_method\") == \"CREDIT CARD\", 1)).alias(\"orders_credit_card\"),\n",
    "            F.count(F.when(F.col(\"payment_method\") == \"CASH\", 1)).alias(\"orders_cash\"),\n",
    "            F.count(F.when(F.col(\"payment_method\") == \"PAYPAL\", 1)).alias(\"orders_paypal\")\n",
    "        )\n",
    "        # Gold audit metadata\n",
    "        .withColumn(\"_gold_created_timestamp\", F.current_timestamp())\n",
    "        .withColumn(\"_gold_aggregation_level\", F.lit(\"DAILY\"))\n",
    "        .orderBy(\"order_date\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447f53a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GOLD LAYER - Customer Lifetime Value\n",
    "@dlt.table(\n",
    "    name=\"lakeflow_gold_customer_ltv\",\n",
    "    comment=\"Gold: Customer lifetime value and segmentation for CRM and marketing\",\n",
    "    table_properties={\n",
    "        \"quality_layer\": \"gold\",\n",
    "        \"aggregation_level\": \"customer\"\n",
    "    }\n",
    ")\n",
    "def lakeflow_gold_customer_ltv():\n",
    "    \"\"\"\n",
    "    Gold layer: Customer-level aggregations for CRM, segmentation, targeting.\n",
    "    \n",
    "    Granularity: Customer (customer_id)\n",
    "    Use case: Customer segmentation, retention analysis, personalization\n",
    "    Processing: Batch aggregation from Silver\n",
    "    \"\"\"\n",
    "    return (\n",
    "        dlt.read(\"lakeflow_silver_orders\")\n",
    "        .groupBy(\"customer_id\")\n",
    "        .agg(\n",
    "            # Purchase frequency\n",
    "            F.count(\"order_id\").alias(\"total_orders\"),\n",
    "            \n",
    "            # Monetary value\n",
    "            F.sum(\"total_amount\").alias(\"lifetime_value\"),\n",
    "            F.avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "            F.max(\"total_amount\").alias(\"max_order_value\"),\n",
    "            \n",
    "            # Recency\n",
    "            F.min(\"order_date\").alias(\"first_order_date\"),\n",
    "            F.max(\"order_date\").alias(\"last_order_date\"),\n",
    "            F.datediff(F.max(\"order_date\"), F.min(\"order_date\")).alias(\"customer_age_days\"),\n",
    "            \n",
    "            # Payment preferences\n",
    "            F.first(F.col(\"payment_method\")).alias(\"preferred_payment_method\")\n",
    "        )\n",
    "        # Customer segmentation (RFM-like)\n",
    "        .withColumn(\"customer_segment\",\n",
    "            F.when(F.col(\"lifetime_value\") > 10000, \"VIP\")\n",
    "             .when(F.col(\"lifetime_value\") > 5000, \"High Value\")\n",
    "             .when(F.col(\"lifetime_value\") > 1000, \"Medium Value\")\n",
    "             .otherwise(\"Low Value\")\n",
    "        )\n",
    "        .withColumn(\"purchase_frequency_segment\",\n",
    "            F.when(F.col(\"total_orders\") >= 10, \"Frequent\")\n",
    "             .when(F.col(\"total_orders\") >= 5, \"Regular\")\n",
    "             .otherwise(\"Occasional\")\n",
    "        )\n",
    "        # Gold audit metadata\n",
    "        .withColumn(\"_gold_created_timestamp\", F.current_timestamp())\n",
    "        .withColumn(\"_gold_aggregation_level\", F.lit(\"CUSTOMER\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd152da8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Monitoring i Troubleshooting\n",
    "\n",
    "### Sprawdzanie statusu pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef14e207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query Event Log dla b≈Çƒôd√≥w\n",
    "errors_df = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        timestamp,\n",
    "        level,\n",
    "        dataset,\n",
    "        message\n",
    "    FROM event_log(system.event_log.kion_lakeflow_pipeline)\n",
    "    WHERE level = 'ERROR'\n",
    "    ORDER BY timestamp DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "errors_df.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e69577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality violations\n",
    "quality_violations = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        dataset,\n",
    "        expectation,\n",
    "        SUM(failed_records) as total_failures,\n",
    "        SUM(passed_records) as total_passed,\n",
    "        ROUND(SUM(failed_records) * 100.0 / (SUM(failed_records) + SUM(passed_records)), 2) as failure_rate_pct\n",
    "    FROM event_log(system.event_log.kion_lakeflow_pipeline)\n",
    "    WHERE event_type = 'data_quality'\n",
    "    GROUP BY dataset, expectation\n",
    "    HAVING SUM(failed_records) > 0\n",
    "    ORDER BY failure_rate_pct DESC\n",
    "\"\"\")\n",
    "quality_violations.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436117b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline execution time trends\n",
    "execution_trends = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        date_trunc('hour', timestamp) as execution_hour,\n",
    "        dataset,\n",
    "        AVG(execution_duration / 1000) as avg_execution_seconds,\n",
    "        SUM(num_output_rows) as total_rows_processed\n",
    "    FROM event_log(system.event_log.kion_lakeflow_pipeline)\n",
    "    WHERE event_type = 'flow_progress'\n",
    "    GROUP BY execution_hour, dataset\n",
    "    ORDER BY execution_hour DESC, dataset\n",
    "\"\"\")\n",
    "execution_trends.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf9a727",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Podsumowanie\n",
    "\n",
    "### Nauczy≈Çe≈õ siƒô:\n",
    "\n",
    "‚úÖ **Koncepcje Lakeflow:**  \n",
    "- Deklaratywne definicje pipeline'√≥w (`@dlt.table()`)  \n",
    "- Automatyczne zarzƒÖdzanie zale≈ºno≈õciami (DAG inference)  \n",
    "- Lakeflow = Delta Live Tables (ta sama technologia, nowa nazwa)  \n",
    "\n",
    "‚úÖ **SQL vs Python API:**  \n",
    "- Python API: `@dlt.table()` - dla complex transforms i data engineering  \n",
    "- SQL API: `CREATE OR REFRESH LIVE TABLE` - dla analytics i prostych agregacji  \n",
    "- Mo≈ºna mieszaƒá oba API w jednym pipeline!  \n",
    "\n",
    "‚úÖ **Materialized Views vs Streaming Tables:**  \n",
    "- Materialized Views: batch processing, full refresh  \n",
    "- Streaming Tables: incremental processing, append-only/upsert  \n",
    "\n",
    "‚úÖ **Data Quality Expectations:**  \n",
    "- `@dlt.expect()`: WARN - loguj naruszenia  \n",
    "- `@dlt.expect_or_drop()`: DROP - usu≈Ñ z≈Çe rekordy  \n",
    "- `@dlt.expect_or_fail()`: FAIL - zatrzymaj pipeline  \n",
    "\n",
    "‚úÖ **Event Log i Lineage:**  \n",
    "- Event Log: szczeg√≥≈Çowy dziennik wszystkich operacji pipeline  \n",
    "- Lineage: automatyczne ≈õledzenie zale≈ºno≈õci miƒôdzy tabelami  \n",
    "- Monitoring: quality metrics, execution times, row counts  \n",
    "\n",
    "‚úÖ **Automatic Orchestration:**  \n",
    "- Dependency resolution: automatyczna kolejno≈õƒá wykonania  \n",
    "- Parallelization: r√≥wnoleg≈Çe przetwarzanie niezale≈ºnych tabel  \n",
    "- Retry logic: automatyczne retry przy b≈Çƒôdach  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Lakeflow upraszcza ETL**: deklaratywna sk≈Çadnia, automatyczna orkiestracja\n",
    "2. **Quality first**: wbudowane expectations zapewniajƒÖ jako≈õƒá danych\n",
    "3. **Observability**: Event Log + Lineage = pe≈Çna widoczno≈õƒá\n",
    "4. **Streaming i Batch**: jeden framework dla obu paradygmat√≥w\n",
    "5. **Production-ready**: retry, checkpointing, monitoring out-of-the-box\n",
    "\n",
    "### Nastƒôpne kroki:\n",
    "- **Notebook 03**: Databricks Jobs Orchestration\n",
    "- **Workshop 02**: Hands-on Lakeflow + Orchestration\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Dodatkowe zasoby\n",
    "\n",
    "- [Lakeflow Documentation](https://docs.databricks.com/delta-live-tables/index.html)\n",
    "- [Lakeflow Best Practices](https://docs.databricks.com/delta-live-tables/best-practices.html)\n",
    "- [Event Log Reference](https://docs.databricks.com/delta-live-tables/observability.html)\n",
    "\n",
    "---\n",
    "\n",
    "## Best Practices - Lakeflow Pipelines\n",
    "\n",
    "**1. Projektowanie Layer√≥w:**\n",
    "```\n",
    "Bronze: @dlt.table() bez expectations (raw = raw)\n",
    "Silver: @dlt.expect_or_drop() dla data quality\n",
    "Gold: @dlt.table() dla agregacji (Silver ju≈º clean)\n",
    "```\n",
    "\n",
    "**2. Naming Convention:**\n",
    "```\n",
    "lakeflow_bronze_orders\n",
    "lakeflow_silver_orders\n",
    "lakeflow_gold_daily_sales\n",
    "```\n",
    "Prefix `lakeflow_` odr√≥≈ºnia tabele Lakeflow od tradycyjnych tabel.\n",
    "\n",
    "**3. Expectations Strategy:**\n",
    "- **Bronze**: Brak expectations (immutable landing zone)\n",
    "- **Silver**: `expect_or_drop` dla critical columns (order_id, customer_id, amount > 0)\n",
    "- **Silver**: `expect` (warn) dla business logic (reasonable_amount < 100000)\n",
    "- **Gold**: Brak expectations (Silver ju≈º zwalidowany)\n",
    "\n",
    "**4. Table Properties:**\n",
    "```python\n",
    "table_properties={\n",
    "    \"quality_layer\": \"silver\",\n",
    "    \"pipelines.autoOptimize.zOrderCols\": \"order_date\",\n",
    "    \"delta.autoOptimize.optimizeWrite\": \"true\"\n",
    "}\n",
    "```\n",
    "\n",
    "**5. Development vs Production:**\n",
    "- **Development mode**: `development: true`, full refresh ka≈ºde uruchomienie\n",
    "- **Production mode**: `development: false`, incremental processing\n",
    "\n",
    "---\n",
    "\n",
    "## Deployment Guide - Jak uruchomiƒá Lakeflow Pipeline?\n",
    "\n",
    "**Krok 1: Przygotowanie notebooka**\n",
    "- Notebook z definicjami tabel (`@dlt.table()`)\n",
    "- Import `dlt` na poczƒÖtku\n",
    "- Zmienne konfiguracyjne (≈õcie≈ºki, schema)\n",
    "\n",
    "**Krok 2: Utworzenie Lakeflow Pipeline (UI)**\n",
    "1. W Databricks UI: **Workflows** ‚Üí **Delta Live Tables**\n",
    "2. **Create Pipeline**\n",
    "3. Skonfiguruj:\n",
    "   - **Pipeline name**: `KION_Lakeflow_Orders_Pipeline`\n",
    "   - **Notebook path**: ≈õcie≈ºka do tego notebooka\n",
    "   - **Target schema**: `{BRONZE_SCHEMA}` (z 00_setup.ipynb)\n",
    "   - **Storage location**: `/mnt/lakeflow/kion_orders`\n",
    "   - **Configuration**: Key-value pairs dla zmiennych\n",
    "\n",
    "**Krok 3: Pipeline Configuration (JSON)**\n",
    "```json\n",
    "{\n",
    "  \"name\": \"KION_Lakeflow_Orders_Pipeline\",\n",
    "  \"storage\": \"/mnt/lakeflow/kion_orders\",\n",
    "  \"target\": \"kion_bronze_schema\",\n",
    "  \"notebooks\": [\n",
    "    {\"path\": \"/Workspace/.../02_lakeflow_pipelines\"}\n",
    "  ],\n",
    "  \"configuration\": {\n",
    "    \"DATASET_BASE_PATH\": \"/Volumes/.../dataset\",\n",
    "    \"CATALOG\": \"kion_catalog\"\n",
    "  },\n",
    "  \"clusters\": [\n",
    "    {\n",
    "      \"label\": \"default\",\n",
    "      \"num_workers\": 2\n",
    "    }\n",
    "  ],\n",
    "  \"continuous\": false,\n",
    "  \"development\": true\n",
    "}\n",
    "```\n",
    "\n",
    "**Krok 4: Uruchomienie**\n",
    "- **Start** ‚Üí Pipeline wykonuje wszystkie tabele zgodnie z dependency graph\n",
    "- Monitor execution w **Event Log**\n",
    "- Sprawd≈∫ quality metrics w **Data Quality** tab\n",
    "\n",
    "**Krok 5: Monitoring**\n",
    "- Event Log: errory, warnings, execution times\n",
    "- Lineage Graph: wizualizacja zale≈ºno≈õci\n",
    "- Data Quality Dashboard: expectations violations\n",
    "\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Problem 1: `dlt module not found`**\n",
    "**RozwiƒÖzanie:** Lakeflow pipelines wymagajƒÖ Databricks Runtime 9.1+. Sprawd≈∫ wersjƒô klastra.\n",
    "\n",
    "**Problem 2: Tabela nie jest tworzona**\n",
    "**RozwiƒÖzanie:** Sprawd≈∫ Event Log dla errors. Czƒôsto problem z ≈õcie≈ºkƒÖ do danych lub schema permissions.\n",
    "\n",
    "**Problem 3: Expectations fail ca≈Çy pipeline**\n",
    "**RozwiƒÖzanie:** U≈ºywaj `@dlt.expect_or_drop()` zamiast `@dlt.expect_or_fail()` dla non-critical rules.\n",
    "\n",
    "**Problem 4: Duplicate records w Silver**\n",
    "**RozwiƒÖzanie:** Dodaj `.dropDuplicates([\"primary_key\"])` w definicji Silver table.\n",
    "\n",
    "---\n",
    "\n",
    "## Nastƒôpne kroki\n",
    "\n",
    "- **Kolejny notebook**: 03_databricks_jobs_orchestration.ipynb\n",
    "- **Warsztat praktyczny**: 02_lakeflow_orchestration_workshop.ipynb\n",
    "- **Dokumentacja**: [Databricks Lakeflow](https://docs.databricks.com/delta-live-tables/index.html)\n",
    "\n",
    "---\n",
    "\n",
    "**üéØ Kluczowe wnioski:**\n",
    "\n",
    "1. **Lakeflow = Deklaratywny**: Definiujesz \"co\", nie \"jak\"\n",
    "2. **Quality first**: Expectations sƒÖ wbudowane, nie custom kod\n",
    "3. **Observability**: Event Log + Lineage = full transparency\n",
    "4. **Production-ready**: Retry, monitoring, orchestration out-of-the-box\n",
    "5. **Team collaboration**: SQL dla analityk√≥w, Python dla engineers"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
