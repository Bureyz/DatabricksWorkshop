{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "048d626b-8bce-4abd-bd13-8bbb8ad5b8c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Jobs Orchestration\n",
    "\n",
    "**KION Training - Dzie≈Ñ 3**\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Agenda\n",
    "\n",
    "1. Wprowadzenie do Databricks Jobs\n",
    "2. Multi-task Jobs i zale≈ºno≈õci\n",
    "3. Task Types: Notebook, DLT, SQL, dbt\n",
    "4. Parametryzacja i Widgets\n",
    "5. Monitoring, Alerting i Retry Logic\n",
    "6. Best Practices dla produkcyjnych Jobs\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Cele szkolenia\n",
    "\n",
    "Po tym module bƒôdziesz potrafiƒá:\n",
    "- Tworzyƒá multi-task Databricks Jobs\n",
    "- Konfigurowaƒá zale≈ºno≈õci miƒôdzy taskami\n",
    "- Parametryzowaƒá workflows\n",
    "- Monitorowaƒá i debugowaƒá Jobs\n",
    "- Implementowaƒá retry logic i alerting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5efdd9d2-e15d-44f8-9b3e-ccd1d2fb4390",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 1Ô∏è‚É£ Wprowadzenie do Databricks Jobs\n",
    "\n",
    "**Databricks Jobs** to zarzƒÖdzany service orkiestracji dla:\n",
    "- ETL/ELT pipelines\n",
    "- Machine Learning workflows\n",
    "- Scheduled reports\n",
    "- Data quality checks\n",
    "\n",
    "### Kluczowe cechy:\n",
    "- **Multi-task workflows**: DAG (Directed Acyclic Graph)\n",
    "- **Task types**: Notebook, DLT, SQL, dbt, Python wheel, JAR\n",
    "- **Scheduling**: cron, continuous, triggered\n",
    "- **Retry logic**: automatyczne retry przy b≈Çƒôdach\n",
    "- **Alerting**: email, webhooks, integrations\n",
    "- **Cost optimization**: spot instances, autoscaling\n",
    "\n",
    "### Jobs vs DLT:\n",
    "\n",
    "| Feature | Databricks Jobs | Delta Live Tables |\n",
    "|---------|----------------|-------------------|\n",
    "| Use Case | General orchestration | ETL pipelines |\n",
    "| Task Types | Notebook, SQL, dbt, DLT | DLT only |\n",
    "| Dependencies | Manual configuration | Automatic |\n",
    "| Data Quality | Custom code | Built-in expectations |\n",
    "| Flexibility | High | Opinionated |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ddea5485-32c3-4774-b622-600cbdbeede8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2Ô∏è‚É£ Multi-task Jobs i zale≈ºno≈õci\n",
    "\n",
    "### Podstawowa struktura Job:\n",
    "\n",
    "```\n",
    "Job: Daily_ETL_Pipeline\n",
    "‚îú‚îÄ‚îÄ Task 1: ingest_raw_data (Notebook)\n",
    "‚îú‚îÄ‚îÄ Task 2: validate_data (SQL) ‚Üí depends_on: Task 1\n",
    "‚îú‚îÄ‚îÄ Task 3: transform_silver (Notebook) ‚Üí depends_on: Task 2\n",
    "‚îî‚îÄ‚îÄ Task 4: aggregate_gold (DLT) ‚Üí depends_on: Task 3\n",
    "```\n",
    "\n",
    "### Przyk≈Çad konfiguracji Job (JSON):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "179b6888-440e-40f4-a8e1-d314072858c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Konfiguracja Job przez Databricks REST API lub UI\n",
    "\n",
    "job_config = {\n",
    "    \"name\": \"KION_Daily_Orders_ETL\",\n",
    "    \"email_notifications\": {\n",
    "        \"on_failure\": [\"data-team@kion.com\"],\n",
    "        \"on_success\": [\"data-team@kion.com\"]\n",
    "    },\n",
    "    \"timeout_seconds\": 7200,  # 2 hours\n",
    "    \"max_concurrent_runs\": 1,\n",
    "    \"format\": \"MULTI_TASK\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_key\": \"ingest_bronze\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/KION/notebooks/01_ingest_orders\",\n",
    "                \"base_parameters\": {\n",
    "                    \"source_path\": \"/Volumes/main/default/kion_data/orders\",\n",
    "                    \"target_table\": \"bronze_orders\"\n",
    "                }\n",
    "            },\n",
    "            \"new_cluster\": {\n",
    "                \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                \"node_type_id\": \"Standard_DS3_v2\",\n",
    "                \"num_workers\": 2,\n",
    "                \"autoscale\": {\n",
    "                    \"min_workers\": 1,\n",
    "                    \"max_workers\": 4\n",
    "                }\n",
    "            },\n",
    "            \"timeout_seconds\": 3600\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"transform_silver\",\n",
    "            \"depends_on\": [{\"task_key\": \"ingest_bronze\"}],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/KION/notebooks/02_transform_silver\",\n",
    "                \"base_parameters\": {\n",
    "                    \"source_table\": \"bronze_orders\",\n",
    "                    \"target_table\": \"silver_orders\"\n",
    "                }\n",
    "            },\n",
    "            \"existing_cluster_id\": \"{{previous_task_cluster}}\"\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"aggregate_gold\",\n",
    "            \"depends_on\": [{\"task_key\": \"transform_silver\"}],\n",
    "            \"sql_task\": {\n",
    "                \"warehouse_id\": \"abc123def456\",\n",
    "                \"query\": {\n",
    "                    \"query_id\": \"gold_aggregations_query_id\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"schedule\": {\n",
    "        \"quartz_cron_expression\": \"0 0 2 * * ?\",  # Daily at 2 AM\n",
    "        \"timezone_id\": \"Europe/Warsaw\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Job configuration ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "879dcdb5-0a6b-4974-a203-4dc7e3f03906",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Dependency patterns:\n",
    "\n",
    "#### 1. Linear pipeline (sequential):\n",
    "```\n",
    "Task A ‚Üí Task B ‚Üí Task C ‚Üí Task D\n",
    "```\n",
    "\n",
    "#### 2. Fan-out (parallel processing):\n",
    "```\n",
    "        Task A\n",
    "       /  |  \\\n",
    "      B   C   D\n",
    "```\n",
    "\n",
    "#### 3. Fan-in (merge results):\n",
    "```\n",
    "    A   B   C\n",
    "     \\  |  /\n",
    "      Task D\n",
    "```\n",
    "\n",
    "#### 4. Diamond (complex):\n",
    "```\n",
    "      Task A\n",
    "      /    \\\n",
    "     B      C\n",
    "      \\    /\n",
    "      Task D\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cc5dd64-9b81-4e5e-beca-57bee8e8be4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 3Ô∏è‚É£ Task Types: Notebook, DLT, SQL, dbt\n",
    "\n",
    "### 1. Notebook Task\n",
    "\n",
    "**Use case**: Python/Scala/R processing, custom logic\n",
    "\n",
    "**Example notebook** (`01_ingest_orders.ipynb`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b4cc2c3-26e8-4278-886b-0e0c2be187dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Notebook Task Example\n",
    "# File: 01_ingest_orders.ipynb\n",
    "\n",
    "# Get parameters from Job\n",
    "dbutils.widgets.text(\"source_path\", \"/Volumes/main/default/kion_data/orders\")\n",
    "dbutils.widgets.text(\"target_table\", \"bronze_orders\")\n",
    "dbutils.widgets.text(\"run_date\", \"\")\n",
    "\n",
    "source_path = dbutils.widgets.get(\"source_path\")\n",
    "target_table = dbutils.widgets.get(\"target_table\")\n",
    "run_date = dbutils.widgets.get(\"run_date\") or datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "print(f\"Starting ingestion from {source_path} to {target_table}\")\n",
    "print(f\"Run date: {run_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d10529ec-b4fa-4ab4-ae85-eac00a223c0c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "\n",
    "# Load data\n",
    "df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(f\"{source_path}/*.csv\")\n",
    ")\n",
    "\n",
    "# Add audit columns\n",
    "df_with_audit = (\n",
    "    df\n",
    "    .withColumn(\"ingestion_timestamp\", current_timestamp())\n",
    "    .withColumn(\"source_file\", input_file_name())\n",
    "    .withColumn(\"run_date\", lit(run_date))\n",
    ")\n",
    "\n",
    "# Write to Delta\n",
    "df_with_audit.write.format(\"delta\").mode(\"append\").saveAsTable(target_table)\n",
    "\n",
    "# Return metrics to Job\n",
    "row_count = df_with_audit.count()\n",
    "dbutils.notebook.exit({\n",
    "    \"status\": \"success\",\n",
    "    \"rows_ingested\": row_count,\n",
    "    \"target_table\": target_table\n",
    "})\n",
    "\n",
    "print(f\"‚úÖ Ingested {row_count} rows to {target_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "641da1d3-7210-45c0-b524-16a34b1d434c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. SQL Task\n",
    "\n",
    "**Use case**: SQL-based transformations, aggregations\n",
    "\n",
    "**Example SQL query**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37c5c4fd-29be-4897-9e1e-016032e519cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL Task - mo≈ºe byƒá zapisane jako SQL Query w Databricks SQL\n",
    "\n",
    "sql_query = \"\"\"\n",
    "-- Gold Layer Aggregation\n",
    "CREATE OR REPLACE TABLE gold_daily_sales AS\n",
    "SELECT \n",
    "    order_date,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers,\n",
    "    COUNT(order_id) as total_orders,\n",
    "    SUM(amount) as total_revenue,\n",
    "    AVG(amount) as avg_order_value,\n",
    "    MAX(amount) as max_order_value,\n",
    "    CURRENT_TIMESTAMP() as calculated_at\n",
    "FROM silver_orders\n",
    "WHERE status = 'completed'\n",
    "GROUP BY order_date\n",
    "ORDER BY order_date DESC\n",
    "\"\"\"\n",
    "\n",
    "# W Job configuration:\n",
    "# \"sql_task\": {\n",
    "#     \"warehouse_id\": \"abc123\",\n",
    "#     \"query\": {\"query_id\": \"saved_query_id\"}\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d72c3a0-dd6e-4a68-a485-50e051fd1f79",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. DLT Task\n",
    "\n",
    "**Use case**: Delta Live Tables pipeline execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ab8be3b-2da7-4c1f-aa35-8c174eecf533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DLT Task configuration\n",
    "dlt_task = {\n",
    "    \"task_key\": \"run_dlt_pipeline\",\n",
    "    \"depends_on\": [{\"task_key\": \"validate_source_data\"}],\n",
    "    \"pipeline_task\": {\n",
    "        \"pipeline_id\": \"abc123-dlt-pipeline-id\",\n",
    "        \"full_refresh\": False  # Incremental by default\n",
    "    }\n",
    "}\n",
    "\n",
    "# DLT pipeline bƒôdzie uruchomiony jako task w wiƒôkszym Job workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c65d0f7-482d-489b-b294-d575690caced",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. dbt Task\n",
    "\n",
    "**Use case**: dbt models execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9a743a0-b501-4f7e-a2f5-dfd379e533bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# dbt Task configuration\n",
    "dbt_task = {\n",
    "    \"task_key\": \"run_dbt_models\",\n",
    "    \"dbt_task\": {\n",
    "        \"project_directory\": \"/Workspace/KION/dbt\",\n",
    "        \"commands\": [\n",
    "            \"dbt deps\",\n",
    "            \"dbt seed\",\n",
    "            \"dbt run --models tag:daily\",\n",
    "            \"dbt test\"\n",
    "        ],\n",
    "        \"profiles_directory\": \"/Workspace/KION/dbt\",\n",
    "        \"warehouse_id\": \"abc123\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# dbt models mogƒÖ byƒá zintegrowane jako tasks w Job workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8781449-7fe6-471b-8dac-ca017d5c42c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Parametryzacja i Widgets\n",
    "\n",
    "### Passing parameters miƒôdzy taskami:\n",
    "\n",
    "#### Metoda 1: Job-level parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4245c1f2-cdbc-4f02-930c-d6c023417712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Job configuration z parametrami\n",
    "job_with_params = {\n",
    "    \"name\": \"Parameterized_ETL_Job\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_key\": \"task_1\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/KION/task1\",\n",
    "                \"base_parameters\": {\n",
    "                    \"environment\": \"production\",\n",
    "                    \"run_date\": \"{{job.start_time.iso_date}}\",  # Dynamic parameter\n",
    "                    \"source_path\": \"/Volumes/main/default/kion_data\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# W notebooku odbieramy parametry:\n",
    "# dbutils.widgets.text(\"environment\", \"dev\")\n",
    "# environment = dbutils.widgets.get(\"environment\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7a7754b-cda5-46a0-be8a-ca8dc2aa9637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Metoda 2: Task output ‚Üí Next task input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83c761ba-6be6-4831-9231-4337acb6ee54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 1 - zwraca output\n",
    "# File: task1_ingest.ipynb\n",
    "\n",
    "# ... ingestion logic ...\n",
    "\n",
    "# Return output\n",
    "import json\n",
    "output = {\n",
    "    \"rows_ingested\": 1000,\n",
    "    \"target_table\": \"bronze_orders\",\n",
    "    \"max_order_date\": \"2024-01-15\"\n",
    "}\n",
    "dbutils.notebook.exit(json.dumps(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7b09f01-f9e0-467e-9f15-30be61853fef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task 2 - odbiera output z Task 1\n",
    "# File: task2_transform.ipynb\n",
    "\n",
    "import json\n",
    "\n",
    "# Get output from previous task\n",
    "task1_output = dbutils.jobs.taskValues.get(\n",
    "    taskKey=\"task_1\",\n",
    "    key=\"default\",\n",
    "    default=\"{}\",\n",
    "    debugValue=\"{}\"\n",
    ")\n",
    "\n",
    "output_dict = json.loads(task1_output)\n",
    "source_table = output_dict.get(\"target_table\")\n",
    "max_date = output_dict.get(\"max_order_date\")\n",
    "\n",
    "print(f\"Processing data from {source_table} up to {max_date}\")\n",
    "\n",
    "# Use in transformation\n",
    "df = spark.table(source_table).filter(f\"order_date <= '{max_date}'\")\n",
    "# ... rest of transformation ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85ba49f5-9845-4360-9de6-614a9610e8bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Metoda 3: Dynamic values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b4d6168-2957-4f4d-8338-d557e9154728",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks wspiera dynamic values w parametrach:\n",
    "\n",
    "dynamic_params = {\n",
    "    \"run_date\": \"{{job.start_time.iso_date}}\",  # YYYY-MM-DD\n",
    "    \"run_timestamp\": \"{{job.start_time}}\",      # Full timestamp\n",
    "    \"job_id\": \"{{job.id}}\",\n",
    "    \"run_id\": \"{{run.id}}\",\n",
    "    \"parent_run_id\": \"{{parent_run.id}}\"  # Dla nested jobs\n",
    "}\n",
    "\n",
    "# Przyk≈Çad u≈ºycia:\n",
    "# base_parameters: {\n",
    "#     \"processing_date\": \"{{job.start_time.iso_date}}\",\n",
    "#     \"job_run_id\": \"{{run.id}}\"\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ebeec0d-1ff7-4201-9fd0-44a6630361a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Monitoring, Alerting i Retry Logic\n",
    "\n",
    "### Retry Logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c03124-f5d7-4bb6-97fc-e2aa92897eed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Task-level retry configuration\n",
    "task_with_retry = {\n",
    "    \"task_key\": \"data_ingestion\",\n",
    "    \"notebook_task\": {\n",
    "        \"notebook_path\": \"/Workspace/KION/ingest\"\n",
    "    },\n",
    "    \"max_retries\": 3,  # Retry up to 3 times\n",
    "    \"min_retry_interval_millis\": 60000,  # Wait 1 minute between retries\n",
    "    \"retry_on_timeout\": True\n",
    "}\n",
    "\n",
    "# Best practice: u≈ºywaj retry dla transient errors (network, API rate limits)\n",
    "# NIE u≈ºywaj retry dla data quality issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f524f538-cb85-438e-bc90-59a270cfd742",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Email Alerting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0d7ebef-b49e-4499-8c90-676198883376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Email notifications configuration\n",
    "email_config = {\n",
    "    \"email_notifications\": {\n",
    "        \"on_start\": [\"ops@kion.com\"],\n",
    "        \"on_success\": [\"data-team@kion.com\"],\n",
    "        \"on_failure\": [\"data-team@kion.com\", \"on-call@kion.com\"],\n",
    "        \"on_duration_warning_threshold_exceeded\": [\"ops@kion.com\"],\n",
    "        \"no_alert_for_skipped_runs\": True\n",
    "    },\n",
    "    \"health\": {\n",
    "        \"rules\": [\n",
    "            {\n",
    "                \"metric\": \"RUN_DURATION_SECONDS\",\n",
    "                \"op\": \"GREATER_THAN\",\n",
    "                \"value\": 3600  # Alert if job runs > 1 hour\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a80149d-3267-442b-b804-c382bda34105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Webhook Integration (Slack, Teams, PagerDuty):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d303ca0c-0b6c-4abd-aaa4-28cb1371c665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Webhook configuration dla Slack\n",
    "webhook_config = {\n",
    "    \"webhook_notifications\": {\n",
    "        \"on_failure\": [\n",
    "            {\n",
    "                \"id\": \"slack-data-alerts\"\n",
    "            }\n",
    "        ],\n",
    "        \"on_success\": [\n",
    "            {\n",
    "                \"id\": \"slack-data-alerts\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Webhook musi byƒá wcze≈õniej skonfigurowany w Databricks UI:\n",
    "# Admin Settings ‚Üí Webhooks ‚Üí Create Webhook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d1f81c9-a049-48ed-8c07-4befe6ad5ce8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Custom monitoring w notebooku:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7e4189d-6541-4262-9af0-c283fb4d5070",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Custom monitoring logic w task notebook\n",
    "\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Load data\n",
    "df = spark.table(\"bronze_orders\")\n",
    "\n",
    "# Data quality checks\n",
    "total_rows = df.count()\n",
    "null_order_ids = df.filter(col(\"order_id\").isNull()).count()\n",
    "null_percentage = (null_order_ids / total_rows) * 100\n",
    "\n",
    "# Alert if quality threshold exceeded\n",
    "if null_percentage > 5:\n",
    "    error_msg = f\"‚ö†Ô∏è Data quality issue: {null_percentage:.2f}% null order_ids\"\n",
    "    print(error_msg)\n",
    "    \n",
    "    # Log to monitoring table\n",
    "    monitoring_df = spark.createDataFrame([\n",
    "        {\n",
    "            \"job_run_id\": dbutils.notebook.entry_point.getDbutils().notebook().getContext().tags().get(\"jobId\").get(),\n",
    "            \"check_type\": \"null_check\",\n",
    "            \"metric_name\": \"null_order_id_percentage\",\n",
    "            \"metric_value\": null_percentage,\n",
    "            \"threshold\": 5.0,\n",
    "            \"status\": \"FAILED\",\n",
    "            \"timestamp\": current_timestamp()\n",
    "        }\n",
    "    ])\n",
    "    monitoring_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"job_monitoring_metrics\")\n",
    "    \n",
    "    # Fail the job\n",
    "    raise Exception(error_msg)\n",
    "\n",
    "print(f\"‚úÖ Quality check passed: {null_percentage:.2f}% null order_ids\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5658cbc-aa23-4469-bc7b-ad046989155f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Monitoring Job runs przez API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91629fee-3888-425e-8ff7-d0cd5cd52395",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Query system tables dla job history\n",
    "job_history = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        job_id,\n",
    "        job_name,\n",
    "        run_id,\n",
    "        start_time,\n",
    "        end_time,\n",
    "        DATEDIFF(SECOND, start_time, end_time) as duration_seconds,\n",
    "        result_state,\n",
    "        task_runs\n",
    "    FROM system.lakeflow.job_runs\n",
    "    WHERE job_name = 'KION_Daily_Orders_ETL'\n",
    "        AND start_time >= current_date() - INTERVAL 7 DAYS\n",
    "    ORDER BY start_time DESC\n",
    "\"\"\")\n",
    "job_history.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "197f07b8-2645-4435-bdd8-df9794ad38df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SLA monitoring - track on-time completion\n",
    "sla_monitoring = spark.sql(\"\"\"\n",
    "    WITH daily_runs AS (\n",
    "        SELECT \n",
    "            DATE(start_time) as run_date,\n",
    "            COUNT(*) as total_runs,\n",
    "            SUM(CASE WHEN result_state = 'SUCCESS' THEN 1 ELSE 0 END) as successful_runs,\n",
    "            SUM(CASE WHEN result_state = 'FAILED' THEN 1 ELSE 0 END) as failed_runs,\n",
    "            AVG(DATEDIFF(SECOND, start_time, end_time)) as avg_duration_seconds\n",
    "        FROM system.lakeflow.job_runs\n",
    "        WHERE job_name = 'KION_Daily_Orders_ETL'\n",
    "            AND start_time >= current_date() - INTERVAL 30 DAYS\n",
    "        GROUP BY run_date\n",
    "    )\n",
    "    SELECT \n",
    "        run_date,\n",
    "        total_runs,\n",
    "        successful_runs,\n",
    "        failed_runs,\n",
    "        ROUND(successful_runs * 100.0 / total_runs, 2) as success_rate_pct,\n",
    "        ROUND(avg_duration_seconds / 60, 2) as avg_duration_minutes\n",
    "    FROM daily_runs\n",
    "    ORDER BY run_date DESC\n",
    "\"\"\")\n",
    "sla_monitoring.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da46bd56-31eb-4644-98f0-de2292b4f5ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ Best Practices dla produkcyjnych Jobs\n",
    "\n",
    "### 1. Cluster Strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db3ab297-0a7a-4d68-80b2-a42c39b151df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Best Practice: Job Clusters (new cluster per run)\n",
    "job_cluster_config = {\n",
    "    \"new_cluster\": {\n",
    "        \"spark_version\": \"13.3.x-scala2.12\",\n",
    "        \"node_type_id\": \"Standard_DS3_v2\",\n",
    "        \"autoscale\": {\n",
    "            \"min_workers\": 1,\n",
    "            \"max_workers\": 8\n",
    "        },\n",
    "        \"spark_conf\": {\n",
    "            \"spark.databricks.delta.optimizeWrite.enabled\": \"true\",\n",
    "            \"spark.databricks.delta.autoCompact.enabled\": \"true\"\n",
    "        },\n",
    "        \"aws_attributes\": {\n",
    "            \"availability\": \"SPOT_WITH_FALLBACK\",  # Cost optimization\n",
    "            \"spot_bid_price_percent\": 100\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Alternative: Cluster reuse dla linked tasks\n",
    "# \"existing_cluster_id\": \"{{previous_task_cluster}}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d67bb337-482e-41cd-8acd-b8be66c5affb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2. Error Handling w Notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe90f107-b1cc-4ee3-b902-364a5dfdb504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Robust error handling w notebook tasks\n",
    "\n",
    "try:\n",
    "    # Main processing logic\n",
    "    df = spark.table(\"bronze_orders\")\n",
    "    \n",
    "    # Validation\n",
    "    assert df.count() > 0, \"Source table is empty\"\n",
    "    \n",
    "    # Transformation\n",
    "    result_df = df.filter(col(\"order_id\").isNotNull())\n",
    "    \n",
    "    # Write result\n",
    "    result_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"silver_orders\")\n",
    "    \n",
    "    # Return success\n",
    "    dbutils.notebook.exit(json.dumps({\n",
    "        \"status\": \"SUCCESS\",\n",
    "        \"rows_processed\": result_df.count()\n",
    "    }))\n",
    "    \n",
    "except AssertionError as e:\n",
    "    # Data validation failure\n",
    "    print(f\"‚ùå Validation Error: {str(e)}\")\n",
    "    dbutils.notebook.exit(json.dumps({\n",
    "        \"status\": \"FAILED\",\n",
    "        \"error_type\": \"VALIDATION_ERROR\",\n",
    "        \"error_message\": str(e)\n",
    "    }))\n",
    "    \n",
    "except Exception as e:\n",
    "    # Unexpected error\n",
    "    print(f\"‚ùå Unexpected Error: {str(e)}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    dbutils.notebook.exit(json.dumps({\n",
    "        \"status\": \"FAILED\",\n",
    "        \"error_type\": \"UNEXPECTED_ERROR\",\n",
    "        \"error_message\": str(e)\n",
    "    }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60f9a124-b278-4cd8-84be-c08b2d5595ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Idempotency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f4b191e-5465-412a-84ee-ae054c6f0235",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Best Practice: Idempotent operations\n",
    "# Job powinien m√≥c byƒá uruchomiony wielokrotnie bez side effects\n",
    "\n",
    "# ‚ùå BAD: Non-idempotent\n",
    "# df.write.format(\"delta\").mode(\"append\").saveAsTable(\"target\")\n",
    "# ^ Re-running duplicates data\n",
    "\n",
    "# ‚úÖ GOOD: Idempotent with overwrite\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"target\")\n",
    "\n",
    "# ‚úÖ GOOD: Idempotent with MERGE\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "target_table = DeltaTable.forName(spark, \"target\")\n",
    "target_table.alias(\"target\").merge(\n",
    "    df.alias(\"source\"),\n",
    "    \"target.order_id = source.order_id\"\n",
    ").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "# ‚úÖ GOOD: Partition-specific overwrite\n",
    "df.write.format(\"delta\").mode(\"overwrite\") \\\n",
    "    .option(\"replaceWhere\", \"order_date = '2024-01-15'\") \\\n",
    "    .saveAsTable(\"target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "074ac56f-6530-44d9-8f09-3e8188e2c436",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Logging i Observability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dbff5af1-e297-422a-9b50-884bbd1f5e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Structured logging\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Setup logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Log key events\n",
    "logger.info(f\"Job started at {datetime.now()}\")\n",
    "logger.info(f\"Processing partition: {run_date}\")\n",
    "\n",
    "# Log metrics\n",
    "start_time = datetime.now()\n",
    "df = spark.table(\"bronze_orders\")\n",
    "row_count = df.count()\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "logger.info(f\"Processed {row_count} rows in {duration}s\")\n",
    "\n",
    "# Persist metrics to monitoring table\n",
    "metrics_df = spark.createDataFrame([{\n",
    "    \"job_name\": \"KION_Daily_ETL\",\n",
    "    \"run_date\": run_date,\n",
    "    \"metric_name\": \"rows_processed\",\n",
    "    \"metric_value\": row_count,\n",
    "    \"execution_time_seconds\": duration,\n",
    "    \"timestamp\": datetime.now()\n",
    "}])\n",
    "metrics_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"job_metrics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b58fb29-6ef9-40ad-9c2b-e96442f4f179",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5. Cost Optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9567f4f2-5c08-4652-9180-f795287d4f16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cost optimization strategies:\n",
    "\n",
    "cost_optimized_config = {\n",
    "    # 1. Use Spot instances\n",
    "    \"aws_attributes\": {\n",
    "        \"availability\": \"SPOT_WITH_FALLBACK\"\n",
    "    },\n",
    "    \n",
    "    # 2. Enable autoscaling\n",
    "    \"autoscale\": {\n",
    "        \"min_workers\": 1,\n",
    "        \"max_workers\": 8\n",
    "    },\n",
    "    \n",
    "    # 3. Set timeouts\n",
    "    \"timeout_seconds\": 3600,\n",
    "    \n",
    "    # 4. Cluster reuse dla linked tasks\n",
    "    # \"existing_cluster_id\": \"{{previous_task_cluster}}\",\n",
    "    \n",
    "    # 5. Photon acceleration\n",
    "    \"runtime_engine\": \"PHOTON\",\n",
    "    \n",
    "    # 6. Right-size clusters\n",
    "    \"node_type_id\": \"Standard_DS3_v2\",  # Choose appropriate size\n",
    "}\n",
    "\n",
    "# 7. Schedule during off-peak hours\n",
    "schedule_config = {\n",
    "    \"quartz_cron_expression\": \"0 0 2 * * ?\",  # 2 AM\n",
    "    \"timezone_id\": \"Europe/Warsaw\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "471afe04-d2c4-4532-8826-01629a3d0ccc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## üî® Kompletny przyk≈Çad: Production-ready Multi-task Job\n",
    "\n",
    "### Scenario: Daily Orders ETL Pipeline\n",
    "```\n",
    "1. validate_source ‚Üí 2. ingest_bronze ‚Üí 3. transform_silver\n",
    "                                          ‚Üì\n",
    "                            4a. aggregate_gold ‚Üê 5. data_quality_check\n",
    "                                          ‚Üì\n",
    "                            4b. customer_metrics\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd455700-75cf-439c-878f-b3fea20f5d2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Complete Job configuration\n",
    "production_job = {\n",
    "    \"name\": \"KION_Daily_Orders_ETL_Production\",\n",
    "    \"email_notifications\": {\n",
    "        \"on_failure\": [\"data-team@kion.com\", \"on-call@kion.com\"],\n",
    "        \"on_success\": [\"data-team@kion.com\"]\n",
    "    },\n",
    "    \"webhook_notifications\": {\n",
    "        \"on_failure\": [{\"id\": \"slack-alerts\"}]\n",
    "    },\n",
    "    \"timeout_seconds\": 7200,\n",
    "    \"max_concurrent_runs\": 1,\n",
    "    \"format\": \"MULTI_TASK\",\n",
    "    \"tasks\": [\n",
    "        # Task 1: Validate source data availability\n",
    "        {\n",
    "            \"task_key\": \"validate_source\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/KION/jobs/00_validate_source\",\n",
    "                \"base_parameters\": {\n",
    "                    \"source_path\": \"/Volumes/main/default/kion_data/orders\",\n",
    "                    \"run_date\": \"{{job.start_time.iso_date}}\"\n",
    "                }\n",
    "            },\n",
    "            \"new_cluster\": {\n",
    "                \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                \"node_type_id\": \"Standard_DS3_v2\",\n",
    "                \"num_workers\": 1\n",
    "            },\n",
    "            \"timeout_seconds\": 600,\n",
    "            \"max_retries\": 2\n",
    "        },\n",
    "        # Task 2: Ingest to Bronze\n",
    "        {\n",
    "            \"task_key\": \"ingest_bronze\",\n",
    "            \"depends_on\": [{\"task_key\": \"validate_source\"}],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/KION/jobs/01_ingest_bronze\",\n",
    "                \"base_parameters\": {\n",
    "                    \"source_path\": \"/Volumes/main/default/kion_data/orders\",\n",
    "                    \"target_table\": \"bronze_orders\",\n",
    "                    \"run_date\": \"{{job.start_time.iso_date}}\"\n",
    "                }\n",
    "            },\n",
    "            \"new_cluster\": {\n",
    "                \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                \"node_type_id\": \"Standard_DS3_v2\",\n",
    "                \"autoscale\": {\"min_workers\": 2, \"max_workers\": 4},\n",
    "                \"aws_attributes\": {\"availability\": \"SPOT_WITH_FALLBACK\"}\n",
    "            },\n",
    "            \"timeout_seconds\": 1800,\n",
    "            \"max_retries\": 3,\n",
    "            \"min_retry_interval_millis\": 120000\n",
    "        },\n",
    "        # Task 3: Transform to Silver\n",
    "        {\n",
    "            \"task_key\": \"transform_silver\",\n",
    "            \"depends_on\": [{\"task_key\": \"ingest_bronze\"}],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/KION/jobs/02_transform_silver\",\n",
    "                \"base_parameters\": {\n",
    "                    \"source_table\": \"bronze_orders\",\n",
    "                    \"target_table\": \"silver_orders\",\n",
    "                    \"run_date\": \"{{job.start_time.iso_date}}\"\n",
    "                }\n",
    "            },\n",
    "            \"existing_cluster_id\": \"{{previous_task_cluster}}\",  # Reuse cluster\n",
    "            \"timeout_seconds\": 1800\n",
    "        },\n",
    "        # Task 4a: Aggregate to Gold (daily sales)\n",
    "        {\n",
    "            \"task_key\": \"aggregate_gold_daily\",\n",
    "            \"depends_on\": [{\"task_key\": \"transform_silver\"}],\n",
    "            \"sql_task\": {\n",
    "                \"warehouse_id\": \"abc123warehouse\",\n",
    "                \"query\": {\"query_id\": \"gold_daily_sales_query\"}\n",
    "            }\n",
    "        },\n",
    "        # Task 4b: Customer metrics (parallel with 4a)\n",
    "        {\n",
    "            \"task_key\": \"aggregate_gold_customers\",\n",
    "            \"depends_on\": [{\"task_key\": \"transform_silver\"}],\n",
    "            \"sql_task\": {\n",
    "                \"warehouse_id\": \"abc123warehouse\",\n",
    "                \"query\": {\"query_id\": \"gold_customer_ltv_query\"}\n",
    "            }\n",
    "        },\n",
    "        # Task 5: Data quality checks\n",
    "        {\n",
    "            \"task_key\": \"data_quality_check\",\n",
    "            \"depends_on\": [\n",
    "                {\"task_key\": \"aggregate_gold_daily\"},\n",
    "                {\"task_key\": \"aggregate_gold_customers\"}\n",
    "            ],\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/KION/jobs/99_quality_checks\",\n",
    "                \"base_parameters\": {\n",
    "                    \"tables_to_check\": \"silver_orders,gold_daily_sales,gold_customer_ltv\",\n",
    "                    \"run_date\": \"{{job.start_time.iso_date}}\"\n",
    "                }\n",
    "            },\n",
    "            \"new_cluster\": {\n",
    "                \"spark_version\": \"13.3.x-scala2.12\",\n",
    "                \"node_type_id\": \"Standard_DS3_v2\",\n",
    "                \"num_workers\": 1\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"schedule\": {\n",
    "        \"quartz_cron_expression\": \"0 0 2 * * ?\",  # Daily at 2 AM\n",
    "        \"timezone_id\": \"Europe/Warsaw\",\n",
    "        \"pause_status\": \"UNPAUSED\"\n",
    "    },\n",
    "    \"health\": {\n",
    "        \"rules\": [\n",
    "            {\n",
    "                \"metric\": \"RUN_DURATION_SECONDS\",\n",
    "                \"op\": \"GREATER_THAN\",\n",
    "                \"value\": 5400  # Alert if > 90 minutes\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"‚úÖ Production Job configuration complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11ca3763-6579-4fd1-975e-a3ffca4dc47d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Podsumowanie\n",
    "\n",
    "### Nauczy≈Çe≈õ siƒô:\n",
    "\n",
    "‚úÖ **Multi-task Jobs**: DAG-based workflow orchestration  \n",
    "‚úÖ **Task Types**: Notebook, SQL, DLT, dbt tasks  \n",
    "‚úÖ **Dependencies**: Sequential, parallel, fan-out/fan-in patterns  \n",
    "‚úÖ **Parametryzacja**: Job parameters, task values, dynamic values  \n",
    "‚úÖ **Monitoring**: Email, webhooks, custom metrics  \n",
    "‚úÖ **Retry Logic**: Automatic retry dla transient errors  \n",
    "‚úÖ **Best Practices**: Idempotency, error handling, cost optimization  \n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Jobs = Orchestration**: Coordinate complex workflows\n",
    "2. **Flexibility**: Mix notebook, SQL, DLT, dbt tasks\n",
    "3. **Resilience**: Retry logic + alerting = robust pipelines\n",
    "4. **Cost-aware**: Spot instances + autoscaling + cluster reuse\n",
    "5. **Observability**: System tables + custom monitoring\n",
    "\n",
    "### Nastƒôpne kroki:\n",
    "- **Notebook 04**: Unity Catalog Governance\n",
    "- **Workshop 02**: Hands-on Lakeflow + Jobs Orchestration\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Dodatkowe zasoby\n",
    "\n",
    "- [Databricks Jobs Documentation](https://docs.databricks.com/workflows/jobs/jobs.html)\n",
    "- [Jobs API Reference](https://docs.databricks.com/api/workspace/jobs)\n",
    "- [Orchestration Best Practices](https://docs.databricks.com/workflows/jobs/jobs-best-practices.html)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_databricks_jobs_orchestration",
   "widgets": {
    "run_date": {
     "currentValue": "",
     "nuid": "fdde0823-251b-444d-830a-f43976f9791f",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "run_date",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "run_date",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "source_path": {
     "currentValue": "/Volumes/main/default/kion_data/orders",
     "nuid": "29436273-a30a-4117-a1fb-62b40afda13a",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "/Volumes/main/default/kion_data/orders",
      "label": null,
      "name": "source_path",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/Volumes/main/default/kion_data/orders",
      "label": null,
      "name": "source_path",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "target_table": {
     "currentValue": "bronze_orders",
     "nuid": "4e371ad1-af66-4bb0-853e-f6fd80878375",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bronze_orders",
      "label": null,
      "name": "target_table",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "bronze_orders",
      "label": null,
      "name": "target_table",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
