Dzień 1 — Fundamentals & Exploration

1. Wprowadzenie do Databricks i architektury Lakehouse
	•	Koncepcja Lakehouse: połączenie Data Lake + Data Warehouse
	•	Elementy platformy Databricks:
	•	Workspace
	•	Catalog Explorer
	•	Repos (integracja z Git)
	•	Volumes i DBFS
	•	Compute: all-purpose clusters, job clusters, serverless SQL
	•	Notebooks:
	•	typy komórek, magic commands (%sql, %python, %fs, %md)
	•	omówienie środowiska runtime i bibliotek
	•	Photon Engine – kiedy działa i co optymalizuje
	•	Omówienie Unity Catalog:
	•	katalogi, schematy, tabele
	•	governance na wysokim poziomie
	•	różnice między hive metastore a UC
	•	Zarządzanie klastrami:
	•	autoscaling
	•	spot instances
	•	init scripts
	•	logging i monitoring

⸻

2. Import i eksploracja danych

Ingest danych oraz formaty i ich charakterystyka
	•	CSV (separator, quoting, escape chars)
	•	JSON multiline / single line
	•	Parquet (kolumnowy format, statystyki, predicate pushdown)
	•	Delta (ACID + transakcje)


Wczytywanie danych – DataFrame Reader
	•	spark.read.format().option().load()
	•	Najważniejsze opcje readera:
	•	header, delimiter, schema, inferSchema, mode, quote, escape
	•	Konstruowanie schem:
	•	StructType / StructField
	•	Podgląd danych:
	•	display(), df.show(), df.printSchema(), df.describe()

Podstawowe operacje eksploracyjne
	•	df.columns, df.dtypes, df.count()
	•	summary() vs describe()
	•	sprawdzenie wartości unikalnych: df.select('col').distinct()
	•	sprawdzanie braków danych

⸻

3. Podstawowe transformacje SQL i PySpark

Transformacje kolumnowe
	•	select()
	•	withColumn()
	•	drop()
	•	alias()
	•	when() / otherwise()
	•	regexp_replace(), trim(), lower(), upper()

Filtry i sortowanie
	•	filter()
	•	where()
	•	orderBy()

Agregacje i grupowanie
	•	groupBy()
	•	agg()
	•	funkcje agregujące:
	•	sum, avg, max, min, count, approx_count_distinct
	•	operacje typu rollup / cube
	•	handling nulls w agregacjach

SQL-equivalent
	•	CREATE VIEW, CREATE OR REPLACE VIEW
	•	SELECT, GROUP BY, HAVING, CASE WHEN
	•	Tworzenie widoków logicznych i tymczasowych (GLOBAL TEMP VIEW)

⸻

4. Czyszczenie danych (Data Cleaning / Data Quality Basics)
	•	Obsługa wartości pustych (null):
	•	fillna(), dropna(), coalesce()
	•	Walidacja typów danych:
	•	cast(), to_date(), to_timestamp()
	•	Usuwanie duplikatów:
	•	dropDuplicates(), deduplikacja po kluczach
	•	Standardyzacja:
	•	formaty dat, tekstów, normalizacja kategorii
	•	Typowe problemy jakości danych:
	•	whitespace
	•	niepoprawne kody
	•	błędne wartości liczbowe
	•	inconsistent formatting

⸻

5. Podstawy pracy z widokami i prostymi workflow
	•	Różnice: VIEW vs TABLE vs DELTA TABLE
	•	Rejestracja tabel w Unity Catalog
	•	Przeglądanie tabel w Catalog Explorer
	•	Tworzenie prostych pipeline’ów notebookowych
	•	Databricks Jobs – overview:
	•	taski
	•	retry
	•	harmonogramy
	•	logowanie wykonania

⸻

⸻

Dzień 2 — Lakehouse & Delta Lake

1. Delta Lake – Operacje i mechanika działania

Delta Lake – core features
	•	Transakcyjność ACID
	•	Delta Log – jak zapisuje diff’y
	•	Schema enforcement
	•	Schema evolution (additive, automatic)
	•	Time Travel
	•	Copy-on-write

Operacje na tabelach
	•	CREATE TABLE USING DELTA
	•	Insert / overwrite:
	•	df.write.format("delta").saveAsTable()
	•	mode("overwrite")
	•	UPDATE, DELETE
	•	MERGE INTO – logika zmian na kluczach
	•	DESCRIBE DETAIL, DESCRIBE HISTORY

Optymalizacja tabel
	•	OPTIMIZE
	•	ZORDER BY
	•	VACUUM – retention, bezpieczeństwo, konsystencja czasowa


2. Architektura Lakehouse i Medallion
	•	Bronze / Silver / Gold – logika warstw
	•	Dlaczego nie ETL → dlaczego ELT
	•	Zasady projektowania pipeline’ów:
	•	raw input → standardization → enrichment → business models
	•	Partitioning strategy (wysoki poziom)
	•	Audyt i lineage: jak tworzyć metadane w każdym kroku
	•	Czym jest data quality w kontekście warstw

⸻

3. Batch & Streaming Load: COPY INTO, Auto Loader, Streaming

COPY INTO
	•	kiedy używać
	•	parametry: FILEFORMAT, VALIDATION_MODE, PATTERN
	•	różnica między COPY INTO a read/write

Auto Loader (CloudFiles)
	•	mechanizm file notification
	•	checkpointing
	•	schema inference
	•	schema evolution – jak działa w praktyce
	•	kiedy użyć “trigger once”

Structured Streaming
	•	architektura micro-batch
	•	readStream() i writeStream()
	•	Triggering: once, processingTime
	•	zarządzanie checkpointami
	•	obsługa upsertów z MERGE na streamingu

⸻

4. Pipeline Bronze → Silver → Gold

Bronze – ingestion
	•	raw load
	•	audit columns:
	•	ingest_ts, source_file, ingested_by
	•	definicje kluczy naturalnych

Silver – cleaning & standardization
	•	poprawa typów
	•	deduplikacja per key
	•	sanity checks
	•	obsługa strukturalnych JSONów (flattening z from_json, explode)

Gold – business models & aggregations
	•	KPI modeling
	•	daily / weekly / monthly aggregates
	•	wide vs tall tables
	•	preferencje organizacyjne:
	•	star schema w Lakehouse
	•	denormalizacja dla performance

⸻

5. Optymalizacja i dobre praktyki

Optymalizacja zapytań
	•	predicate pushdown
	•	file pruning
	•	column pruning
	•	analiza planu fizycznego (explain())

Optymalizacja tabel
	•	Wybór kluczy partycjonowania
	•	ZORDER – przegląd przypadków użycia
	•	Dobór rozmiaru plików (small files problem)
	•	Auto optimize / auto compaction



Dzień 3 — Transformation, Governance & Integrations

1. Zaawansowane transformacje w PySpark

Window Functions
	•	Okna:
	•	rowsBetween, rangeBetween
	•	partitionBy, orderBy
	•	Funkcje:
	•	lag, lead
	•	row_number, dense_rank, rank
	•	rolling windows i agregacje ruchome

Operacje na strukturach złożonych
	•	explode()
	•	posexplode()
	•	sequence()
	•	JSON processing:
	•	from_json(), to_json()
	•	schema_of_json()

Funkcje datowe & czasowe
	•	date_trunc, date_add, add_months, last_day
	•	Funkcje timestampów

⸻

2. Lakeflow – Pipeline’y deklaratywne

Koncepcje Lakeflow
	•	deklaratywny sposób definicji pipeline’ów
	•	SQL vs Python API
	•	materalized views / streaming tables
	•	expectations:
	•	warn
	•	drop
	•	fail

Mechanika działania
	•	event log
	•	lineage per tabela
	•	automatic orchestration

⸻

3. Orkiestracja pipelines – Databricks Jobs

Multi-task Jobs
	•	task types:
	•	notebook
	•	DLT
	•	dbt
	•	SQL task
	•	Dependencies – depends_on
	•	Parametryzacja pipelines:
	•	job parameters
	•	widget parameters (dbutils.widgets)
	•	Alerting i monitoring wykonania

⸻

4. Data Governance & Security – Unity Catalog

Elementy UC
	•	Metastore
	•	Catalog
	•	Schemas
	•	Tables / Views / Functions
	•	Volumes

Zarządzanie dostępami
	•	GRANT, REVOKE
	•	Privileges:
	•	SELECT
	•	MODIFY
	•	CREATE TABLE
	•	EXECUTE
	•	Masking i row-level security
	•	Securable objects – inheritance

Lineage & Audit
	•	analiza lineage end-to-end
	•	przeglądanie metadanych tabel
	•	audit logging i aktywność użytkowników

Delta Sharing
	•	Jak udostępniać dane organizacjom zewnętrznym
	•	Secure data sharing przez protocol Delta

⸻

5. Integracje BI & ML

Power BI
	•	Direct Lake vs Direct Query
	•	optymalne połączenie do Gold Layer
	•	modele semanticzne

SAP Datasphere – overview integracji
	•	łączenie przez JDBC
	•	przepływ danych ELT

Dremio / Athena / inne narzędzia
	•	Federated query engines
	•	kiedy warto

ML – podstawy na Databricks
	•	MLflow concepts:
	•	experiments
	•	metrics
	•	parameters
	•	artifacts
	•	Feature Store – podstawy działania
	•	Proste modele ML Spark MLlib
	•	Wykorzystanie Gold Layer jako datasetu