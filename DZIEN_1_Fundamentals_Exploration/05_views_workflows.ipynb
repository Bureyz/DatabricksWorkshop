{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87d83b18",
   "metadata": {},
   "source": [
    "# Widoki i podstawowe workflow w Databricks - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Nauczenie się różnic między widokami a tabelami, zarządzania obiektami w Unity Catalog oraz tworzenia prostych pipeline'ów notebookowych w Databricks Jobs.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Różnice między VIEW, TABLE i DELTA TABLE\n",
    "- Typy widoków: temp views, global temp views, persistent views\n",
    "- Rejestracja obiektów w Unity Catalog\n",
    "- Przeglądanie metadanych w Catalog Explorer\n",
    "- Proste pipeline'y notebookowe\n",
    "- Wprowadzenie do Databricks Jobs: taski, retry, harmonogramy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c1170",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 1 - Fundamentals & Exploration\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3f679",
   "metadata": {},
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "### Czym są widoki i tabele?\n",
    "\n",
    "W Databricks istnieją trzy główne typy obiektów przechowujących dane:\n",
    "\n",
    "**1. TABLE (Tabela)**\n",
    "- Fizyczny obiekt przechowujący dane na dysku\n",
    "- Dane są zapisane w określonym formacie (Parquet, Delta, CSV, etc.)\n",
    "- Wymaga miejsca na dysku\n",
    "- Trwała (persistent) - przetrwa restart klastra\n",
    "\n",
    "**2. DELTA TABLE (Tabela Delta)**\n",
    "- Specjalny typ tabeli używający formatu Delta Lake\n",
    "- Obsługuje ACID transactions\n",
    "- Umożliwia Time Travel, MERGE, UPDATE, DELETE\n",
    "- Zalecany format dla data lakehouse\n",
    "\n",
    "**3. VIEW (Widok)**\n",
    "- Wirtualny obiekt - nie przechowuje danych fizycznie\n",
    "- Zapisuje tylko definicję zapytania SQL\n",
    "- Wykonywany dynamicznie przy każdym odwołaniu\n",
    "- Przydatny do enkapsulacji logiki biznesowej\n",
    "\n",
    "### Typy widoków w Spark/Databricks:\n",
    "\n",
    "| Typ widoku | Zasięg | Trwałość | Namespace |\n",
    "|------------|--------|----------|----------|\n",
    "| **Temp View** | Sesja Spark (notebook) | Do końca sesji | Lokalny w sesji |\n",
    "| **Global Temp View** | Wszystkie sesje w klastrze | Do restartu klastra | `global_temp` database |\n",
    "| **Persistent View** | Wszystkie użytkownicy | Trwały w Unity Catalog | Catalog.Schema |\n",
    "\n",
    "### Kiedy używać którego?\n",
    "\n",
    "- **Temp View**: Tymczasowe przetwarzanie w jednym notebooku\n",
    "- **Global Temp View**: Współdzielenie danych między notebookami w tym samym klastrze\n",
    "- **Persistent View**: Udostępnianie logiki biznesowej między zespołami\n",
    "- **Delta Table**: Główny format do przechowywania danych w lakehouse\n",
    "\n",
    "### Unity Catalog - hierarchia obiektów:\n",
    "\n",
    "```\n",
    "Metastore\n",
    "  └── Catalog (np. kion_training)\n",
    "      └── Schema/Database (np. bronze, silver, gold)\n",
    "          ├── Tables\n",
    "          ├── Views\n",
    "          ├── Functions\n",
    "          └── Volumes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0fc33a",
   "metadata": {},
   "source": [
    "## Inicjalizacja środowiska\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71596e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c90052b",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aaf91a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "# Ścieżki do danych\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "# Wyświetl kontekst użytkownika (zmienne z 00_setup)\n",
    "print(\"=== Kontekst użytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"Użytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog i schemat jako domyślne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "print(f\"\\n[INFO] Domyślny katalog: {CATALOG}\")\n",
    "print(f\"[INFO] Domyślny schemat: {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b91f765",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Część 1: Przygotowanie danych testowych\n",
    "\n",
    "### 1.1. Wczytanie danych do DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e84c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytaj dane klientów z CSV\n",
    "customers_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Wczytano {customers_df.count()} klientów\")\n",
    "customers_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f9ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytaj dane zamówień z JSON\n",
    "orders_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Wczytano {orders_df.count()} zamówień\")\n",
    "orders_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ac37d7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Część 2: Temporary Views (Widoki tymczasowe)\n",
    "\n",
    "### 2.1. Tworzenie Temp View\n",
    "\n",
    "**Temp View** istnieje tylko w bieżącej sesji Spark (notebooku)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cbc412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utwórz temporary view z DataFrame\n",
    "customers_df.createOrReplaceTempView(\"customers_temp_view\")\n",
    "\n",
    "print(\"[INFO] Utworzono temp view: customers_temp_view\")\n",
    "\n",
    "# Sprawdź, czy widok istnieje\n",
    "print(\"\\n[INFO] Lista temporary views:\")\n",
    "spark.sql(\"SHOW VIEWS\").filter(\"isTemporary = true\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3d6b36",
   "metadata": {},
   "source": [
    "### 2.2. Zapytanie do Temp View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ec5f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapytanie SQL do temp view\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT country, COUNT(*) as customer_count\n",
    "    FROM customers_temp_view\n",
    "    GROUP BY country\n",
    "    ORDER BY customer_count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"[INFO] Liczba klientów według krajów:\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f29646",
   "metadata": {},
   "source": [
    "### 2.3. Usunięcie Temp View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8490cdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usuń temporary view\n",
    "spark.catalog.dropTempView(\"customers_temp_view\")\n",
    "\n",
    "print(\"[INFO] Usunięto temp view: customers_temp_view\")\n",
    "\n",
    "# Weryfikacja\n",
    "try:\n",
    "    spark.sql(\"SELECT * FROM customers_temp_view LIMIT 1\")\n",
    "except Exception as e:\n",
    "    print(f\"[EXPECTED ERROR] Widok nie istnieje: {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232496f1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Część 3: Global Temporary Views\n",
    "\n",
    "### 3.1. Tworzenie Global Temp View\n",
    "\n",
    "**Global Temp View** jest dostępny dla wszystkich sesji w klastrze, ale tylko do restartu klastra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb91eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utwórz global temporary view\n",
    "orders_df.createOrReplaceGlobalTempView(\"orders_global_temp_view\")\n",
    "\n",
    "print(\"[INFO] Utworzono global temp view: orders_global_temp_view\")\n",
    "print(\"[INFO] Dostęp przez namespace: global_temp.orders_global_temp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1861dab1",
   "metadata": {},
   "source": [
    "### 3.2. Zapytanie do Global Temp View\n",
    "\n",
    "**Uwaga:** Musisz użyć prefiksu `global_temp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003e1356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapytanie SQL do global temp view (z prefiksem global_temp)\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT status, COUNT(*) as order_count, SUM(total_amount) as total_revenue\n",
    "    FROM global_temp.orders_global_temp_view\n",
    "    GROUP BY status\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"[INFO] Statystyki zamówień według statusu:\")\n",
    "result_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798f274b",
   "metadata": {},
   "source": [
    "### 3.3. Usunięcie Global Temp View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255eecc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usuń global temporary view\n",
    "spark.catalog.dropGlobalTempView(\"orders_global_temp_view\")\n",
    "\n",
    "print(\"[INFO] Usunięto global temp view: orders_global_temp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377d24b0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Część 4: Delta Tables (Tabele trwałe)\n",
    "\n",
    "### 4.1. Tworzenie Delta Table w Unity Catalog\n",
    "\n",
    "**Delta Table** to fizyczna tabela zapisana w formacie Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea9a0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nazwa tabeli w Unity Catalog\n",
    "CUSTOMERS_TABLE = f\"{CATALOG}.{BRONZE_SCHEMA}.customers\"\n",
    "\n",
    "# Zapisz DataFrame jako Delta Table\n",
    "customers_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(CUSTOMERS_TABLE)\n",
    "\n",
    "print(f\"[INFO] Utworzono Delta Table: {CUSTOMERS_TABLE}\")\n",
    "\n",
    "# Sprawdź liczbę rekordów\n",
    "count = spark.table(CUSTOMERS_TABLE).count()\n",
    "print(f\"[INFO] Liczba rekordów w tabeli: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0375ee",
   "metadata": {},
   "source": [
    "### 4.2. Metadane Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea00aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź szczegóły tabeli\n",
    "print(\"[INFO] Szczegóły Delta Table:\")\n",
    "spark.sql(f\"DESCRIBE DETAIL {CUSTOMERS_TABLE}\").show(truncate=False)\n",
    "\n",
    "# Sprawdź schemat tabeli\n",
    "print(\"\\n[INFO] Schemat tabeli:\")\n",
    "spark.sql(f\"DESCRIBE {CUSTOMERS_TABLE}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e9ea3b",
   "metadata": {},
   "source": [
    "### 4.3. Tworzenie drugiej tabeli (Orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719c98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nazwa tabeli zamówień\n",
    "ORDERS_TABLE = f\"{CATALOG}.{BRONZE_SCHEMA}.orders\"\n",
    "\n",
    "# Zapisz jako Delta Table\n",
    "orders_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(ORDERS_TABLE)\n",
    "\n",
    "print(f\"[INFO] Utworzono Delta Table: {ORDERS_TABLE}\")\n",
    "print(f\"[INFO] Liczba rekordów: {spark.table(ORDERS_TABLE).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b34dfc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Część 5: Persistent Views (Widoki trwałe)\n",
    "\n",
    "### 5.1. Tworzenie Persistent View w Unity Catalog\n",
    "\n",
    "**Persistent View** jest zapisywany w Unity Catalog i dostępny dla wszystkich użytkowników."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12040259",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utwórz persistent view z logiką biznesową\n",
    "VIEW_NAME = f\"{CATALOG}.{SILVER_SCHEMA}.customer_order_summary\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {VIEW_NAME} AS\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.customer_name,\n",
    "        c.country,\n",
    "        COUNT(o.order_id) as total_orders,\n",
    "        COALESCE(SUM(o.total_amount), 0) as total_spent,\n",
    "        COALESCE(AVG(o.total_amount), 0) as avg_order_value\n",
    "    FROM {CUSTOMERS_TABLE} c\n",
    "    LEFT JOIN {ORDERS_TABLE} o ON c.customer_id = o.customer_id\n",
    "    GROUP BY c.customer_id, c.customer_name, c.country\n",
    "\"\"\")\n",
    "\n",
    "print(f\"[INFO] Utworzono persistent view: {VIEW_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd27296d",
   "metadata": {},
   "source": [
    "### 5.2. Zapytanie do Persistent View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b035b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapytanie do widoku - top 10 klientów według wydatków\n",
    "result_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_name,\n",
    "        country,\n",
    "        total_orders,\n",
    "        ROUND(total_spent, 2) as total_spent,\n",
    "        ROUND(avg_order_value, 2) as avg_order_value\n",
    "    FROM {VIEW_NAME}\n",
    "    WHERE total_orders > 0\n",
    "    ORDER BY total_spent DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"[INFO] Top 10 klientów według wydatków:\")\n",
    "result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06b7148",
   "metadata": {},
   "source": [
    "### 5.3. Metadane Persistent View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54924508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź definicję widoku\n",
    "print(\"[INFO] Definicja widoku:\")\n",
    "spark.sql(f\"SHOW CREATE TABLE {VIEW_NAME}\").show(truncate=False)\n",
    "\n",
    "# Lista wszystkich widoków w schemacie\n",
    "print(f\"\\n[INFO] Widoki w schemacie {SILVER_SCHEMA}:\")\n",
    "spark.sql(f\"SHOW VIEWS IN {CATALOG}.{SILVER_SCHEMA}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9129f2e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Część 6: Przeglądanie obiektów w Unity Catalog\n",
    "\n",
    "### 6.1. Lista katalogów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81248bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyświetl wszystkie dostępne katalogi\n",
    "print(\"[INFO] Dostępne katalogi:\")\n",
    "spark.sql(\"SHOW CATALOGS\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c418206",
   "metadata": {},
   "source": [
    "### 6.2. Lista schematów w katalogu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca2c7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyświetl schematy w bieżącym katalogu\n",
    "print(f\"[INFO] Schematy w katalogu {CATALOG}:\")\n",
    "spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b1e98",
   "metadata": {},
   "source": [
    "### 6.3. Lista tabel w schemacie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3027035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyświetl tabele w schemacie bronze\n",
    "print(f\"[INFO] Tabele w schemacie {BRONZE_SCHEMA}:\")\n",
    "spark.sql(f\"SHOW TABLES IN {CATALOG}.{BRONZE_SCHEMA}\").show(truncate=False)\n",
    "\n",
    "# Wyświetl widoki w schemacie silver\n",
    "print(f\"\\n[INFO] Widoki w schemacie {SILVER_SCHEMA}:\")\n",
    "spark.sql(f\"SHOW VIEWS IN {CATALOG}.{SILVER_SCHEMA}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233b47a9",
   "metadata": {},
   "source": [
    "### 6.4. Szczegółowe informacje o tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af47e9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź właściciela, lokalizację, format tabeli\n",
    "print(f\"[INFO] Szczegóły tabeli {CUSTOMERS_TABLE}:\")\n",
    "spark.sql(f\"DESCRIBE EXTENDED {CUSTOMERS_TABLE}\").show(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ae914",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Część 7: Porównanie VIEW vs TABLE\n",
    "\n",
    "### 7.1. Test wydajności: View vs Materialized Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0dba81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test 1: Zapytanie do VIEW (obliczane on-the-fly)\n",
    "start_view = time.time()\n",
    "result_view = spark.sql(f\"SELECT COUNT(*) as cnt FROM {VIEW_NAME}\").collect()\n",
    "time_view = time.time() - start_view\n",
    "\n",
    "print(f\"[BENCHMARK] Czas wykonania VIEW: {time_view:.3f} sekund\")\n",
    "print(f\"[BENCHMARK] Wynik: {result_view[0]['cnt']} rekordów\")\n",
    "\n",
    "# Test 2: Materializacja wyniku jako tabela\n",
    "MATERIALIZED_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.customer_order_summary_materialized\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {MATERIALIZED_TABLE} AS\n",
    "    SELECT * FROM {VIEW_NAME}\n",
    "\"\"\")\n",
    "\n",
    "# Test wydajności tabeli zmaterializowanej\n",
    "start_table = time.time()\n",
    "result_table = spark.sql(f\"SELECT COUNT(*) as cnt FROM {MATERIALIZED_TABLE}\").collect()\n",
    "time_table = time.time() - start_table\n",
    "\n",
    "print(f\"\\n[BENCHMARK] Czas wykonania MATERIALIZED TABLE: {time_table:.3f} sekund\")\n",
    "print(f\"[BENCHMARK] Wynik: {result_table[0]['cnt']} rekordów\")\n",
    "\n",
    "# Porównanie\n",
    "if time_view > time_table:\n",
    "    speedup = (time_view - time_table) / time_view * 100\n",
    "    print(f\"\\n[WYNIK] Materialized table była szybsza o {speedup:.1f}%\")\n",
    "else:\n",
    "    print(f\"\\n[WYNIK] Różnica w wydajności nieznaczna dla tego datasetu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28549a1",
   "metadata": {},
   "source": [
    "### 7.2. Kiedy używać VIEW, a kiedy TABLE?\n",
    "\n",
    "**Użyj VIEW gdy:**\n",
    "- Chcesz enkapsulować logikę biznesową\n",
    "- Dane źródłowe często się zmieniają\n",
    "- Zapytanie jest proste i szybkie\n",
    "- Nie chcesz duplikować danych\n",
    "\n",
    "**Użyj MATERIALIZED TABLE gdy:**\n",
    "- Zapytanie jest skomplikowane i wolne\n",
    "- Dane są czytane bardzo często\n",
    "- Dane źródłowe rzadko się zmieniają\n",
    "- Potrzebujesz indeksowania (ZORDER) dla wydajności"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69205f84",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Część 8: Wprowadzenie do Databricks Jobs\n",
    "\n",
    "### 8.1. Czym jest Databricks Job?\n",
    "\n",
    "**Databricks Job** to mechanizm automatyzacji i orkiestracji notebooków, skryptów lub zapytań SQL.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **Job** - kontener dla jednego lub więcej tasków\n",
    "- **Task** - pojedyncza jednostka pracy (notebook, skrypt, SQL, JAR)\n",
    "- **Cluster** - zasoby obliczeniowe do wykonania zadania\n",
    "- **Trigger** - sposób uruchomienia (manual, scheduled, continuous)\n",
    "- **Retry** - automatyczne ponawianie w przypadku błędu\n",
    "\n",
    "### 8.2. Typy tasków:\n",
    "\n",
    "| Typ tasku | Opis | Use case |\n",
    "|-----------|------|----------|\n",
    "| **Notebook** | Wykonanie notebooka Databricks | ETL pipelines, data processing |\n",
    "| **Python script** | Uruchomienie skryptu .py | Data validation, custom logic |\n",
    "| **JAR** | Uruchomienie aplikacji Spark | Legacy Spark applications |\n",
    "| **SQL** | Wykonanie zapytania SQL | Data transformation, reporting |\n",
    "| **dbt** | Uruchomienie projektu dbt | Modern data transformation |\n",
    "| **Delta Live Tables** | Pipeline DLT | Declarative ETL pipelines |\n",
    "\n",
    "### 8.3. Harmonogramy (Schedules):\n",
    "\n",
    "- **Manual** - uruchomienie na żądanie\n",
    "- **Cron expression** - np. `0 0 * * *` (codziennie o północy)\n",
    "- **Continuous** - ciągłe przetwarzanie (streaming)\n",
    "\n",
    "### 8.4. Retry i timeout:\n",
    "\n",
    "- **Max retries** - maksymalna liczba prób (0-3)\n",
    "- **Retry interval** - czas między próbami (sekundy)\n",
    "- **Timeout** - maksymalny czas wykonania tasku"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e12322",
   "metadata": {},
   "source": [
    "### 8.5. Przykład prostego pipeline'u notebookowego\n",
    "\n",
    "W praktyce pipeline składałby się z kilku notebooków wykonywanych sekwencyjnie:\n",
    "\n",
    "```\n",
    "Pipeline: Daily Customer Analytics\n",
    "├── Task 1: Ingest raw data (notebook: 01_ingest.ipynb)\n",
    "│   └── Retry: 2x, Timeout: 10 min\n",
    "├── Task 2: Clean and transform (notebook: 02_transform.ipynb)\n",
    "│   └── Depends on: Task 1\n",
    "├── Task 3: Aggregate metrics (notebook: 03_aggregate.ipynb)\n",
    "│   └── Depends on: Task 2\n",
    "└── Task 4: Send notification (Python script: notify.py)\n",
    "    └── Depends on: Task 3\n",
    "```\n",
    "\n",
    "**W Databricks UI:**\n",
    "1. Workflows → Create Job\n",
    "2. Add tasks (notebooks)\n",
    "3. Skonfiguruj dependencies między taskami\n",
    "4. Ustaw harmonogram (schedule)\n",
    "5. Skonfiguruj cluster\n",
    "6. Uruchom job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9d584a",
   "metadata": {},
   "source": [
    "### 8.6. Przekazywanie parametrów między notebookami\n",
    "\n",
    "**Metoda 1: Widgets (dla pojedynczego notebooka)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d87c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład: Tworzenie widgetu do parametryzacji\n",
    "# (w Databricks to utworzy interaktywny element UI)\n",
    "\n",
    "# dbutils.widgets.text(\"processing_date\", \"2024-01-01\", \"Processing Date\")\n",
    "# processing_date = dbutils.widgets.get(\"processing_date\")\n",
    "\n",
    "# W tym notebooku używamy bezpośrednio wartości\n",
    "processing_date = \"2024-01-01\"\n",
    "print(f\"[INFO] Processing date: {processing_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1abd807",
   "metadata": {},
   "source": [
    "**Metoda 2: dbutils.notebook.run (dla pipeline'u notebooków)**\n",
    "\n",
    "```python\n",
    "# Uruchomienie kolejnego notebooka z parametrami\n",
    "result = dbutils.notebook.run(\n",
    "    \"./02_next_notebook\",\n",
    "    timeout_seconds=600,\n",
    "    arguments={\"date\": \"2024-01-01\", \"mode\": \"full\"}\n",
    ")\n",
    "print(f\"Notebook result: {result}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df057d57",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Część 9: Demonstracja - Prosty pipeline w jednym notebooku\n",
    "\n",
    "### 9.1. Pipeline: Bronze → Silver → Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7610d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== PIPELINE START ===\")\n",
    "print(f\"Timestamp: {datetime.now()}\")\n",
    "\n",
    "# Step 1: Bronze - dane już wczytane wcześniej\n",
    "print(\"\\n[STEP 1: BRONZE] Dane surowe już istnieją\")\n",
    "print(f\"  - Tabela customers: {spark.table(CUSTOMERS_TABLE).count()} rekordów\")\n",
    "print(f\"  - Tabela orders: {spark.table(ORDERS_TABLE).count()} rekordów\")\n",
    "\n",
    "# Step 2: Silver - widok z logiką biznesową już istnieje\n",
    "print(\"\\n[STEP 2: SILVER] Widok customer_order_summary już istnieje\")\n",
    "silver_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {VIEW_NAME}\").collect()[0]['cnt']\n",
    "print(f\"  - Liczba rekordów w widoku: {silver_count}\")\n",
    "\n",
    "# Step 3: Gold - agregacja na poziomie kraju\n",
    "print(\"\\n[STEP 3: GOLD] Tworzenie agregacji na poziomie kraju\")\n",
    "GOLD_TABLE = f\"{CATALOG}.{GOLD_SCHEMA}.country_metrics\"\n",
    "\n",
    "gold_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        country,\n",
    "        COUNT(DISTINCT customer_id) as total_customers,\n",
    "        SUM(total_orders) as total_orders,\n",
    "        ROUND(SUM(total_spent), 2) as total_revenue,\n",
    "        ROUND(AVG(avg_order_value), 2) as avg_order_value,\n",
    "        current_timestamp() as updated_at\n",
    "    FROM {VIEW_NAME}\n",
    "    WHERE total_orders > 0\n",
    "    GROUP BY country\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "# Zapisz do Gold layer\n",
    "gold_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(GOLD_TABLE)\n",
    "\n",
    "print(f\"  - Utworzono tabelę Gold: {GOLD_TABLE}\")\n",
    "print(f\"  - Liczba krajów: {gold_df.count()}\")\n",
    "\n",
    "print(\"\\n=== PIPELINE COMPLETED ===\")\n",
    "print(f\"Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "178d0906",
   "metadata": {},
   "source": [
    "### 9.2. Weryfikacja wyniku pipeline'u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f25665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyświetl wynik agregacji Gold\n",
    "print(\"[INFO] Metryki według krajów (Gold Layer):\")\n",
    "spark.table(GOLD_TABLE).show(truncate=False)\n",
    "\n",
    "# Sprawdź timestamp aktualizacji\n",
    "latest_update = spark.sql(f\"SELECT MAX(updated_at) as last_update FROM {GOLD_TABLE}\").collect()[0]['last_update']\n",
    "print(f\"\\n[INFO] Ostatnia aktualizacja: {latest_update}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec571c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist - Co powinieneś uzyskać:\n",
    "- [x] Tabela `customers` w schemacie Bronze\n",
    "- [x] Tabela `orders` w schemacie Bronze\n",
    "- [x] Widok `customer_order_summary` w schemacie Silver\n",
    "- [x] Tabela `country_metrics` w schemacie Gold\n",
    "- [x] Zrozumienie różnic między VIEW, TEMP VIEW, GLOBAL TEMP VIEW\n",
    "- [x] Znajomość podstaw Databricks Jobs\n",
    "\n",
    "### Komendy weryfikacyjne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0524d968",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== WERYFIKACJA WYNIKÓW ===\")\n",
    "\n",
    "# 1. Sprawdź tabele Bronze\n",
    "print(\"\\n[1] Tabele Bronze:\")\n",
    "spark.sql(f\"SHOW TABLES IN {CATALOG}.{BRONZE_SCHEMA}\").show(truncate=False)\n",
    "\n",
    "# 2. Sprawdź widoki Silver\n",
    "print(\"\\n[2] Widoki Silver:\")\n",
    "spark.sql(f\"SHOW VIEWS IN {CATALOG}.{SILVER_SCHEMA}\").show(truncate=False)\n",
    "\n",
    "# 3. Sprawdź tabele Gold\n",
    "print(\"\\n[3] Tabele Gold:\")\n",
    "spark.sql(f\"SHOW TABLES IN {CATALOG}.{GOLD_SCHEMA}\").show(truncate=False)\n",
    "\n",
    "# 4. Sprawdź liczby rekordów\n",
    "print(\"\\n[4] Liczba rekordów:\")\n",
    "print(f\"  - customers (Bronze): {spark.table(CUSTOMERS_TABLE).count()}\")\n",
    "print(f\"  - orders (Bronze): {spark.table(ORDERS_TABLE).count()}\")\n",
    "print(f\"  - customer_order_summary (Silver VIEW): {spark.sql(f'SELECT COUNT(*) as cnt FROM {VIEW_NAME}').collect()[0]['cnt']}\")\n",
    "print(f\"  - country_metrics (Gold): {spark.table(GOLD_TABLE).count()}\")\n",
    "\n",
    "print(\"\\n[SUCCESS] Wszystkie obiekty zostały utworzone poprawnie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5a1383",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Problem 1: \"Table or view not found\"\n",
    "**Objawy:**\n",
    "- Błąd podczas zapytania SQL: `Table or view 'xyz' not found`\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Sprawdź, czy obiekt istnieje\n",
    "spark.sql(\"SHOW TABLES IN catalog.schema\").show()\n",
    "\n",
    "# Użyj pełnej ścieżki (three-level namespace)\n",
    "spark.sql(\"SELECT * FROM catalog.schema.table_name\")\n",
    "```\n",
    "\n",
    "### Problem 2: Temp view nie jest widoczny\n",
    "**Objawy:**\n",
    "- Temp view utworzony w jednym notebooku nie działa w innym\n",
    "\n",
    "**Rozwiązanie:**\n",
    "- Temp views są lokalne dla sesji - użyj Global Temp View lub Persistent View\n",
    "- Global Temp View wymaga prefiksu `global_temp.`\n",
    "\n",
    "### Problem 3: \"Permission denied\" przy tworzeniu tabeli\n",
    "**Objawy:**\n",
    "- Brak uprawnień do utworzenia tabeli w schemacie\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```sql\n",
    "-- Administrator musi nadać uprawnienia\n",
    "GRANT CREATE TABLE ON SCHEMA catalog.schema TO `user@domain.com`;\n",
    "GRANT MODIFY ON SCHEMA catalog.schema TO `user@domain.com`;\n",
    "```\n",
    "\n",
    "### Problem 4: View pokazuje stare dane\n",
    "**Objawy:**\n",
    "- Dane w widoku nie są aktualne mimo aktualizacji tabeli źródłowej\n",
    "\n",
    "**Rozwiązanie:**\n",
    "- Widoki są dynamiczne - powinny pokazywać aktualne dane\n",
    "- Sprawdź cache: `spark.catalog.clearCache()`\n",
    "- Dla danych historycznych użyj materializowanej tabeli\n",
    "\n",
    "### Debugging tips:\n",
    "- Sprawdź current catalog: `SELECT current_catalog()`\n",
    "- Sprawdź current schema: `SELECT current_schema()`\n",
    "- Lista wszystkich obiektów: `SHOW TABLES` i `SHOW VIEWS`\n",
    "- Definicja widoku: `SHOW CREATE TABLE view_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37da9913",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### Zarządzanie widokami:\n",
    "- Używaj **Persistent Views** dla logiki biznesowej współdzielonej między zespołami\n",
    "- Używaj **Temp Views** dla tymczasowych przekształceń w ramach jednego notebooka\n",
    "- Dokumentuj logikę widoków w komentarzach SQL\n",
    "- Nazwy widoków powinny jasno wskazywać ich przeznaczenie (np. `vw_customer_360`, `vw_daily_sales`)\n",
    "\n",
    "### Zarządzanie tabelami:\n",
    "- Zawsze używaj **Delta format** dla tabel w lakehouse\n",
    "- Stosuj konwencję nazewnictwa: `{layer}_{domain}_{entity}` (np. `bronze_sales_transactions`)\n",
    "- Dodawaj kolumny audytowe: `created_at`, `updated_at`, `source_system`\n",
    "- Regularnie uruchamiaj `OPTIMIZE` i `VACUUM` dla tabel Delta\n",
    "\n",
    "### Unity Catalog:\n",
    "- Organizuj obiekty w logiczne schematy: bronze, silver, gold\n",
    "- Używaj three-level namespace: `catalog.schema.table`\n",
    "- Nadawaj uprawnienia na poziomie schematu, nie tabeli\n",
    "- Regularnie przeglądaj logi audytowe\n",
    "\n",
    "### Databricks Jobs:\n",
    "- Dziel duże notebooki na mniejsze, modularne taski\n",
    "- Używaj parametryzacji przez widgets\n",
    "- Konfiguruj retry dla tasków podatnych na błędy (np. API calls)\n",
    "- Monitoruj czas wykonania i ustawiaj sensowne timeouty\n",
    "- Używaj alertów e-mail/Slack dla krytycznych pipeline'ów\n",
    "- Testuj pipeline lokalnie przed wdrożeniem do produkcji\n",
    "\n",
    "### Wydajność:\n",
    "- Materializuj widoki, które są często używane w złożonych zapytaniach\n",
    "- Używaj partycjonowania dla dużych tabel\n",
    "- Cache'uj wyniki pośrednie w pipeline'ach\n",
    "- Monitoruj Spark UI dla identyfikacji bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60732f3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "### Co zostało osiągnięte:\n",
    "- Poznanie różnic między VIEW, TABLE i DELTA TABLE\n",
    "- Tworzenie Temp Views, Global Temp Views i Persistent Views\n",
    "- Rejestracja tabel i widoków w Unity Catalog\n",
    "- Przeglądanie metadanych w Catalog Explorer\n",
    "- Zrozumienie podstaw Databricks Jobs i pipeline'ów\n",
    "- Implementacja prostego pipeline'u Bronze → Silver → Gold\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "1. **VIEW vs TABLE**: View nie przechowuje danych, TABLE materializuje wyniki na dysku\n",
    "2. **Delta Lake**: Zalecany format dla wszystkich tabel w lakehouse (ACID, Time Travel, MERGE)\n",
    "3. **Unity Catalog**: Centralne zarządzanie metadanymi, three-level namespace\n",
    "4. **Databricks Jobs**: Automatyzacja i orkiestracja notebooków w pipeline'y\n",
    "5. **Medallion Architecture**: Bronze (raw) → Silver (cleaned) → Gold (aggregated)\n",
    "\n",
    "### Quick Reference - Najważniejsze komendy:\n",
    "\n",
    "| Operacja | PySpark | SQL |\n",
    "|----------|---------|-----|\n",
    "| Temp View | `df.createOrReplaceTempView(\"name\")` | `CREATE TEMP VIEW name AS SELECT ...` |\n",
    "| Global Temp View | `df.createOrReplaceGlobalTempView(\"name\")` | `CREATE GLOBAL TEMP VIEW name AS SELECT ...` |\n",
    "| Persistent View | - | `CREATE VIEW catalog.schema.name AS SELECT ...` |\n",
    "| Delta Table | `df.write.format(\"delta\").saveAsTable(\"name\")` | `CREATE TABLE name USING DELTA AS SELECT ...` |\n",
    "| Lista tabel | `spark.catalog.listTables()` | `SHOW TABLES IN catalog.schema` |\n",
    "| Lista widoków | - | `SHOW VIEWS IN catalog.schema` |\n",
    "| Metadane | - | `DESCRIBE EXTENDED table_name` |\n",
    "| Drop view | `spark.catalog.dropTempView(\"name\")` | `DROP VIEW IF EXISTS name` |\n",
    "\n",
    "### Następne kroki:\n",
    "- **Kolejny notebook**: `DZIEN_2_Lakehouse_Delta_Lake/01_delta_lake_operations.ipynb`\n",
    "- **Warsztat praktyczny**: `workshops/03_views_basic_jobs_workshop.ipynb`\n",
    "- **Materiały dodatkowe**: \n",
    "  - [Unity Catalog documentation](https://docs.databricks.com/data-governance/unity-catalog/index.html)\n",
    "  - [Databricks Jobs documentation](https://docs.databricks.com/workflows/jobs/jobs.html)\n",
    "  - [Delta Lake best practices](https://docs.databricks.com/delta/best-practices.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d0779d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czyszczenie zasobów\n",
    "\n",
    "Posprzątaj zasoby utworzone podczas notebooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f05ee4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UWAGA: Uruchom tylko jeśli chcesz usunąć wszystkie utworzone obiekty\n",
    "\n",
    "# Usuń tabele Bronze\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CUSTOMERS_TABLE}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {ORDERS_TABLE}\")\n",
    "\n",
    "# Usuń widoki i tabele Silver\n",
    "# spark.sql(f\"DROP VIEW IF EXISTS {VIEW_NAME}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {MATERIALIZED_TABLE}\")\n",
    "\n",
    "# Usuń tabele Gold\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {GOLD_TABLE}\")\n",
    "\n",
    "# Wyczyść cache\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "print(\"[INFO] Zasoby zostały wyczyszczone (odkomentuj kod, aby wykonać)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
