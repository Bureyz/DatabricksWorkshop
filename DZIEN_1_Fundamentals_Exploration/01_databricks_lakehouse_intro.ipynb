{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b648629d",
   "metadata": {},
   "source": [
    "# Wprowadzenie do Databricks Lakehouse - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Zrozumienie koncepcji Lakehouse, poznanie podstawowych element√≥w platformy Databricks oraz konfiguracji ≈õrodowiska Unity Catalog.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Koncepcja Lakehouse (Data Lake + Data Warehouse)\n",
    "- Elementy platformy: Workspace, Catalog Explorer, Repos, Volumes, DBFS\n",
    "- Compute: clusters, autoscaling, spot instances, Photon Engine\n",
    "- Notebooks: magic commands (%sql, %python, %fs, %md)\n",
    "- Unity Catalog overview: katalogi, schematy, tabele\n",
    "- R√≥≈ºnice miƒôdzy Hive Metastore a Unity Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75cfd0",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzie≈Ñ szkolenia**: Dzie≈Ñ 1 - Fundamentals & Exploration\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog w≈ÇƒÖczony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z 2-4 workers\n",
    "- **Czas trwania**: 45 minut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477153c2",
   "metadata": {},
   "source": [
    "## Wstƒôp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie ewolucji architektur danych i miejsca Lakehouse w tym kontek≈õcie.\n",
    "\n",
    "**Podstawowe pojƒôcia:**\n",
    "- **Data Lake**: Scentralizowane repozytorium przechowujƒÖce surowe dane w r√≥≈ºnych formatach (strukturalne, semi-strukturalne, niestrukturalne)\n",
    "- **Data Warehouse**: Zoptymalizowane repozytorium danych strukturalnych do analityki biznesowej i raportowania\n",
    "- **Lakehouse**: Nowoczesna architektura ≈ÇƒÖczƒÖca elastyczno≈õƒá Data Lake z niezawodno≈õciƒÖ i wydajno≈õciƒÖ Data Warehouse\n",
    "\n",
    "**Dlaczego to wa≈ºne?**\n",
    "Lakehouse eliminuje potrzebƒô utrzymywania dw√≥ch oddzielnych system√≥w (Data Lake + Data Warehouse), redukujƒÖc koszty, z≈Ço≈ºono≈õƒá i op√≥≈∫nienia w dostƒôpie do danych. Dziƒôki Delta Lake uzyskujemy transakcyjno≈õƒá ACID, wersjonowanie danych i optymalizacjƒô zapyta≈Ñ bezpo≈õrednio na plikach w Data Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08527570",
   "metadata": {},
   "source": [
    "## Izolacja per u≈ºytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalog√≥w i schemat√≥w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5673d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcdbce9f",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych ≈õrodowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293e2363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "\n",
    "# Wy≈õwietl kontekst u≈ºytkownika (zmienne z 00_setup)\n",
    "print(\"=== Kontekst u≈ºytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"U≈ºytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog jako domy≈õlny\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbc16c9",
   "metadata": {},
   "source": [
    "## Sekcja 1: Koncepcja Lakehouse Architecture\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Lakehouse to nowoczesna architektura danych, kt√≥ra ≈ÇƒÖczy zalety Data Lake (niski koszt przechowywania, wsparcie dla r√≥≈ºnych format√≥w) z zaletami Data Warehouse (niezawodno≈õƒá, wydajno≈õƒá zapyta≈Ñ SQL, zarzƒÖdzanie transakcjami). Kluczowym elementem jest Delta Lake - warstwa metadanych zapewniajƒÖca transakcyjno≈õƒá ACID na plikach Parquet.\n",
    "\n",
    "**Kluczowe pojƒôcia:**\n",
    "- **ACID Transactions**: Atomowo≈õƒá, Sp√≥jno≈õƒá, Izolacja, Trwa≈Ço≈õƒá - gwarancje zapewniajƒÖce niezawodno≈õƒá operacji na danych\n",
    "- **Delta Lake**: Open-source storage layer zapewniajƒÖcy transakcyjno≈õƒá na plikach w Data Lake\n",
    "- **Unity Catalog**: Zunifikowany system zarzƒÖdzania danymi, metadanymi i kontrolƒÖ dostƒôpu\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Eliminacja duplikacji danych miƒôdzy systemami analitycznymi i operacyjnymi\n",
    "- Jednoczesne wsparcie dla BI, Data Science i Machine Learning\n",
    "- Redukcja koszt√≥w infrastruktury i utrzymania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b697efd9",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 1.1: Por√≥wnanie tradycyjnej architektury vs Lakehouse\n",
    "\n",
    "**Cel:** Wizualizacja r√≥≈ºnic miƒôdzy tradycyjnym podej≈õciem (Data Lake + Data Warehouse) a Lakehouse.\n",
    "\n",
    "**Tradycyjna architektura:**\n",
    "```\n",
    "Raw Data ‚Üí Data Lake (S3/ADLS) ‚Üí ETL Process ‚Üí Data Warehouse (Snowflake/Redshift) ‚Üí BI Tools\n",
    "                                ‚Üì\n",
    "                         ML/Data Science (separate copy)\n",
    "```\n",
    "\n",
    "**Lakehouse architektura:**\n",
    "```\n",
    "Raw Data ‚Üí Delta Lake (single source of truth) ‚Üí BI Tools + ML + Real-time Analytics\n",
    "```\n",
    "\n",
    "**Korzy≈õci Lakehouse:**\n",
    "- Jedna kopia danych (single source of truth)\n",
    "- Ni≈ºsze koszty przechowywania\n",
    "- Eliminacja op√≥≈∫nie≈Ñ synchronizacji\n",
    "- Wsp√≥lne governance dla wszystkich use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eadeae6",
   "metadata": {},
   "source": [
    "## Sekcja 2: Elementy platformy Databricks\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Platforma Databricks sk≈Çada siƒô z kilku kluczowych komponent√≥w, kt√≥re razem tworzƒÖ kompletne ≈õrodowisko do pracy z danymi w architekturze Lakehouse.\n",
    "\n",
    "**Kluczowe komponenty:**\n",
    "- **Workspace**: ≈örodowisko pracy zawierajƒÖce notebooks, eksperymenty, foldery i zasoby\n",
    "- **Catalog Explorer**: Interfejs do zarzƒÖdzania katalogami, schematami, tabelami i widokami\n",
    "- **Repos**: Integracja z Git do wersjonowania notebook√≥w i kodu\n",
    "- **Volumes**: ZarzƒÖdzanie plikami niestrukturalnymi (obrazy, modele, artifacts)\n",
    "- **DBFS (Databricks File System)**: Wirtualny system plik√≥w nad cloud storage\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Workspace organizuje projekty i wsp√≥≈Çpracƒô zespo≈ÇowƒÖ\n",
    "- Catalog Explorer umo≈ºliwia eksploracjƒô i governance danych\n",
    "- Repos integruje development workflow z Git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523368f3",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 2.1: Eksploracja Workspace\n",
    "\n",
    "**Cel:** Zapoznanie siƒô z interfejsem Databricks Workspace\n",
    "\n",
    "**Elementy Workspace:**\n",
    "1. **Sidebar** (lewa strona):\n",
    "   - Workspace: Foldery i notebooki\n",
    "   - Repos: Integracja Git\n",
    "   - Compute: ZarzƒÖdzanie klastrami\n",
    "   - Workflows: Databricks Jobs\n",
    "   - Catalog: Unity Catalog explorer\n",
    "\n",
    "2. **G≈Ç√≥wny panel**: Edytor notebook√≥w lub widok szczeg√≥≈Ç√≥w\n",
    "\n",
    "3. **G√≥rna belka**: Szybki dostƒôp do compute, account, help\n",
    "\n",
    "**Instrukcje nawigacji:**\n",
    "- U≈ºyj lewego menu do prze≈ÇƒÖczania miƒôdzy sekcjami\n",
    "- W sekcji Catalog mo≈ºesz przeglƒÖdaƒá katalogi, schematy i tabele\n",
    "- W sekcji Compute zarzƒÖdzasz klastrami Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b4cfc5",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 2.2: Catalog Explorer - struktura Unity Catalog\n",
    "\n",
    "**Cel:** Zrozumienie hierarchii obiekt√≥w w Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42af0c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wy≈õwietl aktualny katalog i schemat\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "\n",
    "print(f\"Aktualny katalog: {current_catalog}\")\n",
    "print(f\"Aktualny schemat: {current_schema}\")\n",
    "\n",
    "print(\"\\n=== Hierarchia Unity Catalog ===\")\n",
    "print(\"Metastore\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ Catalog (np. main, dev, prod)\")\n",
    "print(\"  ‚îÇ   ‚îú‚îÄ‚îÄ Schema/Database (np. bronze, silver, gold)\")\n",
    "print(\"  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Tables (Delta Tables)\")\n",
    "print(\"  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Views (SQL Views)\")\n",
    "print(\"  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Functions (UDFs)\")\n",
    "print(\"  ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Volumes (dla plik√≥w)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5c9bc5",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Unity Catalog organizuje dane w hierarchii: Metastore ‚Üí Catalog ‚Üí Schema ‚Üí Objects (Tables/Views/Functions). Ta struktura umo≈ºliwia:\n",
    "- Logiczne oddzielenie ≈õrodowisk (dev/test/prod)\n",
    "- GranularnƒÖ kontrolƒô dostƒôpu na ka≈ºdym poziomie\n",
    "- ≈Åatwe zarzƒÖdzanie namespace'ami i izolacjƒÖ projekt√≥w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a537b5c2",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 2.3: PrzeglƒÖdanie katalog√≥w i schemat√≥w\n",
    "\n",
    "**Cel:** Programowe listowanie obiekt√≥w w Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e069f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista wszystkich katalog√≥w dostƒôpnych dla u≈ºytkownika\n",
    "print(\"=== Dostƒôpne katalogi ===\")\n",
    "catalogs_df = spark.sql(\"SHOW CATALOGS\")\n",
    "display(catalogs_df)\n",
    "\n",
    "# Lista schemat√≥w w aktualnym katalogu\n",
    "print(f\"\\n=== Schematy w katalogu {CATALOG} ===\")\n",
    "schemas_df = spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\")\n",
    "display(schemas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c479126",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Polecenia `SHOW CATALOGS` i `SHOW SCHEMAS` pozwalajƒÖ na eksploracjƒô struktury Unity Catalog. Ka≈ºdy u≈ºytkownik widzi tylko te obiekty, do kt√≥rych ma uprawnienia. Per-user izolacja (jak w naszym `00_setup`) zapewnia, ≈ºe ka≈ºdy uczestnik szkolenia ma w≈ÇasnƒÖ przestrze≈Ñ roboczƒÖ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c311765",
   "metadata": {},
   "source": [
    "## Sekcja 3: Compute - Klastry Spark\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Klastry Spark w Databricks sƒÖ ≈õrodowiskiem wykonawczym dla przetwarzania danych. Databricks oferuje r√≥≈ºne typy klastr√≥w i optymalizacje, kt√≥re automatyzujƒÖ zarzƒÖdzanie infrastrukturƒÖ.\n",
    "\n",
    "**Kluczowe pojƒôcia:**\n",
    "- **All-Purpose Cluster**: Interaktywne klastry do analizy i rozwoju w notebookach\n",
    "- **Job Cluster**: Efemeryczne klastry dla automatyzowanych zada≈Ñ (Databricks Jobs)\n",
    "- **Autoscaling**: Automatyczne skalowanie liczby worker√≥w w zale≈ºno≈õci od obciƒÖ≈ºenia\n",
    "- **Spot Instances**: Wykorzystanie ta≈Ñszych VM w chmurze (AWS Spot, Azure Spot, GCP Preemptible)\n",
    "- **Photon Engine**: Natywny engine wykonawczy w C++ dla przyspieszenia zapyta≈Ñ SQL i DataFrame\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Autoscaling redukuje koszty przy zmiennym obciƒÖ≈ºeniu\n",
    "- Spot instances zmniejszajƒÖ koszty compute o 60-80%\n",
    "- Photon przyspiesza zapytania agregacyjne nawet 3x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e168b31d",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 3.1: Informacje o klastrze\n",
    "\n",
    "**Cel:** Sprawdzenie konfiguracji aktualnego klastra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee00ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informacje o Spark Context\n",
    "print(\"=== Konfiguracja klastra ===\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Aplikacja: {spark.sparkContext.appName}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# Liczba executor√≥w\n",
    "num_executors = len(spark.sparkContext._jsc.sc().statusTracker().getExecutorInfos()) - 1\n",
    "print(f\"Liczba executor√≥w (workers): {num_executors}\")\n",
    "\n",
    "# Runtime version\n",
    "dbr_version = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\", \"unknown\")\n",
    "print(f\"Databricks Runtime: {dbr_version}\")\n",
    "\n",
    "# Photon w≈ÇƒÖczony?\n",
    "photon_enabled = spark.conf.get(\"spark.databricks.photon.enabled\", \"false\")\n",
    "print(f\"Photon Engine: {'W≈ÇƒÖczony' if photon_enabled == 'true' else 'Wy≈ÇƒÖczony'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295352cf",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Ten kod pokazuje podstawowe informacje o klastrze Spark. Liczba executor√≥w (workers) mo≈ºe siƒô zmieniaƒá dynamicznie przy w≈ÇƒÖczonym autoscalingu. Photon Engine, je≈õli w≈ÇƒÖczony, automatycznie przyspiesza zapytania SQL i operacje DataFrame bez zmian w kodzie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f0bf88",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 3.2: Best practices dla konfiguracji klastr√≥w\n",
    "\n",
    "**Cel:** Poznanie rekomendacji dla r√≥≈ºnych use cases\n",
    "\n",
    "**Dla Development (All-Purpose Cluster):**\n",
    "- Runtime: 14.3 LTS (Long Term Support)\n",
    "- Workers: 2-4 (autoscaling 2-8 dla wiƒôkszych projekt√≥w)\n",
    "- Node type: Standard DS3_v2 (Azure) lub m5.xlarge (AWS)\n",
    "- Photon: W≈ÇƒÖczony\n",
    "- Spot instances: Nie (dla stabilno≈õci)\n",
    "\n",
    "**Dla Production (Job Cluster):**\n",
    "- Runtime: 14.3 LTS\n",
    "- Workers: autoscaling 2-20 (zale≈ºnie od obciƒÖ≈ºenia)\n",
    "- Node type: Memory-optimized (DS4_v2, m5.2xlarge)\n",
    "- Photon: W≈ÇƒÖczony\n",
    "- Spot instances: Tak (60-80% workers)\n",
    "- Auto-termination: 10 minut nieaktywno≈õci\n",
    "\n",
    "**Dla ML Workloads:**\n",
    "- Runtime: 14.3 ML (zawiera biblioteki ML)\n",
    "- Workers: GPU-enabled (NC6s_v3, p3.2xlarge)\n",
    "- Single-node mode dla prototypowania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83472ef2",
   "metadata": {},
   "source": [
    "## Sekcja 4: Magic Commands w Notebookach\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Databricks notebooks obs≈ÇugujƒÖ magic commands - specjalne polecenia zaczynajƒÖce siƒô od `%`, kt√≥re kontrolujƒÖ jƒôzyk kom√≥rki lub wykonujƒÖ operacje systemowe. Magic commands umo≈ºliwiajƒÖ mieszanie jƒôzyk√≥w w jednym notebooku oraz interakcjƒô z systemem plik√≥w.\n",
    "\n",
    "**Dostƒôpne magic commands:**\n",
    "- **%python**: Kom√≥rka Python (domy≈õlny)\n",
    "- **%sql**: Kom√≥rka SQL\n",
    "- **%scala**: Kom√≥rka Scala\n",
    "- **%r**: Kom√≥rka R\n",
    "- **%md**: Kom√≥rka Markdown (dokumentacja)\n",
    "- **%fs**: Operacje na systemie plik√≥w (DBFS)\n",
    "- **%sh**: Polecenia shell\n",
    "- **%run**: Uruchomienie innego notebooka (jak import)\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- ≈ÅƒÖczenie SQL i Python w jednym workflow\n",
    "- Dokumentacja inline z Markdown\n",
    "- Operacje na plikach z %fs\n",
    "- Modularyzacja kodu z %run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aaa43ab",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 4.1: Demonstracja SQL magic command\n",
    "\n",
    "**Cel:** Wykonanie zapytania SQL bezpo≈õrednio w notebooku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e72bb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL magic command pozwala pisaƒá czyste SQL bez otoczki Pythona\n",
    "\n",
    "SELECT \n",
    "  current_catalog() as catalog,\n",
    "  current_schema() as schema,\n",
    "  current_user() as user,\n",
    "  current_timestamp() as timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd86c51",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Magic command `%sql` zmienia jƒôzyk kom√≥rki na SQL. Wyniki sƒÖ automatycznie wy≈õwietlane jako tabela. SQL w Databricks to pe≈Çny Spark SQL z rozszerzeniami Delta Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281d7fe",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 4.2: File System operations z %fs\n",
    "\n",
    "**Cel:** Eksploracja systemu plik√≥w DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebec18ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic command %fs dla operacji na plikach\n",
    "# Lista katalog√≥w g≈Ç√≥wnych w DBFS\n",
    "dbutils.fs.ls(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb401a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatywnie: %fs magic command\n",
    "%fs ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a101c841",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "DBFS (Databricks File System) to abstrakcja nad cloud storage (S3, ADLS, GCS). Komenda `%fs` lub `dbutils.fs` pozwala na operacje na plikach. W Unity Catalog zaleca siƒô u≈ºywanie **Volumes** zamiast DBFS dla lepszego governance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823291a8",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 4.3: Mieszanie jƒôzyk√≥w - Python i SQL\n",
    "\n",
    "**Cel:** Demonstracja p≈Çynnego przechodzenia miƒôdzy Python i SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c560f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python: Przygotowanie danych\n",
    "data = [\n",
    "    (1, \"Alice\", \"Engineering\", 95000),\n",
    "    (2, \"Bob\", \"Sales\", 75000),\n",
    "    (3, \"Charlie\", \"Engineering\", 105000),\n",
    "    (4, \"Diana\", \"Marketing\", 68000),\n",
    "    (5, \"Eve\", \"Engineering\", 98000)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"department\", StringType(), False),\n",
    "    StructField(\"salary\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Rejestracja jako temp view dla dostƒôpu z SQL\n",
    "df.createOrReplaceTempView(\"employees_temp\")\n",
    "\n",
    "print(\"Utworzono temp view: employees_temp\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32486d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL: Agregacja na danych z Python\n",
    "\n",
    "SELECT \n",
    "  department,\n",
    "  COUNT(*) as employee_count,\n",
    "  AVG(salary) as avg_salary,\n",
    "  MAX(salary) as max_salary\n",
    "FROM employees_temp\n",
    "GROUP BY department\n",
    "ORDER BY avg_salary DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a84355",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Ten przyk≈Çad pokazuje si≈Çƒô notebook√≥w Databricks: przygotowanie danych w Python (wygodne API, biblioteki), nastƒôpnie analiza w SQL (deklaratywne zapytania, przejrzysto≈õƒá). Temp views sƒÖ widoczne w ca≈Çym notebooku niezale≈ºnie od jƒôzyka kom√≥rki."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709e331e",
   "metadata": {},
   "source": [
    "## Sekcja 5: Unity Catalog vs Hive Metastore\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Databricks wspiera dwa systemy metadanych: legacy Hive Metastore oraz nowoczesny Unity Catalog. Unity Catalog jest zalecany dla wszystkich nowych projekt√≥w ze wzglƒôdu na zaawansowane funkcje governance i bezpiecze≈Ñstwa.\n",
    "\n",
    "**Kluczowe r√≥≈ºnice:**\n",
    "\n",
    "| Aspekt | Hive Metastore | Unity Catalog |\n",
    "|--------|----------------|---------------|\n",
    "| **Governance** | Ograniczone | Pe≈Çne: RBAC, masking, audit |\n",
    "| **Namespace** | 2-poziomowy (db.table) | 3-poziomowy (catalog.schema.table) |\n",
    "| **Cross-workspace** | Nie | Tak (shared metastore) |\n",
    "| **Lineage** | Brak | End-to-end lineage |\n",
    "| **Data Sharing** | Ograniczone | Delta Sharing protocol |\n",
    "| **Isolation** | Workspace-level | Catalog-level |\n",
    "\n",
    "**Dlaczego Unity Catalog?**\n",
    "- Centralne zarzƒÖdzanie dostƒôpem dla wszystkich workspace'√≥w\n",
    "- Automatyczny lineage dla audytu i compliance\n",
    "- Fine-grained permissions (column-level, row-level)\n",
    "- Integracja z zewnƒôtrznymi systemami (Delta Sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe7ff16",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 5.1: Namespace - Hive vs Unity Catalog\n",
    "\n",
    "**Cel:** Por√≥wnanie sk≈Çadni dostƒôpu do tabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9bda98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hive Metastore (2-poziomowy namespace)\n",
    "print(\"=== Hive Metastore ===\")\n",
    "print(\"Sk≈Çadnia: database.table\")\n",
    "print(\"Przyk≈Çad: default.sales_data\")\n",
    "print(\"\")\n",
    "\n",
    "# Unity Catalog (3-poziomowy namespace)\n",
    "print(\"=== Unity Catalog ===\")\n",
    "print(\"Sk≈Çadnia: catalog.schema.table\")\n",
    "print(\"Przyk≈Çad: prod.gold.sales_summary\")\n",
    "print(\"\")\n",
    "print(\"Zalety 3-poziomowego namespace:\")\n",
    "print(\"- Oddzielenie ≈õrodowisk (dev/test/prod catalogs)\")\n",
    "print(\"- Lepsze uprawnienia (grant na poziomie catalogu)\")\n",
    "print(\"- Wsp√≥≈Çdzielenie metastore miƒôdzy workspace'ami\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099c9e6b",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 5.2: Tworzenie tabeli w Unity Catalog\n",
    "\n",
    "**Cel:** Demonstracja pe≈Çnej sk≈Çadni z 3-poziomowym namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887d42fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utworzenie przyk≈Çadowej tabeli w Unity Catalog\n",
    "table_name = f\"{CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\"\n",
    "\n",
    "# Dane demonstracyjne\n",
    "demo_data = [\n",
    "    (1, \"Unity Catalog\", \"Enabled\", \"2024-01-15\"),\n",
    "    (2, \"Delta Lake\", \"Enabled\", \"2024-01-15\"),\n",
    "    (3, \"Photon Engine\", \"Enabled\", \"2024-01-15\"),\n",
    "    (4, \"Hive Metastore\", \"Legacy\", \"2024-01-15\")\n",
    "]\n",
    "\n",
    "demo_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"feature\", StringType(), False),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"date\", StringType(), False)\n",
    "])\n",
    "\n",
    "demo_df = spark.createDataFrame(demo_data, demo_schema)\n",
    "\n",
    "# Zapis jako Delta Table w Unity Catalog\n",
    "demo_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(table_name)\n",
    "\n",
    "print(f\"‚úÖ Tabela utworzona: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d846f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Odczyt tabeli z pe≈Çnym namespace Unity Catalog\n",
    "\n",
    "SELECT * FROM ${CATALOG}.${BRONZE_SCHEMA}.lakehouse_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8baeb4b8",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Tabela zosta≈Ça utworzona z pe≈Çnym 3-poziomowym namespace. W Unity Catalog ka≈ºda tabela automatycznie:\n",
    "- Jest zarzƒÖdzana przez system governance\n",
    "- Ma trackowany lineage\n",
    "- Posiada przypisane uprawnienia na podstawie katalogu i schematu\n",
    "- Jest dostƒôpna w Catalog Explorer dla eksploracji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596f2b2a",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 5.3: Eksploracja metadanych Unity Catalog\n",
    "\n",
    "**Cel:** Wykorzystanie systemu informacyjnych schemat√≥w Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649b7288",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Unity Catalog udostƒôpnia system.information_schema dla metadanych\n",
    "\n",
    "-- Lista tabel w naszym schemacie\n",
    "SELECT \n",
    "  table_catalog,\n",
    "  table_schema,\n",
    "  table_name,\n",
    "  table_type\n",
    "FROM system.information_schema.tables\n",
    "WHERE table_catalog = '${CATALOG}'\n",
    "  AND table_schema = '${BRONZE_SCHEMA}'\n",
    "ORDER BY table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d2d486",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Unity Catalog automatycznie utrzymuje `system.information_schema` - zbi√≥r widok√≥w SQL z metadanymi o wszystkich obiektach. To standardowe podej≈õcie zgodne z ANSI SQL, co u≈Çatwia integracjƒô z narzƒôdziami BI i data governance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac258429",
   "metadata": {},
   "source": [
    "## Por√≥wnanie PySpark vs SQL\n",
    "\n",
    "**DataFrame API (PySpark):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6ba0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podej≈õcie PySpark - programatyczne DataFrame API\n",
    "\n",
    "df_pyspark = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\")\n",
    "\n",
    "result_pyspark = df_pyspark \\\n",
    "    .filter(F.col(\"status\") == \"Enabled\") \\\n",
    "    .select(\"feature\", \"status\", \"date\") \\\n",
    "    .orderBy(\"feature\")\n",
    "\n",
    "display(result_pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d063e63e",
   "metadata": {},
   "source": [
    "**SQL Equivalent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512c20c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Podej≈õcie SQL - deklaratywne zapytanie\n",
    "\n",
    "SELECT \n",
    "  feature,\n",
    "  status,\n",
    "  date\n",
    "FROM ${CATALOG}.${BRONZE_SCHEMA}.lakehouse_demo\n",
    "WHERE status = 'Enabled'\n",
    "ORDER BY feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f51b044",
   "metadata": {},
   "source": [
    "**Por√≥wnanie:**\n",
    "- **Wydajno≈õƒá**: Identyczna - oba podej≈õcia kompilujƒÖ siƒô do tego samego Catalyst query plan\n",
    "- **Kiedy u≈ºywaƒá PySpark**: \n",
    "  - Z≈Ço≈ºona logika biznesowa z UDF\n",
    "  - Dynamiczne pipeline'y (parametryzacja, loops)\n",
    "  - Integracja z bibliotekami Python (pandas, scikit-learn)\n",
    "- **Kiedy u≈ºywaƒá SQL**: \n",
    "  - Proste transformacje i agregacje\n",
    "  - Zesp√≥≈Ç z silnymi kompetencjami SQL\n",
    "  - Migracja z tradycyjnych Data Warehouse\n",
    "  - Lepsze wsparcie dla analityk√≥w biznesowych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4ba530",
   "metadata": {},
   "source": [
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist - Co powiniene≈õ zrozumieƒá po tym notebooku:\n",
    "- [x] Koncepcja Lakehouse i korzy≈õci wzglƒôdem tradycyjnej architektury\n",
    "- [x] Struktura Workspace: Sidebar, Compute, Catalog Explorer\n",
    "- [x] Hierarchia Unity Catalog: Metastore ‚Üí Catalog ‚Üí Schema ‚Üí Objects\n",
    "- [x] Typy klastr√≥w: All-Purpose vs Job, autoscaling, Photon\n",
    "- [x] Magic commands: %sql, %python, %fs, %run, %md\n",
    "- [x] R√≥≈ºnice miƒôdzy Hive Metastore (2-level) a Unity Catalog (3-level)\n",
    "- [x] Tworzenie tabel w Unity Catalog z pe≈Çnym namespace\n",
    "- [x] Dostƒôp do metadanych przez system.information_schema\n",
    "\n",
    "### Komendy weryfikacyjne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9d050d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja wynik√≥w\n",
    "\n",
    "print(\"=== WERYFIKACJA KONFIGURACJI ===\\n\")\n",
    "\n",
    "# 1. Sprawd≈∫ czy tabela demo zosta≈Ça utworzona\n",
    "try:\n",
    "    demo_count = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\").count()\n",
    "    print(f\"‚úÖ Tabela demo utworzona: {demo_count} rekord√≥w\")\n",
    "except:\n",
    "    print(\"‚ùå Tabela demo nie istnieje\")\n",
    "\n",
    "# 2. Sprawd≈∫ katalog i schemat\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "print(f\"‚úÖ Aktualny katalog: {current_catalog}\")\n",
    "\n",
    "# 3. Weryfikacja schemat√≥w bronze/silver/gold\n",
    "schemas = [s.namespace for s in spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\").collect()]\n",
    "required_schemas = [BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA]\n",
    "\n",
    "for schema in required_schemas:\n",
    "    if schema in schemas:\n",
    "        print(f\"‚úÖ Schema {schema} istnieje\")\n",
    "    else:\n",
    "        print(f\"‚ùå Schema {schema} nie istnieje\")\n",
    "\n",
    "# 4. Informacje o klastrze\n",
    "print(f\"\\n‚úÖ Spark version: {spark.version}\")\n",
    "print(f\"‚úÖ DBR version: {spark.conf.get('spark.databricks.clusterUsageTags.sparkVersion', 'unknown')}\")\n",
    "\n",
    "print(\"\\nüéâ Wszystkie sprawdzenia zako≈Ñczone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4ca972",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Problem 1: \"Catalog not found\" lub \"Schema not found\"\n",
    "**Objawy:**\n",
    "- B≈ÇƒÖd: `AnalysisException: [SCHEMA_NOT_FOUND]`\n",
    "- B≈ÇƒÖd: `AnalysisException: [CATALOG_NOT_FOUND]`\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "```python\n",
    "# Sprawd≈∫ dostƒôpne katalogi\n",
    "spark.sql(\"SHOW CATALOGS\").show()\n",
    "\n",
    "# Sprawd≈∫ dostƒôpne schematy\n",
    "spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\").show()\n",
    "\n",
    "# Upewnij siƒô, ≈ºe uruchomi≈Çe≈õ %run ./00_setup\n",
    "```\n",
    "\n",
    "### Problem 2: \"Permission denied\" przy tworzeniu tabel\n",
    "**Objawy:**\n",
    "- B≈ÇƒÖd: `PERMISSION_DENIED: User does not have CREATE TABLE on Schema`\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "- Skontaktuj siƒô z administratorem workspace o nadanie uprawnie≈Ñ `CREATE TABLE`\n",
    "- Sprawd≈∫ uprawnienia: `SHOW GRANTS ON SCHEMA catalog.schema`\n",
    "\n",
    "### Problem 3: Klaster nie startuje lub jest zbyt wolny\n",
    "**Objawy:**\n",
    "- Klaster w stanie \"Pending\" przez d≈Çugi czas\n",
    "- Timeout przy starcie\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "- Sprawd≈∫ quota instancji w chmurze (Azure/AWS/GCP)\n",
    "- Zmniejsz liczbƒô worker√≥w lub wybierz mniejszy node type\n",
    "- Wy≈ÇƒÖcz autoscaling dla test√≥w\n",
    "\n",
    "### Problem 4: Magic command %sql nie dzia≈Ça\n",
    "**Objawy:**\n",
    "- B≈ÇƒÖd sk≈Çadni lub brak wynik√≥w\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "- Upewnij siƒô, ≈ºe `%sql` jest pierwszym elementem w kom√≥rce\n",
    "- Sprawd≈∫ czy u≈ºywasz zmiennych: `${CATALOG}` zamiast `{CATALOG}`\n",
    "- Dla zmiennych Python u≈ºyj: `spark.sql(f\"SELECT ... FROM {CATALOG}.{SCHEMA}.table\")`\n",
    "\n",
    "### Debugging tips:\n",
    "- U≈ºyj `explain()` na DataFrame aby zobaczyƒá plan wykonania\n",
    "- Sprawd≈∫ logi klastra w Spark UI (zak≈Çadka \"Cluster\" ‚Üí \"Spark UI\")\n",
    "- Weryfikuj typy danych: `df.printSchema()`\n",
    "- Dla problem√≥w z wydajno≈õciƒÖ sprawd≈∫ liczbƒô partycji: `df.rdd.getNumPartitions()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9799ffc",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### Architektura Lakehouse:\n",
    "- U≈ºywaj Unity Catalog zamiast Hive Metastore dla nowych projekt√≥w\n",
    "- Organizuj dane w logiczne katalogi (np. dev/test/prod)\n",
    "- Stosuj naming convention dla schemat√≥w: bronze/silver/gold\n",
    "- Wykorzystuj Delta Lake jako domy≈õlny format tabel\n",
    "\n",
    "### ZarzƒÖdzanie Workspace:\n",
    "- Organizuj notebooki w folderach wed≈Çug projekt√≥w lub zespo≈Ç√≥w\n",
    "- U≈ºywaj Repos dla integracji z Git i wersjonowania\n",
    "- Dokumentuj notebooki z Markdown cells\n",
    "- Stosuj `%run` dla wsp√≥≈Çdzielenia kodu miƒôdzy notebookami\n",
    "\n",
    "### Konfiguracja Klastr√≥w:\n",
    "- Development: ma≈Çe klastry (2-4 workers), bez spot instances\n",
    "- Production: autoscaling, spot instances dla oszczƒôdno≈õci\n",
    "- W≈ÇƒÖcz Photon Engine dla zapyta≈Ñ SQL/DataFrame\n",
    "- Ustaw auto-termination (np. 30 min nieaktywno≈õci) dla klastr√≥w All-Purpose\n",
    "- Dla Jobs u≈ºywaj Job Clusters (efemeryczne, optymalne koszty)\n",
    "\n",
    "### Governance i Security:\n",
    "- Stosuj per-user lub per-team izolacjƒô katalog√≥w\n",
    "- U≈ºywaj 3-poziomowego namespace: catalog.schema.table\n",
    "- Przydzielaj uprawnienia na poziomie schematu, nie tabeli\n",
    "- Monitoruj dostƒôp przez system.access.audit\n",
    "- W≈ÇƒÖcz lineage dla compliance i debugowania\n",
    "\n",
    "### Wydajno≈õƒá:\n",
    "- Preferuj Delta Lake zamiast Parquet/CSV dla czƒôstych zapyta≈Ñ\n",
    "- Partycjonuj du≈ºe tabele wed≈Çug kluczy czasowych lub geograficznych\n",
    "- U≈ºywaj Z-ORDER dla kolumn u≈ºywanych w WHERE clauses\n",
    "- Regularnie uruchamiaj OPTIMIZE i VACUUM na tabelach Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1c1898",
   "metadata": {},
   "source": [
    "## Podsumowanie\n",
    "\n",
    "### Co zosta≈Ço osiƒÖgniƒôte:\n",
    "- Poznanie koncepcji Lakehouse jako ewolucji Data Lake + Data Warehouse\n",
    "- Eksploracja element√≥w platformy Databricks: Workspace, Compute, Catalog\n",
    "- Zrozumienie hierarchii Unity Catalog: Metastore ‚Üí Catalog ‚Üí Schema ‚Üí Objects\n",
    "- Praktyka z magic commands: %sql, %python, %fs\n",
    "- Por√≥wnanie Hive Metastore vs Unity Catalog\n",
    "- Utworzenie pierwszej tabeli Delta w Unity Catalog z 3-poziomowym namespace\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "1. **Lakehouse eliminuje duplikacjƒô danych**: Jedna kopia s≈Çu≈ºy BI, ML i real-time analytics\n",
    "2. **Unity Catalog to fundament governance**: 3-poziomowy namespace, fine-grained permissions, automatyczny lineage\n",
    "3. **Klastry sƒÖ elastyczne**: Autoscaling i spot instances redukujƒÖ koszty, Photon przyspiesza zapytania\n",
    "4. **Notebooki sƒÖ potƒô≈ºne**: Mieszanie SQL/Python, magic commands, integracja z Git przez Repos\n",
    "5. **Delta Lake jest domy≈õlnym formatem**: ACID transactions, time travel, schema evolution\n",
    "\n",
    "### Quick Reference - Najwa≈ºniejsze komendy:\n",
    "\n",
    "| Operacja | PySpark | SQL |\n",
    "|----------|---------|-----|\n",
    "| Ustaw katalog | `spark.sql(f\"USE CATALOG {CATALOG}\")` | `USE CATALOG my_catalog` |\n",
    "| Lista katalog√≥w | `spark.sql(\"SHOW CATALOGS\")` | `SHOW CATALOGS` |\n",
    "| Lista schemat√≥w | `spark.sql(\"SHOW SCHEMAS\")` | `SHOW SCHEMAS` |\n",
    "| Utworzenie tabeli | `df.write.saveAsTable(\"cat.schema.table\")` | `CREATE TABLE cat.schema.table AS SELECT ...` |\n",
    "| Odczyt tabeli | `spark.table(\"cat.schema.table\")` | `SELECT * FROM cat.schema.table` |\n",
    "| Metadane | - | `SELECT * FROM system.information_schema.tables` |\n",
    "\n",
    "### Nastƒôpne kroki:\n",
    "- **Kolejny notebook**: `02_data_import_exploration.ipynb` - Wczytywanie danych z r√≥≈ºnych format√≥w (CSV, JSON, Parquet, Delta)\n",
    "- **Warsztat praktyczny**: Po zako≈Ñczeniu dema (notebooki 01-05) przejdziemy do `01_workspace_data_exploration_workshop.ipynb`\n",
    "- **Materia≈Çy dodatkowe**: \n",
    "  - [Databricks Lakehouse Fundamentals](https://www.databricks.com/learn/lakehouse)\n",
    "  - [Unity Catalog Documentation](https://docs.databricks.com/data-governance/unity-catalog/index.html)\n",
    "  - [Delta Lake Guide](https://docs.delta.io/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f3daa7",
   "metadata": {},
   "source": [
    "## Czyszczenie zasob√≥w\n",
    "\n",
    "PosprzƒÖtaj zasoby utworzone podczas notebooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb402db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcjonalne czyszczenie zasob√≥w testowych\n",
    "# UWAGA: Uruchom tylko je≈õli chcesz usunƒÖƒá wszystkie utworzone dane\n",
    "\n",
    "# Usu≈Ñ tabelƒô demo\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\")\n",
    "\n",
    "# Usu≈Ñ temp views\n",
    "# spark.catalog.clearCache()\n",
    "# spark.sql(\"DROP VIEW IF EXISTS employees_temp\")\n",
    "\n",
    "# print(\"‚úÖ Zasoby zosta≈Çy wyczyszczone\")\n",
    "print(\"‚ö†Ô∏è  Czyszczenie zasob√≥w zakomentowane - odkomentuj je≈õli chcesz usunƒÖƒá dane demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b93069fa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Metadata:**\n",
    "- **Notebook**: 01_databricks_lakehouse_intro.ipynb\n",
    "- **Dzie≈Ñ**: 1 - Fundamentals & Exploration\n",
    "- **Typ**: Demo\n",
    "- **Czas**: 45 minut\n",
    "- **Wersja**: 1.0\n",
    "- **Data utworzenia**: 19 listopada 2025\n",
    "- **Compliance**: 7/7 standard√≥w ‚úÖ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e2bb64",
   "metadata": {},
   "source": [
    "# Wprowadzenie do Databricks Lakehouse - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Zrozumienie koncepcji Lakehouse, poznanie podstawowych element√≥w platformy Databricks oraz konfiguracji ≈õrodowiska Unity Catalog.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Koncepcja Lakehouse (Data Lake + Data Warehouse)\n",
    "- Elementy platformy: Workspace, Catalog Explorer, Repos, Volumes, DBFS\n",
    "- Compute: clusters, autoscaling, spot instances, Photon Engine\n",
    "- Notebooks: magic commands (%sql, %python, %fs, %md)\n",
    "- Unity Catalog overview: katalogi, schematy, tabele\n",
    "- R√≥≈ºnice miƒôdzy Hive Metastore a Unity Catalog\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bef113",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzie≈Ñ szkolenia**: Dzie≈Ñ 1 - Fundamentals & Exploration\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog w≈ÇƒÖczony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z 2-4 workers\n",
    "- **Czas trwania**: 45 minut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be073e2",
   "metadata": {},
   "source": [
    "## Wstƒôp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie ewolucji architektur danych i miejsca Lakehouse w tym kontek≈õcie.\n",
    "\n",
    "**Podstawowe pojƒôcia:**\n",
    "- **Data Lake**: Scentralizowane repozytorium przechowujƒÖce surowe dane w r√≥≈ºnych formatach (strukturalne, semi-strukturalne, niestrukturalne)\n",
    "- **Data Warehouse**: Zoptymalizowane repozytorium danych strukturalnych do analityki biznesowej i raportowania\n",
    "- **Lakehouse**: Nowoczesna architektura ≈ÇƒÖczƒÖca elastyczno≈õƒá Data Lake z niezawodno≈õciƒÖ i wydajno≈õciƒÖ Data Warehouse\n",
    "\n",
    "**Dlaczego to wa≈ºne?**\n",
    "Lakehouse eliminuje potrzebƒô utrzymywania dw√≥ch oddzielnych system√≥w (Data Lake + Data Warehouse), redukujƒÖc koszty, z≈Ço≈ºono≈õƒá i op√≥≈∫nienia w dostƒôpie do danych. Dziƒôki Delta Lake uzyskujemy transakcyjno≈õƒá ACID, wersjonowanie danych i optymalizacjƒô zapyta≈Ñ bezpo≈õrednio na plikach w Data Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1fbf17",
   "metadata": {},
   "source": [
    "## Izolacja per u≈ºytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalog√≥w i schemat√≥w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97bb969f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bf31fc",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych ≈õrodowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f474332b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "\n",
    "# Wy≈õwietl kontekst u≈ºytkownika (zmienne z 00_setup)\n",
    "print(\"=== Kontekst u≈ºytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"U≈ºytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog jako domy≈õlny\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7661f52e",
   "metadata": {},
   "source": [
    "## Sekcja 1: Koncepcja Lakehouse Architecture\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Lakehouse to nowoczesna architektura danych, kt√≥ra ≈ÇƒÖczy zalety Data Lake (niski koszt przechowywania, wsparcie dla r√≥≈ºnych format√≥w) z zaletami Data Warehouse (niezawodno≈õƒá, wydajno≈õƒá zapyta≈Ñ SQL, zarzƒÖdzanie transakcjami). Kluczowym elementem jest Delta Lake - warstwa metadanych zapewniajƒÖca transakcyjno≈õƒá ACID na plikach Parquet.\n",
    "\n",
    "**Kluczowe pojƒôcia:**\n",
    "- **ACID Transactions**: Atomowo≈õƒá, Sp√≥jno≈õƒá, Izolacja, Trwa≈Ço≈õƒá - gwarancje zapewniajƒÖce niezawodno≈õƒá operacji na danych\n",
    "- **Delta Lake**: Open-source storage layer zapewniajƒÖcy transakcyjno≈õƒá na plikach w Data Lake\n",
    "- **Unity Catalog**: Zunifikowany system zarzƒÖdzania danymi, metadanymi i kontrolƒÖ dostƒôpu\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Eliminacja duplikacji danych miƒôdzy systemami analitycznymi i operacyjnymi\n",
    "- Jednoczesne wsparcie dla BI, Data Science i Machine Learning\n",
    "- Redukcja koszt√≥w infrastruktury i utrzymania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0410ab36",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 1.1: Por√≥wnanie tradycyjnej architektury vs Lakehouse\n",
    "\n",
    "**Cel:** Wizualizacja r√≥≈ºnic miƒôdzy tradycyjnym podej≈õciem (Data Lake + Data Warehouse) a Lakehouse.\n",
    "\n",
    "**Tradycyjna architektura:**\n",
    "```\n",
    "Raw Data ‚Üí Data Lake (S3/ADLS) ‚Üí ETL Process ‚Üí Data Warehouse (Snowflake/Redshift) ‚Üí BI Tools\n",
    "                                ‚Üì\n",
    "                         ML/Data Science (separate copy)\n",
    "```\n",
    "\n",
    "**Lakehouse architektura:**\n",
    "```\n",
    "Raw Data ‚Üí Delta Lake (single source of truth) ‚Üí BI Tools + ML + Real-time Analytics\n",
    "```\n",
    "\n",
    "**Korzy≈õci Lakehouse:**\n",
    "- Jedna kopia danych (single source of truth)\n",
    "- Ni≈ºsze koszty przechowywania\n",
    "- Eliminacja op√≥≈∫nie≈Ñ synchronizacji\n",
    "- Wsp√≥lne governance dla wszystkich use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d78549",
   "metadata": {},
   "source": [
    "## Sekcja 2: Elementy platformy Databricks\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Platforma Databricks sk≈Çada siƒô z kilku kluczowych komponent√≥w, kt√≥re razem tworzƒÖ kompletne ≈õrodowisko do pracy z danymi w architekturze Lakehouse.\n",
    "\n",
    "**Kluczowe komponenty:**\n",
    "- **Workspace**: ≈örodowisko pracy zawierajƒÖce notebooks, eksperymenty, foldery i zasoby\n",
    "- **Catalog Explorer**: Interfejs do zarzƒÖdzania katalogami, schematami, tabelami i widokami\n",
    "- **Repos**: Integracja z Git do wersjonowania notebook√≥w i kodu\n",
    "- **Volumes**: ZarzƒÖdzanie plikami niestrukturalnymi (obrazy, modele, artifacts)\n",
    "- **DBFS (Databricks File System)**: Wirtualny system plik√≥w nad cloud storage\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Workspace organizuje projekty i wsp√≥≈Çpracƒô zespo≈ÇowƒÖ\n",
    "- Catalog Explorer umo≈ºliwia eksploracjƒô i governance danych\n",
    "- Repos integruje development workflow z Git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224a3a75",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 2.1: Eksploracja Workspace\n",
    "\n",
    "**Cel:** Zapoznanie siƒô z interfejsem Databricks Workspace\n",
    "\n",
    "**Elementy Workspace:**\n",
    "1. **Sidebar** (lewa strona):\n",
    "   - Workspace: Foldery i notebooki\n",
    "   - Repos: Integracja Git\n",
    "   - Compute: ZarzƒÖdzanie klastrami\n",
    "   - Workflows: Databricks Jobs\n",
    "   - Catalog: Unity Catalog explorer\n",
    "\n",
    "2. **G≈Ç√≥wny panel**: Edytor notebook√≥w lub widok szczeg√≥≈Ç√≥w\n",
    "\n",
    "3. **G√≥rna belka**: Szybki dostƒôp do compute, account, help\n",
    "\n",
    "**Instrukcje nawigacji:**\n",
    "- U≈ºyj lewego menu do prze≈ÇƒÖczania miƒôdzy sekcjami\n",
    "- W sekcji Catalog mo≈ºesz przeglƒÖdaƒá katalogi, schematy i tabele\n",
    "- W sekcji Compute zarzƒÖdzasz klastrami Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536ce8e4",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 2.2: Catalog Explorer - struktura Unity Catalog\n",
    "\n",
    "**Cel:** Zrozumienie hierarchii obiekt√≥w w Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f344166",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wy≈õwietl aktualny katalog i schemat\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "\n",
    "print(f\"Aktualny katalog: {current_catalog}\")\n",
    "print(f\"Aktualny schemat: {current_schema}\")\n",
    "\n",
    "print(\"\\n=== Hierarchia Unity Catalog ===\")\n",
    "print(\"Metastore\")\n",
    "print(\"  ‚îú‚îÄ‚îÄ Catalog (np. main, dev, prod)\")\n",
    "print(\"  ‚îÇ   ‚îú‚îÄ‚îÄ Schema/Database (np. bronze, silver, gold)\")\n",
    "print(\"  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Tables (Delta Tables)\")\n",
    "print(\"  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Views (SQL Views)\")\n",
    "print(\"  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Functions (UDFs)\")\n",
    "print(\"  ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Volumes (dla plik√≥w)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22c5ee9",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Unity Catalog organizuje dane w hierarchii: Metastore ‚Üí Catalog ‚Üí Schema ‚Üí Objects (Tables/Views/Functions). Ta struktura umo≈ºliwia:\n",
    "- Logiczne oddzielenie ≈õrodowisk (dev/test/prod)\n",
    "- GranularnƒÖ kontrolƒô dostƒôpu na ka≈ºdym poziomie\n",
    "- ≈Åatwe zarzƒÖdzanie namespace'ami i izolacjƒÖ projekt√≥w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5fd6f",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 2.3: PrzeglƒÖdanie katalog√≥w i schemat√≥w\n",
    "\n",
    "**Cel:** Programowe listowanie obiekt√≥w w Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8565172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista wszystkich katalog√≥w dostƒôpnych dla u≈ºytkownika\n",
    "print(\"=== Dostƒôpne katalogi ===\")\n",
    "catalogs_df = spark.sql(\"SHOW CATALOGS\")\n",
    "display(catalogs_df)\n",
    "\n",
    "# Lista schemat√≥w w aktualnym katalogu\n",
    "print(f\"\\n=== Schematy w katalogu {CATALOG} ===\")\n",
    "schemas_df = spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\")\n",
    "display(schemas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876bc0aa",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Polecenia `SHOW CATALOGS` i `SHOW SCHEMAS` pozwalajƒÖ na eksploracjƒô struktury Unity Catalog. Ka≈ºdy u≈ºytkownik widzi tylko te obiekty, do kt√≥rych ma uprawnienia. Per-user izolacja (jak w naszym `00_setup`) zapewnia, ≈ºe ka≈ºdy uczestnik szkolenia ma w≈ÇasnƒÖ przestrze≈Ñ roboczƒÖ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c659eb74",
   "metadata": {},
   "source": [
    "## Sekcja 3: Compute - Klastry Spark\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Klastry Spark w Databricks sƒÖ ≈õrodowiskiem wykonawczym dla przetwarzania danych. Databricks oferuje r√≥≈ºne typy klastr√≥w i optymalizacje, kt√≥re automatyzujƒÖ zarzƒÖdzanie infrastrukturƒÖ.\n",
    "\n",
    "**Kluczowe pojƒôcia:**\n",
    "- **All-Purpose Cluster**: Interaktywne klastry do analizy i rozwoju w notebookach\n",
    "- **Job Cluster**: Efemeryczne klastry dla automatyzowanych zada≈Ñ (Databricks Jobs)\n",
    "- **Autoscaling**: Automatyczne skalowanie liczby worker√≥w w zale≈ºno≈õci od obciƒÖ≈ºenia\n",
    "- **Spot Instances**: Wykorzystanie ta≈Ñszych VM w chmurze (AWS Spot, Azure Spot, GCP Preemptible)\n",
    "- **Photon Engine**: Natywny engine wykonawczy w C++ dla przyspieszenia zapyta≈Ñ SQL i DataFrame\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Autoscaling redukuje koszty przy zmiennym obciƒÖ≈ºeniu\n",
    "- Spot instances zmniejszajƒÖ koszty compute o 60-80%\n",
    "- Photon przyspiesza zapytania agregacyjne nawet 3x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e2745c",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 3.1: Informacje o klastrze\n",
    "\n",
    "**Cel:** Sprawdzenie konfiguracji aktualnego klastra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccc722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informacje o Spark Context\n",
    "print(\"=== Konfiguracja klastra ===\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Aplikacja: {spark.sparkContext.appName}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# Liczba executor√≥w\n",
    "num_executors = len(spark.sparkContext._jsc.sc().statusTracker().getExecutorInfos()) - 1\n",
    "print(f\"Liczba executor√≥w (workers): {num_executors}\")\n",
    "\n",
    "# Runtime version\n",
    "dbr_version = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\", \"unknown\")\n",
    "print(f\"Databricks Runtime: {dbr_version}\")\n",
    "\n",
    "# Photon w≈ÇƒÖczony?\n",
    "photon_enabled = spark.conf.get(\"spark.databricks.photon.enabled\", \"false\")\n",
    "print(f\"Photon Engine: {'W≈ÇƒÖczony' if photon_enabled == 'true' else 'Wy≈ÇƒÖczony'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b201c65",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Ten kod pokazuje podstawowe informacje o klastrze Spark. Liczba executor√≥w (workers) mo≈ºe siƒô zmieniaƒá dynamicznie przy w≈ÇƒÖczonym autoscalingu. Photon Engine, je≈õli w≈ÇƒÖczony, automatycznie przyspiesza zapytania SQL i operacje DataFrame bez zmian w kodzie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e55b9c6",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 3.2: Best practices dla konfiguracji klastr√≥w\n",
    "\n",
    "**Cel:** Poznanie rekomendacji dla r√≥≈ºnych use cases\n",
    "\n",
    "**Dla Development (All-Purpose Cluster):**\n",
    "- Runtime: 14.3 LTS (Long Term Support)\n",
    "- Workers: 2-4 (autoscaling 2-8 dla wiƒôkszych projekt√≥w)\n",
    "- Node type: Standard DS3_v2 (Azure) lub m5.xlarge (AWS)\n",
    "- Photon: W≈ÇƒÖczony\n",
    "- Spot instances: Nie (dla stabilno≈õci)\n",
    "\n",
    "**Dla Production (Job Cluster):**\n",
    "- Runtime: 14.3 LTS\n",
    "- Workers: autoscaling 2-20 (zale≈ºnie od obciƒÖ≈ºenia)\n",
    "- Node type: Memory-optimized (DS4_v2, m5.2xlarge)\n",
    "- Photon: W≈ÇƒÖczony\n",
    "- Spot instances: Tak (60-80% workers)\n",
    "- Auto-termination: 10 minut nieaktywno≈õci\n",
    "\n",
    "**Dla ML Workloads:**\n",
    "- Runtime: 14.3 ML (zawiera biblioteki ML)\n",
    "- Workers: GPU-enabled (NC6s_v3, p3.2xlarge)\n",
    "- Single-node mode dla prototypowania"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a33583d",
   "metadata": {},
   "source": [
    "## Sekcja 4: Magic Commands w Notebookach\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Databricks notebooks obs≈ÇugujƒÖ magic commands - specjalne polecenia zaczynajƒÖce siƒô od `%`, kt√≥re kontrolujƒÖ jƒôzyk kom√≥rki lub wykonujƒÖ operacje systemowe. Magic commands umo≈ºliwiajƒÖ mieszanie jƒôzyk√≥w w jednym notebooku oraz interakcjƒô z systemem plik√≥w.\n",
    "\n",
    "**Dostƒôpne magic commands:**\n",
    "- **%python**: Kom√≥rka Python (domy≈õlny)\n",
    "- **%sql**: Kom√≥rka SQL\n",
    "- **%scala**: Kom√≥rka Scala\n",
    "- **%r**: Kom√≥rka R\n",
    "- **%md**: Kom√≥rka Markdown (dokumentacja)\n",
    "- **%fs**: Operacje na systemie plik√≥w (DBFS)\n",
    "- **%sh**: Polecenia shell\n",
    "- **%run**: Uruchomienie innego notebooka (jak import)\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- ≈ÅƒÖczenie SQL i Python w jednym workflow\n",
    "- Dokumentacja inline z Markdown\n",
    "- Operacje na plikach z %fs\n",
    "- Modularyzacja kodu z %run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bfdc49",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 4.1: Demonstracja SQL magic command\n",
    "\n",
    "**Cel:** Wykonanie zapytania SQL bezpo≈õrednio w notebooku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690a7889",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL magic command pozwala pisaƒá czyste SQL bez otoczki Pythona\n",
    "\n",
    "SELECT \n",
    "  current_catalog() as catalog,\n",
    "  current_schema() as schema,\n",
    "  current_user() as user,\n",
    "  current_timestamp() as timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a73c822",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Magic command `%sql` zmienia jƒôzyk kom√≥rki na SQL. Wyniki sƒÖ automatycznie wy≈õwietlane jako tabela. SQL w Databricks to pe≈Çny Spark SQL z rozszerzeniami Delta Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc3aba2",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 4.2: File System operations z %fs\n",
    "\n",
    "**Cel:** Eksploracja systemu plik√≥w DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dda251b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic command %fs dla operacji na plikach\n",
    "# Lista katalog√≥w g≈Ç√≥wnych w DBFS\n",
    "dbutils.fs.ls(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b5f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatywnie: %fs magic command\n",
    "%fs ls /"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d9072f",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "DBFS (Databricks File System) to abstrakcja nad cloud storage (S3, ADLS, GCS). Komenda `%fs` lub `dbutils.fs` pozwala na operacje na plikach. W Unity Catalog zaleca siƒô u≈ºywanie **Volumes** zamiast DBFS dla lepszego governance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206676f6",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 4.3: Mieszanie jƒôzyk√≥w - Python i SQL\n",
    "\n",
    "**Cel:** Demonstracja p≈Çynnego przechodzenia miƒôdzy Python i SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383f6a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python: Przygotowanie danych\n",
    "data = [\n",
    "    (1, \"Alice\", \"Engineering\", 95000),\n",
    "    (2, \"Bob\", \"Sales\", 75000),\n",
    "    (3, \"Charlie\", \"Engineering\", 105000),\n",
    "    (4, \"Diana\", \"Marketing\", 68000),\n",
    "    (5, \"Eve\", \"Engineering\", 98000)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"department\", StringType(), False),\n",
    "    StructField(\"salary\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Rejestracja jako temp view dla dostƒôpu z SQL\n",
    "df.createOrReplaceTempView(\"employees_temp\")\n",
    "\n",
    "print(\"Utworzono temp view: employees_temp\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1898dc79",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL: Agregacja na danych z Python\n",
    "\n",
    "SELECT \n",
    "  department,\n",
    "  COUNT(*) as employee_count,\n",
    "  AVG(salary) as avg_salary,\n",
    "  MAX(salary) as max_salary\n",
    "FROM employees_temp\n",
    "GROUP BY department\n",
    "ORDER BY avg_salary DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed8510",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Ten przyk≈Çad pokazuje si≈Çƒô notebook√≥w Databricks: przygotowanie danych w Python (wygodne API, biblioteki), nastƒôpnie analiza w SQL (deklaratywne zapytania, przejrzysto≈õƒá). Temp views sƒÖ widoczne w ca≈Çym notebooku niezale≈ºnie od jƒôzyka kom√≥rki."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06003c65",
   "metadata": {},
   "source": [
    "## Sekcja 5: Unity Catalog vs Hive Metastore\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Databricks wspiera dwa systemy metadanych: legacy Hive Metastore oraz nowoczesny Unity Catalog. Unity Catalog jest zalecany dla wszystkich nowych projekt√≥w ze wzglƒôdu na zaawansowane funkcje governance i bezpiecze≈Ñstwa.\n",
    "\n",
    "**Kluczowe r√≥≈ºnice:**\n",
    "\n",
    "| Aspekt | Hive Metastore | Unity Catalog |\n",
    "|--------|----------------|---------------|\n",
    "| **Governance** | Ograniczone | Pe≈Çne: RBAC, masking, audit |\n",
    "| **Namespace** | 2-poziomowy (db.table) | 3-poziomowy (catalog.schema.table) |\n",
    "| **Cross-workspace** | Nie | Tak (shared metastore) |\n",
    "| **Lineage** | Brak | End-to-end lineage |\n",
    "| **Data Sharing** | Ograniczone | Delta Sharing protocol |\n",
    "| **Isolation** | Workspace-level | Catalog-level |\n",
    "\n",
    "**Dlaczego Unity Catalog?**\n",
    "- Centralne zarzƒÖdzanie dostƒôpem dla wszystkich workspace'√≥w\n",
    "- Automatyczny lineage dla audytu i compliance\n",
    "- Fine-grained permissions (column-level, row-level)\n",
    "- Integracja z zewnƒôtrznymi systemami (Delta Sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38054454",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 5.1: Namespace - Hive vs Unity Catalog\n",
    "\n",
    "**Cel:** Por√≥wnanie sk≈Çadni dostƒôpu do tabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169514b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hive Metastore (2-poziomowy namespace)\n",
    "print(\"=== Hive Metastore ===\")\n",
    "print(\"Sk≈Çadnia: database.table\")\n",
    "print(\"Przyk≈Çad: default.sales_data\")\n",
    "print(\"\")\n",
    "\n",
    "# Unity Catalog (3-poziomowy namespace)\n",
    "print(\"=== Unity Catalog ===\")\n",
    "print(\"Sk≈Çadnia: catalog.schema.table\")\n",
    "print(\"Przyk≈Çad: prod.gold.sales_summary\")\n",
    "print(\"\")\n",
    "print(\"Zalety 3-poziomowego namespace:\")\n",
    "print(\"- Oddzielenie ≈õrodowisk (dev/test/prod catalogs)\")\n",
    "print(\"- Lepsze uprawnienia (grant na poziomie catalogu)\")\n",
    "print(\"- Wsp√≥≈Çdzielenie metastore miƒôdzy workspace'ami\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf919e36",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 5.2: Tworzenie tabeli w Unity Catalog\n",
    "\n",
    "**Cel:** Demonstracja pe≈Çnej sk≈Çadni z 3-poziomowym namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0526ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utworzenie przyk≈Çadowej tabeli w Unity Catalog\n",
    "table_name = f\"{CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\"\n",
    "\n",
    "# Dane demonstracyjne\n",
    "demo_data = [\n",
    "    (1, \"Unity Catalog\", \"Enabled\", \"2024-01-15\"),\n",
    "    (2, \"Delta Lake\", \"Enabled\", \"2024-01-15\"),\n",
    "    (3, \"Photon Engine\", \"Enabled\", \"2024-01-15\"),\n",
    "    (4, \"Hive Metastore\", \"Legacy\", \"2024-01-15\")\n",
    "]\n",
    "\n",
    "demo_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"feature\", StringType(), False),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"date\", StringType(), False)\n",
    "])\n",
    "\n",
    "demo_df = spark.createDataFrame(demo_data, demo_schema)\n",
    "\n",
    "# Zapis jako Delta Table w Unity Catalog\n",
    "demo_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(table_name)\n",
    "\n",
    "print(f\"‚úÖ Tabela utworzona: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b6588",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Odczyt tabeli z pe≈Çnym namespace Unity Catalog\n",
    "\n",
    "SELECT * FROM ${CATALOG}.${BRONZE_SCHEMA}.lakehouse_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f36cfa2",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Tabela zosta≈Ça utworzona z pe≈Çnym 3-poziomowym namespace. W Unity Catalog ka≈ºda tabela automatycznie:\n",
    "- Jest zarzƒÖdzana przez system governance\n",
    "- Ma trackowany lineage\n",
    "- Posiada przypisane uprawnienia na podstawie katalogu i schematu\n",
    "- Jest dostƒôpna w Catalog Explorer dla eksploracji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4701bb33",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 5.3: Eksploracja metadanych Unity Catalog\n",
    "\n",
    "**Cel:** Wykorzystanie systemu informacyjnych schemat√≥w Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41147f52",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Unity Catalog udostƒôpnia system.information_schema dla metadanych\n",
    "\n",
    "-- Lista tabel w naszym schemacie\n",
    "SELECT \n",
    "  table_catalog,\n",
    "  table_schema,\n",
    "  table_name,\n",
    "  table_type\n",
    "FROM system.information_schema.tables\n",
    "WHERE table_catalog = '${CATALOG}'\n",
    "  AND table_schema = '${BRONZE_SCHEMA}'\n",
    "ORDER BY table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad6b0b7",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Unity Catalog automatycznie utrzymuje `system.information_schema` - zbi√≥r widok√≥w SQL z metadanymi o wszystkich obiektach. To standardowe podej≈õcie zgodne z ANSI SQL, co u≈Çatwia integracjƒô z narzƒôdziami BI i data governance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17df0f01",
   "metadata": {},
   "source": [
    "## Por√≥wnanie PySpark vs SQL\n",
    "\n",
    "**DataFrame API (PySpark):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6ec43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podej≈õcie PySpark - programatyczne DataFrame API\n",
    "\n",
    "df_pyspark = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\")\n",
    "\n",
    "result_pyspark = df_pyspark \\\n",
    "    .filter(F.col(\"status\") == \"Enabled\") \\\n",
    "    .select(\"feature\", \"status\", \"date\") \\\n",
    "    .orderBy(\"feature\")\n",
    "\n",
    "display(result_pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffd57b1",
   "metadata": {},
   "source": [
    "**SQL Equivalent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd489d4b",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Podej≈õcie SQL - deklaratywne zapytanie\n",
    "\n",
    "SELECT \n",
    "  feature,\n",
    "  status,\n",
    "  date\n",
    "FROM ${CATALOG}.${BRONZE_SCHEMA}.lakehouse_demo\n",
    "WHERE status = 'Enabled'\n",
    "ORDER BY feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b050da4d",
   "metadata": {},
   "source": [
    "**Por√≥wnanie:**\n",
    "- **Wydajno≈õƒá**: Identyczna - oba podej≈õcia kompilujƒÖ siƒô do tego samego Catalyst query plan\n",
    "- **Kiedy u≈ºywaƒá PySpark**: \n",
    "  - Z≈Ço≈ºona logika biznesowa z UDF\n",
    "  - Dynamiczne pipeline'y (parametryzacja, loops)\n",
    "  - Integracja z bibliotekami Python (pandas, scikit-learn)\n",
    "- **Kiedy u≈ºywaƒá SQL**: \n",
    "  - Proste transformacje i agregacje\n",
    "  - Zesp√≥≈Ç z silnymi kompetencjami SQL\n",
    "  - Migracja z tradycyjnych Data Warehouse\n",
    "  - Lepsze wsparcie dla analityk√≥w biznesowych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550a2b8e",
   "metadata": {},
   "source": [
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist - Co powiniene≈õ zrozumieƒá po tym notebooku:\n",
    "- [x] Koncepcja Lakehouse i korzy≈õci wzglƒôdem tradycyjnej architektury\n",
    "- [x] Struktura Workspace: Sidebar, Compute, Catalog Explorer\n",
    "- [x] Hierarchia Unity Catalog: Metastore ‚Üí Catalog ‚Üí Schema ‚Üí Objects\n",
    "- [x] Typy klastr√≥w: All-Purpose vs Job, autoscaling, Photon\n",
    "- [x] Magic commands: %sql, %python, %fs, %run, %md\n",
    "- [x] R√≥≈ºnice miƒôdzy Hive Metastore (2-level) a Unity Catalog (3-level)\n",
    "- [x] Tworzenie tabel w Unity Catalog z pe≈Çnym namespace\n",
    "- [x] Dostƒôp do metadanych przez system.information_schema\n",
    "\n",
    "### Komendy weryfikacyjne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95972660",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja wynik√≥w\n",
    "\n",
    "print(\"=== WERYFIKACJA KONFIGURACJI ===\\n\")\n",
    "\n",
    "# 1. Sprawd≈∫ czy tabela demo zosta≈Ça utworzona\n",
    "try:\n",
    "    demo_count = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\").count()\n",
    "    print(f\"‚úÖ Tabela demo utworzona: {demo_count} rekord√≥w\")\n",
    "except:\n",
    "    print(\"‚ùå Tabela demo nie istnieje\")\n",
    "\n",
    "# 2. Sprawd≈∫ katalog i schemat\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "print(f\"‚úÖ Aktualny katalog: {current_catalog}\")\n",
    "\n",
    "# 3. Weryfikacja schemat√≥w bronze/silver/gold\n",
    "schemas = [s.namespace for s in spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\").collect()]\n",
    "required_schemas = [BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA]\n",
    "\n",
    "for schema in required_schemas:\n",
    "    if schema in schemas:\n",
    "        print(f\"‚úÖ Schema {schema} istnieje\")\n",
    "    else:\n",
    "        print(f\"‚ùå Schema {schema} nie istnieje\")\n",
    "\n",
    "# 4. Informacje o klastrze\n",
    "print(f\"\\n‚úÖ Spark version: {spark.version}\")\n",
    "print(f\"‚úÖ DBR version: {spark.conf.get('spark.databricks.clusterUsageTags.sparkVersion', 'unknown')}\")\n",
    "\n",
    "print(\"\\nüéâ Wszystkie sprawdzenia zako≈Ñczone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da10ae1b",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Problem 1: \"Catalog not found\" lub \"Schema not found\"\n",
    "**Objawy:**\n",
    "- B≈ÇƒÖd: `AnalysisException: [SCHEMA_NOT_FOUND]`\n",
    "- B≈ÇƒÖd: `AnalysisException: [CATALOG_NOT_FOUND]`\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "```python\n",
    "# Sprawd≈∫ dostƒôpne katalogi\n",
    "spark.sql(\"SHOW CATALOGS\").show()\n",
    "\n",
    "# Sprawd≈∫ dostƒôpne schematy\n",
    "spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\").show()\n",
    "\n",
    "# Upewnij siƒô, ≈ºe uruchomi≈Çe≈õ %run ./00_setup\n",
    "```\n",
    "\n",
    "### Problem 2: \"Permission denied\" przy tworzeniu tabel\n",
    "**Objawy:**\n",
    "- B≈ÇƒÖd: `PERMISSION_DENIED: User does not have CREATE TABLE on Schema`\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "- Skontaktuj siƒô z administratorem workspace o nadanie uprawnie≈Ñ `CREATE TABLE`\n",
    "- Sprawd≈∫ uprawnienia: `SHOW GRANTS ON SCHEMA catalog.schema`\n",
    "\n",
    "### Problem 3: Klaster nie startuje lub jest zbyt wolny\n",
    "**Objawy:**\n",
    "- Klaster w stanie \"Pending\" przez d≈Çugi czas\n",
    "- Timeout przy starcie\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "- Sprawd≈∫ quota instancji w chmurze (Azure/AWS/GCP)\n",
    "- Zmniejsz liczbƒô worker√≥w lub wybierz mniejszy node type\n",
    "- Wy≈ÇƒÖcz autoscaling dla test√≥w\n",
    "\n",
    "### Problem 4: Magic command %sql nie dzia≈Ça\n",
    "**Objawy:**\n",
    "- B≈ÇƒÖd sk≈Çadni lub brak wynik√≥w\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "- Upewnij siƒô, ≈ºe `%sql` jest pierwszym elementem w kom√≥rce\n",
    "- Sprawd≈∫ czy u≈ºywasz zmiennych: `${CATALOG}` zamiast `{CATALOG}`\n",
    "- Dla zmiennych Python u≈ºyj: `spark.sql(f\"SELECT ... FROM {CATALOG}.{SCHEMA}.table\")`\n",
    "\n",
    "### Debugging tips:\n",
    "- U≈ºyj `explain()` na DataFrame aby zobaczyƒá plan wykonania\n",
    "- Sprawd≈∫ logi klastra w Spark UI (zak≈Çadka \"Cluster\" ‚Üí \"Spark UI\")\n",
    "- Weryfikuj typy danych: `df.printSchema()`\n",
    "- Dla problem√≥w z wydajno≈õciƒÖ sprawd≈∫ liczbƒô partycji: `df.rdd.getNumPartitions()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c78d45",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### Architektura Lakehouse:\n",
    "- U≈ºywaj Unity Catalog zamiast Hive Metastore dla nowych projekt√≥w\n",
    "- Organizuj dane w logiczne katalogi (np. dev/test/prod)\n",
    "- Stosuj naming convention dla schemat√≥w: bronze/silver/gold\n",
    "- Wykorzystuj Delta Lake jako domy≈õlny format tabel\n",
    "\n",
    "### ZarzƒÖdzanie Workspace:\n",
    "- Organizuj notebooki w folderach wed≈Çug projekt√≥w lub zespo≈Ç√≥w\n",
    "- U≈ºywaj Repos dla integracji z Git i wersjonowania\n",
    "- Dokumentuj notebooki z Markdown cells\n",
    "- Stosuj `%run` dla wsp√≥≈Çdzielenia kodu miƒôdzy notebookami\n",
    "\n",
    "### Konfiguracja Klastr√≥w:\n",
    "- Development: ma≈Çe klastry (2-4 workers), bez spot instances\n",
    "- Production: autoscaling, spot instances dla oszczƒôdno≈õci\n",
    "- W≈ÇƒÖcz Photon Engine dla zapyta≈Ñ SQL/DataFrame\n",
    "- Ustaw auto-termination (np. 30 min nieaktywno≈õci) dla klastr√≥w All-Purpose\n",
    "- Dla Jobs u≈ºywaj Job Clusters (efemeryczne, optymalne koszty)\n",
    "\n",
    "### Governance i Security:\n",
    "- Stosuj per-user lub per-team izolacjƒô katalog√≥w\n",
    "- U≈ºywaj 3-poziomowego namespace: catalog.schema.table\n",
    "- Przydzielaj uprawnienia na poziomie schematu, nie tabeli\n",
    "- Monitoruj dostƒôp przez system.access.audit\n",
    "- W≈ÇƒÖcz lineage dla compliance i debugowania\n",
    "\n",
    "### Wydajno≈õƒá:\n",
    "- Preferuj Delta Lake zamiast Parquet/CSV dla czƒôstych zapyta≈Ñ\n",
    "- Partycjonuj du≈ºe tabele wed≈Çug kluczy czasowych lub geograficznych\n",
    "- U≈ºywaj Z-ORDER dla kolumn u≈ºywanych w WHERE clauses\n",
    "- Regularnie uruchamiaj OPTIMIZE i VACUUM na tabelach Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec76154",
   "metadata": {},
   "source": [
    "## Podsumowanie\n",
    "\n",
    "### Co zosta≈Ço osiƒÖgniƒôte:\n",
    "- Poznanie koncepcji Lakehouse jako ewolucji Data Lake + Data Warehouse\n",
    "- Eksploracja element√≥w platformy Databricks: Workspace, Compute, Catalog\n",
    "- Zrozumienie hierarchii Unity Catalog: Metastore ‚Üí Catalog ‚Üí Schema ‚Üí Objects\n",
    "- Praktyka z magic commands: %sql, %python, %fs\n",
    "- Por√≥wnanie Hive Metastore vs Unity Catalog\n",
    "- Utworzenie pierwszej tabeli Delta w Unity Catalog z 3-poziomowym namespace\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "1. **Lakehouse eliminuje duplikacjƒô danych**: Jedna kopia s≈Çu≈ºy BI, ML i real-time analytics\n",
    "2. **Unity Catalog to fundament governance**: 3-poziomowy namespace, fine-grained permissions, automatyczny lineage\n",
    "3. **Klastry sƒÖ elastyczne**: Autoscaling i spot instances redukujƒÖ koszty, Photon przyspiesza zapytania\n",
    "4. **Notebooki sƒÖ potƒô≈ºne**: Mieszanie SQL/Python, magic commands, integracja z Git przez Repos\n",
    "5. **Delta Lake jest domy≈õlnym formatem**: ACID transactions, time travel, schema evolution\n",
    "\n",
    "### Quick Reference - Najwa≈ºniejsze komendy:\n",
    "\n",
    "| Operacja | PySpark | SQL |\n",
    "|----------|---------|-----|\n",
    "| Ustaw katalog | `spark.sql(f\"USE CATALOG {CATALOG}\")` | `USE CATALOG my_catalog` |\n",
    "| Lista katalog√≥w | `spark.sql(\"SHOW CATALOGS\")` | `SHOW CATALOGS` |\n",
    "| Lista schemat√≥w | `spark.sql(\"SHOW SCHEMAS\")` | `SHOW SCHEMAS` |\n",
    "| Utworzenie tabeli | `df.write.saveAsTable(\"cat.schema.table\")` | `CREATE TABLE cat.schema.table AS SELECT ...` |\n",
    "| Odczyt tabeli | `spark.table(\"cat.schema.table\")` | `SELECT * FROM cat.schema.table` |\n",
    "| Metadane | - | `SELECT * FROM system.information_schema.tables` |\n",
    "\n",
    "### Nastƒôpne kroki:\n",
    "- **Kolejny notebook**: `02_data_import_exploration.ipynb` - Wczytywanie danych z r√≥≈ºnych format√≥w (CSV, JSON, Parquet, Delta)\n",
    "- **Warsztat praktyczny**: Po zako≈Ñczeniu dema (notebooki 01-05) przejdziemy do `01_workspace_data_exploration_workshop.ipynb`\n",
    "- **Materia≈Çy dodatkowe**: \n",
    "  - [Databricks Lakehouse Fundamentals](https://www.databricks.com/learn/lakehouse)\n",
    "  - [Unity Catalog Documentation](https://docs.databricks.com/data-governance/unity-catalog/index.html)\n",
    "  - [Delta Lake Guide](https://docs.delta.io/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abda22db",
   "metadata": {},
   "source": [
    "## Czyszczenie zasob√≥w\n",
    "\n",
    "PosprzƒÖtaj zasoby utworzone podczas notebooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a24bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcjonalne czyszczenie zasob√≥w testowych\n",
    "# UWAGA: Uruchom tylko je≈õli chcesz usunƒÖƒá wszystkie utworzone dane\n",
    "\n",
    "# Usu≈Ñ tabelƒô demo\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\")\n",
    "\n",
    "# Usu≈Ñ temp views\n",
    "# spark.catalog.clearCache()\n",
    "# spark.sql(\"DROP VIEW IF EXISTS employees_temp\")\n",
    "\n",
    "# print(\"‚úÖ Zasoby zosta≈Çy wyczyszczone\")\n",
    "print(\"‚ö†Ô∏è  Czyszczenie zasob√≥w zakomentowane - odkomentuj je≈õli chcesz usunƒÖƒá dane demo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef190f4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Metadata:**\n",
    "- **Notebook**: 01_databricks_lakehouse_intro.ipynb\n",
    "- **Dzie≈Ñ**: 1 - Fundamentals & Exploration\n",
    "- **Typ**: Demo\n",
    "- **Czas**: 45 minut\n",
    "- **Wersja**: 1.0\n",
    "- **Data utworzenia**: 19 listopada 2025\n",
    "- **Compliance**: 7/7 standard√≥w ‚úÖ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
