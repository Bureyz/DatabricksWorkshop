{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "657047e2-fd5c-4e60-9173-07d93b535bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Wprowadzenie do Databricks Lakehouse - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Zrozumienie koncepcji Lakehouse, poznanie podstawowych elementów platformy Databricks oraz konfiguracji środowiska Unity Catalog.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Koncepcja Lakehouse (Data Lake + Data Warehouse)\n",
    "- Elementy platformy: Workspace, Catalog Explorer, Repos, Volumes, DBFS\n",
    "- Compute: clusters, autoscaling, spot instances, Photon Engine\n",
    "- Notebooks: magic commands (%sql, %python, %md)\n",
    "- Unity Catalog overview: katalogi, schematy, tabele\n",
    "- Różnice między Hive Metastore a Unity Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a99c61e6-53bd-409e-847f-46e32bc931fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 1 - Fundamentals & Exploration\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z 2-4 workers\n",
    "- **Czas trwania**: 20 minut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56333524-0ce0-4ecc-9fd7-22ea9d849be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie ewolucji architektur danych i miejsca Lakehouse w tym kontekście.\n",
    "\n",
    "**Podstawowe pojęcia:**\n",
    "- **Data Lake**: Scentralizowane repozytorium przechowujące surowe dane w różnych formatach (strukturalne, semi-strukturalne, niestrukturalne)\n",
    "- **Data Warehouse**: Zoptymalizowane repozytorium danych strukturalnych do analityki biznesowej i raportowania\n",
    "- **Lakehouse**: Nowoczesna architektura łącząca elastyczność Data Lake z niezawodnością i wydajnością Data Warehouse\n",
    "\n",
    "**Dlaczego to ważne?**\n",
    "Lakehouse eliminuje potrzebę utrzymywania dwóch oddzielnych systemów (Data Lake + Data Warehouse), redukując koszty, złożoność i opóźnienia w dostępie do danych. Dzięki Delta Lake uzyskujemy transakcyjność ACID, wersjonowanie danych i optymalizację zapytań bezpośrednio na plikach w Data Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a45d94d-3139-49cf-bef1-760ae552b359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Izolacja per użytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f4d94df-6da8-468c-84ce-e4601a325e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a822578-04ff-426c-91d0-619b23bf3f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f74fb0b1-9b23-4d51-b1fb-b12913824fc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "\n",
    "# Wyświetl kontekst użytkownika (zmienne z 00_setup)\n",
    "print(\"=== Kontekst użytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"Użytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog jako domyślny\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8883389d-dfce-4ac3-9beb-dee233de75b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 1: Koncepcja Lakehouse Architecture\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Lakehouse to nowoczesna architektura danych, która łączy zalety Data Lake (niski koszt przechowywania, wsparcie dla różnych formatów) z zaletami Data Warehouse (niezawodność, wydajność zapytań SQL, zarządzanie transakcjami). Kluczowym elementem jest Delta Lake - warstwa metadanych zapewniająca transakcyjność ACID na plikach Parquet.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **ACID Transactions**: Atomowość, Spójność, Izolacja, Trwałość - gwarancje zapewniające niezawodność operacji na danych\n",
    "- **Delta Lake**: Open-source storage layer zapewniający transakcyjność na plikach w Data Lake\n",
    "- **Unity Catalog**: Zunifikowany system zarządzania danymi, metadanymi i kontrolą dostępu\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Eliminacja duplikacji danych między systemami analitycznymi i operacyjnymi\n",
    "- Jednoczesne wsparcie dla BI, Data Science i Machine Learning\n",
    "- Redukcja kosztów infrastruktury i utrzymania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad256fd2-ae90-4d9c-8681-be489429f78b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.1: Porównanie tradycyjnej architektury vs Lakehouse\n",
    "\n",
    "**Cel:** Wizualizacja różnic między tradycyjnym podejściem (Data Lake + Data Warehouse) a Lakehouse.\n",
    "\n",
    "**Tradycyjna architektura:**\n",
    "```\n",
    "Raw Data → Data Lake (S3/ADLS) → ETL Process → Data Warehouse (Snowflake/Redshift) → BI Tools\n",
    "                                ↓\n",
    "                         ML/Data Science (separate copy)\n",
    "```\n",
    "\n",
    "**Lakehouse architektura:**\n",
    "```\n",
    "Raw Data → Delta Lake (single source of truth) → BI Tools + ML + Real-time Analytics\n",
    "```\n",
    "\n",
    "**Korzyści Lakehouse:**\n",
    "- Jedna kopia danych (single source of truth)\n",
    "- Niższe koszty przechowywania\n",
    "- Eliminacja opóźnień synchronizacji\n",
    "- Wspólne governance dla wszystkich use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "604dbe6a-70c7-4147-8713-f922e3d88de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 2: Elementy platformy Databricks\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Platforma Databricks składa się z kilku kluczowych komponentów, które razem tworzą kompletne środowisko do pracy z danymi w architekturze Lakehouse.\n",
    "\n",
    "**Kluczowe komponenty:**\n",
    "- **Workspace**: Środowisko pracy zawierające notebooks, eksperymenty, foldery i zasoby\n",
    "- **Catalog Explorer**: Interfejs do zarządzania katalogami, schematami, tabelami i widokami\n",
    "- **Repos**: Integracja z Git do wersjonowania notebooków i kodu\n",
    "- **Volumes**: Zarządzanie plikami niestrukturalnymi (obrazy, modele, artifacts)\n",
    "- **DBFS (Databricks File System)**: Wirtualny system plików nad cloud storage\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Workspace organizuje projekty i współpracę zespołową\n",
    "- Catalog Explorer umożliwia eksplorację i governance danych\n",
    "- Repos integruje development workflow z Git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95bf774f-a63e-4432-ba50-c8da3d316631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.1: Eksploracja Workspace\n",
    "\n",
    "**Cel:** Zapoznanie się z interfejsem Databricks Workspace\n",
    "\n",
    "**Elementy Workspace:**\n",
    "1. **Sidebar** (lewa strona):\n",
    "   - Workspace: Foldery i notebooki\n",
    "   - Repos: Integracja Git\n",
    "   - Compute: Zarządzanie klastrami\n",
    "   - Workflows: Databricks Jobs\n",
    "   - Catalog: Unity Catalog explorer\n",
    "\n",
    "2. **Główny panel**: Edytor notebooków lub widok szczegółów\n",
    "\n",
    "3. **Górna belka**: Szybki dostęp do compute, account, help\n",
    "\n",
    "**Instrukcje nawigacji:**\n",
    "- Użyj lewego menu do przełączania między sekcjami\n",
    "- W sekcji Catalog możesz przeglądać katalogi, schematy i tabele\n",
    "- W sekcji Compute zarządzasz klastrami Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef0fa899-84b5-4b93-835c-f0afe15d4d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.2: Catalog Explorer - struktura Unity Catalog\n",
    "\n",
    "**Cel:** Zrozumienie hierarchii obiektów w Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d181c5-b49b-4c44-bad0-d18ae5988b31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wyświetl aktualny katalog i schemat\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "\n",
    "print(f\"Aktualny katalog: {current_catalog}\")\n",
    "print(f\"Aktualny schemat: {current_schema}\")\n",
    "\n",
    "print(\"\\n=== Hierarchia Unity Catalog ===\")\n",
    "print(\"Metastore\")\n",
    "print(\"  ├── Catalog (np. main, dev, prod)\")\n",
    "print(\"  │   ├── Schema/Database (np. bronze, silver, gold)\")\n",
    "print(\"  │   │   ├── Tables (Delta Tables)\")\n",
    "print(\"  │   │   ├── Views (SQL Views)\")\n",
    "print(\"  │   │   ├── Functions (UDFs)\")\n",
    "print(\"  │   │   └── Volumes (dla plików)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab97de26-6e2a-4b67-87fb-7c9f012c65ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Unity Catalog organizuje dane w hierarchii: Metastore → Catalog → Schema → Objects (Tables/Views/Functions). Ta struktura umożliwia:\n",
    "- Logiczne oddzielenie środowisk (dev/test/prod)\n",
    "- Granularną kontrolę dostępu na każdym poziomie\n",
    "- Łatwe zarządzanie namespace'ami i izolacją projektów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81256c7-e5d6-4494-af87-36671b2da3ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.3: Przeglądanie katalogów i schematów\n",
    "\n",
    "**Cel:** Programowe listowanie obiektów w Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b4a711e-7765-4839-87bf-be0f1cc17263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista wszystkich katalogów dostępnych dla użytkownika\n",
    "print(\"=== Dostępne katalogi ===\")\n",
    "catalogs_df = spark.sql(\"SHOW CATALOGS\")\n",
    "display(catalogs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1acf7186-a6c5-4fe8-a5e2-3f8722729d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista schematów w aktualnym katalogu\n",
    "print(f\"\\n=== Schematy w katalogu {CATALOG} ===\")\n",
    "schemas_df = spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\")\n",
    "display(schemas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b47526a4-c364-44d8-9df1-c19945d0c6eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Polecenia `SHOW CATALOGS` i `SHOW SCHEMAS` pozwalają na eksplorację struktury Unity Catalog. Każdy użytkownik widzi tylko te obiekty, do których ma uprawnienia. Per-user izolacja (jak w naszym `00_setup`) zapewnia, że każdy uczestnik szkolenia ma własną przestrzeń roboczą."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0269415-6dda-473d-a771-236303dbe4c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 3: Compute - Klastry Spark\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Klastry Spark w Databricks są środowiskiem wykonawczym dla przetwarzania danych. Databricks oferuje różne typy klastrów i optymalizacje, które automatyzują zarządzanie infrastrukturą.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **All-Purpose Cluster**: Interaktywne klastry do analizy i rozwoju w notebookach\n",
    "- **Job Cluster**: Efemeryczne klastry dla automatyzowanych zadań (Databricks Jobs)\n",
    "- **Autoscaling**: Automatyczne skalowanie liczby workerów w zależności od obciążenia\n",
    "- **Spot Instances**: Wykorzystanie tańszych VM w chmurze (AWS Spot, Azure Spot, GCP Preemptible)\n",
    "- **Photon Engine**: Natywny engine wykonawczy w C++ dla przyspieszenia zapytań SQL i DataFrame\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Autoscaling redukuje koszty przy zmiennym obciążeniu\n",
    "- Spot instances zmniejszają koszty compute o 60-80%\n",
    "- Photon przyspiesza zapytania agregacyjne nawet 3x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "677ff10d-6ff8-4a96-b12f-5e84aba7f563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.1: Informacje o klastrze\n",
    "\n",
    "**Cel:** Sprawdzenie konfiguracji aktualnego klastra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8c329d-eb70-4c58-a701-ad269466d321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Informacje o Spark Context\n",
    "print(\"=== Konfiguracja klastra ===\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Aplikacja: {spark.sparkContext.appName}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31fa54e2-0f80-4c19-b604-13306507efde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Liczba executorów\n",
    "num_executors = len(spark.sparkContext._jsc.sc().statusTracker().getExecutorInfos()) \n",
    "print(f\"Liczba executorów (workers): {num_executors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91374905-ca6b-46c1-86da-c94a5549cede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Runtime version\n",
    "dbr_version = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\", \"unknown\")\n",
    "print(f\"Databricks Runtime: {dbr_version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7de0918-1b85-4f28-ac88-75b62996290e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Photon włączony?\n",
    "photon_enabled = spark.conf.get(\"spark.databricks.photon.enabled\", \"false\")\n",
    "print(f\"Photon Engine: {'Włączony' if photon_enabled == 'true' else 'Wyłączony'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8caaae99-cbbe-475a-86d3-29e73437c53c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Ten kod pokazuje podstawowe informacje o klastrze Spark. Liczba executorów (workers) może się zmieniać dynamicznie przy włączonym autoscalingu. Photon Engine, jeśli włączony, automatycznie przyspiesza zapytania SQL i operacje DataFrame bez zmian w kodzie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6efe33-547e-46aa-9d14-224669f28b27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.2: Best practices dla konfiguracji klastrów\n",
    "\n",
    "**Cel:** Poznanie rekomendacji dla różnych use cases\n",
    "\n",
    "**Dla Development (All-Purpose Cluster):**\n",
    "- Runtime: 14.3 LTS (Long Term Support)\n",
    "- Workers: 2-4 (autoscaling 2-8 dla większych projektów)\n",
    "- Node type: Standard DS3_v2 (Azure) lub m5.xlarge (AWS)\n",
    "- Photon: Włączony\n",
    "- Spot instances: Nie (dla stabilności)\n",
    "\n",
    "**Dla Production (Job Cluster):**\n",
    "- Runtime: 14.3 LTS\n",
    "- Workers: autoscaling 2-20 (zależnie od obciążenia)\n",
    "- Node type: Memory-optimized (DS4_v2, m5.2xlarge)\n",
    "- Photon: Włączony\n",
    "- Spot instances: Tak (60-80% workers)\n",
    "- Auto-termination: 10 minut nieaktywności\n",
    "\n",
    "**Dla ML Workloads:**\n",
    "- Runtime: 14.3 ML (zawiera biblioteki ML)\n",
    "- Workers: GPU-enabled (NC6s_v3, p3.2xlarge)\n",
    "- Single-node mode dla prototypowania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "566b74f2-66f4-4f1a-b5c9-15dd70cafa28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 4: Magic Commands w Notebookach\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Databricks notebooks obsługują magic commands - specjalne polecenia zaczynające się od `%`, które kontrolują język komórki lub wykonują operacje systemowe. Magic commands umożliwiają mieszanie języków w jednym notebooku oraz interakcję z systemem plików.\n",
    "\n",
    "**Dostępne magic commands:**\n",
    "- **%python**: Komórka Python (domyślny)\n",
    "- **%sql**: Komórka SQL\n",
    "- **%scala**: Komórka Scala\n",
    "- **%r**: Komórka R\n",
    "- **%md**: Komórka Markdown (dokumentacja)\n",
    "- **%fs**: Operacje na systemie plików (DBFS)\n",
    "- **%sh**: Polecenia shell\n",
    "- **%run**: Uruchomienie innego notebooka (jak import)\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Łączenie SQL i Python w jednym workflow\n",
    "- Dokumentacja inline z Markdown\n",
    "- Operacje na plikach z %fs\n",
    "- Modularyzacja kodu z %run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a320a9-fd52-454d-9a08-5c2074746fef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.1: Demonstracja SQL magic command\n",
    "\n",
    "**Cel:** Wykonanie zapytania SQL bezpośrednio w notebooku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cdd5b6e-ce7d-4a23-9056-467163b1657a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL magic command pozwala pisać czyste SQL bez otoczki Pythona\n",
    "\n",
    "SELECT \n",
    "  current_catalog() as catalog,\n",
    "  current_schema() as schema,\n",
    "  current_user() as user,\n",
    "  current_timestamp() as timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72219dd5-27a3-4b77-8ed5-a3d12215b52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Magic command `%sql` zmienia język komórki na SQL. Wyniki są automatycznie wyświetlane jako tabela. SQL w Databricks to pełny Spark SQL z rozszerzeniami Delta Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83c62a01-def3-4a05-9cee-251b9683b774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.2: File System operations z %fs\n",
    "\n",
    "**Cel:** Eksploracja systemu plików DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "132863d8-0852-49fc-bb62-3c4d3a87e1ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Magic command %fs dla operacji na plikach\n",
    "# Lista katalogów głównych w DBFS\n",
    "dbutils.fs.ls(\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "760a718c-6473-4233-83ec-526d06ec733e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "DBFS (Databricks File System) to abstrakcja nad cloud storage (S3, ADLS, GCS). Komenda `%fs` lub `dbutils.fs` pozwala na operacje na plikach. W Unity Catalog zaleca się używanie **Volumes** zamiast DBFS dla lepszego governance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cda7429b-9d22-4a72-99f8-e546014c7529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.3: Mieszanie języków - Python i SQL\n",
    "\n",
    "**Cel:** Demonstracja płynnego przechodzenia między Python i SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039f22a9-d548-4c7b-9532-6ced5708a3aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Python: Przygotowanie danych\n",
    "data = [\n",
    "    (1, \"Alice\", \"Engineering\", 95000),\n",
    "    (2, \"Bob\", \"Sales\", 75000),\n",
    "    (3, \"Charlie\", \"Engineering\", 105000),\n",
    "    (4, \"Diana\", \"Marketing\", 68000),\n",
    "    (5, \"Eve\", \"Engineering\", 98000)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"department\", StringType(), False),\n",
    "    StructField(\"salary\", IntegerType(), False)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Rejestracja jako temp view dla dostępu z SQL\n",
    "df.createOrReplaceTempView(\"employees_temp\")\n",
    "\n",
    "print(\"Utworzono temp view: employees_temp\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f59fce2a-0196-4145-8f6c-24d1f5d70b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL: Agregacja na danych z Python\n",
    "\n",
    "SELECT \n",
    "  department,\n",
    "  COUNT(*) as employee_count,\n",
    "  AVG(salary) as avg_salary,\n",
    "  MAX(salary) as max_salary\n",
    "FROM employees_temp\n",
    "GROUP BY department\n",
    "ORDER BY avg_salary DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9ac5da9-74e5-4025-babf-c3310d15b209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Ten przykład pokazuje siłę notebooków Databricks: przygotowanie danych w Python (wygodne API, biblioteki), następnie analiza w SQL (deklaratywne zapytania, przejrzystość). Temp views są widoczne w całym notebooku niezależnie od języka komórki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cffcf7a7-4b58-4f36-a746-71603cb32a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 5: Unity Catalog vs Hive Metastore\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Databricks wspiera dwa systemy metadanych: legacy Hive Metastore oraz nowoczesny Unity Catalog. Unity Catalog jest zalecany dla wszystkich nowych projektów ze względu na zaawansowane funkcje governance i bezpieczeństwa.\n",
    "\n",
    "**Kluczowe różnice:**\n",
    "\n",
    "| Aspekt | Hive Metastore | Unity Catalog |\n",
    "|--------|----------------|---------------|\n",
    "| **Governance** | Ograniczone | Pełne: RBAC, masking, audit |\n",
    "| **Namespace** | 2-poziomowy (db.table) | 3-poziomowy (catalog.schema.table) |\n",
    "| **Cross-workspace** | Nie | Tak (shared metastore) |\n",
    "| **Lineage** | Brak | End-to-end lineage |\n",
    "| **Data Sharing** | Ograniczone | Delta Sharing protocol |\n",
    "| **Isolation** | Workspace-level | Catalog-level |\n",
    "\n",
    "**Dlaczego Unity Catalog?**\n",
    "- Centralne zarządzanie dostępem dla wszystkich workspace'ów\n",
    "- Automatyczny lineage dla audytu i compliance\n",
    "- Fine-grained permissions (column-level, row-level)\n",
    "- Integracja z zewnętrznymi systemami (Delta Sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f41c629-f1dd-4331-a607-b16b4c7a8314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 5.1: Namespace - Hive vs Unity Catalog\n",
    "\n",
    "**Cel:** Porównanie składni dostępu do tabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62d86c93-7547-4cef-8aa4-5b873e572a3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Hive Metastore (2-poziomowy namespace)\n",
    "print(\"=== Hive Metastore ===\")\n",
    "print(\"Składnia: database.table\")\n",
    "print(\"Przykład: default.sales_data\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "632cf4e5-50c5-4fd6-a122-2345d98bbda0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Unity Catalog (3-poziomowy namespace)\n",
    "print(\"=== Unity Catalog ===\")\n",
    "print(\"Składnia: catalog.schema.table\")\n",
    "print(\"Przykład: prod.gold.sales_summary\")\n",
    "print(\"\")\n",
    "\n",
    "print(\"Zalety 3-poziomowego namespace:\")\n",
    "print(\"- Oddzielenie środowisk (dev/test/prod catalogs)\")\n",
    "print(\"- Lepsze uprawnienia (grant na poziomie catalogu)\")\n",
    "print(\"- Współdzielenie metastore między workspace'ami\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11188262-35c1-49e3-8b92-38362635bd2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 5.2: Tworzenie tabeli w Unity Catalog\n",
    "\n",
    "**Cel:** Demonstracja pełnej składni z 3-poziomowym namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c1afdc-9dba-48ea-a8fd-4f31ff2d05bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Utworzenie przykładowej tabeli w Unity Catalog\n",
    "table_name = f\"{CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\"\n",
    "\n",
    "# Dane demonstracyjne\n",
    "demo_data = [\n",
    "    (1, \"Unity Catalog\", \"Enabled\", \"2024-01-15\"),\n",
    "    (2, \"Delta Lake\", \"Enabled\", \"2024-01-15\"),\n",
    "    (3, \"Photon Engine\", \"Enabled\", \"2024-01-15\"),\n",
    "    (4, \"Hive Metastore\", \"Legacy\", \"2024-01-15\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "607e17b0-12c7-4f01-bbb1-09b9b62a0ad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "demo_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"feature\", StringType(), False),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"date\", StringType(), False)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fd7977b-9031-4744-97eb-828c35a1557b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "demo_df = spark.createDataFrame(demo_data, demo_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec6f87ed-65e9-4727-b1fa-cdbca6eba2dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(demo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40fd65e-3d9b-4ed0-a807-3aa070e50685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zapis jako Delta Table w Unity Catalog\n",
    "demo_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(table_name)\n",
    "\n",
    "print(f\"✅ Tabela utworzona: {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40a22d7f-5ce9-4cb8-832e-fd923f32c2db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"{CATALOG}.{BRONZE_SCHEMA}.{table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97bf40d3-d24d-4089-b971-bd13493037d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(f\"{table_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec8af90-b3b3-43db-a965-d0c39e0c44f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Tabela została utworzona z pełnym 3-poziomowym namespace. W Unity Catalog każda tabela automatycznie:\n",
    "- Jest zarządzana przez system governance\n",
    "- Ma trackowany lineage\n",
    "- Posiada przypisane uprawnienia na podstawie katalogu i schematu\n",
    "- Jest dostępna w Catalog Explorer dla eksploracji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93db4f3-fee5-4c6f-bfd4-c8f7c5da2637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 5.3: Eksploracja metadanych Unity Catalog\n",
    "\n",
    "**Cel:** Wykorzystanie systemu informacyjnych schematów Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d670b941-54a6-4572-9e1d-96623e199e7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Unity Catalog udostępnia system.information_schema dla metadanych\n",
    "\n",
    "-- Lista tabel w naszym schemacie\n",
    "SELECT \n",
    "  table_catalog,\n",
    "  table_schema,\n",
    "  table_name,\n",
    "  table_type\n",
    "FROM system.information_schema.tables\n",
    "--WHERE table_catalog = :CATALOG\n",
    " -- AND table_schema = :BRONZE_SCHEMA #TODO SQL Param\n",
    "ORDER BY table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c83299bc-2015-4d27-82f2-c527bd4a7fb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Unity Catalog automatycznie utrzymuje `system.information_schema` - zbiór widoków SQL z metadanymi o wszystkich obiektach. To standardowe podejście zgodne z ANSI SQL, co ułatwia integrację z narzędziami BI i data governance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dc408bc-fa10-46b1-aadf-e21efee43fd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Porównanie PySpark vs SQL\n",
    "\n",
    "**DataFrame API (PySpark):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1a2858-88a3-4425-b9ae-bbcb8dc3fd7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Podejście PySpark - programatyczne DataFrame API\n",
    "\n",
    "df_pyspark = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\")\n",
    "\n",
    "result_pyspark = df_pyspark \\\n",
    "    .filter(F.col(\"status\") == \"Enabled\") \\\n",
    "    .select(\"feature\", \"status\", \"date\") \\\n",
    "    .orderBy(\"feature\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c101550-b0db-4b2a-bb8b-5656a2b43c5b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result_pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f26fb2-d01e-4ddd-9ba5-a4f125828150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SQL Equivalent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231818c1-6090-40c1-9837-2f3ae9d0c68f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"{CATALOG}.{BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3dff4b4-da92-40a8-9785-eb777db88424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"CATALOG\", \"training_catalog\")\n",
    "dbutils.widgets.text(\"BRONZE_SCHEMA\", \"trainer_bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06523c1b-3a68-42d9-bd21-099e0ff46d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT \n",
    "  feature,\n",
    "  status,\n",
    "  date\n",
    "FROM IDENTIFIER(:CATALOG || '.' || :BRONZE_SCHEMA || '.lakehouse_demo')\n",
    "WHERE status = 'Enabled'\n",
    "ORDER BY feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd223e2-d3f6-42ca-ac53-acbbbea0ebbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Porównanie:**\n",
    "- **Wydajność**: Identyczna - oba podejścia kompilują się do tego samego Catalyst query plan\n",
    "- **Kiedy używać PySpark**: \n",
    "  - Złożona logika biznesowa z UDF\n",
    "  - Dynamiczne pipeline'y (parametryzacja, loops)\n",
    "  - Integracja z bibliotekami Python (pandas, scikit-learn)\n",
    "- **Kiedy używać SQL**: \n",
    "  - Proste transformacje i agregacje\n",
    "  - Zespół z silnymi kompetencjami SQL\n",
    "  - Migracja z tradycyjnych Data Warehouse\n",
    "  - Lepsze wsparcie dla analityków biznesowych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7237cb0f-ce86-4125-9a7e-4889656df750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist - Co powinieneś zrozumieć po tym notebooku:\n",
    "- [x] Koncepcja Lakehouse i korzyści względem tradycyjnej architektury\n",
    "- [x] Struktura Workspace: Sidebar, Compute, Catalog Explorer\n",
    "- [x] Hierarchia Unity Catalog: Metastore → Catalog → Schema → Objects\n",
    "- [x] Typy klastrów: All-Purpose vs Job, autoscaling, Photon\n",
    "- [x] Magic commands: %sql, %python, %fs, %run, %md\n",
    "- [x] Różnice między Hive Metastore (2-level) a Unity Catalog (3-level)\n",
    "- [x] Tworzenie tabel w Unity Catalog z pełnym namespace\n",
    "- [x] Dostęp do metadanych przez system.information_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3f4a541-553c-430b-b6e8-3aabbca452ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Problem 1: \"Catalog not found\" lub \"Schema not found\"\n",
    "**Objawy:**\n",
    "- Błąd: `AnalysisException: [SCHEMA_NOT_FOUND]`\n",
    "- Błąd: `AnalysisException: [CATALOG_NOT_FOUND]`\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Sprawdź dostępne katalogi\n",
    "spark.sql(\"SHOW CATALOGS\").show()\n",
    "\n",
    "# Sprawdź dostępne schematy\n",
    "spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\").show()\n",
    "\n",
    "# Upewnij się, że uruchomiłeś %run ./00_setup\n",
    "```\n",
    "\n",
    "### Problem 2: \"Permission denied\" przy tworzeniu tabel\n",
    "**Objawy:**\n",
    "- Błąd: `PERMISSION_DENIED: User does not have CREATE TABLE on Schema`\n",
    "\n",
    "**Rozwiązanie:**\n",
    "- Skontaktuj się z administratorem workspace o nadanie uprawnień `CREATE TABLE`\n",
    "- Sprawdź uprawnienia: `SHOW GRANTS ON SCHEMA catalog.schema`\n",
    "\n",
    "### Problem 3: Klaster nie startuje lub jest zbyt wolny\n",
    "**Objawy:**\n",
    "- Klaster w stanie \"Pending\" przez długi czas\n",
    "- Timeout przy starcie\n",
    "\n",
    "**Rozwiązanie:**\n",
    "- Sprawdź quota instancji w chmurze (Azure/AWS/GCP)\n",
    "- Zmniejsz liczbę workerów lub wybierz mniejszy node type\n",
    "- Wyłącz autoscaling dla testów\n",
    "\n",
    "### Problem 4: Magic command %sql nie działa\n",
    "**Objawy:**\n",
    "- Błąd składni lub brak wyników\n",
    "\n",
    "**Rozwiązanie:**\n",
    "- Upewnij się, że `%sql` jest pierwszym elementem w komórce\n",
    "- Sprawdź czy używasz zmiennych: `${CATALOG}` zamiast `{CATALOG}`\n",
    "- Dla zmiennych Python użyj: `spark.sql(f\"SELECT ... FROM {CATALOG}.{SCHEMA}.table\")`\n",
    "\n",
    "### Debugging tips:\n",
    "- Użyj `explain()` na DataFrame aby zobaczyć plan wykonania\n",
    "- Sprawdź logi klastra w Spark UI (zakładka \"Cluster\" → \"Spark UI\")\n",
    "- Weryfikuj typy danych: `df.printSchema()`\n",
    "- Dla problemów z wydajnością sprawdź liczbę partycji: `df.rdd.getNumPartitions()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e715d3-bead-4aad-bc6f-9e02f500a4f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "### Architektura Lakehouse:\n",
    "- Używaj Unity Catalog zamiast Hive Metastore dla nowych projektów\n",
    "- Organizuj dane w logiczne katalogi (np. dev/test/prod)\n",
    "- Stosuj naming convention dla schematów: bronze/silver/gold\n",
    "- Wykorzystuj Delta Lake jako domyślny format tabel\n",
    "\n",
    "### Zarządzanie Workspace:\n",
    "- Organizuj notebooki w folderach według projektów lub zespołów\n",
    "- Używaj Repos dla integracji z Git i wersjonowania\n",
    "- Dokumentuj notebooki z Markdown cells\n",
    "- Stosuj `%run` dla współdzielenia kodu między notebookami\n",
    "\n",
    "### Konfiguracja Klastrów:\n",
    "- Development: małe klastry (2-4 workers), bez spot instances\n",
    "- Production: autoscaling, spot instances dla oszczędności\n",
    "- Włącz Photon Engine dla zapytań SQL/DataFrame\n",
    "- Ustaw auto-termination (np. 30 min nieaktywności) dla klastrów All-Purpose\n",
    "- Dla Jobs używaj Job Clusters (efemeryczne, optymalne koszty)\n",
    "\n",
    "### Governance i Security:\n",
    "- Stosuj per-user lub per-team izolację katalogów\n",
    "- Używaj 3-poziomowego namespace: catalog.schema.table\n",
    "- Przydzielaj uprawnienia na poziomie schematu, nie tabeli\n",
    "- Monitoruj dostęp przez system.access.audit\n",
    "- Włącz lineage dla compliance i debugowania\n",
    "\n",
    "### Wydajność:\n",
    "- Preferuj Delta Lake zamiast Parquet/CSV dla częstych zapytań\n",
    "- Partycjonuj duże tabele według kluczy czasowych lub geograficznych\n",
    "- Używaj Z-ORDER dla kolumn używanych w WHERE clauses\n",
    "- Regularnie uruchamiaj OPTIMIZE i VACUUM na tabelach Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16ec104-b474-4943-b590-30dd012a7aad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Podsumowanie\n",
    "\n",
    "### Co zostało osiągnięte:\n",
    "- Poznanie koncepcji Lakehouse jako ewolucji Data Lake + Data Warehouse\n",
    "- Eksploracja elementów platformy Databricks: Workspace, Compute, Catalog\n",
    "- Zrozumienie hierarchii Unity Catalog: Metastore → Catalog → Schema → Objects\n",
    "- Praktyka z magic commands: %sql, %python, %fs\n",
    "- Porównanie Hive Metastore vs Unity Catalog\n",
    "- Utworzenie pierwszej tabeli Delta w Unity Catalog z 3-poziomowym namespace\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "1. **Lakehouse eliminuje duplikację danych**: Jedna kopia służy BI, ML i real-time analytics\n",
    "2. **Unity Catalog to fundament governance**: 3-poziomowy namespace, fine-grained permissions, automatyczny lineage\n",
    "3. **Klastry są elastyczne**: Autoscaling i spot instances redukują koszty, Photon przyspiesza zapytania\n",
    "4. **Notebooki są potężne**: Mieszanie SQL/Python, magic commands, integracja z Git przez Repos\n",
    "5. **Delta Lake jest domyślnym formatem**: ACID transactions, time travel, schema evolution\n",
    "\n",
    "### Quick Reference - Najważniejsze komendy:\n",
    "\n",
    "| Operacja | PySpark | SQL |\n",
    "|----------|---------|-----|\n",
    "| Ustaw katalog | `spark.sql(f\"USE CATALOG {CATALOG}\")` | `USE CATALOG my_catalog` |\n",
    "| Lista katalogów | `spark.sql(\"SHOW CATALOGS\")` | `SHOW CATALOGS` |\n",
    "| Lista schematów | `spark.sql(\"SHOW SCHEMAS\")` | `SHOW SCHEMAS` |\n",
    "| Utworzenie tabeli | `df.write.saveAsTable(\"cat.schema.table\")` | `CREATE TABLE cat.schema.table AS SELECT ...` |\n",
    "| Odczyt tabeli | `spark.table(\"cat.schema.table\")` | `SELECT * FROM cat.schema.table` |\n",
    "| Metadane | - | `SELECT * FROM system.information_schema.tables` |\n",
    "\n",
    "### Następne kroki:\n",
    "- **Kolejny notebook**: `02_data_import_exploration.ipynb` - Wczytywanie danych z różnych formatów (CSV, JSON, Parquet, Delta)\n",
    "- **Warsztat praktyczny**: Po zakończeniu dema (notebooki 01-05) przejdziemy do `01_workspace_data_exploration_workshop.ipynb`\n",
    "- **Materiały dodatkowe**: \n",
    "  - [Databricks Lakehouse Fundamentals](https://www.databricks.com/learn/lakehouse)\n",
    "  - [Unity Catalog Documentation](https://docs.databricks.com/data-governance/unity-catalog/index.html)\n",
    "  - [Delta Lake Guide](https://docs.delta.io/latest/index.html)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6108881231992997,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_databricks_lakehouse_intro",
   "widgets": {
    "BRONZE_SCHEMA": {
     "currentValue": "trainer_bronze",
     "nuid": "3eb823de-3328-4753-a2bf-9d863378b4f1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "trainer_bronze",
      "label": "",
      "name": "BRONZE_SCHEMA",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "trainer_bronze",
      "label": "",
      "name": "BRONZE_SCHEMA",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "CATALOG": {
     "currentValue": "training_catalog",
     "nuid": "ab4cee0e-5693-4e7e-8e80-189b95567447",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "training_catalog",
      "label": null,
      "name": "CATALOG",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "training_catalog",
      "label": null,
      "name": "CATALOG",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
