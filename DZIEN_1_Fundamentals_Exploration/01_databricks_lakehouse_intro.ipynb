{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "657047e2-fd5c-4e60-9173-07d93b535bad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Wprowadzenie do Databricks Lakehouse\n",
    "\n",
    "**Cel szkoleniowy:** Zrozumienie koncepcji Lakehouse, poznanie podstawowych element√≥w platformy Databricks oraz konfiguracji ≈õrodowiska Unity Catalog.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Koncepcja Lakehouse (Data Lake + Data Warehouse)\n",
    "- Elementy platformy: Workspace, Catalog Explorer, Repos, Volumes, DBFS\n",
    "- Compute: clusters, autoscaling, spot instances, Photon Engine\n",
    "- Notebooks: magic commands (%sql, %python, %md)\n",
    "- Unity Catalog overview: katalogi, schematy, tabele\n",
    "- R√≥≈ºnice miƒôdzy Hive Metastore a Unity Catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a99c61e6-53bd-409e-847f-46e32bc931fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzie≈Ñ szkolenia**: Dzie≈Ñ 1 - Fundamentals & Exploration\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog w≈ÇƒÖczony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z 2-4 workers\n",
    "- **Czas trwania**: 20 minut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56333524-0ce0-4ecc-9fd7-22ea9d849be1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wstƒôp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie ewolucji architektur danych i miejsca Lakehouse w tym kontek≈õcie.\n",
    "\n",
    "**Podstawowe pojƒôcia:**\n",
    "- **Data Lake**: Scentralizowane repozytorium przechowujƒÖce surowe dane w r√≥≈ºnych formatach (strukturalne, semi-strukturalne, niestrukturalne)\n",
    "- **Data Warehouse**: Zoptymalizowane repozytorium danych strukturalnych do analityki biznesowej i raportowania\n",
    "- **Lakehouse**: Nowoczesna architektura ≈ÇƒÖczƒÖca elastyczno≈õƒá Data Lake z niezawodno≈õciƒÖ i wydajno≈õciƒÖ Data Warehouse\n",
    "\n",
    "**Dlaczego to wa≈ºne?**\n",
    "Lakehouse eliminuje potrzebƒô utrzymywania dw√≥ch oddzielnych system√≥w (Data Lake + Data Warehouse), redukujƒÖc koszty, z≈Ço≈ºono≈õƒá i op√≥≈∫nienia w dostƒôpie do danych. Dziƒôki Delta Lake uzyskujemy transakcyjno≈õƒá ACID, wersjonowanie danych i optymalizacjƒô zapyta≈Ñ bezpo≈õrednio na plikach w Data Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a45d94d-3139-49cf-bef1-760ae552b359",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Izolacja per u≈ºytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalog√≥w i schemat√≥w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f4d94df-6da8-468c-84ce-e4601a325e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a822578-04ff-426c-91d0-619b23bf3f76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych ≈õrodowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f74fb0b1-9b23-4d51-b1fb-b12913824fc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import re\n",
    "\n",
    "# Wy≈õwietl kontekst u≈ºytkownika (zmienne z 00_setup)\n",
    "print(\"=== Kontekst u≈ºytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"U≈ºytkownik: {raw_user}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6be0ad0d-871c-49b0-ae7c-b5423e4ab8a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ustaw katalog jako domy≈õlny\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8883389d-dfce-4ac3-9beb-dee233de75b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 1: Koncepcja Lakehouse Architecture\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Lakehouse to nowoczesna architektura danych, kt√≥ra ≈ÇƒÖczy zalety Data Lake (niski koszt przechowywania, wsparcie dla r√≥≈ºnych format√≥w) z zaletami Data Warehouse (niezawodno≈õƒá, wydajno≈õƒá zapyta≈Ñ SQL, zarzƒÖdzanie transakcjami). Kluczowym elementem jest Delta Lake - warstwa metadanych zapewniajƒÖca transakcyjno≈õƒá ACID na plikach Parquet.\n",
    "\n",
    "**Kluczowe pojƒôcia:**\n",
    "- **ACID Transactions**: Atomowo≈õƒá, Sp√≥jno≈õƒá, Izolacja, Trwa≈Ço≈õƒá - gwarancje zapewniajƒÖce niezawodno≈õƒá operacji na danych\n",
    "- **Delta Lake**: Open-source storage layer zapewniajƒÖcy transakcyjno≈õƒá na plikach w Data Lake\n",
    "- **Unity Catalog**: Zunifikowany system zarzƒÖdzania danymi, metadanymi i kontrolƒÖ dostƒôpu\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Eliminacja duplikacji danych miƒôdzy systemami analitycznymi i operacyjnymi\n",
    "- Jednoczesne wsparcie dla BI, Data Science i Machine Learning\n",
    "- Redukcja koszt√≥w infrastruktury i utrzymania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad256fd2-ae90-4d9c-8681-be489429f78b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Por√≥wnanie tradycyjnej architektury vs Lakehouse\n",
    "\n",
    "**Cel:** Wizualizacja r√≥≈ºnic miƒôdzy tradycyjnym podej≈õciem (Data Lake + Data Warehouse) a Lakehouse.\n",
    "\n",
    "**Tradycyjna architektura:**\n",
    "```\n",
    "Raw Data ‚Üí Data Lake (S3/ADLS) ‚Üí ETL Process ‚Üí Data Warehouse (Snowflake/Redshift) ‚Üí BI Tools\n",
    "                                ‚Üì\n",
    "                         ML/Data Science (separate copy)\n",
    "```\n",
    "\n",
    "**Lakehouse architektura:**\n",
    "```\n",
    "Raw Data ‚Üí Delta Lake (single source of truth) ‚Üí BI Tools + ML + Real-time Analytics\n",
    "```\n",
    "\n",
    "**Korzy≈õci Lakehouse:**\n",
    "- Jedna kopia danych (single source of truth)\n",
    "- Ni≈ºsze koszty przechowywania\n",
    "- Eliminacja op√≥≈∫nie≈Ñ synchronizacji\n",
    "- Wsp√≥lne governance dla wszystkich use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "604dbe6a-70c7-4147-8713-f922e3d88de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 2: Elementy platformy Databricks\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Platforma Databricks sk≈Çada siƒô z kilku kluczowych komponent√≥w, kt√≥re razem tworzƒÖ kompletne ≈õrodowisko do pracy z danymi w architekturze Lakehouse.\n",
    "\n",
    "**Kluczowe komponenty:**\n",
    "- **Workspace**: ≈örodowisko pracy zawierajƒÖce notebooks, eksperymenty, foldery i zasoby\n",
    "- **Catalog Explorer**: Interfejs do zarzƒÖdzania katalogami, schematami, tabelami i widokami\n",
    "- **Git Folders (dawniej Repos)**: Integracja z Git do wersjonowania notebook√≥w i kodu\n",
    "- **Volumes**: ZarzƒÖdzanie plikami niestrukturalnymi (obrazy, modele, artifacts)\n",
    "- **DBFS (Databricks File System)**: Wirtualny system plik√≥w nad cloud storage\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Workspace organizuje projekty i wsp√≥≈Çpracƒô zespo≈ÇowƒÖ\n",
    "- Catalog Explorer umo≈ºliwia eksploracjƒô i governance danych\n",
    "- Git Folders integruje development workflow z Git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95bf774f-a63e-4432-ba50-c8da3d316631",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 2.1: Eksploracja Workspace\n",
    "\n",
    "**Cel:** Zapoznanie siƒô z interfejsem Databricks Workspace\n",
    "\n",
    "**Elementy Workspace:**\n",
    "1. **Sidebar** (lewa strona):\n",
    "   - Workspace: Foldery i notebooki\n",
    "   - Git Folders: Integracja Git\n",
    "   - Compute: ZarzƒÖdzanie klastrami\n",
    "   - Workflows: Databricks Jobs\n",
    "   - Catalog: Unity Catalog explorer\n",
    "\n",
    "2. **G≈Ç√≥wny panel**: Edytor notebook√≥w lub widok szczeg√≥≈Ç√≥w\n",
    "\n",
    "3. **G√≥rna belka**: Szybki dostƒôp do compute, account, help\n",
    "\n",
    "**Instrukcje nawigacji:**\n",
    "- U≈ºyj lewego menu do prze≈ÇƒÖczania miƒôdzy sekcjami\n",
    "- W sekcji Catalog mo≈ºesz przeglƒÖdaƒá katalogi, schematy i tabele\n",
    "- W sekcji Compute zarzƒÖdzasz klastrami Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef0fa899-84b5-4b93-835c-f0afe15d4d57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 2.2: Catalog Explorer - struktura Unity Catalog\n",
    "\n",
    "**Cel:** Zrozumienie hierarchii obiekt√≥w w Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e7d181c5-b49b-4c44-bad0-d18ae5988b31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wy≈õwietl aktualny katalog i schemat\n",
    "current_catalog = spark.sql(\"SELECT current_catalog()\").collect()[0][0]\n",
    "current_schema = spark.sql(\"SELECT current_schema()\").collect()[0][0]\n",
    "\n",
    "print(f\"Aktualny katalog: {current_catalog}\")\n",
    "print(f\"Aktualny schemat: {current_schema}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0069c11c-67d5-4a54-a1a7-fbb67f5d80e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Hierarchia Unity Catalog:**\n",
    "\n",
    "```\n",
    "Metastore\n",
    "  ‚îú‚îÄ‚îÄ Catalog (np. main, dev, prod)\n",
    "  ‚îÇ   ‚îú‚îÄ‚îÄ Schema/Database (np. bronze, silver, gold)\n",
    "  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Tables (Delta Tables)\n",
    "  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Views (SQL Views)\n",
    "  ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Functions (UDFs)\n",
    "  ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Volumes (dla plik√≥w)\n",
    "```\n",
    "\n",
    "Unity Catalog organizuje dane w trzech poziomach: `catalog.schema.table`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab97de26-6e2a-4b67-87fb-7c9f012c65ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Unity Catalog organizuje dane w hierarchii: Metastore ‚Üí Catalog ‚Üí Schema ‚Üí Objects (Tables/Views/Functions). Ta struktura umo≈ºliwia:\n",
    "- Logiczne oddzielenie ≈õrodowisk (dev/test/prod)\n",
    "- GranularnƒÖ kontrolƒô dostƒôpu na ka≈ºdym poziomie\n",
    "- ≈Åatwe zarzƒÖdzanie namespace'ami i izolacjƒÖ projekt√≥w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a81256c7-e5d6-4494-af87-36671b2da3ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 2.3: PrzeglƒÖdanie katalog√≥w i schemat√≥w\n",
    "\n",
    "**Cel:** Programowe listowanie obiekt√≥w w Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b4a711e-7765-4839-87bf-be0f1cc17263",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista wszystkich katalog√≥w dostƒôpnych dla u≈ºytkownika\n",
    "catalogs_df = spark.sql(\"SHOW CATALOGS\")\n",
    "display(catalogs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1acf7186-a6c5-4fe8-a5e2-3f8722729d73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista schemat√≥w w aktualnym katalogu\n",
    "schemas_df = spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\")\n",
    "display(schemas_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b47526a4-c364-44d8-9df1-c19945d0c6eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Polecenia `SHOW CATALOGS` i `SHOW SCHEMAS` pozwalajƒÖ na eksploracjƒô struktury Unity Catalog. Ka≈ºdy u≈ºytkownik widzi tylko te obiekty, do kt√≥rych ma uprawnienia. Per-user izolacja (jak w naszym `00_setup`) zapewnia, ≈ºe ka≈ºdy uczestnik szkolenia ma w≈ÇasnƒÖ przestrze≈Ñ roboczƒÖ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b04ffdba-2d7b-4363-8c15-9312d59f107a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## 2.4 Git Folders (Repos) i integracja z Git \n",
    "\n",
    "W praktyce praca z kodem w Databricks powinna opieraƒá siƒô o **Git Folders** (dawniej Repos), a nie o pojedyncze, osierocone notebooki w Workspace.\n",
    "\n",
    "Typowy workflow:\n",
    "\n",
    "1. **Utw√≥rz Git Folder** w Databricks: `Workspace ‚Üí Git Folders ‚Üí Add Repo`.\n",
    "2. **Po≈ÇƒÖcz z Git** (GitHub / Azure DevOps / inne).\n",
    "3. Pracuj na **branchach feature** (np. `feature/cleaning-module`).\n",
    "4. Regularnie:\n",
    "   - commituj i pushuj zmiany z Databricks do zdalnego repo,\n",
    "   - tw√≥rz PR i merguj do main/dev.\n",
    "\n",
    "Dobre praktyki:\n",
    "\n",
    "- Jeden repo per projekt/domenƒô (np. `databricks-dea-training-kion`).\n",
    "- Nie pracujemy w **Workspace root** ‚Äì zawsze w **Git Folders**.\n",
    "- Notebooki szkoleniowe, dane testowe i README mogƒÖ byƒá w jednym repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3b97f4c-f2d7-476b-876c-03e662fc8258",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 2.5 Volumes vs DBFS ‚Äì gdzie trzymaƒá pliki?\n",
    "\n",
    "W nowych workspace'ach opartych na Unity Catalog preferowanym miejscem przechowywania plik√≥w sƒÖ **Volumes**.\n",
    "\n",
    "- `dbfs:/` traktujemy jako warstwƒô **legacy** lub obszar pomocniczy.\n",
    "- `volume://catalog.schema.volume_name` to w pe≈Çni zarzƒÖdzany, kontrolowany przez UC obszar danych (uprawnienia, audit, lineage).\n",
    "\n",
    "Przyk≈Çad definicji Volume (SQL):\n",
    "\n",
    "```sql\n",
    "CREATE VOLUME IF NOT EXISTS ${catalog}.${schema}.training_volume\n",
    "COMMENT 'Obszar roboczy na potrzeby szkole≈Ñ';\n",
    "```\n",
    "\n",
    "Przyk≈Çad u≈ºycia w PySpark:\n",
    "\n",
    "```python\n",
    "catalog = dbutils.widgets.get(\"catalog\")\n",
    "schema = dbutils.widgets.get(\"schema\")\n",
    "\n",
    "volume_path = f\"volume://{catalog}.{schema}.training_volume\"\n",
    "display(dbutils.fs.ls(volume_path))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59fe7bbe-41c2-4fc0-8247-61dbd858854f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## 2.6 Serverless SQL / SQL Warehouse ‚Äì kiedy zamiast clustra notebookowego?\n",
    "\n",
    "Opr√≥cz klastr√≥w notebookowych Databricks oferuje **SQL Warehouse (serverless)** ‚Äì silnik zapyta≈Ñ SQL zoptymalizowany pod BI i analitykƒô ad‚Äëhoc.\n",
    "\n",
    "Kiedy u≈ºywaƒá:\n",
    "- Raportowanie w Power BI / innych narzƒôdziach BI.\n",
    "- Analitycy biznesowi / power users, kt√≥rzy pracujƒÖ g≈Ç√≥wnie w SQL.\n",
    "- Interaktywne dashboardy i zapytania ad‚Äëhoc do warstwy **Gold**.\n",
    "\n",
    "R√≥≈ºnice do all‚Äëpurpose cluster:\n",
    "- Rozliczanie w oparciu o **DBU SQL** (inne stawki).\n",
    "- Automatyczny provisioning / scaling.\n",
    "- Izolacja obciƒÖ≈ºenia BI od klastr√≥w in≈ºynierskich.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0269415-6dda-473d-a771-236303dbe4c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 3: Compute - Klastry Spark i Serverless\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Klastry Spark w Databricks sƒÖ ≈õrodowiskiem wykonawczym dla przetwarzania danych. Tradycyjnie zarzƒÖdzamy klastrami (All-Purpose, Job), ale platforma coraz mocniej ewoluuje w stronƒô **Serverless Compute**, gdzie infrastruktura jest w pe≈Çni zarzƒÖdzana przez Databricks, startuje natychmiastowo i skaluje siƒô automatycznie.\n",
    "\n",
    "**Kluczowe pojƒôcia:**\n",
    "- **All-Purpose Cluster**: Interaktywne klastry do analizy i rozwoju w notebookach.\n",
    "- **Job Cluster**: Efemeryczne klastry dla automatyzowanych zada≈Ñ.\n",
    "- **Serverless Compute**: Bezobs≈Çugowa warstwa obliczeniowa (dostƒôpna dla SQL Warehouses, Jobs, a coraz czƒô≈õciej dla Notebook√≥w).\n",
    "- **Autoscaling**: Automatyczne skalowanie liczby worker√≥w.\n",
    "- **Photon Engine**: Natywny engine wykonawczy w C++ przyspieszajƒÖcy zapytania.\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Serverless eliminuje czas oczekiwania na start klastra (instant startup).\n",
    "- Autoscaling redukuje koszty przy zmiennym obciƒÖ≈ºeniu.\n",
    "- Photon przyspiesza zapytania agregacyjne nawet 3x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "677ff10d-6ff8-4a96-b12f-5e84aba7f563",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 3.1: Informacje o klastrze\n",
    "\n",
    "**Cel:** Sprawdzenie konfiguracji aktualnego klastra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d8c329d-eb70-4c58-a701-ad269466d321",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Informacje o Spark Context\n",
    "spark_version = spark.version\n",
    "app_name = spark.sparkContext.appName\n",
    "master = spark.sparkContext.master"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92b57d4d-6aa9-4bdf-9bea-0b239e059ad5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Konfiguracja klastra:**\n",
    "\n",
    "Podstawowe informacje o ≈õrodowisku Spark i Databricks Runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e683c880-a33e-42c3-a526-c6da591bc240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"Spark Version: {spark_version}\")\n",
    "print(f\"Aplikacja: {app_name}\")\n",
    "print(f\"Master: {master}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "31fa54e2-0f80-4c19-b604-13306507efde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Liczba executor√≥w\n",
    "num_executors = len(spark.sparkContext._jsc.sc().statusTracker().getExecutorInfos()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91374905-ca6b-46c1-86da-c94a5549cede",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Runtime version\n",
    "dbr_version = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\", \"unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7de0918-1b85-4f28-ac88-75b62996290e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Photon w≈ÇƒÖczony?\n",
    "photon_enabled = spark.conf.get(\"spark.databricks.photon.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8caaae99-cbbe-475a-86d3-29e73437c53c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Ten kod pokazuje podstawowe informacje o klastrze Spark. Liczba executor√≥w (workers) mo≈ºe siƒô zmieniaƒá dynamicznie przy w≈ÇƒÖczonym autoscalingu. Photon Engine, je≈õli w≈ÇƒÖczony, automatycznie przyspiesza zapytania SQL i operacje DataFrame bez zmian w kodzie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6efe33-547e-46aa-9d14-224669f28b27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 3.2: Best practices dla konfiguracji klastr√≥w\n",
    "\n",
    "**Cel:** Poznanie rekomendacji dla r√≥≈ºnych use cases\n",
    "\n",
    "**Dla Development (All-Purpose Cluster):**\n",
    "- Runtime: 14.3 LTS (Long Term Support)\n",
    "- Workers: 2-4 (autoscaling 2-8 dla wiƒôkszych projekt√≥w)\n",
    "- Node type: Standard DS3_v2 (Azure) lub m5.xlarge (AWS)\n",
    "- Photon: W≈ÇƒÖczony\n",
    "- Spot instances: Nie (dla stabilno≈õci)\n",
    "\n",
    "**Dla Production (Job Cluster):**\n",
    "- Runtime: 14.3 LTS\n",
    "- Workers: autoscaling 2-20 (zale≈ºnie od obciƒÖ≈ºenia)\n",
    "- Node type: Memory-optimized (DS4_v2, m5.2xlarge)\n",
    "- Photon: W≈ÇƒÖczony\n",
    "- Spot instances: Tak (60-80% workers)\n",
    "- Auto-termination: 10 minut nieaktywno≈õci\n",
    "\n",
    "**Dla ML Workloads:**\n",
    "- Runtime: 14.3 ML (zawiera biblioteki ML)\n",
    "- Workers: GPU-enabled (NC6s_v3, p3.2xlarge)\n",
    "- Single-node mode dla prototypowania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "566b74f2-66f4-4f1a-b5c9-15dd70cafa28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 4: Magic Commands w Notebookach\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Databricks notebooks obs≈ÇugujƒÖ magic commands - specjalne polecenia zaczynajƒÖce siƒô od `%`, kt√≥re kontrolujƒÖ jƒôzyk kom√≥rki lub wykonujƒÖ operacje systemowe. Magic commands umo≈ºliwiajƒÖ mieszanie jƒôzyk√≥w w jednym notebooku oraz interakcjƒô z systemem plik√≥w.\n",
    "\n",
    "**Dostƒôpne magic commands:**\n",
    "- **%python**: Kom√≥rka Python (domy≈õlny)\n",
    "- **%sql**: Kom√≥rka SQL\n",
    "- **%scala**: Kom√≥rka Scala\n",
    "- **%r**: Kom√≥rka R\n",
    "- **%md**: Kom√≥rka Markdown (dokumentacja)\n",
    "- **%fs**: Operacje na systemie plik√≥w (DBFS)\n",
    "- **%sh**: Polecenia shell\n",
    "- **%run**: Uruchomienie innego notebooka (jak import)\n",
    "- **%pip**: Instalacja bibliotek Python (notebook-scoped)\n",
    "- **%skip**: Skipuje komorke\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- ≈ÅƒÖczenie SQL i Python w jednym workflow\n",
    "- Dokumentacja inline z Markdown\n",
    "- Operacje na plikach z %fs\n",
    "- Modularyzacja kodu z %run\n",
    "- ZarzƒÖdzanie zale≈ºno≈õciami z %pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14084b35-dd2b-4bc3-abcb-b170d0e48117",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Monitoring i logowanie ‚Äì gdzie szukaƒá problem√≥w?\n",
    "\n",
    "Przy pracy z klastrami i Jobami warto znaƒá podstawowe miejsca, gdzie szukamy informacji diagnostycznych:\n",
    "\n",
    "- **Cluster ‚Üí Event log** ‚Äì start/stop clustra, autoscaling, b≈Çƒôdy infrastruktury.\n",
    "- **Spark UI** (zak≈Çadki Jobs, SQL, Storage, Environment) ‚Äì plan wykonania, shufflowanie, b≈Çƒôdy na poziomie zada≈Ñ.\n",
    "- **Driver / Executor logs** ‚Äì szczeg√≥≈Çowe stacktrace'y wyjƒÖtk√≥w Pythona/Scali.\n",
    "- **Job Run page** ‚Äì status poszczeg√≥lnych task√≥w, retry, czas wykonania.\n",
    "\n",
    "Dobre praktyki:\n",
    "- Przy d≈Çu≈ºszych pipeline'ach zawsze sprawdzaj Spark UI (sekcja SQL/Jobs).\n",
    "- Krytyczne logi aplikacyjne zapisuj do tabel Delta / storage, a nie tylko do log√≥w clustra.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15a320a9-fd52-454d-9a08-5c2074746fef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 4.1: Demonstracja SQL magic command\n",
    "\n",
    "**Cel:** Wykonanie zapytania SQL bezpo≈õrednio w notebooku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0cdd5b6e-ce7d-4a23-9056-467163b1657a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL magic command pozwala pisaƒá czyste SQL bez otoczki Pythona\n",
    "\n",
    "SELECT \n",
    "  current_catalog() as catalog,\n",
    "  current_schema() as schema,\n",
    "  current_user() as user,\n",
    "  current_timestamp() as timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72219dd5-27a3-4b77-8ed5-a3d12215b52f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Magic command `%sql` zmienia jƒôzyk kom√≥rki na SQL. Wyniki sƒÖ automatycznie wy≈õwietlane jako tabela. SQL w Databricks to pe≈Çny Spark SQL z rozszerzeniami Delta Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83c62a01-def3-4a05-9cee-251b9683b774",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 4.2: File System operations z fs\n",
    "\n",
    "**Cel:** Eksploracja systemu plik√≥w DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "132863d8-0852-49fc-bb62-3c4d3a87e1ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista katalog√≥w g≈Ç√≥wnych w DBFS\n",
    "dbutils.fs.ls(\"/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "760a718c-6473-4233-83ec-526d06ec733e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "DBFS (Databricks File System) to abstrakcja nad cloud storage (S3, ADLS, GCS). Komenda `%fs` lub `dbutils.fs` pozwala na operacje na plikach. W Unity Catalog zaleca siƒô u≈ºywanie **Volumes** zamiast DBFS dla lepszego governance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cda7429b-9d22-4a72-99f8-e546014c7529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 4.3: Mieszanie jƒôzyk√≥w - Python i SQL\n",
    "\n",
    "**Cel:** Demonstracja p≈Çynnego przechodzenia miƒôdzy Python i SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "039f22a9-d548-4c7b-9532-6ced5708a3aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Python: Definicja danych surowych\n",
    "data = [\n",
    "    (1, \"Alice\", \"Engineering\", 95000),\n",
    "    (2, \"Bob\", \"Sales\", 75000),\n",
    "    (3, \"Charlie\", \"Engineering\", 105000),\n",
    "    (4, \"Diana\", \"Marketing\", 68000),\n",
    "    (5, \"Eve\", \"Engineering\", 98000)\n",
    "]\n",
    "\n",
    "# Definicja schematu\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), False),\n",
    "    StructField(\"department\", StringType(), False),\n",
    "    StructField(\"salary\", IntegerType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "289d1ce0-479b-47f5-9e30-50c09b90c688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Utworzenie DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e34557e1-9ac9-4f3a-88c9-3f40d3c8426f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rejestracja jako temp view dla dostƒôpu z SQL\n",
    "df.createOrReplaceTempView(\"employees_temp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4071b2d3-e8f0-4027-a850-c18c2fc7b71a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Temp view utworzony: employees_temp**\n",
    "\n",
    "Temp view pozwala na dostƒôp do DataFrame z kom√≥rek SQL w tym samym notebooku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f59fce2a-0196-4145-8f6c-24d1f5d70b13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- SQL: Agregacja na danych z Python\n",
    "\n",
    "SELECT \n",
    "  department,\n",
    "  COUNT(*) as employee_count,\n",
    "  AVG(salary) as avg_salary,\n",
    "  MAX(salary) as max_salary\n",
    "FROM employees_temp\n",
    "GROUP BY department\n",
    "ORDER BY avg_salary DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a9ac5da9-74e5-4025-babf-c3310d15b209",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Ten przyk≈Çad pokazuje si≈Çƒô notebook√≥w Databricks: przygotowanie danych w Python (wygodne API, biblioteki), nastƒôpnie analiza w SQL (deklaratywne zapytania, przejrzysto≈õƒá). Temp views sƒÖ widoczne w ca≈Çym notebooku niezale≈ºnie od jƒôzyka kom√≥rki."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef0c1cd-1d7c-4fad-910b-8087ab6fc581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 4.4: ZarzƒÖdzanie bibliotekami (%pip)\n",
    "\n",
    "W Databricks mo≈ºemy instalowaƒá biblioteki Python specyficzne dla danego notebooka (notebook-scoped libraries) u≈ºywajƒÖc komendy `%pip`. Jest to zalecane podej≈õcie zamiast instalacji globalnej na klastrze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b998dba-508e-4ee7-a9a5-96f8419efe33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Instalacja biblioteki emoji\n",
    "%pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "731f59a0-93e7-4245-b94e-504b7b0c5bfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "print(emoji.emojize('Databricks is :fire:'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6662cc7-46a9-4f65-915a-18079ed97f68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 4.5: Databricks Assistant (AI)\n",
    "\n",
    "W roku 2025 praca z kodem jest wspomagana przez AI. Databricks posiada wbudowanego asystenta (**Databricks Assistant**), kt√≥ry jest ≈õwiadomy kontekstu Twoich danych (zna schematy tabel w Unity Catalog!).\n",
    "\n",
    "**Jak korzystaƒá?**\n",
    "1. Skr√≥t **Cmd+I** (Mac) lub **Ctrl+I** (Windows) wewnƒÖtrz kom√≥rki.\n",
    "2. Panel boczny \"Assistant\".\n",
    "\n",
    "**Do czego s≈Çu≈ºy?**\n",
    "- **Generowanie kodu**: \"Napisz zapytanie SQL, kt√≥re policzy ≈õredniƒÖ sprzeda≈º po regionach z tabeli sales\".\n",
    "- **Wyja≈õnianie kodu**: Zaznacz skomplikowany fragment i zapytaj \"Explain this code\".\n",
    "- **Fixing errors**: Gdy kom√≥rka zwr√≥ci b≈ÇƒÖd, kliknij \"Diagnose Error\" ‚Äì asystent wyja≈õni przyczynƒô i zaproponuje poprawkƒô.\n",
    "- **Transformacja**: \"Przepisz ten kod z PySpark na SQL\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5d45e64-069d-4a60-b9b7-8d3fcb777f7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cffcf7a7-4b58-4f36-a746-71603cb32a1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 5: Unity Catalog vs Hive Metastore\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Databricks wspiera dwa systemy metadanych: legacy Hive Metastore oraz nowoczesny Unity Catalog. Unity Catalog jest zalecany dla wszystkich nowych projekt√≥w ze wzglƒôdu na zaawansowane funkcje governance i bezpiecze≈Ñstwa.\n",
    "\n",
    "**Kluczowe r√≥≈ºnice:**\n",
    "\n",
    "| Aspekt | Hive Metastore | Unity Catalog |\n",
    "|--------|----------------|---------------|\n",
    "| **Governance** | Ograniczone | Pe≈Çne: RBAC, masking, audit |\n",
    "| **Namespace** | 2-poziomowy (db.table) | 3-poziomowy (catalog.schema.table) |\n",
    "| **Cross-workspace** | Nie | Tak (shared metastore) |\n",
    "| **Lineage** | Brak | End-to-end lineage |\n",
    "| **Data Sharing** | Ograniczone | Delta Sharing protocol |\n",
    "| **Isolation** | Workspace-level | Catalog-level |\n",
    "\n",
    "**Dlaczego Unity Catalog?**\n",
    "- Centralne zarzƒÖdzanie dostƒôpem dla wszystkich workspace'√≥w\n",
    "- Automatyczny lineage dla audytu i compliance\n",
    "- Fine-grained permissions (column-level, row-level)\n",
    "- Integracja z zewnƒôtrznymi systemami (Delta Sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f41c629-f1dd-4331-a607-b16b4c7a8314",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 5.1: Namespace - Hive vs Unity Catalog\n",
    "\n",
    "**Cel:** Por√≥wnanie sk≈Çadni dostƒôpu do tabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ff6cc60-b5c6-4119-849f-76a3c175f19b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Hive Metastore (legacy)**\n",
    "\n",
    "- **Sk≈Çadnia**: `database.table`\n",
    "- **Przyk≈Çad**: `default.sales_data`\n",
    "- **Ograniczenia**: Brak fine-grained permissions, brak lineage, workspace isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da5a2ba-76c0-4230-b5ee-a5a47656beb0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Unity Catalog (nowoczesny)**\n",
    "\n",
    "- **Sk≈Çadnia**: `catalog.schema.table`\n",
    "- **Przyk≈Çad**: `prod.gold.sales_summary`\n",
    "\n",
    "**Zalety 3-poziomowego namespace:**\n",
    "- Oddzielenie ≈õrodowisk (dev/test/prod catalogs)\n",
    "- Lepsze uprawnienia (grant na poziomie catalogu)\n",
    "- Wsp√≥≈Çdzielenie metastore miƒôdzy workspace'ami\n",
    "- End-to-end lineage\n",
    "- Fine-grained access control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11188262-35c1-49e3-8b92-38362635bd2b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 5.2: Tworzenie tabeli w Unity Catalog\n",
    "\n",
    "**Cel:** Demonstracja pe≈Çnej sk≈Çadni z 3-poziomowym namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19c1afdc-9dba-48ea-a8fd-4f31ff2d05bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Utworzenie przyk≈Çadowej tabeli w Unity Catalog\n",
    "table_name = f\"{CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\"\n",
    "\n",
    "# Dane demonstracyjne\n",
    "demo_data = [\n",
    "    (1, \"Unity Catalog\", \"Enabled\", \"2024-01-15\"),\n",
    "    (2, \"Delta Lake\", \"Enabled\", \"2024-01-15\"),\n",
    "    (3, \"Photon Engine\", \"Enabled\", \"2024-01-15\"),\n",
    "    (4, \"Hive Metastore\", \"Legacy\", \"2024-01-15\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd8f2775-8274-4791-ab51-d147a1be2078",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definicja schematu\n",
    "demo_schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), False),\n",
    "    StructField(\"feature\", StringType(), False),\n",
    "    StructField(\"status\", StringType(), False),\n",
    "    StructField(\"date\", StringType(), False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a60aa549-2b4b-4714-944c-49cd6ab52d59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Utworzenie DataFrame\n",
    "demo_df = spark.createDataFrame(demo_data, demo_schema)\n",
    "display(demo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e40fd65e-3d9b-4ed0-a807-3aa070e50685",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zapis jako Delta Table w Unity Catalog\n",
    "demo_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1506a541-232b-4b70-b15e-17fe667a9944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weryfikacja\n",
    "display(spark.table(table_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17ffb9ec-997d-4305-affd-991831d89bfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Sukces!** Tabela zosta≈Ça utworzona w Unity Catalog. Mo≈ºemy teraz zweryfikowaƒá jej istnienie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "490272be-2a84-46e2-a66d-0d6d1d573ff8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### üéØ Zadanie: Sprawd≈∫ w UI\n",
    "\n",
    "1. Kliknij **Catalog** w lewym menu bocznym.\n",
    "2. Znajd≈∫ sw√≥j katalog (nazwa w zmiennej `CATALOG`, np. `ecommerce_platform_...`).\n",
    "3. Rozwi≈Ñ schemat `bronze` (lub inny zdefiniowany w `BRONZE_SCHEMA`).\n",
    "4. Kliknij na tabelƒô `lakehouse_demo`.\n",
    "5. Zobacz zak≈Çadki: **Sample Data** (podglƒÖd) oraz **Lineage** (pochodzenie danych)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ec8af90-b3b3-43db-a965-d0c39e0c44f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Tabela zosta≈Ça utworzona z pe≈Çnym 3-poziomowym namespace. W Unity Catalog ka≈ºda tabela automatycznie:\n",
    "- Jest zarzƒÖdzana przez system governance\n",
    "- Ma trackowany lineage\n",
    "- Posiada przypisane uprawnienia na podstawie katalogu i schematu\n",
    "- Jest dostƒôpna w Catalog Explorer dla eksploracji\n",
    "\n",
    "**Managed vs External Tables:**\n",
    "Powy≈ºsza tabela to **Managed Table**. Databricks zarzƒÖdza zar√≥wno metadanymi, jak i plikami danych (w domy≈õlnym storage katalogu/schematu). Usuniƒôcie tabeli (`DROP TABLE`) usuwa r√≥wnie≈º dane.\n",
    "\n",
    "**External Table** powstaje, gdy podamy `LOCATION 'path'`. Wtedy `DROP TABLE` usuwa tylko metadane, a pliki pozostajƒÖ w storage'u."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfeca6d4-c870-4788-aa01-7dfe3acc3780",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Bonus: Delta Time Travel (Teaser)\n",
    "\n",
    "Ka≈ºda operacja na tabeli Delta jest rejestrowana w transaction logu. Dziƒôki temu mo≈ºemy ≈õledziƒá historiƒô zmian i cofaƒá siƒô w czasie (Time Travel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f56ed7b3-67af-47e2-821a-abcabe4a484c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e16fc12-df73-414e-af57-a9002b9a34c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY ecommerce_platform_trainer.bronze.lakehouse_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c93db4f3-fee5-4c6f-bfd4-c8f7c5da2637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 5.3: Eksploracja metadanych Unity Catalog\n",
    "\n",
    "**Cel:** Wykorzystanie systemu informacyjnych schemat√≥w Unity Catalog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d670b941-54a6-4572-9e1d-96623e199e7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Unity Catalog udostƒôpnia system.information_schema dla metadanych\n",
    "\n",
    "-- Lista tabel w naszym schemacie\n",
    "SELECT \n",
    "  table_catalog,\n",
    "  table_schema,\n",
    "  table_name,\n",
    "  table_type\n",
    "FROM system.information_schema.tables\n",
    "WHERE table_catalog = 'ecommerce_platform_trainer'\n",
    "  AND table_schema = 'bronze'\n",
    "ORDER BY table_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c83299bc-2015-4d27-82f2-c527bd4a7fb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Unity Catalog automatycznie utrzymuje `system.information_schema` - zbi√≥r widok√≥w SQL z metadanymi o wszystkich obiektach. To standardowe podej≈õcie zgodne z ANSI SQL, co u≈Çatwia integracjƒô z narzƒôdziami BI i data governance.\n",
    "\n",
    "**System Tables:**\n",
    "Warto wiedzieƒá, ≈ºe Unity Catalog udostƒôpnia r√≥wnie≈º tabele systemowe (wymaga w≈ÇƒÖczenia przez admina):\n",
    "- `system.billing.usage`: szczeg√≥≈Çowe dane o kosztach (DBU)\n",
    "- `system.access.audit`: logi audytowe (kto, co, kiedy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dc408bc-fa10-46b1-aadf-e21efee43fd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Por√≥wnanie PySpark vs SQL\n",
    "\n",
    "**DataFrame API (PySpark):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a1a2858-88a3-4425-b9ae-bbcb8dc3fd7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Podej≈õcie PySpark - programatyczne DataFrame API\n",
    "\n",
    "df_pyspark = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.lakehouse_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf9b31f8-ef80-4b2c-9d88-4a7106d1f7c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "result_pyspark = df_pyspark \\\n",
    "    .filter(F.col(\"status\") == \"Enabled\") \\\n",
    "    .select(\"feature\", \"status\", \"date\") \\\n",
    "    .orderBy(\"feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd1e2252-4c73-4961-b13c-e88b905ec60f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(result_pyspark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26f26fb2-d01e-4ddd-9ba5-a4f125828150",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SQL Equivalent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "231818c1-6090-40c1-9837-2f3ae9d0c68f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(f\"{CATALOG}.{BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96b7c9b1-a1a1-4c15-b6df-8249cd755b2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from \n",
    "ecommerce_platform_trainer.bronze.lakehouse_demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "169a685d-24ac-4f56-b608-8a240f2cd18f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Parametryzacja z Databricks Widgets\n",
    "\n",
    "Poni≈ºej u≈ºywamy mechanizmu **Widgets**, kt√≥ry pozwala na tworzenie interaktywnych kontrolek w notebooku. Dziƒôki temu mo≈ºemy przekazywaƒá parametry (np. nazwy tabel, daty) do kodu SQL i Python, co u≈Çatwia budowanie uniwersalnych raport√≥w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3dff4b4-da92-40a8-9785-eb777db88424",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parametryzacja z Databricks Widgets\n",
    "# Ustawiamy warto≈õci domy≈õlne na podstawie zmiennych z 00_setup (je≈õli dostƒôpne)\n",
    "# Dziƒôki temu SQL cells bƒôdƒÖ korzystaƒá z tego samego katalogu co Python cells\n",
    "\n",
    "default_catalog = CATALOG if 'CATALOG' in locals() else \"ecommerce_platform_trainer\"\n",
    "default_schema = BRONZE_SCHEMA if 'BRONZE_SCHEMA' in locals() else \"bronze\"\n",
    "\n",
    "dbutils.widgets.text(\"CATALOG\", default_catalog)\n",
    "dbutils.widgets.text(\"BRONZE_SCHEMA\", default_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06523c1b-3a68-42d9-bd21-099e0ff46d49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "SELECT \n",
    "  feature,\n",
    "  status,\n",
    "  date\n",
    "FROM IDENTIFIER(:CATALOG || '.' || :BRONZE_SCHEMA || '.lakehouse_demo')\n",
    "WHERE status = 'Enabled'\n",
    "ORDER BY feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd223e2-d3f6-42ca-ac53-acbbbea0ebbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Por√≥wnanie:**\n",
    "- **Wydajno≈õƒá**: Identyczna - oba podej≈õcia kompilujƒÖ siƒô do tego samego Catalyst query plan\n",
    "- **Kiedy u≈ºywaƒá PySpark**: \n",
    "  - Z≈Ço≈ºona logika biznesowa z UDF\n",
    "  - Dynamiczne pipeline'y (parametryzacja, loops)\n",
    "  - Integracja z bibliotekami Python (pandas, scikit-learn)\n",
    "- **Kiedy u≈ºywaƒá SQL**: \n",
    "  - Proste transformacje i agregacje\n",
    "  - Zesp√≥≈Ç z silnymi kompetencjami SQL\n",
    "  - Migracja z tradycyjnych Data Warehouse\n",
    "  - Lepsze wsparcie dla analityk√≥w biznesowych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7237cb0f-ce86-4125-9a7e-4889656df750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist - Co powiniene≈õ zrozumieƒá po tym notebooku:\n",
    "- [x] Koncepcja Lakehouse i korzy≈õci wzglƒôdem tradycyjnej architektury\n",
    "- [x] Struktura Workspace: Sidebar, Compute, Catalog Explorer\n",
    "- [x] Hierarchia Unity Catalog: Metastore ‚Üí Catalog ‚Üí Schema ‚Üí Objects\n",
    "- [x] Typy klastr√≥w: All-Purpose vs Job, autoscaling, Photon\n",
    "- [x] Magic commands: %sql, %python, %fs, %run, %md\n",
    "- [x] R√≥≈ºnice miƒôdzy Hive Metastore (2-level) a Unity Catalog (3-level)\n",
    "- [x] Tworzenie tabel w Unity Catalog z pe≈Çnym namespace\n",
    "- [x] Dostƒôp do metadanych przez system.information_schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3f4a541-553c-430b-b6e8-3aabbca452ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Problem 1: \"Catalog not found\" lub \"Schema not found\"\n",
    "**Objawy:**\n",
    "- B≈ÇƒÖd: `AnalysisException: [SCHEMA_NOT_FOUND]`\n",
    "- B≈ÇƒÖd: `AnalysisException: [CATALOG_NOT_FOUND]`\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "```python\n",
    "# Sprawd≈∫ dostƒôpne katalogi\n",
    "spark.sql(\"SHOW CATALOGS\").show()\n",
    "\n",
    "# Sprawd≈∫ dostƒôpne schematy\n",
    "spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\").show()\n",
    "\n",
    "# Upewnij siƒô, ≈ºe uruchomi≈Çe≈õ %run ./00_setup\n",
    "```\n",
    "\n",
    "### Problem 2: \"Permission denied\" przy tworzeniu tabel\n",
    "**Objawy:**\n",
    "- B≈ÇƒÖd: `PERMISSION_DENIED: User does not have CREATE TABLE on Schema`\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "- Skontaktuj siƒô z administratorem workspace o nadanie uprawnie≈Ñ `CREATE TABLE`\n",
    "- Sprawd≈∫ uprawnienia: `SHOW GRANTS ON SCHEMA catalog.schema`\n",
    "\n",
    "### Problem 3: Klaster nie startuje lub jest zbyt wolny\n",
    "**Objawy:**\n",
    "- Klaster w stanie \"Pending\" przez d≈Çugi czas\n",
    "- Timeout przy starcie\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "- Sprawd≈∫ quota instancji w chmurze (Azure/AWS/GCP)\n",
    "- Zmniejsz liczbƒô worker√≥w lub wybierz mniejszy node type\n",
    "- Wy≈ÇƒÖcz autoscaling dla test√≥w\n",
    "\n",
    "### Problem 4: Magic command %sql nie dzia≈Ça\n",
    "**Objawy:**\n",
    "- B≈ÇƒÖd sk≈Çadni lub brak wynik√≥w\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "- Upewnij siƒô, ≈ºe `%sql` jest pierwszym elementem w kom√≥rce\n",
    "- Sprawd≈∫ czy u≈ºywasz zmiennych: `${CATALOG}` zamiast `{CATALOG}`\n",
    "- Dla zmiennych Python u≈ºyj: `spark.sql(f\"SELECT ... FROM {CATALOG}.{SCHEMA}.table\")`\n",
    "\n",
    "### Debugging tips:\n",
    "- U≈ºyj `explain()` na DataFrame aby zobaczyƒá plan wykonania\n",
    "- Sprawd≈∫ logi klastra w Spark UI (zak≈Çadka \"Cluster\" ‚Üí \"Spark UI\")\n",
    "- Weryfikuj typy danych: `df.printSchema()`\n",
    "- Dla problem√≥w z wydajno≈õciƒÖ sprawd≈∫ liczbƒô partycji: `df.rdd.getNumPartitions()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46e715d3-bead-4aad-bc6f-9e02f500a4f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "### Architektura Lakehouse:\n",
    "- U≈ºywaj Unity Catalog zamiast Hive Metastore dla nowych projekt√≥w\n",
    "- Organizuj dane w logiczne katalogi (np. dev/test/prod)\n",
    "- Stosuj naming convention dla schemat√≥w: bronze/silver/gold\n",
    "- Wykorzystuj Delta Lake jako domy≈õlny format tabel\n",
    "\n",
    "### ZarzƒÖdzanie Workspace:\n",
    "- Organizuj notebooki w folderach wed≈Çug projekt√≥w lub zespo≈Ç√≥w\n",
    "- U≈ºywaj Git Folders (Repos) dla integracji z Git i wersjonowania\n",
    "- Dokumentuj notebooki z Markdown cells\n",
    "- Stosuj `%run` dla wsp√≥≈Çdzielenia kodu miƒôdzy notebookami\n",
    "\n",
    "### Konfiguracja Klastr√≥w:\n",
    "- Development: ma≈Çe klastry (2-4 workers), bez spot instances\n",
    "- Production: autoscaling, spot instances dla oszczƒôdno≈õci\n",
    "- W≈ÇƒÖcz Photon Engine dla zapyta≈Ñ SQL/DataFrame\n",
    "- Ustaw auto-termination (np. 30 min nieaktywno≈õci) dla klastr√≥w All-Purpose\n",
    "- Dla Jobs u≈ºywaj Job Clusters (efemeryczne, optymalne koszty)\n",
    "\n",
    "### Governance i Security:\n",
    "- Stosuj per-user lub per-team izolacjƒô katalog√≥w\n",
    "- U≈ºywaj 3-poziomowego namespace: catalog.schema.table\n",
    "- Przydzielaj uprawnienia na poziomie schematu, nie tabeli\n",
    "- Monitoruj dostƒôp przez system.access.audit\n",
    "- W≈ÇƒÖcz lineage dla compliance i debugowania\n",
    "\n",
    "### Wydajno≈õƒá:\n",
    "- Preferuj Delta Lake zamiast Parquet/CSV dla czƒôstych zapyta≈Ñ\n",
    "- Partycjonuj du≈ºe tabele wed≈Çug kluczy czasowych lub geograficznych\n",
    "- U≈ºywaj Z-ORDER dla kolumn u≈ºywanych w WHERE clauses\n",
    "- Regularnie uruchamiaj OPTIMIZE i VACUUM na tabelach Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a16ec104-b474-4943-b590-30dd012a7aad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Podsumowanie\n",
    "\n",
    "### Co zosta≈Ço osiƒÖgniƒôte:\n",
    "- Poznanie koncepcji Lakehouse jako ewolucji Data Lake + Data Warehouse\n",
    "- Eksploracja element√≥w platformy Databricks: Workspace, Compute, Catalog\n",
    "- Zrozumienie hierarchii Unity Catalog: Metastore ‚Üí Catalog ‚Üí Schema ‚Üí Objects\n",
    "- Praktyka z magic commands: %sql, %python, %fs, %pip\n",
    "- Por√≥wnanie Hive Metastore vs Unity Catalog\n",
    "- Utworzenie pierwszej tabeli Delta w Unity Catalog z 3-poziomowym namespace\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "1. **Lakehouse eliminuje duplikacjƒô danych**: Jedna kopia s≈Çu≈ºy BI, ML i real-time analytics\n",
    "2. **Unity Catalog to fundament governance**: 3-poziomowy namespace, fine-grained permissions, automatyczny lineage\n",
    "3. **Klastry sƒÖ elastyczne**: Autoscaling i spot instances redukujƒÖ koszty, Photon przyspiesza zapytania\n",
    "4. **Notebooki sƒÖ potƒô≈ºne**: Mieszanie SQL/Python, magic commands, integracja z Git przez Git Folders\n",
    "5. **Delta Lake jest domy≈õlnym formatem**: ACID transactions, time travel, schema evolution\n",
    "\n",
    "### Quick Reference - Najwa≈ºniejsze komendy:\n",
    "\n",
    "| Operacja | PySpark | SQL |\n",
    "|----------|---------|-----|\n",
    "| Ustaw katalog | `spark.sql(f\"USE CATALOG {CATALOG}\")` | `USE CATALOG my_catalog` |\n",
    "| Lista katalog√≥w | `spark.sql(\"SHOW CATALOGS\")` | `SHOW CATALOGS` |\n",
    "| Lista schemat√≥w | `spark.sql(\"SHOW SCHEMAS\")` | `SHOW SCHEMAS` |\n",
    "| Utworzenie tabeli | `df.write.saveAsTable(\"cat.schema.table\")` | `CREATE TABLE cat.schema.table AS SELECT ...` |\n",
    "| Odczyt tabeli | `spark.table(\"cat.schema.table\")` | `SELECT * FROM cat.schema.table` |\n",
    "| Metadane | - | `SELECT * FROM system.information_schema.tables` |\n",
    "| Instalacja lib | `%pip install package` | - |\n",
    "\n",
    "### Nastƒôpne kroki:\n",
    "- **Kolejny notebook**: `02_data_import_exploration.ipynb` - Wczytywanie danych z r√≥≈ºnych format√≥w (CSV, JSON, Parquet, Delta)\n",
    "- **Warsztat praktyczny**: Po zako≈Ñczeniu dema (notebooki 01-05) przejdziemy do `01_workspace_data_exploration_workshop.ipynb`\n",
    "- **Materia≈Çy dodatkowe**: \n",
    "  - [Databricks Lakehouse Fundamentals](https://www.databricks.com/learn/lakehouse)\n",
    "  - [Unity Catalog Documentation](https://docs.databricks.com/data-governance/unity-catalog/index.html)\n",
    "  - [Delta Lake Guide](https://docs.delta.io/latest/index.html)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8151009896109070,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_databricks_lakehouse_intro",
   "widgets": {
    "BRONZE_SCHEMA": {
     "currentValue": "bronze",
     "nuid": "3eb823de-3328-4753-a2bf-9d863378b4f1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "bronze",
      "label": null,
      "name": "BRONZE_SCHEMA",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "bronze",
      "label": null,
      "name": "BRONZE_SCHEMA",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "CATALOG": {
     "currentValue": "ecommerce_platform_trainer",
     "nuid": "ab4cee0e-5693-4e7e-8e80-189b95567447",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "ecommerce_platform_trainer",
      "label": null,
      "name": "CATALOG",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "ecommerce_platform_trainer",
      "label": null,
      "name": "CATALOG",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
