{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c8c4ae6-ba62-4942-a5a7-a41db20064dc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Podstawowe transformacje SQL i PySpark - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Opanowanie podstawowych transformacji danych w PySpark DataFrame API i SQL, zrozumienie ekwiwalentności obu podejść, umiejętność wyboru odpowiedniego narzędzia do zadania\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Transformacje kolumnowe: select(), withColumn(), drop(), alias()\n",
    "- Logika warunkowa: when() / otherwise() oraz CASE WHEN\n",
    "- Operacje tekstowe: regexp_replace(), trim(), lower(), upper()\n",
    "- Filtry i sortowanie: filter(), where(), orderBy()\n",
    "- Agregacje: groupBy(), agg(), rollup(), cube()\n",
    "- SQL equivalents i porównanie wydajności"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "958d123b-4af6-45cd-88d3-b65522c60340",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 1 - Fundamentals & Exploration\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z 2-4 workers\n",
    "- **Czas trwania**: 60 minut\n",
    "- **Prerekvizity**: 02_data_import_exploration.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "634cc23e-f13b-43e4-9b89-2f8d8d1b1ed1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie fundamentów transformacji danych w Spark i związku między DataFrame API a SQL\n",
    "\n",
    "**Podstawowe pojęcia:**\n",
    "- **DataFrame API (PySpark)**: Programatyczne API do manipulacji danymi używające metod i funkcji Pythona\n",
    "- **Spark SQL**: Deklaratywny język zapytań SQL na DataFrames\n",
    "- **Catalyst Optimizer**: Silnik optymalizacji Spark, który kompiluje zarówno DataFrame API jak i SQL do tego samego execution plan\n",
    "- **Lazy Evaluation**: Transformacje nie są wykonywane natychmiast, tylko budują DAG (Directed Acyclic Graph) wykonywany przy akcji\n",
    "- **Transformations vs Actions**: Transformacje (select, filter, groupBy) są lazy, akcje (show, count, collect) triggerują wykonanie\n",
    "\n",
    "**Dlaczego to ważne?**\n",
    "Spark oferuje dwa równoważne sposoby transformacji danych: DataFrame API (PySpark) i SQL. Oba kompilują się do tego samego execution plan przez Catalyst Optimizer, więc wydajność jest identyczna. Wybór zależy od preferencji zespołu, złożoności logiki i integracji z innymi narzędziami. Znajomość obu podejść jest kluczowa dla Data Engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "374ee190-dc21-47c0-a039-ed08cf32a655",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Izolacja per użytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25438ef5-15df-4a88-95d5-796cb3e53196",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61067945-e87d-40f1-931c-57f3c983a78d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7df2f166-39d3-43e3-ae4b-16e43511e349",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Wyświetl kontekst użytkownika (zmienne z 00_setup)\n",
    "print(\"=== Kontekst użytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"Użytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog i schemat jako domyślne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "print(\"\\n=== Konfiguracja zakończona pomyślnie ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdd817a1-fceb-4321-9010-c33ac05a9c85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Przygotowanie danych\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Załadujemy dane z Unity Catalog Volume - bezpiecznego, zarządzanego storage dla plików. Użyjemy danych customers i orders do demonstracji transformacji.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **Volume**: Kontener dla plików w Unity Catalog z kontrolą dostępu\n",
    "- **createOrReplaceTempView**: Rejestracja DataFrame jako temporary view dla zapytań SQL\n",
    "- **Temp View**: Widok tymczasowy dostępny tylko w bieżącej sesji Spark\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Ładowanie danych źródłowych z managed storage\n",
    "- Przygotowanie danych do transformacji\n",
    "- Umożliwienie zapytań SQL na DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5317f6cd-5997-49da-9dbe-1771a4c41e82",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Wczytanie danych z Volume\n",
    "\n",
    "**Cel:** Załadowanie danych customers i orders z Unity Catalog Volume\n",
    "\n",
    "**Podejście:**\n",
    "1. Zdefiniowanie ścieżek do plików w Volume\n",
    "2. Wczytanie CSV i JSON\n",
    "3. Rejestracja jako temporary views dla SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d325fec0-ce20-4c7c-a67e-c8bfec2f3c8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: CSV i JSON files w volume\n",
    "# VARIABLE: df_customers, df_orders - DataFrames z danymi\n",
    "\n",
    "# Ścieżki do plików w Volume\n",
    "customers_path = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "orders_path = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "\n",
    "# Wczytanie customers (CSV)\n",
    "df_customers = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(customers_path)\n",
    "\n",
    "# Wczytanie orders (JSON)\n",
    "df_orders = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(orders_path)\n",
    "\n",
    "# Rejestracja jako temporary views dla SQL queries\n",
    "df_customers.createOrReplaceTempView(\"customers\")\n",
    "df_orders.createOrReplaceTempView(\"orders\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "987237ae-cf1d-40e1-bae9-0dd975e97339",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(\"orders\").limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b428b44a-7398-4e88-984b-8186c2576c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "100eabf7-353d-4f00-9bd3-68d9250f6ab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "select * from customers limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0646c29c-396f-4f1b-9093-8480d51ad6d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a878f2bb-0784-49ec-849b-c2a0455c57a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== Dane załadowane ===\")\n",
    "print(f\"Customers: {df_customers.count()} rekordów, {len(df_customers.columns)} kolumn\")\n",
    "print(f\"Orders: {df_orders.count()} rekordów, {len(df_orders.columns)} kolumn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1be96e9-a5a5-42d6-9d3f-fea427ad1a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Załadowaliśmy dane z Volume używając `spark.read` z opcjami specyficznymi dla formatów. `createOrReplaceTempView()` rejestruje DataFrame jako SQL view, co pozwala na zapytania SQL. Views są tymczasowe i dostępne tylko w bieżącej sesji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b12f35a-2082-4ead-8519-6319e5552914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 1: Transformacje kolumnowe\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Transformacje kolumnowe to najczęstsze operacje w data engineering: selekcja potrzebnych kolumn, dodawanie nowych kolumn obliczeniowych, zmiana nazw i usuwanie zbędnych kolumn. Spark oferuje bogate API do manipulacji kolumnami.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **select()**: Wybór i projekcja kolumn (PySpark)\n",
    "- **withColumn()**: Dodanie lub nadpisanie kolumny\n",
    "- **drop()**: Usunięcie kolumn\n",
    "- **alias()**: Zmiana nazwy kolumny lub wyrażenia\n",
    "- **col() / F.col()**: Referencja do kolumny w DataFrame\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Selekcja tylko potrzebnych kolumn (column pruning dla wydajności)\n",
    "- Kalkulacje biznesowe (np. total_price = quantity * unit_price)\n",
    "- Renaming dla czytelności\n",
    "- Czyszczenie niepotrzebnych kolumn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a92c1a86-15e3-4fb4-ab0b-52dc1e756c34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.1: Selekcja kolumn - select()\n",
    "\n",
    "**Cel:** Wybór konkretnych kolumn z DataFrame\n",
    "\n",
    "**Podejście:**\n",
    "1. Selekcja po nazwach kolumn\n",
    "2. Selekcja z aliasami\n",
    "3. Selekcja z obliczeniami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b2ebb4e-49ea-4551-9ddb-9ae219b664ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_selected - DataFrame z wybranymi kolumnami\n",
    "\n",
    "# PySpark: Selekcja kolumn\n",
    "df_selected = df_customers.select(\n",
    "    \"customer_id\",\n",
    "    \"first_name\",\n",
    "    \"last_name\",\n",
    "    \"email\",\n",
    "    \"city\",\n",
    "    \"country\"\n",
    ")\n",
    "\n",
    "print(\"=== PySpark: select() ===\")\n",
    "display(df_selected.limit(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e990f2-8379-45b2-8603-19ab3a458b8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Selekcja z aliasami\n",
    "df_selected_alias = df_customers.select(\n",
    "    F.col(\"customer_id\").alias(\"id\"),\n",
    "    F.concat_ws(' ', F.col(\"first_name\"), F.col(\"last_name\")).alias(\"full_name\"),\n",
    "    F.col(\"email\"),\n",
    "    F.concat_ws(\", \", F.col(\"city\"), F.col(\"country\")).alias(\"location\")\n",
    ")\n",
    "\n",
    "print(\"\\n=== PySpark: select() z aliasami i concat ===\")\n",
    "display(df_selected_alias.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57b21fc1-3651-4a4b-a6bc-29e4b1ded330",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.2: SQL Equivalent - SELECT\n",
    "\n",
    "**Cel:** Ta sama operacja w SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4982595-c6eb-42fc-85e2-d07a8d289adc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        email,\n",
    "        city,\n",
    "        country\n",
    "    FROM customers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "695100f2-e31b-422d-bd15-47689f055d6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    " SELECT \n",
    "        customer_id AS id,\n",
    "        first_name || ' ' || last_name AS full_name,\n",
    "        email,\n",
    "        CONCAT(city, ', ', country) AS location\n",
    "    FROM customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43cbd4ae-42e0-4a76-84df-e2ba3b3c9258",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.3: Dodawanie nowych kolumn - withColumn()\n",
    "\n",
    "**Cel:** Tworzenie nowych kolumn na podstawie istniejących"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21a6ca18-45ad-4841-8a25-78727178a399",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df_customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53488e7e-bd4f-42b4-9368-f46da3b67cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_with_new_cols - DataFrame z nowymi kolumnami\n",
    "\n",
    "# PySpark: withColumn() - dodawanie nowych kolumn\n",
    "df_with_new_cols = df_customers \\\n",
    "    .withColumn(\"full_name\", F.upper(F.concat_ws(' ', F.col(\"first_name\"), F.col(\"last_name\")))) \\\n",
    "    .withColumn(\"email_domain\", F.split(F.col(\"email\"), \"@\").getItem(1)) \\\n",
    "    .withColumn(\"registration_year\", F.year(F.col(\"registration_date\"))) \\\n",
    "    .withColumn(\"is_premium\", F.when(F.col(\"customer_segment\") == \"Premium\", True).otherwise(False))\n",
    "\n",
    "\n",
    "\n",
    "print(\"=== PySpark: withColumn() ===\")\n",
    "display(df_with_new_cols.select(\n",
    "    \"first_name\",\"last_name\", \"full_name\", \n",
    "    \"email\", \"email_domain\",\n",
    "    \"registration_date\", \"registration_year\",\n",
    "    \"customer_segment\", \"is_premium\"\n",
    ").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70a3bcb2-3494-4530-b356-ca8d538dcf7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.4: SQL Equivalent - kolumny w SELECT\n",
    "\n",
    "**Cel:** Tworzenie nowych kolumn w SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "341bcf07-7f9e-4ca5-9999-c8af32b3dbdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "%sql\n",
    "-- SQL: Nowe kolumny w SELECT\n",
    "    SELECT \n",
    "        first_name || ' ' || last_name AS full_name,\n",
    "        UPPER(first_name || ' ' || last_name) AS full_name_upper,\n",
    "        email,\n",
    "        -- GET pobiera element o podanym indeksie z tablicy zwróconej przez SPLIT (indeksowanie od 0)\n",
    "        get(SPLIT(email, '@'), 1) AS email_domain,\n",
    "        registration_date,\n",
    "        YEAR(registration_date) AS registration_year,\n",
    "        customer_segment,\n",
    "        customer_segment = 'Premium' AS is_premium\n",
    "    FROM customers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8135f662-03c5-489f-a3bc-f0719879fe9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Null-e w agregacjach – na co uważać?\n",
    "\n",
    "Przy agregacjach warto pamiętać, że:\n",
    "\n",
    "- `COUNT(*)` liczy wszystkie wiersze,\n",
    "- `COUNT(kolumna)` liczy tylko wiersze, gdzie `kolumna IS NOT NULL`,\n",
    "- `COUNT(DISTINCT kolumna)` liczy unikalne wartości **niepuste**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4f94676-e911-43e1-8c1f-aaccfa08ee6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.5: Usuwanie kolumn - drop()\n",
    "\n",
    "**Cel:** Usunięcie niepotrzebnych kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee9b81fa-97b4-47a7-98e2-956a465ca318",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_dropped - DataFrame z usuniętymi kolumnami\n",
    "\n",
    "# PySpark: drop() - usuwanie kolumn\n",
    "df_dropped = df_customers.drop(\"phone\", \"postal_code\", \"notes\")\n",
    "\n",
    "print(\"=== PySpark: drop() ===\")\n",
    "print(f\"Kolumny przed drop: {len(df_customers.columns)}\")\n",
    "print(f\"Kolumny po drop: {len(df_dropped.columns)}\")\n",
    "print(f\"Usunięte kolumny: phone, postal_code, notes\")\n",
    "display(df_dropped.limit(3))\n",
    "\n",
    "# SQL: Wybór wszystkich kolumn OPRÓCZ niektórych (trzeba wylistować ręcznie)\n",
    "# W SQL nie ma drop(), więc trzeba wybrać tylko potrzebne kolumny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8979c737-205a-43c2-91a3-d010935c9661",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 2: Logika warunkowa\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Logika warunkowa pozwala na tworzenie wartości kolumn bazując na warunkach. Jest fundamentem business logic w transformacjach danych - kategoryzacja, flagowanie, mapowanie wartości.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **when() / otherwise()**: Konstrukcja IF-THEN-ELSE w PySpark\n",
    "- **CASE WHEN**: Konstrukcja warunkowa w SQL\n",
    "- **Chaining conditions**: Łańcuchowe warunki (if-elif-else)\n",
    "- **Boolean expressions**: Wyrażenia logiczne jako warunki\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Kategoryzacja klientów (VIP, Standard, New)\n",
    "- Status mapping (active/inactive)\n",
    "- Business rules (discounts, flags)\n",
    "- Data quality flags (valid/invalid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4326617-bdad-4583-9a04-377f418f8460",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.1: Logika warunkowa - when() / otherwise()\n",
    "\n",
    "**Cel:** Kategoryzacja klientów na podstawie istniejących danych (customer_segment, registration_date)\n",
    "\n",
    "**Podejście:**\n",
    "1. Proste when-otherwise (IF-ELSE)\n",
    "2. Łańcuchowe when (IF-ELIF-ELSE)\n",
    "3. Multiple conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44f16248-e03e-4d8d-8e22-a355d13b85c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_categorized - DataFrame z kategoriami klientów\n",
    "\n",
    "# Dodanie sztucznej kolumny account_value na podstawie istniejących danych dla demonstracji\n",
    "df_customers_with_value = df_customers.withColumn(\n",
    "    \"account_value\", \n",
    "    F.when(F.col(\"customer_segment\") == \"Premium\", F.rand() * 10000 + 5000)\n",
    "     .when(F.col(\"customer_segment\") == \"Standard\", F.rand() * 5000 + 1000)\n",
    "     .otherwise(F.rand() * 1000 + 100)\n",
    ")\n",
    "\n",
    "# PySpark: when() / otherwise() - logika warunkowa\n",
    "df_categorized = df_customers_with_value.withColumn(\n",
    "    \"customer_tier\",\n",
    "    F.when(F.col(\"account_value\") >= 8000, \"Platinum\")\n",
    "     .when(F.col(\"account_value\") >= 3000, \"Gold\")\n",
    "     .when(F.col(\"account_value\") >= 1000, \"Silver\")\n",
    "     .otherwise(\"Bronze\")\n",
    ")\n",
    "\n",
    "# Dodanie flagi VIP (multiple conditions)\n",
    "df_categorized = df_categorized.withColumn(\n",
    "    \"is_vip\",\n",
    "    F.when(\n",
    "        (F.col(\"account_value\") > 5000) & \n",
    "        (F.col(\"customer_segment\") == \"Premium\"),\n",
    "        True\n",
    "    ).otherwise(False)\n",
    ")\n",
    "\n",
    "# Kategoryzacja na podstawie daty rejestracji\n",
    "df_categorized = df_categorized.withColumn(\n",
    "    \"registration_period\",\n",
    "    F.when(F.col(\"registration_date\") >= \"2025-01-01\", \"Recent\")\n",
    "     .when(F.col(\"registration_date\") >= \"2024-01-01\", \"This Year\")\n",
    "     .otherwise(\"Older\")\n",
    ")\n",
    "\n",
    "print(\"=== PySpark: when() / otherwise() ===\")\n",
    "display(df_categorized.select(\n",
    "    \"first_name\", \"last_name\",\n",
    "    \"customer_segment\", \"account_value\", \"customer_tier\",\n",
    "    \"is_vip\", \"registration_date\", \"registration_period\"\n",
    ").orderBy(F.desc(\"account_value\")).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f045d878-8ceb-49ca-96d8-2513fa64f0fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.2: SQL Equivalent - CASE WHEN\n",
    "\n",
    "**Cel:** Ta sama logika warunkowa w SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50226a5f-e125-418f-b012-2b6cc19d1603",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Najpierw utwórz temporary view z account_value\n",
    "df_customers_with_value.createOrReplaceTempView(\"customers_enriched\")\n",
    "\n",
    "# SQL: CASE WHEN - logika warunkowa\n",
    "df_categorized_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        first_name,\n",
    "        last_name,\n",
    "        customer_segment,\n",
    "        account_value,\n",
    "        CASE \n",
    "            WHEN account_value >= 8000 THEN 'Platinum'\n",
    "            WHEN account_value >= 3000 THEN 'Gold'\n",
    "            WHEN account_value >= 1000 THEN 'Silver'\n",
    "            ELSE 'Bronze'\n",
    "        END AS customer_tier,\n",
    "        CASE \n",
    "            WHEN account_value > 5000 AND customer_segment = 'Premium' THEN TRUE\n",
    "            ELSE FALSE\n",
    "        END AS is_vip,\n",
    "        registration_date,\n",
    "        CASE \n",
    "            WHEN registration_date >= '2025-01-01' THEN 'Recent'\n",
    "            WHEN registration_date >= '2024-01-01' THEN 'This Year'\n",
    "            ELSE 'Older'\n",
    "        END AS registration_period\n",
    "    FROM customers_enriched\n",
    "    ORDER BY account_value DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== SQL: CASE WHEN ===\")\n",
    "display(df_categorized_sql.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64ddd098-864e-4b09-9be0-7fa1e9f956ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 3: Operacje tekstowe\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Operacje tekstowe są kluczowe w data cleaning i standaryzacji. Spark oferuje bogate API do manipulacji stringami - od prostych (trim, upper) po złożone (regex, split, concat).\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **trim() / ltrim() / rtrim()**: Usuwanie whitespace\n",
    "- **upper() / lower() / initcap()**: Zmiana wielkości liter\n",
    "- **regexp_replace()**: Zastępowanie wzorców regex\n",
    "- **concat() / concat_ws()**: Łączenie stringów\n",
    "- **split() / substring()**: Dzielenie i wycinanie stringów\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Data cleaning (whitespace, case normalization)\n",
    "- Parsing (extracting parts of strings)\n",
    "- Formatting (phone numbers, codes)\n",
    "- Anonymization (masking sensitive data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f7945d6-206e-4d67-8603-72d386ad879d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.1: Operacje tekstowe - trim, upper, lower\n",
    "\n",
    "**Cel:** Czyszczenie i normalizacja pól tekstowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1504ed0d-df67-44ac-8fcb-6e6d4fb72bfb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_cleaned_text - DataFrame z oczyszczonymi polami tekstowymi\n",
    "\n",
    "# PySpark: Operacje tekstowe\n",
    "df_cleaned_text = df_customers \\\n",
    "    .withColumn(\"first_name_clean\", F.trim(F.col(\"first_name\"))) \\\n",
    "    .withColumn(\"last_name_clean\", F.trim(F.col(\"last_name\"))) \\\n",
    "    .withColumn(\"full_name_upper\", F.upper(F.concat_ws(' ', F.trim(F.col(\"first_name\")), F.trim(F.col(\"last_name\"))))) \\\n",
    "    .withColumn(\"full_name_title\", F.initcap(F.concat_ws(' ', F.trim(F.col(\"first_name\")), F.trim(F.col(\"last_name\"))))) \\\n",
    "    .withColumn(\"email_lower\", F.lower(F.trim(F.col(\"email\")))) \\\n",
    "    .withColumn(\"country_code\", F.upper(F.col(\"country\")))\n",
    "\n",
    "print(\"=== PySpark: Operacje tekstowe ===\")\n",
    "display(df_cleaned_text.select(\n",
    "    \"first_name\", \"last_name\", \"first_name_clean\", \"last_name_clean\", \n",
    "    \"full_name_upper\", \"full_name_title\",\n",
    "    \"email\", \"email_lower\",\n",
    "    \"country\", \"country_code\"\n",
    ").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dfd2a42e-bb9b-474b-b36c-5d52104c6274",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.2: SQL Equivalent - funkcje tekstowe\n",
    "\n",
    "**Cel:** Te same operacje w SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "527d729b-2aad-4d1b-a1bb-7b814c54f864",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL: Funkcje tekstowe\n",
    "df_cleaned_text_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        first_name,\n",
    "        last_name,\n",
    "        TRIM(first_name) AS first_name_clean,\n",
    "        TRIM(last_name) AS last_name_clean,\n",
    "        UPPER(TRIM(first_name) || ' ' || TRIM(last_name)) AS full_name_upper,\n",
    "        INITCAP(TRIM(first_name) || ' ' || TRIM(last_name)) AS full_name_title,\n",
    "        email,\n",
    "        LOWER(TRIM(email)) AS email_lower,\n",
    "        country,\n",
    "        UPPER(country) AS country_code\n",
    "    FROM customers\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== SQL: Funkcje tekstowe ===\")\n",
    "display(df_cleaned_text_sql.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf646c33-49ad-4467-a323-523331d3116d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.3: Regex i zaawansowane operacje\n",
    "\n",
    "**Cel:** Używanie regex do czyszczenia i parsowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79526a3a-da34-487b-9fb0-be95c1408f9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_regex - DataFrame z operacjami regex\n",
    "\n",
    "# PySpark: regexp_replace, split, substring\n",
    "df_regex = df_customers \\\n",
    "    .withColumn(\"phone_digits_only\", \n",
    "        F.regexp_replace(F.col(\"phone\"), \"[^0-9]\", \"\")\n",
    "    ) \\\n",
    "    .withColumn(\"email_username\",\n",
    "        F.split(F.col(\"email\"), \"@\").getItem(0)\n",
    "    ) \\\n",
    "    .withColumn(\"email_domain\",\n",
    "        F.split(F.col(\"email\"), \"@\").getItem(1)\n",
    "    ) \\\n",
    "    .withColumn(\"customer_initials\",\n",
    "        F.concat(\n",
    "            F.substring(F.col(\"first_name\"), 1, 1),\n",
    "            F.lit(\".\"),\n",
    "            F.substring(F.col(\"last_name\"), 1, 1),\n",
    "            F.lit(\".\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "print(\"=== PySpark: Regex i parsing ===\")\n",
    "display(df_regex.select(\n",
    "    \"phone\", \"phone_digits_only\",\n",
    "    \"email\", \"email_username\", \"email_domain\",\n",
    "    \"first_name\", \"last_name\", \"customer_initials\"\n",
    ").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a58f65-fe98-492f-ab21-c6d906d51e3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.4: SQL Equivalent - regex i parsing\n",
    "\n",
    "**Cel:** Te same operacje regex w SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fcc2a0a-f682-439c-88a5-4c436e9fb60f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL: Regex i funkcje tekstowe\n",
    "df_regex_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        phone,\n",
    "        REGEXP_REPLACE(phone, '[^0-9]', '') AS phone_digits_only,\n",
    "        email,\n",
    "        SPLIT(email, '@')[0] AS email_username,\n",
    "        SPLIT(email, '@')[1] AS email_domain,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        CONCAT(\n",
    "            SUBSTRING(first_name, 1, 1), '.', \n",
    "            SUBSTRING(last_name, 1, 1), '.'\n",
    "        ) AS customer_initials\n",
    "    FROM customers\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== SQL: Regex i parsing ===\")\n",
    "display(df_regex_sql.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b429af1-2cad-486f-a851-5e4de93bc01e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 4: Filtry i sortowanie\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Filtry redukują zbiór danych do interesujących nas rekordów, a sortowanie organizuje wyniki. Są to fundamentalne operacje analityczne, często łączone z agregacjami.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **filter() / where()**: Filtrowanie rekordów (synonimy)\n",
    "- **orderBy() / sort()**: Sortowanie (synonimy)\n",
    "- **Boolean conditions**: Warunki logiczne (AND, OR, NOT)\n",
    "- **Multiple conditions**: Łączenie wielu warunków\n",
    "- **asc() / desc()**: Kierunek sortowania\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Filtrowanie do interesujących rekordów (active customers, recent orders)\n",
    "-排序 dla analiz (top customers, oldest records)\n",
    "- Pre-aggregation filtering (WHERE przed GROUP BY)\n",
    "- Post-aggregation filtering (HAVING po GROUP BY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70a15337-047c-4834-a4e5-35269903504f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.1: Filtry - filter() / where()\n",
    "\n",
    "**Cel:** Filtrowanie rekordów na podstawie warunków\n",
    "\n",
    "**Podejście:**\n",
    "1. Proste filtry\n",
    "2. Multiple conditions (AND, OR)\n",
    "3. Filtry na wartościach null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0790130-dcbf-4d4d-8fcd-80d427cd648d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_filtered - DataFrame z przefiltrowanymi rekordami\n",
    "\n",
    "# PySpark: filter() / where() - identyczne\n",
    "df_filtered_premium = df_customers.filter(F.col(\"customer_segment\") == \"Premium\")\n",
    "\n",
    "print(\"=== PySpark: filter() - Premium customers ===\")\n",
    "print(f\"Total customers: {df_customers.count()}\")\n",
    "print(f\"Premium customers: {df_filtered_premium.count()}\")\n",
    "display(df_filtered_premium.limit(5))\n",
    "\n",
    "# Multiple conditions z AND (&)\n",
    "df_filtered_multi = df_customers.filter(\n",
    "    (F.col(\"customer_segment\") == \"Premium\") & \n",
    "    (F.col(\"country\") == \"USA\") &\n",
    "    (F.col(\"state\").isNotNull())\n",
    ")\n",
    "\n",
    "print(\"\\n=== PySpark: Multiple conditions (AND) ===\")\n",
    "print(f\"Premium USA customers with state info: {df_filtered_multi.count()}\")\n",
    "display(df_filtered_multi.limit(5))\n",
    "\n",
    "# OR conditions (|)\n",
    "df_filtered_or = df_customers.filter(\n",
    "    (F.col(\"country\") == \"USA\") | \n",
    "    (F.col(\"country\") == \"Poland\")\n",
    ")\n",
    "\n",
    "print(\"\\n=== PySpark: OR conditions ===\")\n",
    "print(f\"Customers from USA or Poland: {df_filtered_or.count()}\")\n",
    "\n",
    "# NOT condition (~) i null check\n",
    "df_filtered_not_null = df_customers.filter(\n",
    "    F.col(\"email\").isNotNull() &\n",
    "    ~F.col(\"email\").contains(\"example.com\")\n",
    ")\n",
    "\n",
    "print(\"\\n=== PySpark: NOT i null check ===\")\n",
    "print(f\"Customers with valid email (not null, not example.com): {df_filtered_not_null.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c6ae207-0ceb-4cbe-a2ed-8979fca57b00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.2: SQL Equivalent - WHERE\n",
    "\n",
    "**Cel:** Te same filtry w SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64a79a5f-ea94-44ad-b1a7-b3ace8830f0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL: WHERE clause\n",
    "df_filtered_premium_sql = spark.sql(\"\"\"\n",
    "    SELECT * FROM customers\n",
    "    WHERE customer_segment = 'Premium'\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== SQL: WHERE - Premium customers ===\")\n",
    "print(f\"Premium customers: {df_filtered_premium_sql.count()}\")\n",
    "display(df_filtered_premium_sql.limit(5))\n",
    "\n",
    "# Multiple conditions AND\n",
    "df_filtered_multi_sql = spark.sql(\"\"\"\n",
    "    SELECT * FROM customers\n",
    "    WHERE customer_segment = 'Premium'\n",
    "        AND country = 'USA'\n",
    "        AND state IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== SQL: Multiple conditions (AND) ===\")\n",
    "print(f\"Count: {df_filtered_multi_sql.count()}\")\n",
    "\n",
    "# OR conditions\n",
    "df_filtered_or_sql = spark.sql(\"\"\"\n",
    "    SELECT * FROM customers\n",
    "    WHERE country = 'USA' \n",
    "       OR country = 'Poland'\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== SQL: OR conditions ===\")\n",
    "print(f\"Count: {df_filtered_or_sql.count()}\")\n",
    "\n",
    "# NOT i null check\n",
    "df_filtered_not_null_sql = spark.sql(\"\"\"\n",
    "    SELECT * FROM customers\n",
    "    WHERE email IS NOT NULL\n",
    "        AND email NOT LIKE '%example.com%'\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== SQL: NOT i null check ===\")\n",
    "print(f\"Count: {df_filtered_not_null_sql.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9678aa19-36ba-4a65-a05c-4f54c3d7982e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.3: Sortowanie - orderBy() / sort()\n",
    "\n",
    "**Cel:** Sortowanie wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21df07d1-f0b4-4fbd-93dc-f7ed4142ed10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_sorted - DataFrame posortowany\n",
    "\n",
    "# PySpark: orderBy() / sort() - identyczne\n",
    "df_sorted_desc = df_customers.orderBy(F.desc(\"registration_date\"))\n",
    "\n",
    "print(\"=== PySpark: orderBy() DESC (registration_date) ===\")\n",
    "display(df_sorted_desc.select(\"first_name\", \"last_name\", \"registration_date\", \"country\").limit(10))\n",
    "\n",
    "# Multiple columns sort\n",
    "df_sorted_multi = df_customers.orderBy(\n",
    "    F.col(\"country\").asc(),\n",
    "    F.col(\"customer_segment\").desc(),\n",
    "    F.col(\"registration_date\").desc()\n",
    ")\n",
    "\n",
    "print(\"\\n=== PySpark: Multiple columns sort (country ASC, segment DESC, date DESC) ===\")\n",
    "display(df_sorted_multi.select(\"first_name\", \"last_name\", \"country\", \"customer_segment\", \"registration_date\").limit(10))\n",
    "\n",
    "# Null handling w sortowaniu\n",
    "df_sorted_nulls = df_customers.orderBy(\n",
    "    F.col(\"email\").asc_nulls_last()\n",
    ")\n",
    "\n",
    "print(\"\\n=== PySpark: Sort with nulls handling (nulls last) ===\")\n",
    "display(df_sorted_nulls.select(\"first_name\", \"last_name\", \"email\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb601d0-d8d0-4c8d-a071-e73f0371d9ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.4: SQL Equivalent - ORDER BY\n",
    "\n",
    "**Cel:** Te same sortowania w SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c975153f-685e-4cc1-9283-53f65c6aae56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL: ORDER BY\n",
    "df_sorted_desc_sql = spark.sql(\"\"\"\n",
    "    SELECT first_name, last_name, registration_date, country\n",
    "    FROM customers\n",
    "    ORDER BY registration_date DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== SQL: ORDER BY DESC ===\")\n",
    "display(df_sorted_desc_sql.limit(10))\n",
    "\n",
    "# Multiple columns\n",
    "df_sorted_multi_sql = spark.sql(\"\"\"\n",
    "    SELECT first_name, last_name, country, customer_segment, registration_date\n",
    "    FROM customers\n",
    "    ORDER BY country ASC, customer_segment DESC, registration_date DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== SQL: Multiple columns ORDER BY ===\")\n",
    "display(df_sorted_multi_sql.limit(10))\n",
    "\n",
    "# Nulls handling\n",
    "df_sorted_nulls_sql = spark.sql(\"\"\"\n",
    "    SELECT first_name, last_name, email\n",
    "    FROM customers\n",
    "    ORDER BY email ASC NULLS LAST\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== SQL: ORDER BY with NULLS LAST ===\")\n",
    "display(df_sorted_nulls_sql.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7f56789-c847-4423-81b6-9583fc50bd0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 5: Agregacje\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Agregacje redukują wiele rekordów do wartości podsumowujących. Są fundamentem analityki biznesowej - KPIs, metryki, raporty. Spark oferuje standardowe agregacje (sum, avg, count) oraz zaawansowane (rollup, cube).\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **groupBy()**: Grupowanie po kolumnach\n",
    "- **agg()**: Agregacje (sum, avg, count, min, max)\n",
    "- **rollup()**: Hierarchiczne podsumowania (subtotals)\n",
    "- **cube()**: Wszystkie kombinacje wymiarów (cross-tabulation)\n",
    "- **HAVING**: Filtrowanie wyników agregacji\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- KPIs per country, per customer tier\n",
    "- Sales reports (total revenue, average order value)\n",
    "- Cohort analysis (customers per age group)\n",
    "- Multi-dimensional reporting (rollup, cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12a2f3cd-912b-49d2-8f6b-157f74de4e3b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 5.1: Podstawowe agregacje - groupBy() + agg()\n",
    "\n",
    "**Cel:** Obliczenie statystyk per grupa\n",
    "\n",
    "**Podejście:**\n",
    "1. Proste groupBy + count\n",
    "2. Multiple aggregations\n",
    "3. Aliasy dla czytelności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82ff4d75-6546-4715-a04d-634a66c6abad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_agg - DataFrame z agregacjami\n",
    "\n",
    "# Dodajmy kolumnę account_value na podstawie customer_segment dla demonstracji agregacji\n",
    "df_customers_for_agg = df_customers.withColumn(\n",
    "    \"account_value\",\n",
    "    F.when(F.col(\"customer_segment\") == \"Premium\", F.rand() * 5000 + 5000)\n",
    "     .when(F.col(\"customer_segment\") == \"Standard\", F.rand() * 3000 + 2000)\n",
    "     .otherwise(F.rand() * 1000 + 500)\n",
    ")\n",
    "\n",
    "# PySpark: groupBy() + agg()\n",
    "df_agg_country = df_customers_for_agg.groupBy(\"country\").agg(\n",
    "    F.count(\"*\").alias(\"customer_count\"),\n",
    "    F.sum(\"account_value\").alias(\"total_account_value\"),\n",
    "    F.avg(\"account_value\").alias(\"avg_account_value\"),\n",
    "    F.max(\"account_value\").alias(\"max_account_value\"),\n",
    "    F.count(F.when(F.col(\"customer_segment\") == \"Premium\", 1)).alias(\"premium_count\")\n",
    ").orderBy(F.desc(\"customer_count\"))\n",
    "\n",
    "print(\"=== PySpark: groupBy() + agg() per country ===\")\n",
    "display(df_agg_country)\n",
    "\n",
    "# Multiple dimensions grouping\n",
    "df_agg_multi = df_customers_for_agg \\\n",
    "    .withColumn(\"registration_year\", F.year(F.col(\"registration_date\"))) \\\n",
    "    .groupBy(\"country\", \"customer_segment\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"count\"),\n",
    "        F.avg(\"account_value\").alias(\"avg_account_value\")\n",
    "    ) \\\n",
    "    .orderBy(\"country\", \"customer_segment\")\n",
    "\n",
    "print(\"\\n=== PySpark: groupBy() multiple dimensions ===\")\n",
    "display(df_agg_multi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e71d76-f418-40f0-86f9-ffa64c8a879a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 5.2: SQL Equivalent - GROUP BY\n",
    "\n",
    "**Cel:** Te same agregacje w SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58bf0ff9-7966-4e22-944c-068ad51b481e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_customers_for_agg.createOrReplaceTempView(\"customers_with_value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a79f50ec-ad0d-47aa-8959-1e794bc32f99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL: GROUP BY\n",
    "df_agg_country_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        country,\n",
    "        COUNT(*) AS customer_count,\n",
    "        SUM(account_value) AS total_account_value,\n",
    "        AVG(account_value) AS avg_account_value,\n",
    "        MAX(account_value) AS max_account_value,\n",
    "        COUNT(CASE WHEN customer_segment = 'Premium' THEN 1 END) AS premium_count\n",
    "    FROM customers_with_value\n",
    "    GROUP BY country\n",
    "    ORDER BY customer_count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== SQL: GROUP BY per country ===\")\n",
    "display(df_agg_country_sql)\n",
    "\n",
    "# Multiple dimensions\n",
    "df_agg_multi_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        country,\n",
    "        customer_segment,\n",
    "        COUNT(*) AS count,\n",
    "        AVG(account_value) AS avg_account_value\n",
    "    FROM customers_with_value\n",
    "    GROUP BY country, customer_segment\n",
    "    ORDER BY country, customer_segment\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== SQL: GROUP BY multiple dimensions ===\")\n",
    "display(df_agg_multi_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee85556-f1d7-4c6c-bf31-741467859c40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 5.3: HAVING - filtrowanie wyników agregacji\n",
    "\n",
    "**Cel:** Filtrowanie grup po agregacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "921b7909-721e-443d-8e34-1f7a445c2577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers_for_agg\n",
    "# VARIABLE: df_having - DataFrame z filtrem po agregacji\n",
    "\n",
    "# PySpark: HAVING equivalent - filter po agg\n",
    "df_having = df_customers_for_agg.groupBy(\"country\").agg(\n",
    "    F.count(\"*\").alias(\"customer_count\"),\n",
    "    F.avg(\"account_value\").alias(\"avg_account_value\")\n",
    ").filter(F.col(\"customer_count\") >= 50).orderBy(F.desc(\"avg_account_value\"))\n",
    "\n",
    "print(\"=== PySpark: HAVING equivalent (countries with >= 50 customers) ===\")\n",
    "display(df_having)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e33f7e8-7c77-442a-8bfc-78c8acb6e0b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 5.4: SQL Equivalent - HAVING\n",
    "\n",
    "**Cel:** HAVING clause w SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22614933-4fee-4167-a381-0f0572fafa28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL: HAVING\n",
    "df_having_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        country,\n",
    "        COUNT(*) AS customer_count,\n",
    "        AVG(account_value) AS avg_account_value\n",
    "    FROM customers_with_value\n",
    "    GROUP BY country\n",
    "    HAVING COUNT(*) >= 50\n",
    "    ORDER BY avg_account_value DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== SQL: HAVING clause ===\")\n",
    "display(df_having_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4b3fe46f-0b4e-42f0-90af-29f5482e5f6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 5.5: Zaawansowane agregacje - rollup() i cube()\n",
    "\n",
    "**Cel:** Hierarchiczne podsumowania i cross-tabulation\n",
    "\n",
    "**Wyjaśnienie:**\n",
    "- **rollup(A, B)**: Tworzy subtotals hierarchicznie: (A,B), (A), ()\n",
    "- **cube(A, B)**: Tworzy wszystkie kombinacje: (A,B), (A), (B), ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "82bd48e7-a969-4ff6-81f2-932f6ee5879e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers_for_agg\n",
    "# VARIABLE: df_rollup, df_cube - DataFrames z zaawansowanymi agregacjami\n",
    "\n",
    "# PySpark: rollup() - hierarchiczne subtotals\n",
    "df_rollup = df_customers_for_agg \\\n",
    "    .withColumn(\"registration_year\", F.year(F.col(\"registration_date\"))) \\\n",
    "    .rollup(\"country\", \"customer_segment\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"count\"),\n",
    "        F.sum(\"account_value\").alias(\"total_account_value\")\n",
    "    ) \\\n",
    "    .orderBy(F.col(\"country\").asc_nulls_last(), F.col(\"customer_segment\").asc_nulls_last())\n",
    "\n",
    "print(\"=== PySpark: rollup() - hierarchiczne subtotals ===\")\n",
    "print(\"Zawiera: (country, customer_segment), (country), (grand_total)\")\n",
    "display(df_rollup)\n",
    "\n",
    "# PySpark: cube() - wszystkie kombinacje wymiarów\n",
    "df_cube = df_customers_for_agg \\\n",
    "    .withColumn(\"registration_year\", F.year(F.col(\"registration_date\"))) \\\n",
    "    .cube(\"country\", \"customer_segment\") \\\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"count\"),\n",
    "        F.sum(\"account_value\").alias(\"total_account_value\")\n",
    "    ) \\\n",
    "    .orderBy(F.col(\"country\").asc_nulls_last(), F.col(\"customer_segment\").asc_nulls_last())\n",
    "\n",
    "print(\"\\n=== PySpark: cube() - wszystkie kombinacje ===\")\n",
    "print(\"Zawiera: (country, customer_segment), (country), (customer_segment), (grand_total)\")\n",
    "display(df_cube)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ea9c516-3280-415b-8a8e-846e637c6891",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 5.6: SQL Equivalent - ROLLUP i CUBE\n",
    "\n",
    "**Cel:** Te same operacje w SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e6873d2-e451-4912-a00c-b2ce57876199",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SQL: ROLLUP\n",
    "df_rollup_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        country,\n",
    "        customer_segment,\n",
    "        COUNT(*) AS count,\n",
    "        SUM(account_value) AS total_account_value\n",
    "    FROM customers_with_value\n",
    "    GROUP BY ROLLUP(country, customer_segment)\n",
    "    ORDER BY country ASC NULLS LAST, customer_segment ASC NULLS LAST\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== SQL: ROLLUP ===\")\n",
    "display(df_rollup_sql)\n",
    "\n",
    "# SQL: CUBE\n",
    "df_cube_sql = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        country,\n",
    "        customer_segment,\n",
    "        COUNT(*) AS count,\n",
    "        SUM(account_value) AS total_account_value\n",
    "    FROM customers_with_value\n",
    "    GROUP BY CUBE(country, customer_segment)\n",
    "    ORDER BY country ASC NULLS LAST, customer_segment ASC NULLS LAST\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== SQL: CUBE ===\")\n",
    "display(df_cube_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67a68af1-f6a9-41f0-b43f-13191796290b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Porównanie PySpark vs SQL - Podsumowanie\n",
    "\n",
    "**DataFrame API (PySpark):**\n",
    "\n",
    "**Zalety:**\n",
    "- Programatyczne API - łatwa integracja z Python code\n",
    "- Type safety (PyCharm autocomplete, type hints)\n",
    "- Łatwe łańcuchowanie transformacji (method chaining)\n",
    "- Lepsza dla dynamicznych pipeline'ów (parametryzacja, loops)\n",
    "- Integracja z Python libraries (pandas, numpy)\n",
    "\n",
    "**Wady:**\n",
    "- Bardziej verbose dla prostych zapytań\n",
    "- Wymaga znajomości PySpark API\n",
    "- Trudniejsze dla bardzo złożonych joinów\n",
    "\n",
    "**Kiedy używać PySpark:**\n",
    "- Złożone pipeline'y ETL z logiką biznesową\n",
    "- Integracja z ML (MLlib, scikit-learn)\n",
    "- Dynamiczne transformacje (parametryzowane, warunkowe)\n",
    "- Gdy zespół preferuje programatyczny styl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13af1a8b-78c4-4a13-9ebd-3c47a3b4faa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Porównanie PySpark vs SQL - Kompleksowe przykłady\n",
    "\n",
    "**DataFrame API (PySpark):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36316929-6d67-4f52-87fe-e30763a3f635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład: Kompleksowe czyszczenie danych w PySpark\n",
    "\n",
    "df_cleaned_pyspark = df_customers \\\n",
    "    .fillna({\"city\": \"Unknown\", \"country\": \"Unknown\"}) \\\n",
    "    .dropna(subset=[\"customer_id\"]) \\\n",
    "    .withColumn(\"full_name\", F.trim(F.initcap(F.concat_ws(' ', F.col(\"first_name\"), F.col(\"last_name\"))))) \\\n",
    "    .withColumn(\"email\", F.lower(F.trim(F.col(\"email\")))) \\\n",
    "    .dropDuplicates([\"customer_id\"]) \\\n",
    "    .withColumn(\"registration_year\", F.year(F.col(\"registration_date\"))) \\\n",
    "    .filter(F.col(\"registration_year\") >= 2020)\n",
    "\n",
    "print(f\"Rekordy po czyszczeniu (PySpark): {df_cleaned_pyspark.count()}\")\n",
    "display(df_cleaned_pyspark.select(\"customer_id\", \"full_name\", \"email\", \"customer_segment\", \"registration_year\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3ecefb2-08ef-491b-a542-ef8e8e1d340e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SQL Equivalent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06e3c8c5-82e1-45ca-96ec-72fdc2e14523",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Najpierw utwórz temporary view\n",
    "df_customers.createOrReplaceTempView(\"customers_raw\")\n",
    "\n",
    "# SQL approach\n",
    "df_cleaned_sql = spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "    customer_id,\n",
    "    TRIM(INITCAP(first_name || ' ' || last_name)) as full_name,\n",
    "    LOWER(TRIM(email)) as email,\n",
    "    customer_segment,\n",
    "    YEAR(registration_date) as registration_year,\n",
    "    COALESCE(city, 'Unknown') as city,\n",
    "    COALESCE(country, 'Unknown') as country,\n",
    "    phone,\n",
    "    registration_date\n",
    "FROM customers_raw\n",
    "WHERE customer_id IS NOT NULL\n",
    "    AND YEAR(registration_date) >= 2020\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Rekordy po czyszczeniu (SQL): {df_cleaned_sql.count()}\")\n",
    "display(df_cleaned_sql.select(\"customer_id\", \"full_name\", \"email\", \"customer_segment\", \"registration_year\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07adef23-cd5a-4da9-802c-14955aa24c06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SQL Equivalent:**\n",
    "\n",
    "**Zalety:**\n",
    "- Deklaratywny - zwięzły i czytelny\n",
    "- Standardowy język - znany analitykom biznesowym\n",
    "- Świetny dla ad-hoc analiz\n",
    "- Czytelniejszy dla złożonych joinów i subqueries\n",
    "- Łatwy do debugowania (explain plan)\n",
    "\n",
    "**Wady:**\n",
    "- Mniej elastyczny dla dynamicznych pipeline'ów\n",
    "- Trudniejsza parametryzacja\n",
    "- Słabsza integracja z Python ecosystem\n",
    "- String interpolation może być ryzykowna (SQL injection risk)\n",
    "\n",
    "**Kiedy używać SQL:**\n",
    "- Ad-hoc analityka i eksploracja\n",
    "- Raportowanie i dashboardy\n",
    "- Gdy zespół preferuje SQL\n",
    "- Proste do średnio złożone transformacje\n",
    "- Widoki i materialized views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4af740a7-5886-4578-961d-f24efd3c36de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Porównanie:**\n",
    "- **Wydajność**: Identyczna - Catalyst optimizer kompiluje oba do tego samego execution plan\n",
    "- **Best practice**: Używaj tego, co jest bardziej czytelne dla konkretnego przypadku\n",
    "- **Hybrydowe podejście**: Kombinuj oba (DataFrame API dla pipeline, SQL dla final aggregations)\n",
    "- **Zespół**: Wybór zależy od umiejętności zespołu (Data Engineers → PySpark, Analysts → SQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f561500-7c25-41d4-aa24-b34acb2c7697",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist - Co powinieneś uzyskać:\n",
    "- [ ] Dane załadowane z Volume (customers, orders)\n",
    "- [ ] Temporary views utworzone dla SQL queries\n",
    "- [ ] Transformacje kolumnowe działają (select, withColumn, drop, alias)\n",
    "- [ ] Logika warunkowa działa (when/otherwise, CASE WHEN)\n",
    "- [ ] Operacje tekstowe działają (trim, upper, lower, regex)\n",
    "- [ ] Filtry i sortowanie działają (filter, orderBy)\n",
    "- [ ] Agregacje działają (groupBy, agg, rollup, cube)\n",
    "- [ ] SQL equivalents dają identyczne wyniki co PySpark\n",
    "\n",
    "### Komendy weryfikacyjne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef190240-dd2c-4bd9-9cb8-f8dd83d45838",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weryfikacja wyników\n",
    "\n",
    "print(\"=== WERYFIKACJA WYNIKÓW ===\\n\")\n",
    "\n",
    "# 1. Sprawdź czy temporary views istnieją\n",
    "print(\"1. Temporary views:\")\n",
    "temp_views = spark.catalog.listTables()\n",
    "for view in temp_views:\n",
    "    if view.name in [\"customers\", \"orders\"]:\n",
    "        print(f\"   ✓ {view.name} - {view.tableType}\")\n",
    "\n",
    "# 2. Porównaj wyniki PySpark vs SQL (should be identical)\n",
    "pyspark_count = df_customers.filter(F.col(\"customer_segment\") == \"Premium\").count()\n",
    "sql_count = spark.sql(\"SELECT COUNT(*) FROM customers WHERE customer_segment = 'Premium'\").collect()[0][0]\n",
    "\n",
    "print(f\"\\n2. PySpark vs SQL consistency:\")\n",
    "print(f\"   PySpark filter count: {pyspark_count}\")\n",
    "print(f\"   SQL WHERE count: {sql_count}\")\n",
    "print(f\"   Match: {'✓' if pyspark_count == sql_count else '✗'}\")\n",
    "\n",
    "# 3. Sprawdź agregacje\n",
    "agg_result = df_customers.agg(\n",
    "    F.count(\"*\").alias(\"total\"),\n",
    "    F.countDistinct(\"customer_segment\").alias(\"segments\"),\n",
    "    F.countDistinct(\"country\").alias(\"countries\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\n3. Agregacje:\")\n",
    "print(f\"   Total customers: {agg_result['total']}\")\n",
    "print(f\"   Unique segments: {agg_result['segments']}\")\n",
    "print(f\"   Unique countries: {agg_result['countries']}\")\n",
    "\n",
    "# 4. Sprawdź transformation chains\n",
    "chained_result = df_customers \\\n",
    "    .filter(F.col(\"country\") == \"USA\") \\\n",
    "    .withColumn(\"tier\", \n",
    "        F.when(F.col(\"customer_segment\") == \"Premium\", \"VIP\")\n",
    "         .otherwise(\"Standard\")\n",
    "    ) \\\n",
    "    .groupBy(\"tier\") \\\n",
    "    .agg(F.count(\"*\").alias(\"count\")) \\\n",
    "    .orderBy(\"tier\")\n",
    "\n",
    "print(f\"\\n4. Transformation chain (USA customers by tier):\")\n",
    "chained_result.show()\n",
    "\n",
    "# Testy asercyjne\n",
    "try:\n",
    "    assert pyspark_count == sql_count, \"PySpark i SQL dają różne wyniki!\"\n",
    "    assert agg_result['total'] > 0, \"Brak danych w DataFrame!\"\n",
    "    print(\"\\n✓ Wszystkie testy przeszły pomyślnie!\")\n",
    "except AssertionError as e:\n",
    "    print(f\"\\n✗ Test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "614ceb4a-7b7b-4b9d-8b63-36945b1348a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Problem 1: AnalysisException - kolumna nie istnieje\n",
    "**Objawy:**\n",
    "- `AnalysisException: cannot resolve 'column_name'`\n",
    "- Błąd przy select() lub withColumn()\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Sprawdź dostępne kolumny\n",
    "print(df.columns)\n",
    "\n",
    "# Sprawdź schemat\n",
    "df.printSchema()\n",
    "\n",
    "# Case sensitivity - Spark SQL jest case-insensitive, ale DataFrame API może być\n",
    "df.select(F.col(\"`Column Name`\"))  # Użyj backticks dla nazw z spacjami\n",
    "```\n",
    "\n",
    "### Problem 2: TypeError przy multiple conditions\n",
    "**Objawy:**\n",
    "- `TypeError: unsupported operand type(s) for &`\n",
    "- Błąd przy łączeniu warunków\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# BŁĄD: Brak nawiasów wokół warunków\n",
    "df.filter(F.col(\"age\") > 18 & F.col(\"country\") == \"Poland\")\n",
    "\n",
    "# POPRAWNIE: Nawiasy wokół każdego warunku\n",
    "df.filter((F.col(\"age\") > 18) & (F.col(\"country\") == \"Poland\"))\n",
    "\n",
    "# Alternatywnie: Użyj metod\n",
    "df.filter(F.col(\"age\") > 18).filter(F.col(\"country\") == \"Poland\")\n",
    "```\n",
    "\n",
    "### Problem 3: Aggregation without groupBy\n",
    "**Objawy:**\n",
    "- Błąd przy próbie agregacji bez grupowania\n",
    "- Unexpected results\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Agregacja całego DataFrame (bez groupBy)\n",
    "df.agg(F.sum(\"amount\"), F.avg(\"age\"))\n",
    "\n",
    "# Agregacja per grupa (z groupBy)\n",
    "df.groupBy(\"country\").agg(F.sum(\"amount\"), F.avg(\"age\"))\n",
    "\n",
    "# BŁĄD: agg() bez groupBy gdy chcesz per grupa\n",
    "# df.agg(F.sum(\"amount\")).groupBy(\"country\")  # Źle!\n",
    "```\n",
    "\n",
    "### Problem 4: SQL view not found\n",
    "**Objawy:**\n",
    "- `Table or view not found: view_name`\n",
    "- SQL query fails\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Sprawdź dostępne views\n",
    "spark.catalog.listTables()\n",
    "\n",
    "# Utwórz view jeśli nie istnieje\n",
    "df.createOrReplaceTempView(\"view_name\")\n",
    "\n",
    "# Global temp view (dostępny w całej aplikacji)\n",
    "df.createOrReplaceGlobalTempView(\"view_name\")\n",
    "# Użycie: spark.sql(\"SELECT * FROM global_temp.view_name\")\n",
    "```\n",
    "\n",
    "### Debugging tips:\n",
    "- Użyj `.explain()` aby zobaczyć execution plan i zidentyfikować bottlenecks\n",
    "- `.show(truncate=False)` dla pełnych wartości kolumn\n",
    "- `.printSchema()` dla sprawdzenia typów danych\n",
    "- `df.columns` dla listy kolumn\n",
    "- `.count()` jest action - triggeruje execution (użyj ostrożnie w loops)\n",
    "- Cache DataFrame jeśli używasz wielokrotnie: `df.cache()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0531eeec-ee7a-4c89-a2c0-b47d88d95c04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "### Wydajność:\n",
    "- Używaj column pruning (select tylko potrzebne kolumny) aby zmniejszyć shuffle data\n",
    "- Filter pushdown - filtruj jak najwcześniej w pipeline\n",
    "- Predicate pushdown - Spark automatycznie przesuwa filtry do źródła danych\n",
    "- Cache DataFrames używane wielokrotnie: `df.cache()` lub `df.persist()`\n",
    "- Unikaj `collect()` na dużych DataFrames (OOM risk)\n",
    "- Używaj `broadcast()` dla małych DataFrames w joinach\n",
    "\n",
    "### Jakość kodu:\n",
    "- Nazywaj kolumny czytelnie używając `.alias()`\n",
    "- Komentuj złożoną logikę biznesową\n",
    "- Łańcuchuj transformacje czytelnie (jedna transformacja per linię)\n",
    "- Używaj zmiennych dla czytelności zamiast głębokich nested expressions\n",
    "- Konsystencja: Wybierz PySpark LUB SQL dla całego pipeline'u (nie mieszaj bez potrzeby)\n",
    "- Type hints w Python dla DataFrame-returning functions\n",
    "\n",
    "### Transformacje:\n",
    "- Preferuj `filter()` przed `groupBy()` (pre-aggregation filtering)\n",
    "- `when().otherwise()` jest lazy evaluated - bezpieczne dla null values\n",
    "- `withColumn()` nadpisuje kolumnę jeśli już istnieje (użyj świadomie)\n",
    "- Unikaj UDFs (User Defined Functions) - wolniejsze niż built-in functions\n",
    "- Używaj built-in functions (F.*) zamiast własnych - zoptymalizowane przez Catalyst\n",
    "\n",
    "### SQL vs PySpark:\n",
    "- SQL dla ad-hoc analiz i eksploracji\n",
    "- PySpark dla production pipelines i ETL\n",
    "- Dokumentuj wybór (dlaczego SQL vs PySpark w danym przypadku)\n",
    "- Testuj oba podejścia dla złożonych zapytań (readability vs maintainability)\n",
    "\n",
    "### Governance:\n",
    "- Temporary views są session-scoped - nie używaj dla shared data\n",
    "- Persistent views (CREATE VIEW) dla reusable logic\n",
    "- Dokumentuj business logic w komentarzach i docstrings\n",
    "- Version control dla notebooks (Git integration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ffaa30ea-9ae1-4019-a462-09c7b2eddf6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Podsumowanie\n",
    "\n",
    "### Co zostało osiągnięte:\n",
    "- Załadowanie danych z Unity Catalog Volume (customers.csv, orders.json)\n",
    "- Transformacje kolumnowe (select, withColumn, drop, alias)\n",
    "- Logika warunkowa (when/otherwise, CASE WHEN)\n",
    "- Operacje tekstowe (trim, upper, lower, regex, split)\n",
    "- Filtry i sortowanie (filter, where, orderBy, multiple conditions)\n",
    "- Agregacje (groupBy, agg, rollup, cube)\n",
    "- Porównanie PySpark DataFrame API vs SQL (identyczna wydajność)\n",
    "- Zrozumienie Catalyst Optimizer i lazy evaluation\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "1. **PySpark i SQL są równoważne**: Catalyst Optimizer kompiluje oba do tego samego execution plan\n",
    "2. **Lazy evaluation**: Transformacje budują DAG, execution następuje przy action\n",
    "3. **Column pruning i filter pushdown**: Spark automatycznie optymalizuje queries\n",
    "4. **Wybór narzędzia**: Zależy od use case, zespołu i czytelności (nie od wydajności)\n",
    "\n",
    "### Quick Reference - Najważniejsze komendy:\n",
    "\n",
    "| Operacja | PySpark | SQL |\n",
    "|----------|---------|-----|\n",
    "| Selekcja kolumn | `df.select(\"col1\", \"col2\")` | `SELECT col1, col2 FROM table` |\n",
    "| Nowa kolumna | `df.withColumn(\"new\", expr)` | `SELECT *, expr AS new FROM table` |\n",
    "| Filtr | `df.filter(F.col(\"age\") > 18)` | `SELECT * FROM table WHERE age > 18` |\n",
    "| Sortowanie | `df.orderBy(F.desc(\"amount\"))` | `SELECT * FROM table ORDER BY amount DESC` |\n",
    "| Agregacja | `df.groupBy(\"country\").agg(F.sum(\"sales\"))` | `SELECT country, SUM(sales) FROM table GROUP BY country` |\n",
    "| Logika warunkowa | `F.when(cond, val).otherwise(default)` | `CASE WHEN cond THEN val ELSE default END` |\n",
    "| Operacje tekstowe | `F.upper(F.trim(F.col(\"name\")))` | `UPPER(TRIM(name))` |\n",
    "| Join | `df1.join(df2, \"key\")` | `SELECT * FROM t1 JOIN t2 ON t1.key = t2.key` |\n",
    "\n",
    "### Następne kroki:\n",
    "- **Kolejny notebook**: 04_data_cleaning_quality.ipynb - Data quality i czyszczenie danych\n",
    "- **Warsztat praktyczny**: 02_transformations_cleaning_workshop.ipynb\n",
    "- **Materiały dodatkowe**: \n",
    "  - [Spark SQL Guide](https://spark.apache.org/docs/latest/sql-programming-guide.html)\n",
    "  - [PySpark API Reference](https://spark.apache.org/docs/latest/api/python/)\n",
    "- **Zadanie domowe**: Wykonaj top 10 customers per country używając zarówno PySpark jak i SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1ad69e2-7eae-44f4-82b2-efdca36163bb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Czyszczenie zasobów\n",
    "\n",
    "Posprzątaj zasoby utworzone podczas notebooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca6fa7b6-a9b1-48ce-962c-75a6e303d530",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Opcjonalne czyszczenie zasobów testowych\n",
    "# UWAGA: Uruchom tylko jeśli chcesz usunąć wszystkie utworzone dane\n",
    "\n",
    "# Usuń temporary views\n",
    "spark.catalog.dropTempView(\"customers\")\n",
    "spark.catalog.dropTempView(\"orders\")\n",
    "\n",
    "# Wyczyść cache\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "print(\"Temporary views i cache zostały wyczyszczone\")\n",
    "print(\"Dane źródłowe w Volume pozostają nienaruszone\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7433496500350564,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "03_basic_transformations_sql_pyspark",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
