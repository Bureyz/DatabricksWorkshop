{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d762e60b-4016-493d-a790-17dce06e2417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Widoki w Databricks\n",
    "\n",
    "**Cel szkoleniowy:** Nauczenie się różnic między widokami a tabelami, zarządzania obiektami w Unity Catalog oraz tworzenia prostych pipeline'ów notebookowych w Databricks Jobs.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Różnice między VIEW, TABLE i DELTA TABLE\n",
    "- Typy widoków: temp views, global temp views, persistent views\n",
    "- Rejestracja obiektów w Unity Catalog\n",
    "- Przeglądanie metadanych w Catalog Explorer\n",
    "- Proste pipeline'y notebookowe\n",
    "- Wprowadzenie do Databricks Jobs: taski, retry, harmonogramy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eff41f20-1d23-4061-8a4d-4a998f20aa17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 1 - Fundamentals & Exploration\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f454dd8f-321d-4e7b-8069-e337bcb2db41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "### Czym są widoki i tabele?\n",
    "\n",
    "W Databricks istnieją trzy główne typy obiektów przechowujących dane:\n",
    "\n",
    "**1. TABLE (Tabela)**\n",
    "- Fizyczny obiekt przechowujący dane na dysku\n",
    "- Dane są zapisane w określonym formacie (Parquet, Delta, CSV, etc.)\n",
    "- Wymaga miejsca na dysku\n",
    "- Trwała (persistent) - przetrwa restart klastra\n",
    "\n",
    "**2. DELTA TABLE (Tabela Delta)**\n",
    "- Specjalny typ tabeli używający formatu Delta Lake\n",
    "- Obsługuje ACID transactions\n",
    "- Umożliwia Time Travel, MERGE, UPDATE, DELETE\n",
    "- Zalecany format dla data lakehouse\n",
    "\n",
    "**3. VIEW (Widok)**\n",
    "- Wirtualny obiekt - nie przechowuje danych fizycznie\n",
    "- Zapisuje tylko definicję zapytania SQL\n",
    "- Wykonywany dynamicznie przy każdym odwołaniu\n",
    "- Przydatny do enkapsulacji logiki biznesowej\n",
    "\n",
    "### Typy widoków w Spark/Databricks:\n",
    "\n",
    "| Typ widoku | Zasięg | Trwałość | Namespace |\n",
    "|------------|--------|----------|----------|\n",
    "| **Temp View** | Sesja Spark (notebook) | Do końca sesji | Lokalny w sesji |\n",
    "| **Global Temp View** | Wszystkie sesje w klastrze | Do restartu klastra | `global_temp` database |\n",
    "| **Persistent View** | Wszystkie użytkownicy | Trwały w Unity Catalog | Catalog.Schema |\n",
    "\n",
    "### Kiedy używać którego?\n",
    "\n",
    "- **Temp View**: Tymczasowe przetwarzanie w jednym notebooku\n",
    "- **Global Temp View**: Współdzielenie danych między notebookami w tym samym klastrze\n",
    "- **Persistent View**: Udostępnianie logiki biznesowej między zespołami\n",
    "- **Delta Table**: Główny format do przechowywania danych w lakehouse\n",
    "\n",
    "### Unity Catalog - hierarchia obiektów:\n",
    "\n",
    "```\n",
    "Metastore\n",
    "  └── Catalog (np. kion_training)\n",
    "      └── Schema/Database (np. bronze, silver, gold)\n",
    "          ├── Tables\n",
    "          ├── Views\n",
    "          ├── Functions\n",
    "          └── Volumes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1945624-1ca2-428f-94e1-97478685ee0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inicjalizacja środowiska\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3054e0f5-1250-4684-9bd8-e70d3492635d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a36ad357-f48f-41a7-b6b4-ed46c0f7f178",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "092e1d6e-9ca3-439e-851d-cd68293f8145",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "# Ścieżki do danych\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "# Wyświetl kontekst użytkownika (zmienne z 00_setup)\n",
    "print(\"=== Kontekst użytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"Użytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog i schemat jako domyślne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "print(f\"\\n Domyślny katalog: {CATALOG}\")\n",
    "print(f\" Domyślny schemat: {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "201aada9-9e3d-41e4-848b-59e84f647bd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Część 1: Przygotowanie danych testowych\n",
    "\n",
    "### 1.1. Wczytanie danych do DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1350902-7665-40bc-b8fe-eef190a26750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wczytaj dane klientów z CSV\n",
    "customers_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "print(f\" Wczytano {customers_df.count()} klientów\")\n",
    "customers_df.display(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d303b872-1206-4413-9524-08ba7295242e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wczytaj dane zamówień z JSON\n",
    "orders_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "print(f\" Wczytano {orders_df.count()} zamówień\")\n",
    "orders_df.display(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "309bb4cc-a989-4cde-bdda-f1a32ad1ae9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Część 2: Temporary Views (Widoki tymczasowe)\n",
    "\n",
    "### 2.1. Tworzenie Temp View\n",
    "\n",
    "**Temp View** istnieje tylko w bieżącej sesji Spark (notebooku)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e2fd39c7-cc4b-4b84-9ff6-aa2aeb7398a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Utwórz temporary view z DataFrame\n",
    "customers_df.createOrReplaceTempView(\"customers_temp_view\")\n",
    "\n",
    "print(\" Utworzono temp view: customers_temp_view\")\n",
    "\n",
    "# Sprawdź, czy widok istnieje\n",
    "print(\"\\n Lista temporary views:\")\n",
    "spark.sql(\"SHOW VIEWS\").filter(\"isTemporary = true\").display(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e44badad-7c24-46f1-89e5-103010e4d44d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2. Zapytanie do Temp View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "820b1330-fa69-4aab-a5d5-e759ed454e7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zapytanie SQL do temp view\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT country, COUNT(*) as customer_count\n",
    "    FROM customers_temp_view\n",
    "    GROUP BY country\n",
    "    ORDER BY customer_count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\" Liczba klientów według krajów:\")\n",
    "result_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "160695f6-310f-476d-b138-922f1e59ffd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.3. Usunięcie Temp View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7d8ae6a-97f4-46ec-9bcf-7dd8a25ec1a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Usuń temporary view\n",
    "spark.catalog.dropTempView(\"customers_temp_view\")\n",
    "\n",
    "print(\" Usunięto temp view: customers_temp_view\")\n",
    "\n",
    "# Weryfikacja\n",
    "try:\n",
    "    spark.sql(\"SELECT * FROM customers_temp_view LIMIT 1\")\n",
    "except Exception as e:\n",
    "    print(f\"[EXPECTED ERROR] Widok nie istnieje: {str(e)[:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cc62a97-5dd2-45d4-b3e2-d43924ffa148",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Część 3: Global Temporary Views\n",
    "\n",
    "### 3.1. Tworzenie Global Temp View\n",
    "\n",
    "**Global Temp View** jest dostępny dla wszystkich sesji w klastrze, ale tylko do restartu klastra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15e94282-13d4-4d15-99cd-8191a6338e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Utwórz global temporary view\n",
    "orders_df.createOrReplaceGlobalTempView(\"orders_global_temp_view\")\n",
    "\n",
    "print(\" Utworzono global temp view: orders_global_temp_view\")\n",
    "print(\" Dostęp przez namespace: global_temp.orders_global_temp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5805c9f3-4c55-44e1-b7d0-928c7ce666d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.2. Zapytanie do Global Temp View\n",
    "\n",
    "**Uwaga:** Musisz użyć prefiksu `global_temp`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5e3c0e6-8dd3-47d6-b2a4-d5624c3893e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zapytanie SQL do global temp view (z prefiksem global_temp)\n",
    "result_df = spark.sql(\"\"\"\n",
    "    SELECT store_id, COUNT(*) as order_count, SUM(total_amount) as total_revenue\n",
    "    FROM global_temp.orders_global_temp_view\n",
    "    GROUP BY store_id\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\" Statystyki zamówień według store_id:\")\n",
    "result_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1d3551c-9013-491c-8744-4c4539306aec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.3. Usunięcie Global Temp View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2270c6c-3081-45f1-8d41-4339003cf914",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Usuń global temporary view\n",
    "spark.catalog.dropGlobalTempView(\"orders_global_temp_view\")\n",
    "\n",
    "print(\" Usunięto global temp view: orders_global_temp_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6b8a1b0-e99b-410d-b630-6466ce3604c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Część 4: Delta Tables (Tabele trwałe)\n",
    "\n",
    "### 4.1. Tworzenie Delta Table w Unity Catalog\n",
    "\n",
    "**Delta Table** to fizyczna tabela zapisana w formacie Delta Lake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c50a94d6-91cb-4566-b51c-320aecb502f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Nazwa tabeli w Unity Catalog\n",
    "CUSTOMERS_TABLE = f\"{CATALOG}.{BRONZE_SCHEMA}.customers\"\n",
    "\n",
    "# Zapisz DataFrame jako Delta Table\n",
    "customers_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(CUSTOMERS_TABLE)\n",
    "\n",
    "print(f\" Utworzono Delta Table: {CUSTOMERS_TABLE}\")\n",
    "\n",
    "# Sprawdź liczbę rekordów\n",
    "count = spark.table(CUSTOMERS_TABLE).count()\n",
    "print(f\" Liczba rekordów w tabeli: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84e90dd2-711b-4768-a150-7c740a17c03a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.2. Metadane Delta Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3225a980-9973-440f-b443-41043140c7c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdź szczegóły tabeli\n",
    "print(\" Szczegóły Delta Table:\")\n",
    "spark.sql(f\"DESCRIBE DETAIL {CUSTOMERS_TABLE}\").display(truncate=False)\n",
    "\n",
    "# Sprawdź schemat tabeli\n",
    "print(\"\\n Schemat tabeli:\")\n",
    "spark.sql(f\"DESCRIBE {CUSTOMERS_TABLE}\").display(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "156607b2-aa10-4ec5-9744-c86be87c37a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4.3. Tworzenie drugiej tabeli (Orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f211033e-e332-40f9-8512-3fe877e3c062",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Nazwa tabeli zamówień\n",
    "ORDERS_TABLE = f\"{CATALOG}.{BRONZE_SCHEMA}.orders\"\n",
    "\n",
    "# Zapisz jako Delta Table\n",
    "orders_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(ORDERS_TABLE)\n",
    "\n",
    "print(f\" Utworzono Delta Table: {ORDERS_TABLE}\")\n",
    "print(f\" Liczba rekordów: {spark.table(ORDERS_TABLE).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a99a6e9d-f114-4830-b61c-574c530c863f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Część 5: Persistent Views (Widoki trwałe)\n",
    "\n",
    "### 5.1. Tworzenie Persistent View w Unity Catalog\n",
    "\n",
    "**Persistent View** jest zapisywany w Unity Catalog i dostępny dla wszystkich użytkowników."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1df65897-49df-4c52-a5de-89bb5a16b672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Utwórz persistent view z logiką biznesową\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {SILVER_SCHEMA}\")\n",
    "VIEW_NAME = f\"customer_order_summary\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW `{VIEW_NAME}` AS\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        concat_ws(' ', c.first_name, c.last_name) as customer_name,\n",
    "        c.country,\n",
    "        COUNT(o.order_id) as total_orders,\n",
    "        COALESCE(SUM(o.total_amount), 0) as total_spent,\n",
    "        COALESCE(AVG(o.total_amount), 0) as avg_order_value\n",
    "    FROM {CUSTOMERS_TABLE} c\n",
    "    LEFT JOIN {ORDERS_TABLE} o ON c.customer_id = o.customer_id\n",
    "    GROUP BY c.customer_id, c.first_name, c.last_name, c.country\n",
    "\"\"\")\n",
    "\n",
    "print(f\" Utworzono persistent view: {VIEW_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d602f5b-2e2a-4ef6-b42d-54a5462855f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.2. Zapytanie do Persistent View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af1717eb-ce15-4a89-9679-30e1af4d0a67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zapytanie do widoku - top 10 klientów według wydatków\n",
    "result_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_name,\n",
    "        country,\n",
    "        total_orders,\n",
    "        ROUND(total_spent, 2) as total_spent,\n",
    "        ROUND(avg_order_value, 2) as avg_order_value\n",
    "    FROM {VIEW_NAME}\n",
    "    WHERE total_orders > 0\n",
    "    ORDER BY total_spent DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\" Top 10 klientów według wydatków:\")\n",
    "result_df.display(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6884dd9-986a-4f52-8da6-c51dd4fd19d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 5.3. Metadane Persistent View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef0396af-1b6f-4d18-b4ae-a987d99152b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdź definicję widoku\n",
    "print(\" Definicja widoku:\")\n",
    "spark.sql(f\"SHOW CREATE TABLE {VIEW_NAME}\").display(truncate=False)\n",
    "\n",
    "# Lista wszystkich widoków w schemacie\n",
    "print(f\"\\n Widoki w schemacie {SILVER_SCHEMA}:\")\n",
    "spark.sql(f\"SHOW VIEWS IN {CATALOG}.{SILVER_SCHEMA}\").display(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "045a1b30-a4b5-47a5-a5f1-214b39675257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Część 6: Przeglądanie obiektów w Unity Catalog\n",
    "\n",
    "### 6.1. Lista katalogów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8ce6fa9-29ec-4314-8ad8-e765180d6645",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wyświetl wszystkie dostępne katalogi\n",
    "print(\" Dostępne katalogi:\")\n",
    "spark.sql(\"SHOW CATALOGS\").display(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f03df828-0798-482f-a7f7-17c032dd7f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6.2. Lista schematów w katalogu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27dc13e0-b481-439f-a47e-95af122a44ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wyświetl schematy w bieżącym katalogu\n",
    "print(f\" Schematy w katalogu {CATALOG}:\")\n",
    "spark.sql(f\"SHOW SCHEMAS IN {CATALOG}\").display(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6149dd28-50b5-4b59-9438-60d70c87cc63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6.3. Lista tabel w schemacie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b00b2deb-b665-4b0a-b2fb-f7d410a8a2e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wyświetl tabele w schemacie bronze\n",
    "print(f\" Tabele w schemacie {BRONZE_SCHEMA}:\")\n",
    "spark.sql(f\"SHOW TABLES IN {CATALOG}.{BRONZE_SCHEMA}\").display(truncate=False)\n",
    "\n",
    "# Wyświetl widoki w schemacie silver\n",
    "print(f\"\\n Widoki w schemacie {SILVER_SCHEMA}:\")\n",
    "spark.sql(f\"SHOW VIEWS IN {CATALOG}.{SILVER_SCHEMA}\").display(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1bdc2d4-cb4d-4f8a-adbd-da5b3b52dc8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6.4. Szczegółowe informacje o tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "375885b0-2120-402d-a08d-8f4be6cc8865",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdź właściciela, lokalizację, format tabeli\n",
    "print(f\" Szczegóły tabeli {CUSTOMERS_TABLE}:\")\n",
    "spark.sql(f\"DESCRIBE EXTENDED {CUSTOMERS_TABLE}\").display(100, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8817b7e-1392-431d-9d3a-c8d9484f4aad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Część 7: Porównanie VIEW vs TABLE\n",
    "\n",
    "### 7.1. Test wydajności: View vs Materialized Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c2b5816-f43a-456b-ae30-c511198d1121",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test 1: Zapytanie do VIEW (obliczane on-the-fly)\n",
    "start_view = time.time()\n",
    "result_view = spark.sql(f\"SELECT COUNT(*) as cnt FROM {VIEW_NAME}\").collect()\n",
    "time_view = time.time() - start_view\n",
    "\n",
    "print(f\"[BENCHMARK] Czas wykonania VIEW: {time_view:.3f} sekund\")\n",
    "print(f\"[BENCHMARK] Wynik: {result_view[0]['cnt']} rekordów\")\n",
    "\n",
    "# Test 2: Materializacja wyniku jako tabela\n",
    "MATERIALIZED_TABLE = f\"{CATALOG}.{SILVER_SCHEMA}.customer_order_summary_materialized\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TABLE {MATERIALIZED_TABLE} AS\n",
    "    SELECT * FROM {VIEW_NAME}\n",
    "\"\"\")\n",
    "\n",
    "# Test wydajności tabeli zmaterializowanej\n",
    "start_table = time.time()\n",
    "result_table = spark.sql(f\"SELECT COUNT(*) as cnt FROM {MATERIALIZED_TABLE}\").collect()\n",
    "time_table = time.time() - start_table\n",
    "\n",
    "print(f\"\\n[BENCHMARK] Czas wykonania MATERIALIZED TABLE: {time_table:.3f} sekund\")\n",
    "print(f\"[BENCHMARK] Wynik: {result_table[0]['cnt']} rekordów\")\n",
    "\n",
    "# Porównanie\n",
    "if time_view > time_table:\n",
    "    speedup = (time_view - time_table) / time_view * 100\n",
    "    print(f\"\\n[WYNIK] Materialized table była szybsza o {speedup:.1f}%\")\n",
    "else:\n",
    "    print(f\"\\n[WYNIK] Różnica w wydajności nieznaczna dla tego datasetu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b25b517-181b-4ee7-9b6a-4f63fb45e4c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 7.2. Kiedy używać VIEW, a kiedy TABLE?\n",
    "\n",
    "**Użyj VIEW gdy:**\n",
    "- Chcesz enkapsulować logikę biznesową\n",
    "- Dane źródłowe często się zmieniają\n",
    "- Zapytanie jest proste i szybkie\n",
    "- Nie chcesz duplikować danych\n",
    "\n",
    "**Użyj MATERIALIZED TABLE gdy:**\n",
    "- Zapytanie jest skomplikowane i wolne\n",
    "- Dane są czytane bardzo często\n",
    "- Dane źródłowe rzadko się zmieniają\n",
    "- Potrzebujesz indeksowania (ZORDER) dla wydajności"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65f2faa4-d5c9-46a0-a505-bf77297bf348",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Część 8: Wprowadzenie do Databricks Jobs\n",
    "\n",
    "### 8.1. Czym jest Databricks Job?\n",
    "\n",
    "**Databricks Job** to mechanizm automatyzacji i orkiestracji notebooków, skryptów lub zapytań SQL.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **Job** - kontener dla jednego lub więcej tasków\n",
    "- **Task** - pojedyncza jednostka pracy (notebook, skrypt, SQL, JAR)\n",
    "- **Cluster** - zasoby obliczeniowe do wykonania zadania\n",
    "- **Trigger** - sposób uruchomienia (manual, scheduled, continuous)\n",
    "- **Retry** - automatyczne ponawianie w przypadku błędu\n",
    "\n",
    "### 8.2. Typy tasków:\n",
    "\n",
    "| Typ tasku | Opis | Use case |\n",
    "|-----------|------|----------|\n",
    "| **Notebook** | Wykonanie notebooka Databricks | ETL pipelines, data processing |\n",
    "| **Python script** | Uruchomienie skryptu .py | Data validation, custom logic |\n",
    "| **JAR** | Uruchomienie aplikacji Spark | Legacy Spark applications |\n",
    "| **SQL** | Wykonanie zapytania SQL | Data transformation, reporting |\n",
    "| **dbt** | Uruchomienie projektu dbt | Modern data transformation |\n",
    "| **Delta Live Tables** | Pipeline DLT | Declarative ETL pipelines |\n",
    "\n",
    "### 8.3. Harmonogramy (Schedules):\n",
    "\n",
    "- **Manual** - uruchomienie na żądanie\n",
    "- **Cron expression** - np. `0 0 * * *` (codziennie o północy)\n",
    "- **Continuous** - ciągłe przetwarzanie (streaming)\n",
    "\n",
    "### 8.4. Retry i timeout:\n",
    "\n",
    "- **Max retries** - maksymalna liczba prób (0-3)\n",
    "- **Retry interval** - czas między próbami (sekundy)\n",
    "- **Timeout** - maksymalny czas wykonania tasku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25602b20-1ffd-4c5d-9506-503c07261187",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8.5. Przykład prostego pipeline'u notebookowego\n",
    "\n",
    "W praktyce pipeline składałby się z kilku notebooków wykonywanych sekwencyjnie:\n",
    "\n",
    "```\n",
    "Pipeline: Daily Customer Analytics\n",
    "├── Task 1: Ingest raw data (notebook: 01_ingest.ipynb)\n",
    "│   └── Retry: 2x, Timeout: 10 min\n",
    "├── Task 2: Clean and transform (notebook: 02_transform.ipynb)\n",
    "│   └── Depends on: Task 1\n",
    "├── Task 3: Aggregate metrics (notebook: 03_aggregate.ipynb)\n",
    "│   └── Depends on: Task 2\n",
    "└── Task 4: Send notification (Python script: notify.py)\n",
    "    └── Depends on: Task 3\n",
    "```\n",
    "\n",
    "**W Databricks UI:**\n",
    "1. Workflows → Create Job\n",
    "2. Add tasks (notebooks)\n",
    "3. Skonfiguruj dependencies między taskami\n",
    "4. Ustaw harmonogram (schedule)\n",
    "5. Skonfiguruj cluster\n",
    "6. Uruchom job"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c168cf5e-fac5-4b84-96b1-91291e615ec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Tworzenie prostego Job’a w UI – krok po kroku\n",
    "\n",
    "1. Przejdź do **Workflows → Jobs** i kliknij **Create Job**.\n",
    "2. Nazwij Job, wybierz typ taska **Notebook** i wskaż notebook (np. pipeline Bronze→Silver→Gold).\n",
    "3. Wybierz cluster:\n",
    "   - istniejący all-purpose (dev), albo\n",
    "   - nowy job cluster z odpowiednim Databricks Runtime.\n",
    "4. (Opcjonalnie) dodaj **parametry**:\n",
    "   - job parameters,\n",
    "   - lub mapowanie na widgety w notebooku (`dbutils.widgets.get`).\n",
    "5. Skonfiguruj **schedule** (np. codziennie o 07:00) lub pozostaw uruchamianie ręczne.\n",
    "6. Ustaw **Retry** (np. 2–3 próby) oraz ewentualny **timeout**.\n",
    "\n",
    "Po pierwszych uruchomieniach:\n",
    "\n",
    "- obserwuj czas wykonania,\n",
    "- zaglądaj do Spark UI,\n",
    "- sprawdzaj liczbę rekordów między warstwami (Bronze/Silver/Gold)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7365e3a1-9fa6-4077-b081-e399f6037ca4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Gdzie szukać logów i szczegółów wykonań Jobów?\n",
    "\n",
    "Po uruchomieniu Databricks Jobów najważniejsze miejsca diagnostyczne:\n",
    "\n",
    "1. **Job run details (Jobs → konkretny run)**:\n",
    "   - status poszczególnych tasków,\n",
    "   - czas wykonania,\n",
    "   - liczba retry,\n",
    "   - powiązany cluster.\n",
    "\n",
    "2. **Spark UI**:\n",
    "   - zakładka **SQL** – plany wykonania zapytań,\n",
    "   - zakładka **Jobs** – DAG zadań Spark,\n",
    "   - zakładka **Storage** – cache / shuffle,\n",
    "   - zakładka **Environment** – konfiguracja clustra i aplikacji.\n",
    "\n",
    "3. **Driver / Executor logs**:\n",
    "   - szczegółowe stacktrace’y wyjątków Pythona/Scali,\n",
    "   - logi aplikacyjne.\n",
    "\n",
    "Dobre praktyki:\n",
    "\n",
    "- Przy krytycznych pipeline’ach ETL zawsze sprawdzaj Spark UI przynajmniej przy pierwszych wdrożeniach.\n",
    "- Jeśli Job kończy się statusem `SUCCESS`, ale dane wyglądają podejrzanie – porównaj liczbę wierszy i proste agregaty między warstwami."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "771935c6-5de3-48f6-bf40-1c5d5b098b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 8.6. Przekazywanie parametrów między notebookami\n",
    "\n",
    "**Metoda 1: Widgets (dla pojedynczego notebooka)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4140850d-d324-4bf7-b4b8-850155204840",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład: Tworzenie widgetu do parametryzacji\n",
    "# (w Databricks to utworzy interaktywny element UI)\n",
    "\n",
    "# dbutils.widgets.text(\"processing_date\", \"2024-01-01\", \"Processing Date\")\n",
    "# processing_date = dbutils.widgets.get(\"processing_date\")\n",
    "\n",
    "# W tym notebooku używamy bezpośrednio wartości\n",
    "processing_date = \"2024-01-01\"\n",
    "print(f\" Processing date: {processing_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03b9262f-6ecd-4292-9916-3a0ad93812c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Metoda 2: dbutils.notebook.run (dla pipeline'u notebooków)**\n",
    "\n",
    "```python\n",
    "# Uruchomienie kolejnego notebooka z parametrami\n",
    "result = dbutils.notebook.run(\n",
    "    \"./02_next_notebook\",\n",
    "    timeout_seconds=600,\n",
    "    arguments={\"date\": \"2024-01-01\", \"mode\": \"full\"}\n",
    ")\n",
    "print(f\"Notebook result: {result}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d1a757da-22df-4834-9a25-d53dd213686b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Część 9: Demonstracja - Prosty pipeline w jednym notebooku\n",
    "\n",
    "### 9.1. Pipeline: Bronze → Silver → Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10beb87a-9721-45a7-aa4c-6ba33fa8875f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== PIPELINE START ===\")\n",
    "print(f\"Timestamp: {datetime.now()}\")\n",
    "\n",
    "# Step 1: Bronze - dane już wczytane wcześniej\n",
    "print(\"\\n[STEP 1: BRONZE] Dane surowe już istnieją\")\n",
    "print(f\"  - Tabela customers: {spark.table(CUSTOMERS_TABLE).count()} rekordów\")\n",
    "print(f\"  - Tabela orders: {spark.table(ORDERS_TABLE).count()} rekordów\")\n",
    "\n",
    "# Step 2: Silver - widok z logiką biznesową już istnieje\n",
    "print(\"\\n[STEP 2: SILVER] Widok customer_order_summary już istnieje\")\n",
    "silver_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {VIEW_NAME}\").collect()[0]['cnt']\n",
    "print(f\"  - Liczba rekordów w widoku: {silver_count}\")\n",
    "\n",
    "# Step 3: Gold - agregacja na poziomie kraju\n",
    "print(\"\\n[STEP 3: GOLD] Tworzenie agregacji na poziomie kraju\")\n",
    "GOLD_TABLE = f\"{CATALOG}.{GOLD_SCHEMA}.country_metrics\"\n",
    "\n",
    "gold_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        country,\n",
    "        COUNT(DISTINCT customer_id) as total_customers,\n",
    "        SUM(total_orders) as total_orders,\n",
    "        ROUND(SUM(total_spent), 2) as total_revenue,\n",
    "        ROUND(AVG(avg_order_value), 2) as avg_order_value,\n",
    "        current_timestamp() as updated_at\n",
    "    FROM {VIEW_NAME}\n",
    "    WHERE total_orders > 0\n",
    "    GROUP BY country\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "# Zapisz do Gold layer\n",
    "gold_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(GOLD_TABLE)\n",
    "\n",
    "print(f\"  - Utworzono tabelę Gold: {GOLD_TABLE}\")\n",
    "print(f\"  - Liczba krajów: {gold_df.count()}\")\n",
    "\n",
    "print(\"\\n=== PIPELINE COMPLETED ===\")\n",
    "print(f\"Timestamp: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd498666-ba3b-408e-b77f-c1e4024859bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 9.2. Weryfikacja wyniku pipeline'u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ff3a3b6-c5a4-440e-a1cb-5c41add4e4af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wyświetl wynik agregacji Gold\n",
    "print(\" Metryki według krajów (Gold Layer):\")\n",
    "spark.table(GOLD_TABLE).display(truncate=False)\n",
    "\n",
    "# Sprawdź timestamp aktualizacji\n",
    "latest_update = spark.sql(f\"SELECT MAX(updated_at) as last_update FROM {GOLD_TABLE}\").collect()[0]['last_update']\n",
    "print(f\"\\n Ostatnia aktualizacja: {latest_update}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23ff24df-44bf-4700-af63-3f9db04a8715",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist - Co powinieneś uzyskać:\n",
    "- [x] Tabela `customers` w schemacie Bronze\n",
    "- [x] Tabela `orders` w schemacie Bronze\n",
    "- [x] Widok `customer_order_summary` w schemacie Silver\n",
    "- [x] Tabela `country_metrics` w schemacie Gold\n",
    "- [x] Zrozumienie różnic między VIEW, TEMP VIEW, GLOBAL TEMP VIEW\n",
    "- [x] Znajomość podstaw Databricks Jobs\n",
    "\n",
    "### Komendy weryfikacyjne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d17256d1-56db-4e2a-a9c2-04d5320e9ae1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=== WERYFIKACJA WYNIKÓW ===\")\n",
    "\n",
    "# 1. Sprawdź tabele Bronze\n",
    "print(\"\\n[1] Tabele Bronze:\")\n",
    "spark.sql(f\"SHOW TABLES IN {CATALOG}.{BRONZE_SCHEMA}\").display(truncate=False)\n",
    "\n",
    "# 2. Sprawdź widoki Silver\n",
    "print(\"\\n[2] Widoki Silver:\")\n",
    "spark.sql(f\"SHOW VIEWS IN {CATALOG}.{SILVER_SCHEMA}\").display(truncate=False)\n",
    "\n",
    "# 3. Sprawdź tabele Gold\n",
    "print(\"\\n[3] Tabele Gold:\")\n",
    "spark.sql(f\"SHOW TABLES IN {CATALOG}.{GOLD_SCHEMA}\").display(truncate=False)\n",
    "\n",
    "# 4. Sprawdź liczby rekordów\n",
    "print(\"\\n[4] Liczba rekordów:\")\n",
    "print(f\"  - customers (Bronze): {spark.table(CUSTOMERS_TABLE).count()}\")\n",
    "print(f\"  - orders (Bronze): {spark.table(ORDERS_TABLE).count()}\")\n",
    "print(f\"  - customer_order_summary (Silver VIEW): {spark.sql(f'SELECT COUNT(*) as cnt FROM {VIEW_NAME}').collect()[0]['cnt']}\")\n",
    "print(f\"  - country_metrics (Gold): {spark.table(GOLD_TABLE).count()}\")\n",
    "\n",
    "print(\"\\n[SUCCESS] Wszystkie obiekty zostały utworzone poprawnie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af4d58d2-b61a-40f9-bd5c-2de3ec0c6b22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Problem 1: \"Table or view not found\"\n",
    "**Objawy:**\n",
    "- Błąd podczas zapytania SQL: `Table or view 'xyz' not found`\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Sprawdź, czy obiekt istnieje\n",
    "spark.sql(\"SHOW TABLES IN catalog.schema\").display()\n",
    "\n",
    "# Użyj pełnej ścieżki (three-level namespace)\n",
    "spark.sql(\"SELECT * FROM catalog.schema.table_name\")\n",
    "```\n",
    "\n",
    "### Problem 2: Temp view nie jest widoczny\n",
    "**Objawy:**\n",
    "- Temp view utworzony w jednym notebooku nie działa w innym\n",
    "\n",
    "**Rozwiązanie:**\n",
    "- Temp views są lokalne dla sesji - użyj Global Temp View lub Persistent View\n",
    "- Global Temp View wymaga prefiksu `global_temp.`\n",
    "\n",
    "### Problem 3: \"Permission denied\" przy tworzeniu tabeli\n",
    "**Objawy:**\n",
    "- Brak uprawnień do utworzenia tabeli w schemacie\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```sql\n",
    "-- Administrator musi nadać uprawnienia\n",
    "GRANT CREATE TABLE ON SCHEMA catalog.schema TO `user@domain.com`;\n",
    "GRANT MODIFY ON SCHEMA catalog.schema TO `user@domain.com`;\n",
    "```\n",
    "\n",
    "### Problem 4: View pokazuje stare dane\n",
    "**Objawy:**\n",
    "- Dane w widoku nie są aktualne mimo aktualizacji tabeli źródłowej\n",
    "\n",
    "**Rozwiązanie:**\n",
    "- Widoki są dynamiczne - powinny pokazywać aktualne dane\n",
    "- Sprawdź cache: `spark.catalog.clearCache()`\n",
    "- Dla danych historycznych użyj materializowanej tabeli\n",
    "\n",
    "### Debugging tips:\n",
    "- Sprawdź current catalog: `SELECT current_catalog()`\n",
    "- Sprawdź current schema: `SELECT current_schema()`\n",
    "- Lista wszystkich obiektów: `SHOW TABLES` i `SHOW VIEWS`\n",
    "- Definicja widoku: `SHOW CREATE TABLE view_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e33c9a71-5b2a-4a08-96aa-d95ddc2d54cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "### Zarządzanie widokami:\n",
    "- Używaj **Persistent Views** dla logiki biznesowej współdzielonej między zespołami\n",
    "- Używaj **Temp Views** dla tymczasowych przekształceń w ramach jednego notebooka\n",
    "- Dokumentuj logikę widoków w komentarzach SQL\n",
    "- Nazwy widoków powinny jasno wskazywać ich przeznaczenie (np. `vw_customer_360`, `vw_daily_sales`)\n",
    "\n",
    "### Zarządzanie tabelami:\n",
    "- Zawsze używaj **Delta format** dla tabel w lakehouse\n",
    "- Stosuj konwencję nazewnictwa: `{layer}_{domain}_{entity}` (np. `bronze_sales_transactions`)\n",
    "- Dodawaj kolumny audytowe: `created_at`, `updated_at`, `source_system`\n",
    "- Regularnie uruchamiaj `OPTIMIZE` i `VACUUM` dla tabel Delta\n",
    "\n",
    "### Unity Catalog:\n",
    "- Organizuj obiekty w logiczne schematy: bronze, silver, gold\n",
    "- Używaj three-level namespace: `catalog.schema.table`\n",
    "- Nadawaj uprawnienia na poziomie schematu, nie tabeli\n",
    "- Regularnie przeglądaj logi audytowe\n",
    "\n",
    "### Databricks Jobs:\n",
    "- Dziel duże notebooki na mniejsze, modularne taski\n",
    "- Używaj parametryzacji przez widgets\n",
    "- Konfiguruj retry dla tasków podatnych na błędy (np. API calls)\n",
    "- Monitoruj czas wykonania i ustawiaj sensowne timeouty\n",
    "- Używaj alertów e-mail/Slack dla krytycznych pipeline'ów\n",
    "- Testuj pipeline lokalnie przed wdrożeniem do produkcji\n",
    "\n",
    "### Wydajność:\n",
    "- Materializuj widoki, które są często używane w złożonych zapytaniach\n",
    "- Używaj partycjonowania dla dużych tabel\n",
    "- Cache'uj wyniki pośrednie w pipeline'ach\n",
    "- Monitoruj Spark UI dla identyfikacji bottlenecks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2b33a9a-6929-4965-ab40-0694bab1a5d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "### Co zostało osiągnięte:\n",
    "- Poznanie różnic między VIEW, TABLE i DELTA TABLE\n",
    "- Tworzenie Temp Views, Global Temp Views i Persistent Views\n",
    "- Rejestracja tabel i widoków w Unity Catalog\n",
    "- Przeglądanie metadanych w Catalog Explorer\n",
    "- Zrozumienie podstaw Databricks Jobs i pipeline'ów\n",
    "- Implementacja prostego pipeline'u Bronze → Silver → Gold\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "1. **VIEW vs TABLE**: View nie przechowuje danych, TABLE materializuje wyniki na dysku\n",
    "2. **Delta Lake**: Zalecany format dla wszystkich tabel w lakehouse (ACID, Time Travel, MERGE)\n",
    "3. **Unity Catalog**: Centralne zarządzanie metadanymi, three-level namespace\n",
    "4. **Databricks Jobs**: Automatyzacja i orkiestracja notebooków w pipeline'y\n",
    "5. **Medallion Architecture**: Bronze (raw) → Silver (cleaned) → Gold (aggregated)\n",
    "\n",
    "### Quick Reference - Najważniejsze komendy:\n",
    "\n",
    "| Operacja | PySpark | SQL |\n",
    "|----------|---------|-----|\n",
    "| Temp View | `df.createOrReplaceTempView(\"name\")` | `CREATE TEMP VIEW name AS SELECT ...` |\n",
    "| Global Temp View | `df.createOrReplaceGlobalTempView(\"name\")` | `CREATE GLOBAL TEMP VIEW name AS SELECT ...` |\n",
    "| Persistent View | - | `CREATE VIEW catalog.schema.name AS SELECT ...` |\n",
    "| Delta Table | `df.write.format(\"delta\").saveAsTable(\"name\")` | `CREATE TABLE name USING DELTA AS SELECT ...` |\n",
    "| Lista tabel | `spark.catalog.listTables()` | `SHOW TABLES IN catalog.schema` |\n",
    "| Lista widoków | - | `SHOW VIEWS IN catalog.schema` |\n",
    "| Metadane | - | `DESCRIBE EXTENDED table_name` |\n",
    "| Drop view | `spark.catalog.dropTempView(\"name\")` | `DROP VIEW IF EXISTS name` |\n",
    "\n",
    "### Następne kroki:\n",
    "- **Kolejny notebook**: `DZIEN_2_Lakehouse_Delta_Lake/01_delta_lake_operations.ipynb`\n",
    "- **Warsztat praktyczny**: `workshops/03_views_basic_jobs_workshop.ipynb`\n",
    "- **Materiały dodatkowe**: \n",
    "  - [Unity Catalog documentation](https://docs.databricks.com/data-governance/unity-catalog/index.html)\n",
    "  - [Databricks Jobs documentation](https://docs.databricks.com/workflows/jobs/jobs.html)\n",
    "  - [Delta Lake best practices](https://docs.databricks.com/delta/best-practices.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84236668-9e43-4769-8bf6-1005931407ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Czyszczenie zasobów\n",
    "\n",
    "Posprzątaj zasoby utworzone podczas notebooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f56075b-4e3a-4cc4-9cdb-8358cc4f2672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UWAGA: Uruchom tylko jeśli chcesz usunąć wszystkie utworzone obiekty\n",
    "\n",
    "# Usuń tabele Bronze\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CUSTOMERS_TABLE}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {ORDERS_TABLE}\")\n",
    "\n",
    "# Usuń widoki i tabele Silver\n",
    "# spark.sql(f\"DROP VIEW IF EXISTS {VIEW_NAME}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {MATERIALIZED_TABLE}\")\n",
    "\n",
    "# Usuń tabele Gold\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {GOLD_TABLE}\")\n",
    "\n",
    "# Wyczyść cache\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "print(\" Zasoby zostały wyczyszczone (odkomentuj kod, aby wykonać)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_views_workflows",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
