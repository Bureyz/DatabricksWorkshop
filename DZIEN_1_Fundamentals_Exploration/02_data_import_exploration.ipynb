{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31c3897a",
   "metadata": {},
   "source": [
    "# Databricks Training: Import i eksploracja danych\n",
    "\n",
    "**Modu≈Ç:** Fundamentals & Data Exploration  \n",
    "**Temat:** DataFrame Reader API, opcje wczytywania, konstrukcja schemat√≥w, podstawowe operacje eksploracyjne  \n",
    "**Czas trwania:** 45 minut  \n",
    "**Poziom:** PoczƒÖtkujƒÖcy\n",
    "\n",
    "---\n",
    "\n",
    "## Cel szkoleniowy\n",
    "\n",
    "W tym notebooku nauczysz siƒô:\n",
    "- U≈ºywaƒá DataFrame Reader API do wczytywania danych z r√≥≈ºnych format√≥w\n",
    "- Konfigurowaƒá opcje czytania danych (header, delimiter, inferSchema)\n",
    "- Rƒôcznie definiowaƒá schematy za pomocƒÖ StructType i StructField\n",
    "- Wykonywaƒá podstawowe operacje eksploracyjne na DataFrame (columns, dtypes, count, describe, summary)\n",
    "- Por√≥wnywaƒá automatyczne wykrywanie schemat√≥w z rƒôcznƒÖ definicjƒÖ\n",
    "\n",
    "---\n",
    "\n",
    "## Kontekst biznesowy\n",
    "\n",
    "Organizacja KION zarzƒÖdza danymi z r√≥≈ºnych ≈∫r√≥de≈Ç:\n",
    "- **Klienci** - dane w formacie CSV\n",
    "- **Zam√≥wienia** - dane w formacie JSON\n",
    "- **Produkty** - dane w formacie Parquet\n",
    "\n",
    "Zadaniem data engineera jest:\n",
    "1. Zaimportowaƒá dane z r√≥≈ºnych format√≥w do Databricks\n",
    "2. Zrozumieƒá strukturƒô danych i typy kolumn\n",
    "3. Przygotowaƒá dane do dalszych transformacji\n",
    "\n",
    "---\n",
    "\n",
    "## Podstawy teoretyczne\n",
    "\n",
    "### DataFrame Reader API\n",
    "\n",
    "Databricks udostƒôpnia uniwersalne API do wczytywania danych:\n",
    "\n",
    "```python\n",
    "spark.read.format(\"format\").option(\"key\", \"value\").load(\"path\")\n",
    "```\n",
    "\n",
    "Wspierane formaty:\n",
    "- **CSV** - dane tabelaryczne z separatorami\n",
    "- **JSON** - dane semi-strukturalne\n",
    "- **Parquet** - kolumnowy format binarny z kompresjƒÖ\n",
    "- **Delta** - format transakcyjny (om√≥wiony w kolejnych modu≈Çach)\n",
    "\n",
    "### Opcje wczytywania danych\n",
    "\n",
    "Dla **CSV**:\n",
    "- `header=True` - pierwsza linia zawiera nazwy kolumn\n",
    "- `inferSchema=True` - automatyczne wykrywanie typ√≥w danych\n",
    "- `delimiter=\",\"` - separator kolumn\n",
    "\n",
    "Dla **JSON**:\n",
    "- `multiLine=True` - JSON roz≈Ço≈ºony na wiele linii\n",
    "- `inferSchema=True` - wykrywanie struktury zagnie≈ºd≈ºonej\n",
    "\n",
    "### Definicja schematu\n",
    "\n",
    "Zalecana praktyka: **rƒôczne definiowanie schematu** zamiast automatycznego wykrywania.\n",
    "\n",
    "Korzy≈õci:\n",
    "- Szybsze wczytywanie (brak konieczno≈õci skanowania danych)\n",
    "- Kontrola typ√≥w danych\n",
    "- Unikniƒôcie b≈Çƒôd√≥w przy niepoprawnych danych\n",
    "\n",
    "Przyk≈Çad:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "```\n",
    "\n",
    "### Operacje eksploracyjne\n",
    "\n",
    "| Operacja | Opis |\n",
    "|----------|------|\n",
    "| `df.columns` | Lista nazw kolumn |\n",
    "| `df.dtypes` | Lista typ√≥w kolumn |\n",
    "| `df.count()` | Liczba wierszy |\n",
    "| `df.printSchema()` | Struktura schematu |\n",
    "| `df.describe()` | Statystyki opisowe (count, mean, stddev, min, max) |\n",
    "| `df.summary()` | Rozszerzone statystyki (+ percentyle) |\n",
    "| `df.show(n)` | Wy≈õwietl n wierszy |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb83c3a3",
   "metadata": {},
   "source": [
    "## Inicjalizacja ≈õrodowiska\n",
    "\n",
    "Wykonujemy centralny skrypt konfiguracyjny:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796995b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7901a15",
   "metadata": {},
   "source": [
    "## Konfiguracja notebooka\n",
    "\n",
    "Definiujemy zmienne specyficzne dla tego notebooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a405a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ≈öcie≈ºki do katalog√≥w z danymi (podkatalogi w DATASET_BASE_PATH z 00_setup)\n",
    "CUSTOMERS_PATH = f\"{DATASET_BASE_PATH}/customers\"\n",
    "ORDERS_PATH = f\"{DATASET_BASE_PATH}/orders\"\n",
    "PRODUCTS_PATH = f\"{DATASET_BASE_PATH}/products\"\n",
    "\n",
    "# ≈öcie≈ºki do konkretnych plik√≥w\n",
    "CUSTOMERS_CSV = f\"{CUSTOMERS_PATH}/customers.csv\"\n",
    "ORDERS_JSON = f\"{ORDERS_PATH}/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{PRODUCTS_PATH}/products.parquet\"\n",
    "\n",
    "print(f\"≈öcie≈ºka do pliku customers CSV: {CUSTOMERS_CSV}\")\n",
    "print(f\"≈öcie≈ºka do pliku orders JSON: {ORDERS_JSON}\")\n",
    "print(f\"≈öcie≈ºka do pliku products Parquet: {PRODUCTS_PARQUET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db8bbe5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czƒô≈õƒá 1: Import danych CSV (Customers)\n",
    "\n",
    "### 1.1. Wczytanie CSV z automatycznym wykrywaniem schematu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21751d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytanie danych CSV z automatycznym wykrywaniem schematu\n",
    "customers_auto_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")       # Pierwsza linia to nazwy kolumn\n",
    "    .option(\"inferSchema\", \"true\")  # Automatyczne wykrywanie typ√≥w danych\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "# Wy≈õwietl schemat\n",
    "print(\"[INFO] Schemat wykryty automatycznie:\")\n",
    "customers_auto_df.printSchema()\n",
    "\n",
    "# Wy≈õwietl pr√≥bkƒô danych\n",
    "print(\"\\n[INFO] Pr√≥bka danych (5 wierszy):\")\n",
    "customers_auto_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385fde35",
   "metadata": {},
   "source": [
    "### 1.2. Eksploracja danych CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ac44ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista nazw kolumn\n",
    "print(\"[INFO] Kolumny DataFrame:\")\n",
    "print(customers_auto_df.columns)\n",
    "\n",
    "# Lista typ√≥w danych\n",
    "print(\"\\n[INFO] Typy danych kolumn:\")\n",
    "print(customers_auto_df.dtypes)\n",
    "\n",
    "# Liczba wierszy\n",
    "row_count = customers_auto_df.count()\n",
    "print(f\"\\n[INFO] Liczba wierszy: {row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c82570",
   "metadata": {},
   "source": [
    "### 1.3. Statystyki opisowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76cba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statystyki opisowe (count, mean, stddev, min, max)\n",
    "print(\"[INFO] Statystyki opisowe (describe):\")\n",
    "customers_auto_df.describe().show()\n",
    "\n",
    "# Rozszerzone statystyki (+ percentyle)\n",
    "print(\"\\n[INFO] Rozszerzone statystyki (summary):\")\n",
    "customers_auto_df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e82ca61",
   "metadata": {},
   "source": [
    "### 1.4. Rƒôczna definicja schematu dla CSV\n",
    "\n",
    "**Best Practice:** Definiowanie schematu manualnie zapewnia kontrolƒô i wydajno≈õƒá."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc2cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType\n",
    "\n",
    "# Definicja schematu dla customers\n",
    "# Struktura: customer_id (int), customer_name (string), email (string), country (string), registration_date (timestamp)\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"customer_name\", StringType(), nullable=True),\n",
    "    StructField(\"email\", StringType(), nullable=True),\n",
    "    StructField(\"country\", StringType(), nullable=True),\n",
    "    StructField(\"registration_date\", TimestampType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Wczytanie danych CSV z rƒôcznie zdefiniowanym schematem\n",
    "customers_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(customers_schema)  # U≈ºycie zdefiniowanego schematu\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "print(\"[INFO] Schemat zdefiniowany rƒôcznie:\")\n",
    "customers_df.printSchema()\n",
    "\n",
    "print(\"\\n[INFO] Pr√≥bka danych z rƒôcznym schematem:\")\n",
    "customers_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8057607",
   "metadata": {},
   "source": [
    "### ‚úÖ ƒÜwiczenie 1: Wczytanie CSV z w≈Çasnymi opcjami\n",
    "\n",
    "**Zadanie:** Wczytaj plik CSV `customers.csv` ponownie, ale:\n",
    "- Ustaw delimiter na `;` (celowo b≈Çƒôdny, aby zobaczyƒá efekt)\n",
    "- Wy≈ÇƒÖcz `inferSchema`\n",
    "- Zaobserwuj, jak to wp≈Çywa na wynik\n",
    "\n",
    "**Hint:** U≈ºyj `.option(\"delimiter\", \";\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f39ae3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wczytaj customers.csv z delimitorem `;` i bez inferSchema\n",
    "# customers_wrong_df = spark.read.format(\"csv\")...\n",
    "\n",
    "# TODO: Wy≈õwietl schemat i 5 wierszy\n",
    "# customers_wrong_df.printSchema()\n",
    "# customers_wrong_df.show(5, truncate=False)\n",
    "\n",
    "# Zaobserwuj, ≈ºe wszystkie dane bƒôdƒÖ w jednej kolumnie\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44724e60",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czƒô≈õƒá 2: Import danych JSON (Orders)\n",
    "\n",
    "### 2.1. Wczytanie JSON z automatycznym wykrywaniem schematu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d930e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytanie danych JSON z automatycznym wykrywaniem schematu\n",
    "orders_auto_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "print(\"[INFO] Schemat JSON wykryty automatycznie:\")\n",
    "orders_auto_df.printSchema()\n",
    "\n",
    "print(\"\\n[INFO] Pr√≥bka danych JSON:\")\n",
    "orders_auto_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456e251a",
   "metadata": {},
   "source": [
    "### 2.2. Eksploracja danych JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fbb50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liczba kolumn i wierszy\n",
    "print(f\"[INFO] Liczba kolumn: {len(orders_auto_df.columns)}\")\n",
    "print(f\"[INFO] Nazwy kolumn: {orders_auto_df.columns}\")\n",
    "print(f\"[INFO] Liczba wierszy: {orders_auto_df.count()}\")\n",
    "\n",
    "# Typy danych\n",
    "print(\"\\n[INFO] Typy danych:\")\n",
    "for col_name, col_type in orders_auto_df.dtypes:\n",
    "    print(f\"  - {col_name}: {col_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78342d4",
   "metadata": {},
   "source": [
    "### 2.3. Rƒôczna definicja schematu dla JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960d978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "# Definicja schematu dla orders\n",
    "# Struktura: order_id (int), customer_id (int), order_date (timestamp), total_amount (double), status (string)\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"customer_id\", IntegerType(), nullable=True),\n",
    "    StructField(\"order_date\", TimestampType(), nullable=True),\n",
    "    StructField(\"total_amount\", DoubleType(), nullable=True),\n",
    "    StructField(\"status\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Wczytanie danych JSON z rƒôcznie zdefiniowanym schematem\n",
    "orders_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .schema(orders_schema)\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "print(\"[INFO] Schemat JSON zdefiniowany rƒôcznie:\")\n",
    "orders_df.printSchema()\n",
    "\n",
    "print(\"\\n[INFO] Pr√≥bka danych z rƒôcznym schematem:\")\n",
    "orders_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f1881c",
   "metadata": {},
   "source": [
    "### 2.4. Statystyki opisowe dla JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84998245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statystyki dla kolumn numerycznych\n",
    "print(\"[INFO] Statystyki dla orders:\")\n",
    "orders_df.select(\"order_id\", \"customer_id\", \"total_amount\").describe().show()\n",
    "\n",
    "# Rozk≈Çad warto≈õci dla kolumny 'status'\n",
    "print(\"\\n[INFO] Rozk≈Çad warto≈õci w kolumnie 'status':\")\n",
    "orders_df.groupBy(\"status\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc76411",
   "metadata": {},
   "source": [
    "### ‚úÖ ƒÜwiczenie 2: Eksploracja danych JSON\n",
    "\n",
    "**Zadanie:**\n",
    "1. Policz liczbƒô unikalnych klient√≥w w DataFrame `orders_df`\n",
    "2. Znajd≈∫ warto≈õƒá maksymalnƒÖ i minimalnƒÖ w kolumnie `total_amount`\n",
    "3. Policz liczbƒô zam√≥wie≈Ñ w ka≈ºdym statusie\n",
    "\n",
    "**Hint:** U≈ºyj `.select()`, `.distinct()`, `.count()`, `.agg()`, `.groupBy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e65df26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Policz liczbƒô unikalnych customer_id\n",
    "# unique_customers = orders_df.select(\"customer_id\").distinct().count()\n",
    "# print(f\"Liczba unikalnych klient√≥w: {unique_customers}\")\n",
    "\n",
    "# TODO: Znajd≈∫ min i max total_amount\n",
    "# from pyspark.sql.functions import min, max\n",
    "# orders_df.select(min(\"total_amount\"), max(\"total_amount\")).show()\n",
    "\n",
    "# TODO: Policz zam√≥wienia wed≈Çug statusu\n",
    "# orders_df.groupBy(\"status\").count().show()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1fd8d5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czƒô≈õƒá 3: Import danych Parquet (Products)\n",
    "\n",
    "### 3.1. Wczytanie Parquet (schemat wbudowany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192fa909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parquet zawiera ju≈º schemat wbudowany - nie trzeba go definiowaƒá\n",
    "products_df = (\n",
    "    spark.read\n",
    "    .format(\"parquet\")\n",
    "    .load(PRODUCTS_PARQUET)\n",
    ")\n",
    "\n",
    "print(\"[INFO] Schemat Parquet (wbudowany):\")\n",
    "products_df.printSchema()\n",
    "\n",
    "print(\"\\n[INFO] Pr√≥bka danych Parquet:\")\n",
    "products_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "661e20b9",
   "metadata": {},
   "source": [
    "### 3.2. Eksploracja danych Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2d1958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Podstawowe informacje\n",
    "print(f\"[INFO] Liczba kolumn: {len(products_df.columns)}\")\n",
    "print(f\"[INFO] Nazwy kolumn: {products_df.columns}\")\n",
    "print(f\"[INFO] Liczba produkt√≥w: {products_df.count()}\")\n",
    "\n",
    "# Typy danych\n",
    "print(\"\\n[INFO] Typy danych:\")\n",
    "for col_name, col_type in products_df.dtypes:\n",
    "    print(f\"  - {col_name}: {col_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb4fa56",
   "metadata": {},
   "source": [
    "### 3.3. Statystyki opisowe dla Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412e9754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statystyki dla kolumn numerycznych\n",
    "print(\"[INFO] Statystyki dla products:\")\n",
    "products_df.describe().show()\n",
    "\n",
    "# Rozszerzone statystyki\n",
    "print(\"\\n[INFO] Rozszerzone statystyki:\")\n",
    "products_df.summary().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1d0284",
   "metadata": {},
   "source": [
    "### 3.4. Rozk≈Çad kategorii produkt√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95f1044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawd≈∫, czy istnieje kolumna 'category' i wy≈õwietl rozk≈Çad\n",
    "if \"category\" in products_df.columns:\n",
    "    print(\"[INFO] Rozk≈Çad kategorii produkt√≥w:\")\n",
    "    products_df.groupBy(\"category\").count().orderBy(\"count\", ascending=False).show()\n",
    "else:\n",
    "    print(\"[INFO] Kolumna 'category' nie istnieje w danych produkt√≥w\")\n",
    "    print(f\"[INFO] Dostƒôpne kolumny: {products_df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8673d033",
   "metadata": {},
   "source": [
    "### ‚úÖ ƒÜwiczenie 3: Analiza danych Parquet\n",
    "\n",
    "**Zadanie:**\n",
    "1. Znajd≈∫ ≈õredniƒÖ cenƒô produktu (je≈õli kolumna `price` istnieje)\n",
    "2. Znajd≈∫ 5 najdro≈ºszych produkt√≥w\n",
    "3. Policz produkty wed≈Çug kategorii (je≈õli kolumna istnieje)\n",
    "\n",
    "**Hint:** U≈ºyj `.agg()`, `.orderBy()`, `.limit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43072bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Oblicz ≈õredniƒÖ cenƒô\n",
    "# from pyspark.sql.functions import avg\n",
    "# products_df.select(avg(\"price\")).show()\n",
    "\n",
    "# TODO: Znajd≈∫ 5 najdro≈ºszych produkt√≥w\n",
    "# products_df.orderBy(col(\"price\").desc()).limit(5).show(truncate=False)\n",
    "\n",
    "# TODO: Policz produkty wed≈Çug kategorii\n",
    "# if \"category\" in products_df.columns:\n",
    "#     products_df.groupBy(\"category\").count().show()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338f4f5b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czƒô≈õƒá 4: Por√≥wnanie wydajno≈õci\n",
    "\n",
    "### 4.1. Wczytanie CSV: inferSchema vs rƒôczny schemat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27ced5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test 1: Automatyczne wykrywanie schematu\n",
    "start_auto = time.time()\n",
    "df_auto = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "count_auto = df_auto.count()  # Action - wymusza wykonanie\n",
    "time_auto = time.time() - start_auto\n",
    "\n",
    "print(f\"[BENCHMARK] Wczytanie CSV z inferSchema: {time_auto:.3f} sekund\")\n",
    "print(f\"[BENCHMARK] Liczba wierszy: {count_auto}\")\n",
    "\n",
    "# Test 2: Rƒôczne zdefiniowanie schematu\n",
    "start_manual = time.time()\n",
    "df_manual = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(customers_schema)\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "count_manual = df_manual.count()  # Action - wymusza wykonanie\n",
    "time_manual = time.time() - start_manual\n",
    "\n",
    "print(f\"\\n[BENCHMARK] Wczytanie CSV z rƒôcznym schematem: {time_manual:.3f} sekund\")\n",
    "print(f\"[BENCHMARK] Liczba wierszy: {count_manual}\")\n",
    "\n",
    "# Por√≥wnanie\n",
    "speedup = (time_auto - time_manual) / time_auto * 100\n",
    "print(f\"\\n[WYNIK] Przyspieszenie: {speedup:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b6cad",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Czƒô≈õƒá 5: Best Practices\n",
    "\n",
    "### Zalecenia przy importowaniu danych:\n",
    "\n",
    "1. **Zawsze definiuj schemat rƒôcznie**\n",
    "   - Szybsze wczytywanie\n",
    "   - Kontrola typ√≥w danych\n",
    "   - Unikniƒôcie b≈Çƒôd√≥w typu\n",
    "\n",
    "2. **Wybieraj odpowiedni format**\n",
    "   - **Parquet** - najlepszy dla analityki (kolumnowy, kompresja)\n",
    "   - **CSV** - ≈Çatwy do debugowania, ale wolniejszy\n",
    "   - **JSON** - elastyczny dla danych semi-strukturalnych\n",
    "\n",
    "3. **U≈ºywaj partycjonowania**\n",
    "   - Przyspieszenie zapyta≈Ñ filtrujƒÖcych\n",
    "   - Przyk≈Çad: partycjonowanie zam√≥wie≈Ñ wed≈Çug daty\n",
    "\n",
    "4. **Sprawdzaj jako≈õƒá danych od razu**\n",
    "   - `count()` - sprawd≈∫ liczbƒô wierszy\n",
    "   - `describe()` - sprawd≈∫ rozk≈Çad warto≈õci\n",
    "   - `printSchema()` - zweryfikuj typy\n",
    "\n",
    "5. **U≈ºywaj `limit()` podczas eksperymentowania**\n",
    "   - Przyspiesza iteracje nad kodem\n",
    "   - Przyk≈Çad: `df.limit(1000).show()`\n",
    "\n",
    "6. **Dokumentuj schematy**\n",
    "   - U≈Çatwia utrzymanie kodu\n",
    "   - Przyk≈Çad: komentarze przy definicji StructType\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e724300",
   "metadata": {},
   "source": [
    "## Podsumowanie\n",
    "\n",
    "W tym notebooku nauczy≈Çe≈õ siƒô:\n",
    "\n",
    "‚úÖ **DataFrame Reader API**\n",
    "- Wczytywanie danych z CSV, JSON, Parquet\n",
    "- Konfiguracja opcji (header, inferSchema, delimiter)\n",
    "\n",
    "‚úÖ **Rƒôczna definicja schematu**\n",
    "- Tworzenie schemat√≥w za pomocƒÖ StructType i StructField\n",
    "- Por√≥wnanie wydajno≈õci: inferSchema vs rƒôczny schemat\n",
    "\n",
    "‚úÖ **Operacje eksploracyjne**\n",
    "- Podstawowe operacje: columns, dtypes, count\n",
    "- Statystyki: describe(), summary()\n",
    "- Grupowanie i agregacja\n",
    "\n",
    "‚úÖ **Best Practices**\n",
    "- Zalecenie rƒôcznego definiowania schematu\n",
    "- Wyb√≥r formatu danych dla r√≥≈ºnych scenariuszy\n",
    "- Sprawdzanie jako≈õci danych\n",
    "\n",
    "---\n",
    "\n",
    "### Nastƒôpne kroki:\n",
    "\n",
    "üìå **Kolejny notebook:** `03_basic_transformations_sql_pyspark.ipynb`  \n",
    "üìå **Temat:** Transformacje danych, operacje SQL i PySpark API\n",
    "\n",
    "---\n",
    "\n",
    "## Zasoby dodatkowe\n",
    "\n",
    "- [Databricks - Reading Data](https://docs.databricks.com/ingestion/index.html)\n",
    "- [PySpark DataFrame API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)\n",
    "- [Schema Evolution in Spark](https://docs.databricks.com/delta/schema-evolution.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7699b3b6",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Zwalniamy zasoby (opcjonalne):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477d403d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Od≈ÇƒÖczenie tymczasowych widok√≥w (je≈õli by≈Çy tworzone)\n",
    "# spark.catalog.dropTempView(\"customers_view\")\n",
    "\n",
    "print(\"[INFO] Notebook zako≈Ñczony pomy≈õlnie\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
