{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a6bc86c4-58e5-4f4f-af86-f3311937bb70",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Databricks Training: Import i eksploracja danych\n",
    "\n",
    "**Modu≈Ç:** Fundamentals & Data Exploration  \n",
    "**Temat:** DataFrame Reader API, opcje wczytywania, konstrukcja schemat√≥w, podstawowe operacje eksploracyjne  \n",
    "**Czas trwania:** 20 minut  \n",
    "**Poziom:** PoczƒÖtkujƒÖcy\n",
    "\n",
    "---\n",
    "\n",
    "## Cel szkoleniowy\n",
    "\n",
    "W tym notebooku nauczysz siƒô:\n",
    "- U≈ºywaƒá DataFrame Reader API do wczytywania danych z r√≥≈ºnych format√≥w\n",
    "- Konfigurowaƒá opcje czytania danych (header, delimiter, inferSchema)\n",
    "- Rƒôcznie definiowaƒá schematy za pomocƒÖ StructType i StructField\n",
    "- Wykonywaƒá podstawowe operacje eksploracyjne na DataFrame (columns, dtypes, count, describe, summary)\n",
    "- Por√≥wnywaƒá automatyczne wykrywanie schemat√≥w z rƒôcznƒÖ definicjƒÖ\n",
    "\n",
    "---\n",
    "\n",
    "## Kontekst biznesowy\n",
    "\n",
    "Organizacja KION zarzƒÖdza danymi z r√≥≈ºnych ≈∫r√≥de≈Ç:\n",
    "- **Klienci** - dane w formacie CSV\n",
    "- **Zam√≥wienia** - dane w formacie JSON\n",
    "- **Produkty** - dane w formacie Parquet\n",
    "\n",
    "Zadaniem data engineera jest:\n",
    "1. Zaimportowaƒá dane z r√≥≈ºnych format√≥w do Databricks\n",
    "2. Zrozumieƒá strukturƒô danych i typy kolumn\n",
    "3. Przygotowaƒá dane do dalszych transformacji\n",
    "\n",
    "---\n",
    "\n",
    "## Podstawy teoretyczne\n",
    "\n",
    "### DataFrame Reader API\n",
    "\n",
    "Databricks udostƒôpnia uniwersalne API do wczytywania danych:\n",
    "\n",
    "```python\n",
    "spark.read.format(\"format\").option(\"key\", \"value\").load(\"path\")\n",
    "```\n",
    "\n",
    "Wspierane formaty:\n",
    "- **CSV** - dane tabelaryczne z separatorami\n",
    "- **JSON** - dane semi-strukturalne\n",
    "- **Parquet** - kolumnowy format binarny z kompresjƒÖ\n",
    "- **Delta** - format transakcyjny (om√≥wiony w kolejnych modu≈Çach)\n",
    "\n",
    "### Opcje wczytywania danych\n",
    "\n",
    "Dla **CSV**:\n",
    "- `header=True` - pierwsza linia zawiera nazwy kolumn\n",
    "- `inferSchema=True` - automatyczne wykrywanie typ√≥w danych\n",
    "- `delimiter=\",\"` - separator kolumn\n",
    "\n",
    "Dla **JSON**:\n",
    "- `multiLine=True` - JSON roz≈Ço≈ºony na wiele linii\n",
    "- `inferSchema=True` - wykrywanie struktury zagnie≈ºd≈ºonej\n",
    "\n",
    "### Definicja schematu\n",
    "\n",
    "Zalecana praktyka: **rƒôczne definiowanie schematu** zamiast automatycznego wykrywania.\n",
    "\n",
    "Korzy≈õci:\n",
    "- Szybsze wczytywanie (brak konieczno≈õci skanowania danych)\n",
    "- Kontrola typ√≥w danych\n",
    "- Unikniƒôcie b≈Çƒôd√≥w przy niepoprawnych danych\n",
    "\n",
    "Przyk≈Çad:\n",
    "\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"name\", StringType(), True)\n",
    "])\n",
    "```\n",
    "\n",
    "### Operacje eksploracyjne\n",
    "\n",
    "| Operacja | Opis |\n",
    "|----------|------|\n",
    "| `df.columns` | Lista nazw kolumn |\n",
    "| `df.dtypes` | Lista typ√≥w kolumn |\n",
    "| `df.count()` | Liczba wierszy |\n",
    "| `df.printSchema()` | Struktura schematu |\n",
    "| `df.describe()` | Statystyki opisowe (count, mean, stddev, min, max) |\n",
    "| `df.summary()` | Rozszerzone statystyki (+ percentyle) |\n",
    "| `df.show(n)` | Wy≈õwietl n wierszy |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8cf6c95b-2145-4ffe-a23d-8118983d4dd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inicjalizacja ≈õrodowiska\n",
    "\n",
    "Wykonujemy centralny skrypt konfiguracyjny:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea9ed832-63c8-4788-8c7f-256fcf2ac6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9981cb88-642b-4f84-8a87-ebf5d572e504",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja notebooka\n",
    "\n",
    "Definiujemy zmienne specyficzne dla tego notebooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f11c5fa-8097-4ebe-89cf-93833a9e6f72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ≈öcie≈ºki do katalog√≥w z danymi (podkatalogi w DATASET_BASE_PATH z 00_setup)\n",
    "CUSTOMERS_PATH = f\"{DATASET_BASE_PATH}/customers\"\n",
    "ORDERS_PATH = f\"{DATASET_BASE_PATH}/orders\"\n",
    "PRODUCTS_PATH = f\"{DATASET_BASE_PATH}/products\"\n",
    "\n",
    "# ≈öcie≈ºki do konkretnych plik√≥w\n",
    "CUSTOMERS_CSV = f\"{CUSTOMERS_PATH}/customers.csv\"\n",
    "ORDERS_JSON = f\"{ORDERS_PATH}/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{PRODUCTS_PATH}/products.parquet\"\n",
    "\n",
    "print(f\"≈öcie≈ºka do pliku customers CSV: {CUSTOMERS_CSV}\")\n",
    "print(f\"≈öcie≈ºka do pliku orders JSON: {ORDERS_JSON}\")\n",
    "print(f\"≈öcie≈ºka do pliku products Parquet: {PRODUCTS_PARQUET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "889facc3-12b1-46b1-ac58-004aef6b0971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Czƒô≈õƒá 1: Import danych CSV (Customers)\n",
    "\n",
    "### 1.1. Wczytanie CSV z automatycznym wykrywaniem schematu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e44f411-bfe5-4692-8552-458c1dcfad50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wczytanie danych CSV z automatycznym wykrywaniem schematu\n",
    "customers_auto_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")       # Pierwsza linia to nazwy kolumn\n",
    "    .option(\"inferSchema\", \"true\")  # Automatyczne wykrywanie typ√≥w danych\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bf57a69-f1da-43f1-ba1c-7adec2725f80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wy≈õwietl schemat\n",
    "print(\" Schemat wykryty automatycznie:\")\n",
    "customers_auto_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "44eb5392-ef2f-41d4-bf26-057fcd3d1654",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n Pr√≥bka danych (5 wierszy):\")\n",
    "customers_auto_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9442b0e9-3cad-4495-984d-e7e7f7b73e99",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wy≈õwietl pr√≥bkƒô danych\n",
    "print(\"\\n Pr√≥bka danych :\")\n",
    "display(customers_auto_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b54f9b85-8dc4-4446-9164-cf1b07518eee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.2. Eksploracja danych CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bb59e98-a08e-46c0-9a35-7cddc6bce458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista nazw kolumn\n",
    "print(\" Kolumny DataFrame:\")\n",
    "customers_auto_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8274f51b-13c1-4a57-bfd6-2b921ff6eda3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista typ√≥w danych\n",
    "print(\"\\n Typy danych kolumn:\")\n",
    "customers_auto_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3287a960-4d7a-498f-aebc-31cc03abfebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Liczba wierszy\n",
    "row_count = customers_auto_df.count()\n",
    "print(f\"\\nLiczba wierszy: {row_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7437bcb1-5c86-400a-b99b-009f7ceeaec0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.3. Statystyki opisowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c0ea429-e348-41c1-a2c1-275d16952a03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# Definicja schematu dla customers\n",
    "# Struktura rzeczywista: customer_id, first_name, last_name, email, phone, city, state, country, registration_date, customer_segment\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), nullable=False),\n",
    "    StructField(\"first_name\", StringType(), nullable=True),\n",
    "    StructField(\"last_name\", StringType(), nullable=True), \n",
    "    StructField(\"email\", StringType(), nullable=True),\n",
    "    StructField(\"phone\", StringType(), nullable=True),\n",
    "    StructField(\"city\", StringType(), nullable=True),\n",
    "    StructField(\"state\", StringType(), nullable=True),\n",
    "    StructField(\"country\", StringType(), nullable=True),\n",
    "    StructField(\"registration_date\", StringType(), nullable=True),  # Date as string, will convert later\n",
    "    StructField(\"customer_segment\", StringType(), nullable=True)\n",
    "])\n",
    "\n",
    "# Statystyki opisowe (count, mean, stddev, min, max)\n",
    "print(\" Statystyki opisowe (describe):\")\n",
    "customers_auto_df.describe().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbd046cf-ee66-4ecb-9ba8-d8291c194052",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rozszerzone statystyki (+ percentyle)\n",
    "print(\"\\n Rozszerzone statystyki (summary):\")\n",
    "customers_auto_df.summary().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10f0f50b-3fa1-452a-85fd-f0df5773f08c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Rozszerzone opcje readera ‚Äì typowe problemy produkcyjne\n",
    "\n",
    "W ≈õrodowisku produkcyjnym czƒôsto spotykamy pliki CSV z:\n",
    "\n",
    "- innym separatorem (`;` zamiast `,`),\n",
    "- cudzys≈Çowami wewnƒÖtrz p√≥l,\n",
    "- uszkodzonymi wierszami.\n",
    "\n",
    "Kluczowe opcje:\n",
    "\n",
    "- `delimiter` ‚Äì niestandardowy separator kolumn,\n",
    "- `quote` ‚Äì znak otwierajƒÖcy/zamykajƒÖcy pola tekstowe,\n",
    "- `escape` ‚Äì spos√≥b ‚Äûucieczki‚Äù znak√≥w specjalnych,\n",
    "- `mode` ‚Äì spos√≥b obs≈Çugi b≈Çƒôdnych rekord√≥w (`PERMISSIVE`, `DROPMALFORMED`, `FAILFAST`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5071f0b-5611-4d63-9e21-3f649dae5ee6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1.4. Rƒôczna definicja schematu dla CSV\n",
    "\n",
    "**Best Practice:** Definiowanie schematu manualnie zapewnia kontrolƒô i wydajno≈õƒá."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0a84ce7-93e5-4da7-8492-e6ca35479b17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1cdaa60-b1b1-46de-8604-7ae9aaa4c320",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Definicja schematu dla customers\n",
    "# Struktura: customer_id (int), first_name (string), last_name (string), email (string), city (string), country (string), registration_date (timestamp)\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), nullable=False),\n",
    "    StructField(\"first_name\", StringType(), nullable=True),\n",
    "    StructField(\"last_name\", StringType(), nullable=True),\n",
    "    StructField(\"city\", StringType(), nullable=True),\n",
    "    StructField(\"email\", StringType(), nullable=True),\n",
    "    StructField(\"country\", StringType(), nullable=True),\n",
    "    StructField(\"registration_date\", TimestampType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2bd147be-df3f-4274-8228-bf8cf6a626ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wczytanie danych CSV z rƒôcznie zdefiniowanym schematem\n",
    "customers_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(customers_schema)  # U≈ºycie zdefiniowanego schematu\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5468f78b-50f0-4032-94af-b1dcba5dd4df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n Pr√≥bka danych z rƒôcznym schematem:\")\n",
    "customers_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1abe1977-dc27-4f2b-8193-5da7611fb9b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚úÖ ƒÜwiczenie 1: Wczytanie CSV z w≈Çasnymi opcjami\n",
    "\n",
    "**Zadanie:** Wczytaj plik CSV `customers.csv` ponownie, ale:\n",
    "- Ustaw delimiter na `;` (celowo b≈Çƒôdny, aby zobaczyƒá efekt)\n",
    "- Wy≈ÇƒÖcz `inferSchema`\n",
    "- Zaobserwuj, jak to wp≈Çywa na wynik\n",
    "\n",
    "**Hint:** U≈ºyj `.option(\"delimiter\", \";\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "355f246d-7ae9-49b7-b552-f2702d99311b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Wczytaj customers.csv z delimitorem `;` i bez inferSchema\n",
    "# customers_wrong_df = spark.read.format(\"csv\")...\n",
    "\n",
    "# TODO: Wy≈õwietl schemat i 5 wierszy\n",
    "# customers_wrong_df.printSchema()\n",
    "# customers_wrong_df.show(5, truncate=False)\n",
    "\n",
    "# Zaobserwuj, ≈ºe wszystkie dane bƒôdƒÖ w jednej kolumnie\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2599e27-eda1-4c27-885d-3e96326d9a6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Czƒô≈õƒá 2: Import danych JSON (Orders)\n",
    "\n",
    "### 2.1. Wczytanie JSON z automatycznym wykrywaniem schematu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "574e1299-3b2b-4ee2-93bb-b2a4e136776e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wczytanie danych JSON z automatycznym wykrywaniem schematu\n",
    "orders_auto_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "print(\" Schemat JSON wykryty automatycznie:\")\n",
    "orders_auto_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed55989b-08d3-4660-a0cc-02cbbfddee90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n Pr√≥bka danych JSON:\")\n",
    "orders_auto_df.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "438958ac-583a-469e-aea4-aafe0780a54e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.2. Eksploracja danych JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17c7d1a3-7296-463d-916b-dfecac27c277",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Liczba kolumn i wierszy\n",
    "print(f\" Liczba kolumn: {len(orders_auto_df.columns)}\")\n",
    "print(f\" Nazwy kolumn: {orders_auto_df.columns}\")\n",
    "print(f\" Liczba wierszy: {orders_auto_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "151a08c4-7463-4c4d-9870-3bafbba9a101",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Typy danych\n",
    "print(\"\\n Typy danych:\")\n",
    "for col_name, col_type in orders_auto_df.dtypes:\n",
    "    print(f\"  - {col_name}: {col_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d607c2-8aa5-4bbe-ad9c-1af6495d66a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.3. Rƒôczna definicja schematu dla JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c826f976-b255-4e73-83fc-bcc6b3f5238b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType, StringType, StructField, StructType, TimestampType, IntegerType\n",
    "\n",
    "# Definicja schematu dla orders\n",
    "# Struktura rzeczywista: order_id, customer_id, product_id, store_id, order_datetime, quantity, unit_price, discount_percent, total_amount, payment_method\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), nullable=True),  # Mo≈ºe byƒá null w danych\n",
    "    StructField(\"customer_id\", StringType(), nullable=False),\n",
    "    StructField(\"product_id\", StringType(), nullable=False),\n",
    "    StructField(\"store_id\", StringType(), nullable=False),\n",
    "    StructField(\"order_datetime\", StringType(), nullable=True),  # String, will convert to timestamp later\n",
    "    StructField(\"quantity\", IntegerType(), nullable=False),\n",
    "    StructField(\"unit_price\", DoubleType(), nullable=False),\n",
    "    StructField(\"discount_percent\", IntegerType(), nullable=False),\n",
    "    StructField(\"total_amount\", DoubleType(), nullable=False),\n",
    "    StructField(\"payment_method\", StringType(), nullable=False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630d2a6d-8307-47ea-8de2-abacfe368d11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), nullable=False),\n",
    "    StructField(\"customer_id\", StringType(), nullable=True),\n",
    "    StructField(\"order_datetime\", TimestampType(), nullable=True),\n",
    "    StructField(\"total_amount\", DoubleType(), nullable=True),\n",
    "    StructField(\"status\", StringType(), nullable=True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a928b826-9d57-43ae-826a-f50d97830be8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wczytanie danych JSON z rƒôcznie zdefiniowanym schematem\n",
    "orders_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .schema(orders_schema)\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "print(\" Schemat JSON zdefiniowany rƒôcznie:\")\n",
    "orders_df.printSchema()\n",
    "\n",
    "print(\"\\n Pr√≥bka danych z rƒôcznym schematem:\")\n",
    "orders_df.display(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69067b3e-a06e-4e93-bdfa-b9bd4cc945cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.4. Statystyki opisowe dla JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f74cfb11-9531-4779-9b4a-2cb6fa8ae265",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Statystyki dla kolumn numerycznych\n",
    "print(\" Statystyki dla orders:\")\n",
    "orders_df.select(\"order_id\", \"customer_id\", \"total_amount\").describe().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74f09cf3-b798-4341-b2a8-8a22b1ca9038",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rozk≈Çad zam√≥wie≈Ñ wed≈Çug metody p≈Çatno≈õci\n",
    "print(\"\\n Rozk≈Çad zam√≥wie≈Ñ wed≈Çug metody p≈Çatno≈õci (payment_method):\")\n",
    "orders_df.groupBy(\"payment_method\").count().orderBy(\"count\", ascending=False).display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f286bb44-3d18-4746-a701-aaea6e6fdaaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Top 10 klient√≥w z najwiƒôkszƒÖ liczbƒÖ zam√≥wie≈Ñ\n",
    "print(\"\\n Top 10 klient√≥w z najwiƒôkszƒÖ liczbƒÖ zam√≥wie≈Ñ:\")\n",
    "orders_auto_df.groupBy(\"customer_id\").count().orderBy(\"count\", ascending=False).limit(10).display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cceece4c-cf6a-4254-95de-300e002e5088",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚úÖ ƒÜwiczenie 2: Eksploracja danych JSON\n",
    "\n",
    "**Zadanie:**\n",
    "1. Policz liczbƒô unikalnych klient√≥w w DataFrame `orders_auto_df`\n",
    "2. Znajd≈∫ warto≈õƒá maksymalnƒÖ i minimalnƒÖ w kolumnie `total_amount`\n",
    "3. Policz liczbƒô zam√≥wie≈Ñ wed≈Çug metody p≈Çatno≈õci (`payment_method`)\n",
    "\n",
    "**Hint:** U≈ºyj `.select()`, `.distinct()`, `.count()`, `.agg()`, `.groupBy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f87ce39c-404a-49aa-9b3f-1926a0270f9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO: Policz liczbƒô unikalnych customer_id\n",
    "# unique_customers = orders_auto_df.select(\"customer_id\").distinct().count()\n",
    "# print(f\"Liczba unikalnych klient√≥w: {unique_customers}\")\n",
    "\n",
    "# TODO: Znajd≈∫ min i max total_amount\n",
    "# from pyspark.sql.functions import min, max\n",
    "# orders_auto_df.select(min(\"total_amount\"), max(\"total_amount\")).display()\n",
    "\n",
    "# TODO: Policz zam√≥wienia wed≈Çug metody p≈Çatno≈õci\n",
    "# orders_auto_df.groupBy(\"payment_method\").count().display()\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9f2041e-010b-42d1-b577-0a31eb841d95",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Czƒô≈õƒá 3: Import danych Parquet (Products)\n",
    "\n",
    "### 3.1. Wczytanie Parquet (schemat wbudowany)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "607e760f-8e10-45a7-9341-59f7da46a630",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Parquet zawiera ju≈º schemat wbudowany - nie trzeba go definiowaƒá\n",
    "products_df = (\n",
    "    spark.read\n",
    "    .format(\"parquet\")\n",
    "    .load(PRODUCTS_PARQUET)\n",
    ")\n",
    "\n",
    "print(\" Schemat Parquet (wbudowany):\")\n",
    "products_df.printSchema()\n",
    "\n",
    "print(\"\\n Pr√≥bka danych Parquet:\")\n",
    "products_df.display(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08f2a68f-7205-49c4-a823-18d44f4d50d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.2. Eksploracja danych Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da237eab-76fe-4a9a-9e89-c98b159d5c96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Podstawowe informacje\n",
    "print(f\" Liczba kolumn: {len(products_df.columns)}\")\n",
    "print(f\" Nazwy kolumn: {products_df.columns}\")\n",
    "print(f\" Liczba produkt√≥w: {products_df.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3483321-4da9-4cd9-975b-5504dd4d9d2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Typy danych\n",
    "print(\"\\n Typy danych:\")\n",
    "for col_name, col_type in products_df.dtypes:\n",
    "    print(f\"  - {col_name}: {col_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d9b0fd-684e-4db2-9b60-95b243b5896d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3.3. Statystyki opisowe dla Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c333b1b-91a9-4da1-b891-c7c650f55e3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Statystyki dla kolumn numerycznych\n",
    "print(\" Statystyki dla products:\")\n",
    "products_df.describe().display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36e56f2d-d3d2-4ca5-9304-2c75230d6cc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Rozszerzone statystyki\n",
    "print(\"\\n Rozszerzone statystyki:\")\n",
    "products_df.summary().display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2904a8b9-de25-499c-a8ea-b52a59b0aa32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### ‚úÖ ƒÜwiczenie 3: Analiza danych Parquet\n",
    "\n",
    "**Zadanie:**\n",
    "1. Znajd≈∫ ≈õredniƒÖ cenƒô produktu (je≈õli kolumna `price` istnieje)\n",
    "2. Znajd≈∫ 5 najdro≈ºszych produkt√≥w\n",
    "3. Policz produkty wed≈Çug kategorii (je≈õli kolumna istnieje)\n",
    "\n",
    "**Hint:** U≈ºyj `.agg()`, `.orderBy()`, `.limit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2e96126-6ca7-4963-bd34-9108e4ad6ff0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg\n",
    "# TODO: Oblicz ≈õredniƒÖ cenƒô\n",
    "products_df.select(avg(\"_____\")).display()\n",
    "\n",
    "# TODO: Znajd≈∫ 5 najdro≈ºszych produkt√≥w\n",
    "products_df.orderBy(col(\"_____\").desc()).limit(5).display()\n",
    "\n",
    "# TODO: Policz produkty wed≈Çug kategorii\n",
    "if \"category\" in products_df.columns:\n",
    "    products_df.groupBy(\"category\").count().display()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bb09b67-4724-4fbc-8d4d-08cb6dad06c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Czƒô≈õƒá 4: Por√≥wnanie wydajno≈õci\n",
    "\n",
    "### 4.1. Wczytanie CSV: inferSchema vs rƒôczny schemat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21c1e752-420b-4f81-99ca-0943eca7cb6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test 1: Automatyczne wykrywanie schematu\n",
    "start_auto = time.time()\n",
    "df_auto = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "count_auto = df_auto.count()  # Action - wymusza wykonanie\n",
    "time_auto = time.time() - start_auto\n",
    "\n",
    "print(f\"[BENCHMARK] Wczytanie CSV z inferSchema: {time_auto:.3f} sekund\")\n",
    "print(f\"[BENCHMARK] Liczba wierszy: {count_auto}\")\n",
    "\n",
    "# Test 2: Rƒôczne zdefiniowanie schematu\n",
    "start_manual = time.time()\n",
    "df_manual = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .schema(customers_schema)\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "count_manual = df_manual.count()  # Action - wymusza wykonanie\n",
    "time_manual = time.time() - start_manual\n",
    "\n",
    "print(f\"\\n[BENCHMARK] Wczytanie CSV z rƒôcznym schematem: {time_manual:.3f} sekund\")\n",
    "print(f\"[BENCHMARK] Liczba wierszy: {count_manual}\")\n",
    "\n",
    "# Por√≥wnanie\n",
    "speedup = (time_auto - time_manual) / time_auto * 100\n",
    "print(f\"\\n[WYNIK] Przyspieszenie: {speedup:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3d4e576-2385-455d-bdce-d782b705729f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Czƒô≈õƒá 5: Best Practices\n",
    "\n",
    "### Zalecenia przy importowaniu danych:\n",
    "\n",
    "1. **Zawsze definiuj schemat rƒôcznie**\n",
    "   - Szybsze wczytywanie\n",
    "   - Kontrola typ√≥w danych\n",
    "   - Unikniƒôcie b≈Çƒôd√≥w typu\n",
    "\n",
    "2. **Wybieraj odpowiedni format**\n",
    "   - **Parquet** - najlepszy dla analityki (kolumnowy, kompresja)\n",
    "   - **CSV** - ≈Çatwy do debugowania, ale wolniejszy\n",
    "   - **JSON** - elastyczny dla danych semi-strukturalnych\n",
    "\n",
    "3. **U≈ºywaj partycjonowania**\n",
    "   - Przyspieszenie zapyta≈Ñ filtrujƒÖcych\n",
    "   - Przyk≈Çad: partycjonowanie zam√≥wie≈Ñ wed≈Çug daty\n",
    "\n",
    "4. **Sprawdzaj jako≈õƒá danych od razu**\n",
    "   - `count()` - sprawd≈∫ liczbƒô wierszy\n",
    "   - `describe()` - sprawd≈∫ rozk≈Çad warto≈õci\n",
    "   - `printSchema()` - zweryfikuj typy\n",
    "\n",
    "5. **U≈ºywaj `limit()` podczas eksperymentowania**\n",
    "   - Przyspiesza iteracje nad kodem\n",
    "   - Przyk≈Çad: `df.limit(1000).display()`\n",
    "\n",
    "6. **Dokumentuj schematy**\n",
    "   - U≈Çatwia utrzymanie kodu\n",
    "   - Przyk≈Çad: komentarze przy definicji StructType\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f118114-afa2-4aa5-b224-caffe767626f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Podsumowanie\n",
    "\n",
    "W tym notebooku nauczy≈Çe≈õ siƒô:\n",
    "\n",
    "‚úÖ **DataFrame Reader API**\n",
    "- Wczytywanie danych z CSV, JSON, Parquet\n",
    "- Konfiguracja opcji (header, inferSchema, delimiter)\n",
    "\n",
    "‚úÖ **Rƒôczna definicja schematu**\n",
    "- Tworzenie schemat√≥w za pomocƒÖ StructType i StructField\n",
    "- Por√≥wnanie wydajno≈õci: inferSchema vs rƒôczny schemat\n",
    "\n",
    "‚úÖ **Operacje eksploracyjne**\n",
    "- Podstawowe operacje: columns, dtypes, count\n",
    "- Statystyki: describe(), summary()\n",
    "- Grupowanie i agregacja\n",
    "\n",
    "‚úÖ **Best Practices**\n",
    "- Zalecenie rƒôcznego definiowania schematu\n",
    "- Wyb√≥r formatu danych dla r√≥≈ºnych scenariuszy\n",
    "- Sprawdzanie jako≈õci danych\n",
    "\n",
    "---\n",
    "\n",
    "### Nastƒôpne kroki:\n",
    "\n",
    "üìå **Kolejny notebook:** `03_basic_transformations_sql_pyspark.ipynb`  \n",
    "üìå **Temat:** Transformacje danych, operacje SQL i PySpark API\n",
    "\n",
    "---\n",
    "\n",
    "## Zasoby dodatkowe\n",
    "\n",
    "- [Databricks - Reading Data](https://docs.databricks.com/ingestion/index.html)\n",
    "- [PySpark DataFrame API](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_data_import_exploration",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}