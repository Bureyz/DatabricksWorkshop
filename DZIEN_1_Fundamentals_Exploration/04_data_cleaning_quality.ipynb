{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d8de6a-6ffc-4a65-a1b0-1890f3387beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Czyszczenie i jakość danych - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Opanowanie technik identyfikacji i rozwiązywania problemów jakości danych, zrozumienie strategii obsługi wartości pustych, walidacji typów, deduplikacji i standaryzacji danych\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Obsługa wartości pustych: fillna(), dropna(), coalesce()\n",
    "- Walidacja typów: cast(), to_date(), to_timestamp()\n",
    "- Deduplikacja: dropDuplicates() - all columns vs key columns\n",
    "- Standardyzacja: formaty dat, tekstów, kategorii\n",
    "- Typowe problemy jakości: whitespace, niepoprawne kody, inconsistent formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d3bbc6e-e99c-4689-8b12-86040c06dba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 1 - Fundamentals & Exploration\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z 2-4 workers\n",
    "- **Czas trwania**: 20 minut\n",
    "- **Prerekvizity**: 03_basic_transformations_sql_pyspark.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e47df0c8-4b57-45d2-bd2f-3a524198eab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie fundamentów data quality i technik czyszczenia danych\n",
    "\n",
    "**Podstawowe pojęcia:**\n",
    "- **Data Quality**: Miara przydatności danych do ich zamierzonego celu\n",
    "- **Data Cleansing**: Proces identyfikacji i korygowania błędów w danych\n",
    "- **Data Validation**: Weryfikacja zgodności danych z regułami biznesowymi\n",
    "- **Data Standardization**: Ujednolicenie formatów i reprezentacji danych\n",
    "- **Data Profiling**: Analiza struktury, zawartości i relacji w danych\n",
    "\n",
    "**Wymiary jakości danych:**\n",
    "- **Completeness**: Czy wszystkie wymagane dane są obecne\n",
    "- **Accuracy**: Czy dane są poprawne i zgodne z rzeczywistością\n",
    "- **Consistency**: Czy dane są spójne w całym systemie\n",
    "- **Timeliness**: Czy dane są aktualne\n",
    "- **Validity**: Czy dane spełniają reguły biznesowe\n",
    "- **Uniqueness**: Czy rekordy są unikalne\n",
    "\n",
    "**Dlaczego to ważne?**\n",
    "Niska jakość danych prowadzi do błędnych analiz, złych decyzji biznesowych i utraty zaufania do systemu. Czyszczenie danych to często 60-80% czasu w projektach data engineering. Systematyczne podejście do data quality zapewnia wiarygodność całego pipeline'u danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae724e7-9abf-4a35-80a5-08e9c8e9b55c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Izolacja per użytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d33558-b424-4e25-ab88-4da72280f040",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "497f6109-4d20-48b3-b7be-dd8f207a5109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b15728b9-230e-458c-b7fe-84a7c8b27798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Wyświetl kontekst użytkownika (zmienne z 00_setup)\n",
    "print(\"=== Kontekst użytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"Użytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog i schemat jako domyślne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "print(\"\\n=== Konfiguracja zakończona pomyślnie ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c8aaa9-b9c7-43fe-a718-21cd1f96d858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 1: Wczytanie danych z dataset\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "W tym notebooku wykorzystujemy pliki z lokalnego folderu `dataset/` (Dzień 1-2 szkolenia). Pliki CSV, JSON i Parquet są ładowane bezpośrednio z systemu plików używając ścieżki `DATASET_BASE_PATH` z `00_setup.ipynb`.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **DATASET_BASE_PATH**: Ścieżka do folderu dataset/ zdefiniowana w 00_setup.ipynb\n",
    "- **CSV Reader**: spark.read.format(\"csv\") z opcjami (header, inferSchema)\n",
    "- **Schema inference**: Automatyczne wykrywanie typów vs jawna definicja schematu\n",
    "- **File paths**: Absolutne ścieżki do plików w lokalnym systemie\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Wczytywanie raw files z folderu dataset/\n",
    "- Eksploracja i profilowanie danych przed czyszczeniem\n",
    "- Przygotowanie do załadowania do Delta tables (Bronze layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2ee3ace-8a8b-4fa9-bad8-04b0f4e2418d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.1: Wczytanie danych customers z dataset\n",
    "\n",
    "**Cel:** Załadowanie danych klientów z pliku CSV przechowywanego w folderze dataset/\n",
    "\n",
    "**Podejście:**\n",
    "1. Zdefiniowanie ścieżki używając DATASET_BASE_PATH z 00_setup.ipynb\n",
    "2. Wczytanie CSV z opcjami (header, inferSchema)\n",
    "3. Podstawowa eksploracja załadowanych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84389d0f-05ab-4c08-a3a7-099069cb1c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: CSV file: {DATASET_BASE_PATH}/customers/customers.csv\n",
    "# VARIABLE: df_customers - DataFrame z danymi klientów\n",
    "\n",
    "# Ścieżka do pliku w dataset\n",
    "customers_path = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "\n",
    "# Wczytanie danych\n",
    "df_customers = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(customers_path)\n",
    "\n",
    "# Podstawowa eksploracja\n",
    "print(\"=== Dane klientów załadowane ===\")\n",
    "print(f\"Liczba rekordów: {df_customers.count()}\")\n",
    "print(f\"Liczba kolumn: {len(df_customers.columns)}\")\n",
    "print(f\"\\nKolumny: {df_customers.columns}\")\n",
    "\n",
    "# Schemat danych\n",
    "print(\"\\n=== Schemat danych ===\")\n",
    "df_customers.printSchema()\n",
    "\n",
    "# Podgląd pierwszych rekordów\n",
    "print(\"\\n=== Pierwsze 10 rekordów ===\")\n",
    "display(df_customers.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c54d00ee-c148-4ff8-b1a9-5b0cc29f4610",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Ścieżka do pliku używa zmiennej `DATASET_BASE_PATH` zdefiniowanej w `00_setup.ipynb`, która wskazuje na folder `dataset/` w workspace. Opcja `inferSchema=true` automatycznie wykrywa typy danych, co jest wygodne dla eksploracji, ale w produkcji zaleca się jawne definiowanie schematów dla wydajności i kontroli."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fcc0ef1-e096-4e35-95fe-92df5681abc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.2: Data Profiling - analiza jakości danych\n",
    "\n",
    "**Cel:** Identyfikacja problemów jakości w załadowanych danych przed rozpoczęciem czyszczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c72c6a73-48b0-4712-b96c-fce6312339e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: profiling_report - dict ze statystykami jakości\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY PROFILING REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Completeness - Analiza wartości null\n",
    "print(\"\\n1. COMPLETENESS - Wartości null per kolumna:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for col_name in df_customers.columns:\n",
    "    null_count = df_customers.filter(F.col(col_name).isNull()).count()\n",
    "    total_count = df_customers.count()\n",
    "    null_pct = (null_count / total_count) * 100\n",
    "    print(f\"  {col_name:20s}: {null_count:4d} nulls ({null_pct:5.1f}%)\")\n",
    "\n",
    "# 2. Uniqueness - Analiza duplikatów\n",
    "print(\"\\n2. UNIQUENESS - Analiza duplikatów:\")\n",
    "print(\"-\" * 80)\n",
    "total_rows = df_customers.count()\n",
    "unique_rows = df_customers.distinct().count()\n",
    "duplicate_rows = total_rows - unique_rows\n",
    "print(f\"  Total rows: {total_rows}\")\n",
    "print(f\"  Unique rows: {unique_rows}\")\n",
    "print(f\"  Duplicate rows: {duplicate_rows}\")\n",
    "print(f\"  Duplication rate: {(duplicate_rows/total_rows)*100:.1f}%\")\n",
    "\n",
    "# 3. Consistency - Analiza wartości unikalnych w kluczowych kolumnach\n",
    "print(\"\\n3. CONSISTENCY - Wartości unikalne:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for col_name in df_customers.columns:\n",
    "    distinct_count = df_customers.select(col_name).distinct().count()\n",
    "    print(f\"  {col_name:20s}: {distinct_count:4d} unikalnych wartości\")\n",
    "\n",
    "# 4. Accuracy - Przykładowe wartości\n",
    "print(\"\\n4. ACCURACY - Przykładowe wartości (pierwsze 5):\")\n",
    "print(\"-\" * 80)\n",
    "display(df_customers.limit(5))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROFILING COMPLETED\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c334e71-0521-44f6-a728-8306e58fddff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 2: Obsługa wartości pustych\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Brakujące wartości są jednym z najczęstszych problemów jakości danych. Strategia obsługi null values zależy od kontekstu biznesowego i charakteru danych. Niepoprawna obsługa może prowadzić do błędów w analizach i modelach ML.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **fillna()**: Wypełnienie wartości null określoną wartością lub strategią\n",
    "- **dropna()**: Usunięcie rekordów zawierających wartości null\n",
    "- **coalesce()**: Wybór pierwszej niepustej wartości z wielu kolumn\n",
    "- **Imputation**: Statystyczne metody wypełniania (mean, median, mode)\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Uzupełnianie brakujących wartości sensownymi defaultami\n",
    "- Usuwanie rekordów z krytycznymi brakującymi danymi\n",
    "- Fallback do alternatywnych źródeł danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b239f4ac-bd75-41a3-85f6-e6c8bdcbeda0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.1: Wypełnianie wartości null (fillna)\n",
    "\n",
    "**Cel:** Uzupełnienie brakujących wartości w kolumnach różnymi strategiami\n",
    "\n",
    "**Podejście:**\n",
    "1. Identyfikacja kolumn z null values\n",
    "2. Wybór odpowiedniej strategii per kolumna\n",
    "3. Zastosowanie fillna() z słownikiem wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f0e7095-14bf-4600-8e87-7d5280d444b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_filled - DataFrame z wypełnionymi wartościami null\n",
    "\n",
    "# Strategia wypełniania wartości null - tylko dla kolumn które rzeczywiście mogą mieć nulls\n",
    "fill_values = {\n",
    "    \"phone\": \"brak telefonu\",\n",
    "    \"city\": \"Unknown\",\n",
    "    \"state\": \"Unknown\",\n",
    "    \"country\": \"Unknown\"\n",
    "}\n",
    "\n",
    "# Wypełnienie wartości null\n",
    "df_filled = df_customers.fillna(fill_values)\n",
    "\n",
    "# Weryfikacja zmian\n",
    "print(\"=== Porównanie przed i po fillna ===\")\n",
    "for col_name in fill_values.keys():\n",
    "    before_nulls = df_customers.filter(F.col(col_name).isNull()).count()\n",
    "    after_nulls = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "    print(f\"{col_name:15s}: {before_nulls:3d} nulls → {after_nulls:3d} nulls\")\n",
    "\n",
    "# Przykładowe rekordy po wypełnieniu\n",
    "print(\"\\n=== Przykładowe wypełnione rekordy ===\")\n",
    "display(df_filled.filter(\n",
    "    df_customers[\"phone\"].isNull() | \n",
    "    df_customers[\"city\"].isNull() |\n",
    "    df_customers[\"state\"].isNull()\n",
    ").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e1ca53-8820-4535-85cf-4fc3239f11f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.2: Usuwanie rekordów z wartościami null (dropna)\n",
    "\n",
    "**Cel:** Usunięcie rekordów z brakującymi wartościami w kluczowych kolumnach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b54ac61-9b65-43ca-9594-c351604ac707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_valid - DataFrame z usuniętymi rekordami z null w kluczowych kolumnach\n",
    "\n",
    "# Usuń rekordy bez kluczowych informacji (customer_id jest wymagane)\n",
    "df_valid = df_customers.dropna(subset=[\"customer_id\"])\n",
    "\n",
    "# Weryfikacja zmian\n",
    "print(\"=== Porównanie przed i po dropna ===\")\n",
    "print(f\"Liczba rekordów PRZED: {df_customers.count()}\")\n",
    "print(f\"Liczba rekordów PO: {df_valid.count()}\")\n",
    "print(f\"Usunięto rekordów: {df_customers.count() - df_valid.count()}\")\n",
    "\n",
    "# Sprawdzenie, czy są jeszcze null w customer_id\n",
    "null_ids = df_valid.filter(F.col(\"customer_id\").isNull()).count()\n",
    "print(f\"\\nNull w customer_id po dropna: {null_ids}\")\n",
    "\n",
    "# Alternatywne użycie: dropna z how='all' (usuwa tylko jeśli wszystkie kolumny są null)\n",
    "df_any_data = df_customers.dropna(how='all')\n",
    "print(f\"\\nRekordy z jakimikolwiek danymi: {df_any_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "771829fd-0deb-480a-8612-22d6e0681da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.3: Wybór pierwszej niepustej wartości (coalesce)\n",
    "\n",
    "**Cel:** Użycie coalesce() do fallback między alternatywnymi źródłami danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5561f47d-aae3-46f0-88b9-d0228a706693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_with_contact - DataFrame z nową kolumną primary_contact\n",
    "\n",
    "from pyspark.sql.functions import coalesce, lit\n",
    "\n",
    "# Przykład: Stwórz primary_contact wybierając pierwszą niepustą wartość\n",
    "df_with_contact = df_customers.withColumn(\n",
    "    \"primary_contact\",\n",
    "    coalesce(F.col(\"email\"), F.col(\"phone\"), lit(\"brak kontaktu\"))\n",
    ")\n",
    "\n",
    "# Stwórz pełny adres z dostępnych pól\n",
    "df_with_contact = df_with_contact.withColumn(\n",
    "    \"full_address\",\n",
    "    coalesce(\n",
    "        F.concat_ws(\", \", F.col(\"city\"), F.col(\"state\"), F.col(\"country\")),\n",
    "        F.concat_ws(\", \", F.col(\"city\"), F.col(\"country\")),\n",
    "        F.col(\"country\"),\n",
    "        lit(\"Address Unknown\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Weryfikacja\n",
    "print(\"=== Analiza primary_contact ===\")\n",
    "contact_stats = df_with_contact.groupBy(\"primary_contact\").count().orderBy(F.desc(\"count\"))\n",
    "display(contact_stats.limit(10))\n",
    "\n",
    "# Przykłady\n",
    "print(\"\\n=== Przykładowe rekordy z primary_contact i full_address ===\")\n",
    "display(df_with_contact.select(\"customer_id\", \"email\", \"phone\", \"primary_contact\", \"city\", \"state\", \"country\", \"full_address\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50d87ed-2e88-41e8-a281-7aadc227e06a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 3: Walidacja i konwersja typów danych\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Niepoprawne typy danych są częstym problemem podczas ładowania danych z plików tekstowych (CSV, JSON). Konwersje typu muszą być wykonane bezpiecznie z obsługą błędów, aby uniknąć utraty danych lub niepoprawnych wyników analiz.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **cast()**: Konwersja typu danych (string → int, date, timestamp)\n",
    "- **to_date()**: Parsowanie stringów na DateType z określeniem formatu\n",
    "- **to_timestamp()**: Parsowanie stringów na TimestampType\n",
    "- **try_cast()**: Bezpieczna konwersja zwracająca null przy błędzie (Spark 3.4+)\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Walidacja typów po załadowaniu CSV z inferSchema\n",
    "- Parsowanie dat w niestandardowych formatach\n",
    "- Konwersja typów przed złączeniami i agregacjami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e057d71-27de-46e4-8351-bc0043852b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.1: Konwersja typów numerycznych (cast)\n",
    "\n",
    "**Cel:** Bezpieczna konwersja stringów na typy numeryczne z walidacją\n",
    "\n",
    "**Podejście:**\n",
    "1. Czyszczenie wartości przed konwersją (usunięcie znaków specjalnych)\n",
    "2. Konwersja typu używając cast()\n",
    "3. Walidacja zakresu wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "133bf1e7-20ba-4c5d-bf39-96d20bc625a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_typed - DataFrame z poprawnie skonwertowanymi typami\n",
    "\n",
    "# Przykład: Walidacja dat rejestracji i dodanie flag jakości\n",
    "df_typed = df_customers\n",
    "\n",
    "# Parsowanie registration_date i walidacja zakresu\n",
    "df_typed = df_typed.withColumn(\n",
    "    \"registration_date_parsed\",\n",
    "    F.to_date(F.col(\"registration_date\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "# Walidacja zakresu dat (2020-2026)\n",
    "df_typed = df_typed.withColumn(\n",
    "    \"registration_date_valid\",\n",
    "    (F.col(\"registration_date_parsed\").isNotNull()) & \n",
    "    (F.col(\"registration_date_parsed\") >= \"2020-01-01\") & \n",
    "    (F.col(\"registration_date_parsed\") <= \"2026-12-31\")\n",
    ")\n",
    "\n",
    "# Dodaj wiek konta w dniach\n",
    "df_typed = df_typed.withColumn(\n",
    "    \"account_age_days\",\n",
    "    F.datediff(F.current_date(), F.col(\"registration_date_parsed\"))\n",
    ")\n",
    "\n",
    "# Walidacja email format\n",
    "df_typed = df_typed.withColumn(\n",
    "    \"email_valid\",\n",
    "    (F.col(\"email\").isNotNull()) & \n",
    "    F.col(\"email\").rlike(\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\")\n",
    ")\n",
    "\n",
    "# Statystyki konwersji\n",
    "total = df_typed.count()\n",
    "valid_dates = df_typed.filter(F.col(\"registration_date_valid\") == True).count()\n",
    "invalid_dates = df_typed.filter(F.col(\"registration_date_valid\") == False).count()\n",
    "valid_emails = df_typed.filter(F.col(\"email_valid\") == True).count()\n",
    "\n",
    "print(\"=== Statystyki walidacji ===\")\n",
    "print(f\"Total: {total}\")\n",
    "print(f\"Valid registration dates: {valid_dates} ({(valid_dates/total)*100:.1f}%)\")\n",
    "print(f\"Invalid registration dates: {invalid_dates} ({(invalid_dates/total)*100:.1f}%)\")\n",
    "print(f\"Valid emails: {valid_emails} ({(valid_emails/total)*100:.1f}%)\")\n",
    "\n",
    "# Przykłady nieprawidłowych wartości\n",
    "if invalid_dates > 0:\n",
    "    print(\"\\n=== Przykłady nieprawidłowych dat rejestracji ===\")\n",
    "    display(df_typed.filter(F.col(\"registration_date_valid\") == False).select(\"customer_id\", \"registration_date\", \"registration_date_parsed\", \"registration_date_valid\").limit(5))\n",
    "\n",
    "# Schemat po konwersji\n",
    "print(\"\\n=== Schemat po konwersji ===\")\n",
    "df_typed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "993abebe-2a10-4750-94e4-9edd132fccc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.2: Konwersja dat (to_date)\n",
    "\n",
    "**Cel:** Parsowanie stringów na DateType z obsługą wielu formatów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c49468-ea46-4f0d-9d28-690643fa3248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_with_dates - DataFrame z poprawnie sparsowanymi datami\n",
    "\n",
    "# Przykład: Konwersja kolumny registration_date (string) na date\n",
    "# Strategia: Próbuj różne popularne formaty używając coalesce()\n",
    "\n",
    "df_with_dates = df_customers.withColumn(\n",
    "    \"registration_date_parsed\",\n",
    "    coalesce(\n",
    "        F.to_date(F.col(\"registration_date\"), \"yyyy-MM-dd\"),     # Format: 2024-01-15\n",
    "        F.to_date(F.col(\"registration_date\"), \"dd/MM/yyyy\"),     # Format: 15/01/2024\n",
    "        F.to_date(F.col(\"registration_date\"), \"MM-dd-yyyy\"),     # Format: 01-15-2024\n",
    "        F.to_date(F.col(\"registration_date\"), \"yyyy.MM.dd\"),     # Format: 2024.01.15\n",
    "        F.to_date(F.col(\"registration_date\"))                    # Automatyczne wykrywanie\n",
    "    )\n",
    ")\n",
    "\n",
    "# Walidacja konwersji\n",
    "total = df_with_dates.count()\n",
    "parsed = df_with_dates.filter(F.col(\"registration_date_parsed\").isNotNull()).count()\n",
    "failed = df_with_dates.filter(\n",
    "    F.col(\"registration_date\").isNotNull() & \n",
    "    F.col(\"registration_date_parsed\").isNull()\n",
    ").count()\n",
    "\n",
    "print(\"=== Statystyki konwersji dat ===\")\n",
    "print(f\"Total: {total}\")\n",
    "print(f\"Poprawnie sparsowane: {parsed} ({(parsed/total)*100:.1f}%)\")\n",
    "print(f\"Nie udało się sparsować: {failed} ({(failed/total)*100:.1f}%)\")\n",
    "\n",
    "# Przykłady konwersji\n",
    "print(\"\\n=== Przykładowe konwersje dat ===\")\n",
    "display(df_with_dates.select(\"customer_id\", \"registration_date\", \"registration_date_parsed\").limit(10))\n",
    "\n",
    "# Rekordy z błędami parsowania\n",
    "if failed > 0:\n",
    "    print(\"\\n=== Rekordy z błędami parsowania ===\")\n",
    "    display(df_with_dates.filter(\n",
    "        F.col(\"registration_date\").isNotNull() & \n",
    "        F.col(\"registration_date_parsed\").isNull()\n",
    "    ).select(\"customer_id\", \"registration_date\", \"registration_date_parsed\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09bd7ce6-fbf1-44d0-8f29-d1835d3bbfa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.3: Konwersja timestamp i obliczenia czasowe\n",
    "\n",
    "**Cel:** Konwersja na timestamp i wykonanie obliczeń czasowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f3b5d8-50c6-46fe-b0cf-b95d13fde87a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_with_dates\n",
    "# VARIABLE: df_with_timestamp - DataFrame z timestamp i obliczeniami\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp, current_timestamp, current_date, datediff\n",
    "\n",
    "# Konwersja registration_date_parsed na timestamp (dodaje czas 00:00:00)\n",
    "df_with_timestamp = df_with_dates.withColumn(\n",
    "    \"registration_timestamp\",\n",
    "    F.to_timestamp(F.col(\"registration_date_parsed\"))\n",
    ")\n",
    "\n",
    "# Obliczenia czasowe\n",
    "df_with_timestamp = df_with_timestamp \\\n",
    "    .withColumn(\"current_date\", current_date()) \\\n",
    "    .withColumn(\"days_since_registration\", \n",
    "        datediff(F.col(\"current_date\"), F.col(\"registration_date_parsed\"))\n",
    "    )\n",
    "\n",
    "# Statystyki\n",
    "print(\"=== Statystyki czasowe ===\")\n",
    "df_with_timestamp.select(\n",
    "    F.min(\"days_since_registration\").alias(\"min_days\"),\n",
    "    F.max(\"days_since_registration\").alias(\"max_days\"),\n",
    "    F.avg(\"days_since_registration\").alias(\"avg_days\")\n",
    ").show()\n",
    "\n",
    "# Przykłady\n",
    "print(\"\\n=== Przykładowe obliczenia czasowe ===\")\n",
    "display(df_with_timestamp.select(\n",
    "    \"customer_id\",\n",
    "    \"registration_date\",\n",
    "    \"registration_date_parsed\",\n",
    "    \"registration_timestamp\",\n",
    "    \"days_since_registration\"\n",
    ").orderBy(F.desc(\"days_since_registration\")).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d796273-f020-436e-bb3c-9ce06ae27354",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 4: Deduplikacja danych\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Duplikaty są powszechnym problemem jakości danych wynikającym z błędów w systemach źródłowych, wielokrotnego ładowania tych samych danych lub błędów w procesach ETL. Strategia deduplikacji zależy od kontekstu biznesowego.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **dropDuplicates()**: Usunięcie duplikatów na podstawie wszystkich lub wybranych kolumn\n",
    "- **Exact duplicates**: Rekordy identyczne we wszystkich kolumnach\n",
    "- **Key duplicates**: Rekordy z tym samym kluczem biznesowym\n",
    "- **Deduplication strategy**: Którą wersję rekordu zachować (first, last, newest)\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Usuwanie duplikatów po key columns (customer_id)\n",
    "- Zachowanie najnowszej wersji rekordu (last insert wins)\n",
    "- Identyfikacja potencjalnych duplikatów (fuzzy matching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "338e49bb-24f7-4612-a0e5-5320428ee8e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.1: Deduplikacja - wszystkie kolumny\n",
    "\n",
    "**Cel:** Usunięcie rekordów całkowicie identycznych (exact duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc07a589-27b8-4ccf-8bf3-e2ed1e0094b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_distinct - DataFrame bez dokładnych duplikatów\n",
    "\n",
    "# Usunięcie exact duplicates (wszystkie kolumny identyczne)\n",
    "df_distinct = df_customers.distinct()\n",
    "\n",
    "# Statystyki\n",
    "total = df_customers.count()\n",
    "distinct = df_distinct.count()\n",
    "duplicates = total - distinct\n",
    "\n",
    "print(\"=== Deduplikacja - wszystkie kolumny ===\")\n",
    "print(f\"Total rekordów: {total}\")\n",
    "print(f\"Unikalne rekordy: {distinct}\")\n",
    "print(f\"Usunięte duplikaty: {duplicates}\")\n",
    "print(f\"Duplication rate: {(duplicates/total)*100:.1f}%\")\n",
    "\n",
    "# Identyfikacja duplikatów przed usunięciem\n",
    "from pyspark.sql.functions import count as spark_count\n",
    "\n",
    "duplicated_records = df_customers \\\n",
    "    .groupBy(df_customers.columns) \\\n",
    "    .agg(spark_count(\"*\").alias(\"count\")) \\\n",
    "    .filter(F.col(\"count\") > 1) \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "if duplicated_records.count() > 0:\n",
    "    print(\"\\n=== Przykłady zduplikowanych rekordów ===\")\n",
    "    display(duplicated_records.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1711638e-b2b3-4c65-987a-925ff3176546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.2: Deduplikacja per key columns\n",
    "\n",
    "**Cel:** Usunięcie duplikatów na podstawie klucza biznesowego (customer_id), zachowując najnowszy rekord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "697fb1a6-1703-483e-b4d9-355f8fc2e1a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_deduped - DataFrame z usuniętymi duplikatami per customer_id\n",
    "\n",
    "# Strategia 1: dropDuplicates() - zachowuje pierwszy napotkany rekord\n",
    "df_deduped_simple = df_customers.dropDuplicates([\"customer_id\"])\n",
    "\n",
    "print(\"=== Deduplikacja per customer_id (strategia simple) ===\")\n",
    "print(f\"Przed: {df_customers.count()} rekordów\")\n",
    "print(f\"Po: {df_deduped_simple.count()} rekordów\")\n",
    "print(f\"Usunięto: {df_customers.count() - df_deduped_simple.count()} duplikatów\")\n",
    "\n",
    "# Strategia 2: Window function - zachowaj najnowszy rekord (jeśli mamy timestamp)\n",
    "# Zakładamy, że mamy kolumnę created_at lub inny timestamp\n",
    "\n",
    "if \"created_at\" in df_customers.columns or \"last_updated\" in df_customers.columns:\n",
    "    from pyspark.sql.window import Window\n",
    "    \n",
    "    timestamp_col = \"created_at\" if \"created_at\" in df_customers.columns else \"last_updated\"\n",
    "    \n",
    "    # Okno partycjonowane po customer_id, sortowane po timestamp desc\n",
    "    window_spec = Window.partitionBy(\"customer_id\").orderBy(F.desc(timestamp_col))\n",
    "    \n",
    "    df_deduped = df_customers \\\n",
    "        .withColumn(\"row_num\", F.row_number().over(window_spec)) \\\n",
    "        .filter(F.col(\"row_num\") == 1) \\\n",
    "        .drop(\"row_num\")\n",
    "    \n",
    "    print(f\"\\n=== Deduplikacja per customer_id (strategia: najnowszy rekord) ===\")\n",
    "    print(f\"Przed: {df_customers.count()} rekordów\")\n",
    "    print(f\"Po: {df_deduped.count()} rekordów\")\n",
    "    print(f\"Usunięto: {df_customers.count() - df_deduped.count()} duplikatów\")\n",
    "else:\n",
    "    # Jeśli brak timestamp, użyj prostej strategii\n",
    "    df_deduped = df_deduped_simple\n",
    "    print(\"\\n(Brak kolumny timestamp - użyto prostej strategii)\")\n",
    "\n",
    "# Identyfikacja duplikatów przed usunięciem\n",
    "duplicate_ids = df_customers \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(spark_count(\"*\").alias(\"count\")) \\\n",
    "    .filter(F.col(\"count\") > 1) \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "if duplicate_ids.count() > 0:\n",
    "    print(f\"\\n=== {duplicate_ids.count()} customer_id z duplikatami ===\")\n",
    "    display(duplicate_ids.limit(10))\n",
    "    \n",
    "    # Przykłady duplikatów\n",
    "    sample_duplicate_id = duplicate_ids.first()[\"customer_id\"]\n",
    "    print(f\"\\n=== Przykład duplikatów dla customer_id={sample_duplicate_id} ===\")\n",
    "    display(df_customers.filter(F.col(\"customer_id\") == sample_duplicate_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a564b0b-a641-440e-87b7-39e634d32224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 5: Standardyzacja danych\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Standaryzacja polega na ujednoliceniu formatów i reprezentacji danych zgodnie z ustalonymi regułami biznesowymi. Niestandardowe dane (różne case, whitespace, formaty) utrudniają analizy, złączenia i agregacje.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **Text standardization**: trim(), lower(), upper(), initcap()\n",
    "- **Pattern standardization**: regexp_replace() dla kodów, telefonów\n",
    "- **Format standardization**: Ujednolicenie formatów dat, adresów\n",
    "- **Categorical standardization**: Mapowanie wariantów na standardowe wartości\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Ujednolicenie wielkości liter w polach tekstowych\n",
    "- Usunięcie whitespace z początku i końca\n",
    "- Standaryzacja kodów krajów, telefonów, kodów pocztowych\n",
    "- Konsolidacja wariantów kategorii (Active/active/ACTIVE → Active)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80f07b2-120c-4d91-a4af-cfcb6a264dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 5.1: Standaryzacja tekstu\n",
    "\n",
    "**Cel:** Oczyszczenie i standaryzacja pól tekstowych (trim, case, whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3edc8e5-f85a-4676-af98-ef0411ba67ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_standardized - DataFrame ze standaryzowanymi polami tekstowymi\n",
    "\n",
    "# Standaryzacja pól tekstowych\n",
    "df_standardized = df_customers\n",
    "\n",
    "# 1. Trim whitespace z wszystkich string columns\n",
    "for col_name in [\"first_name\", \"last_name\", \"email\", \"phone\", \"city\", \"state\", \"country\"]:\n",
    "    if col_name in df_standardized.columns:\n",
    "        df_standardized = df_standardized.withColumn(\n",
    "            col_name,\n",
    "            F.trim(F.col(col_name))\n",
    "        )\n",
    "\n",
    "# 2. Standaryzacja konkretnych kolumn\n",
    "# first_name, last_name: Title Case\n",
    "df_standardized = df_standardized.withColumn(\n",
    "    \"first_name\",\n",
    "    F.initcap(F.col(\"first_name\"))\n",
    ").withColumn(\n",
    "    \"last_name\", \n",
    "    F.initcap(F.col(\"last_name\"))\n",
    ")\n",
    "\n",
    "# email: lowercase (standard dla emaili)\n",
    "df_standardized = df_standardized.withColumn(\n",
    "    \"email\",\n",
    "    F.lower(F.col(\"email\"))\n",
    ")\n",
    "\n",
    "# country: uppercase (standard ISO dla kodów krajów)\n",
    "df_standardized = df_standardized.withColumn(\n",
    "    \"country\",\n",
    "    F.upper(F.col(\"country\"))\n",
    ")\n",
    "\n",
    "# city: Title Case\n",
    "df_standardized = df_standardized.withColumn(\n",
    "    \"city\",\n",
    "    F.initcap(F.col(\"city\"))\n",
    ")\n",
    "\n",
    "# Porównanie przed i po\n",
    "print(\"=== Porównanie standaryzacji ===\")\n",
    "display(df_customers.select(\"first_name\", \"last_name\", \"email\", \"city\", \"country\").limit(5))\n",
    "print(\"\\n↓↓↓ PO STANDARYZACJI ↓↓↓\\n\")\n",
    "display(df_standardized.select(\"first_name\", \"last_name\", \"email\", \"city\", \"country\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68584674-f31c-4315-b93f-6027094167c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 5.2: Standaryzacja kodów i kategorii\n",
    "\n",
    "**Cel:** Ujednolicenie formatów kodów i mapowanie wariantów kategorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "168546b9-4a72-4645-a8a3-20dfda790e0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_standardized\n",
    "# VARIABLE: df_codes_standardized - DataFrame ze standaryzowanymi kodami\n",
    "\n",
    "df_codes_standardized = df_standardized\n",
    "\n",
    "# 1. Standaryzacja phone numbers (usunięcie wszystkich non-digit, format międzynarodowy)\n",
    "if \"phone\" in df_codes_standardized.columns:\n",
    "    df_codes_standardized = df_codes_standardized.withColumn(\n",
    "        \"phone_standardized\",\n",
    "        F.when(F.col(\"phone\").isNotNull(),\n",
    "            F.concat(\n",
    "                F.when(F.col(\"phone\").startswith(\"+\"), \"\")\n",
    "                 .otherwise(\"+1-\"),  # Domyślny prefiks dla USA\n",
    "                F.regexp_replace(F.col(\"phone\"), \"[^0-9]\", \"\")\n",
    "            )\n",
    "        ).otherwise(F.col(\"phone\"))\n",
    "    )\n",
    "\n",
    "# 2. Standaryzacja customer_segment (konsystentne nazewnictwo)\n",
    "if \"customer_segment\" in df_codes_standardized.columns:\n",
    "    df_codes_standardized = df_codes_standardized.withColumn(\n",
    "        \"customer_segment_standardized\",\n",
    "        F.when(F.upper(F.trim(F.col(\"customer_segment\"))) == \"PREMIUM\", \"Premium\")\n",
    "         .when(F.upper(F.trim(F.col(\"customer_segment\"))) == \"STANDARD\", \"Standard\")\n",
    "         .when(F.upper(F.trim(F.col(\"customer_segment\"))) == \"BASIC\", \"Basic\")\n",
    "         .otherwise(\"Unknown\")\n",
    "    )\n",
    "\n",
    "# 3. Standaryzacja country codes (3-letter ISO codes jako przykład)\n",
    "df_codes_standardized = df_codes_standardized.withColumn(\n",
    "    \"country_iso\",\n",
    "    F.when(F.upper(F.col(\"country\")) == \"USA\", \"USA\")\n",
    "     .when(F.upper(F.col(\"country\")) == \"POLAND\", \"POL\")\n",
    "     .when(F.upper(F.col(\"country\")) == \"GERMANY\", \"DEU\")\n",
    "     .when(F.upper(F.col(\"country\")) == \"FRANCE\", \"FRA\")\n",
    "     .otherwise(F.upper(F.col(\"country\")))\n",
    ")\n",
    "\n",
    "# Weryfikacja standaryzacji\n",
    "print(\"=== Standaryzacja kodów i kategorii ===\")\n",
    "\n",
    "if \"phone\" in df_codes_standardized.columns:\n",
    "    print(\"\\n--- Phone numbers ---\")\n",
    "    display(df_codes_standardized.select(\"phone\", \"phone_standardized\").limit(5))\n",
    "\n",
    "if \"customer_segment\" in df_codes_standardized.columns:\n",
    "    print(\"\\n--- Customer segments ---\")\n",
    "    display(df_codes_standardized.groupBy(\"customer_segment\", \"customer_segment_standardized\").count().orderBy(\"customer_segment\"))\n",
    "\n",
    "print(\"\\n--- Country codes ---\")\n",
    "display(df_codes_standardized.groupBy(\"country\", \"country_iso\").count().orderBy(\"country\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23750ae6-0a82-46fe-826e-5c6c44877b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Porównanie PySpark vs SQL\n",
    "\n",
    "**DataFrame API (PySpark):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b7ebef-9ca4-4017-8dd5-38e345a53866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład: Kompleksowe czyszczenie danych w PySpark\n",
    "\n",
    "df_cleaned_pyspark = df_customers \\\n",
    "    .fillna({\"city\": \"Unknown\", \"country\": \"USA\"}) \\\n",
    "    .dropna(subset=[\"customer_id\"]) \\\n",
    "    .withColumn(\"first_name\", F.trim(F.initcap(F.col(\"first_name\")))) \\\n",
    "    .withColumn(\"last_name\", F.trim(F.initcap(F.col(\"last_name\")))) \\\n",
    "    .withColumn(\"email\", F.lower(F.trim(F.col(\"email\")))) \\\n",
    "    .dropDuplicates([\"customer_id\"])\n",
    "\n",
    "print(f\"Rekordy po czyszczeniu (PySpark): {df_cleaned_pyspark.count()}\")\n",
    "display(df_cleaned_pyspark.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb295ae-7f38-4e70-9c94-ffd19771a918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SQL Equivalent:**\n",
    "\n",
    "```sql\n",
    "# Najpierw utwórz temporary view\n",
    "df_customers.createOrReplaceTempView(\"customers_raw\")\n",
    "\n",
    "# SQL approach\n",
    "df_cleaned_sql = spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "    customer_id,\n",
    "    TRIM(INITCAP(first_name)) as first_name,\n",
    "    TRIM(INITCAP(last_name)) as last_name,\n",
    "    LOWER(TRIM(email)) as email,\n",
    "    COALESCE(city, 'Unknown') as city,\n",
    "    COALESCE(country, 'Unknown') as country,\n",
    "    phone,\n",
    "    registration_date,\n",
    "    customer_segment\n",
    "FROM customers_raw\n",
    "WHERE customer_id IS NOT NULL\n",
    "    AND email IS NOT NULL\n",
    "    AND email LIKE '%@%'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Rekordy po czyszczeniu (SQL): {df_cleaned_sql.count()}\")\n",
    "display(df_cleaned_sql.limit(5))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5349481-bf50-4ecb-b13f-e4577f325e25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Najpierw utwórz temporary view\n",
    "df_customers.createOrReplaceTempView(\"customers_raw\")\n",
    "\n",
    "# SQL approach\n",
    "df_cleaned_sql = spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "    customer_id,\n",
    "    TRIM(INITCAP(first_name)) as first_name,\n",
    "    TRIM(INITCAP(last_name)) as last_name,\n",
    "    LOWER(TRIM(email)) as email,\n",
    "    COALESCE(city, 'Unknown') as city,\n",
    "    COALESCE(country, 'USA') as country,\n",
    "    phone,\n",
    "    registration_date,\n",
    "    customer_segment\n",
    "FROM customers_raw\n",
    "WHERE customer_id IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Rekordy po czyszczeniu (SQL): {df_cleaned_sql.count()}\")\n",
    "display(df_cleaned_sql.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754c4117-f1f6-439e-8e17-534fdc0f8f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Porównanie:**\n",
    "- **Wydajność**: Identyczna - Catalyst optimizer kompiluje oba do tego samego execution plan\n",
    "- **Kiedy używać PySpark**: Łańcuchowe transformacje, dynamiczne pipeline'y, integracja z Python libraries\n",
    "- **Kiedy używać SQL**: Złożone joiny, window functions, analitycy preferujący SQL syntax\n",
    "- **Best practice**: Używaj tego, co jest bardziej czytelne dla zespołu i przypadku użycia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afad2cee-3fc2-431c-81ae-6afa560d3560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist - Co powinieneś uzyskać:\n",
    "- [ ] Dane załadowane z dataset/ (customers.csv używając DATASET_BASE_PATH)\n",
    "- [ ] Data profiling report wykonany\n",
    "- [ ] Null values obsłużone (fillna/dropna/coalesce)\n",
    "- [ ] Typy danych skonwertowane i zwalidowane (cast, to_date)\n",
    "- [ ] Duplikaty usunięte (dropDuplicates)\n",
    "- [ ] Pola tekstowe standaryzowane (trim, case)\n",
    "- [ ] Kody i kategorie zunifikowane\n",
    "- [ ] Brak błędów w execution\n",
    "\n",
    "### Komendy weryfikacyjne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfde7220-5244-4cc2-9d2d-ef793d4cb65d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weryfikacja wyników - użyj df_codes_standardized jako final\n",
    "\n",
    "df_final = df_codes_standardized\n",
    "\n",
    "print(\"=== WERYFIKACJA WYNIKÓW ===\\n\")\n",
    "\n",
    "# 1. Podstawowe statystyki\n",
    "print(\"1. Podstawowe statystyki:\")\n",
    "print(f\"   Liczba rekordów: {df_final.count()}\")\n",
    "print(f\"   Liczba kolumn: {len(df_final.columns)}\")\n",
    "\n",
    "# 2. Null values per kolumna (sprawdź tylko pierwsze 10 kolumn)\n",
    "print(\"\\n2. Null values per kolumna (próbka):\")\n",
    "for col_name in df_final.columns[:10]:  # Pokaż tylko pierwsze 10\n",
    "    null_count = df_final.filter(F.col(col_name).isNull()).count()\n",
    "    print(f\"   {col_name:20s}: {null_count:4d} nulls\")\n",
    "\n",
    "# 3. Duplikaty\n",
    "print(\"\\n3. Analiza duplikatów:\")\n",
    "total = df_final.count()\n",
    "distinct = df_final.distinct().count()\n",
    "print(f\"   Total: {total}, Distinct: {distinct}, Duplicates: {total - distinct}\")\n",
    "\n",
    "# 4. Schemat danych (tylko typy)\n",
    "print(\"\\n4. Schemat danych:\")\n",
    "for field in df_final.schema.fields[:10]:  # Pokaż tylko pierwsze 10\n",
    "    print(f\"   {field.name:20s}: {field.dataType}\")\n",
    "\n",
    "# 5. Podgląd danych\n",
    "print(\"\\n5. Podgląd wyczyszczonych danych:\")\n",
    "display(df_final.select(\"customer_id\", \"first_name\", \"last_name\", \"email\", \"customer_segment\", \"country\").limit(10))\n",
    "\n",
    "# 6. Testy asercyjne\n",
    "try:\n",
    "    assert df_final.count() > 0, \"BŁĄD: DataFrame jest pusty\"\n",
    "    assert df_final.filter(F.col(\"customer_id\").isNull()).count() == 0, \"BŁĄD: Null values w customer_id\"\n",
    "    assert df_final.count() == df_final.dropDuplicates([\"customer_id\"]).count(), \"BŁĄD: Duplikaty w customer_id\"\n",
    "    print(\"\\n✓ Wszystkie testy przeszły pomyślnie!\")\n",
    "except AssertionError as e:\n",
    "    print(f\"\\n✗ Test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ddcecda-332b-4036-a6ed-038d134bafb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Problem 1: Błąd przy wczytywaniu z Volume\n",
    "**Objawy:**\n",
    "- FileNotFoundException\n",
    "- \"Path does not exist\"\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Sprawdź zawartość DATASET_BASE_PATH\n",
    "import os\n",
    "if os.path.exists(DATASET_BASE_PATH):\n",
    "    print(f\"✓ DATASET_BASE_PATH exists: {DATASET_BASE_PATH}\")\n",
    "    for folder in os.listdir(DATASET_BASE_PATH):\n",
    "        print(f\"  - {folder}/\")\n",
    "else:\n",
    "    print(f\"✗ DATASET_BASE_PATH not found: {DATASET_BASE_PATH}\")\n",
    "\n",
    "# Lista plików w customers/\n",
    "customers_dir = f\"{DATASET_BASE_PATH}/customers\"\n",
    "if os.path.exists(customers_dir):\n",
    "    for file in os.listdir(customers_dir):\n",
    "        print(f\"  - {file}\")\n",
    "```\n",
    "\n",
    "### Problem 2: InferSchema daje niepoprawne typy\n",
    "**Objawy:** \n",
    "- Kolumny numeryczne jako StringType\n",
    "- Daty nie są rozpoznawane\n",
    "\n",
    "**Rozwiązanie:**\n",
    "Zdefiniuj schemat jawnie:\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"registration_date\", StringType(), True),\n",
    "    StructField(\"customer_segment\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema).load(customers_path)\n",
    "```\n",
    "\n",
    "### Problem 3: Konwersja dat zwraca null\n",
    "**Objawy:**\n",
    "- to_date() zwraca null dla wszystkich wartości\n",
    "- Niepoprawny format daty\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Sprawdź przykładowe wartości\n",
    "df.select(\"registration_date\").show(5, truncate=False)\n",
    "\n",
    "# Wypróbuj różne formaty\n",
    "df.select(\n",
    "    F.col(\"registration_date\"),\n",
    "    F.to_date(F.col(\"registration_date\"), \"yyyy-MM-dd\").alias(\"format1\"),\n",
    "    F.to_date(F.col(\"registration_date\"), \"dd/MM/yyyy\").alias(\"format2\"),\n",
    "    F.to_date(F.col(\"registration_date\"), \"MM-dd-yyyy\").alias(\"format3\")\n",
    ").show(5)\n",
    "```\n",
    "\n",
    "### Debugging tips:\n",
    "- Użyj `.explain()` aby zobaczyć plan wykonania\n",
    "- Sprawdź typy: `df.printSchema()`\n",
    "- Podgląd danych: `display(df.limit(10))`\n",
    "- Monitoruj null counts po każdej transformacji\n",
    "- Użyj `.show(truncate=False)` dla pełnych wartości"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d767b746-5b28-4c79-915c-0ecdfae8b815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "### Wydajność:\n",
    "- Użyj jawnych schematów zamiast inferSchema w produkcji (szybsze, bardziej przewidywalne)\n",
    "- dropDuplicates() jest operacją shuffle - użyj ostrożnie na dużych danych\n",
    "- Partycjonuj dane przed deduplikacją per klucz biznesowy\n",
    "- Cache() DataFrames używanych wielokrotnie\n",
    "\n",
    "### Jakość kodu:\n",
    "- Zawsze waliduj dane po konwersji typów (null counts, value ranges)\n",
    "- Dodawaj flagi jakości (email_valid, date_parsed_success) dla auditability\n",
    "- Loguj statystyki czyszczenia (ile rekordów usunięto, wypełniono)\n",
    "- Zachowaj oryginalne wartości w osobnych kolumnach dla troubleshootingu\n",
    "\n",
    "### Data Quality:\n",
    "- Definiuj jasne reguły biznesowe dla każdego pola (allowed values, ranges)\n",
    "- Implementuj quality checks jako assertions w pipeline\n",
    "- Używaj coalesce() dla fallback strategies zamiast prostego fillna()\n",
    "- Dokumentuj decyzje o czyszczeniu (dlaczego usunięto, dlaczego fillna)\n",
    "\n",
    "### Governance:\n",
    "- Wszystkie transformacje muszą być deterministyczne (powtarzalne)\n",
    "- Audit trail: timestamp, user, operation w metadata columns\n",
    "- Zachowaj raw data w Bronze layer (nigdy nie nadpisuj źródła)\n",
    "- Używaj DATASET_BASE_PATH z 00_setup.ipynb dla spójnych ścieżek do danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "629e9ca1-ea24-493e-82ad-133cbce3fb72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Podsumowanie\n",
    "\n",
    "### Co zostało osiągnięte:\n",
    "- Załadowanie danych z folderu dataset/ (używając DATASET_BASE_PATH)\n",
    "- Data profiling i identyfikacja problemów jakości\n",
    "- Obsługa wartości null (fillna, dropna, coalesce)\n",
    "- Walidacja i konwersja typów (cast, to_date, to_timestamp)\n",
    "- Deduplikacja rekordów (distinct, dropDuplicates, window functions)\n",
    "- Standaryzacja tekstu i kodów (trim, case, regexp_replace)\n",
    "- Porównanie PySpark vs SQL approaches\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "1. **Data profiling first**: Zawsze analizuj dane przed rozpoczęciem czyszczenia\n",
    "2. **Context matters**: Strategia czyszczenia zależy od kontekstu biznesowego\n",
    "3. **Validation is critical**: Zawsze waliduj wyniki konwersji i transformacji\n",
    "4. **Document decisions**: Loguj statystyki i decyzje dla auditability\n",
    "\n",
    "### Quick Reference - Najważniejsze komendy:\n",
    "\n",
    "| Operacja | PySpark | SQL |\n",
    "|----------|---------|-----|\n",
    "| Fill nulls | `df.fillna({\"col\": \"value\"})` | `COALESCE(col, 'value')` |\n",
    "| Drop nulls | `df.dropna(subset=[\"col\"])` | `WHERE col IS NOT NULL` |\n",
    "| Parse date | `to_date(col(\"date\"), \"yyyy-MM-dd\")` | `TO_DATE(date, 'yyyy-MM-dd')` |\n",
    "| Remove duplicates | `df.dropDuplicates([\"id\"])` | `SELECT DISTINCT` |\n",
    "| Trim whitespace | `trim(col(\"name\"))` | `TRIM(name)` |\n",
    "| Lowercase | `lower(col(\"email\"))` | `LOWER(email)` |\n",
    "| Uppercase | `upper(col(\"country\"))` | `UPPER(country)` |\n",
    "| Title case | `initcap(col(\"city\"))` | `INITCAP(city)` |\n",
    "\n",
    "### Następne kroki:\n",
    "- **Kolejny notebook**: 05_views_workflows.ipynb - Persistent views i basic workflows\n",
    "- **Warsztat praktyczny**: 02_transformations_cleaning_workshop.ipynb\n",
    "- **Materiały dodatkowe**: \n",
    "  - Databricks Data Quality Guide\n",
    "  - Delta Lake best practices documentation\n",
    "- **Zadanie domowe**: Zastosuj poznane techniki czyszczenia na dataset orders (orders_batch.json z dataset/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da07bba-6574-4094-955d-5e3b2e1f2347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Czyszczenie zasobów\n",
    "\n",
    "Posprzątaj zasoby utworzone podczas notebooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a480f43-f0b1-4b84-a13a-fab63379f2a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Opcjonalne czyszczenie zasobów testowych\n",
    "# UWAGA: Uruchom tylko jeśli chcesz usunąć wszystkie utworzone dane\n",
    "\n",
    "# Usuń temporary views\n",
    "spark.catalog.dropTempView(\"customers_raw\")\n",
    "\n",
    "# Wyczyść cache\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "print(\"Temporary views i cache zostały wyczyszczone\")\n",
    "print(\"Dane źródłowe w Volume pozostają nienaruszone\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_data_cleaning_quality",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
