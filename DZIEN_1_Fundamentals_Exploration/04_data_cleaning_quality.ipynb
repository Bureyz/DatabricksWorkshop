{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "27d8de6a-6ffc-4a65-a1b0-1890f3387beb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Czyszczenie i jakość danych\n",
    "\n",
    "**Cel szkoleniowy:** Opanowanie technik identyfikacji i rozwiązywania problemów jakości danych, zrozumienie strategii obsługi wartości pustych, walidacji typów, deduplikacji i standaryzacji danych\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Obsługa wartości pustych: fillna(), dropna(), coalesce()\n",
    "- Walidacja typów: cast(), to_date(), to_timestamp()\n",
    "- Deduplikacja: dropDuplicates() - all columns vs key columns\n",
    "- Standardyzacja: formaty dat, tekstów, kategorii\n",
    "- Typowe problemy jakości: whitespace, niepoprawne kody, inconsistent formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d3bbc6e-e99c-4689-8b12-86040c06dba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 1 - Fundamentals & Exploration\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z 2-4 workers\n",
    "- **Czas trwania**: 20 minut\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e47df0c8-4b57-45d2-bd2f-3a524198eab8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie fundamentów data quality i technik czyszczenia danych\n",
    "\n",
    "**Podstawowe pojęcia:**\n",
    "- **Data Quality**: Miara przydatności danych do ich zamierzonego celu\n",
    "- **Data Cleansing**: Proces identyfikacji i korygowania błędów w danych\n",
    "- **Data Validation**: Weryfikacja zgodności danych z regułami biznesowymi\n",
    "- **Data Standardization**: Ujednolicenie formatów i reprezentacji danych\n",
    "- **Data Profiling**: Analiza struktury, zawartości i relacji w danych\n",
    "\n",
    "**Wymiary jakości danych:**\n",
    "- **Completeness**: Czy wszystkie wymagane dane są obecne\n",
    "- **Accuracy**: Czy dane są poprawne i zgodne z rzeczywistością\n",
    "- **Consistency**: Czy dane są spójne w całym systemie\n",
    "- **Timeliness**: Czy dane są aktualne\n",
    "- **Validity**: Czy dane spełniają reguły biznesowe\n",
    "- **Uniqueness**: Czy rekordy są unikalne\n",
    "\n",
    "**Dlaczego to ważne?**\n",
    "Niska jakość danych prowadzi do błędnych analiz, złych decyzji biznesowych i utraty zaufania do systemu. Czyszczenie danych to często 60-80% czasu w projektach data engineering. Systematyczne podejście do data quality zapewnia wiarygodność całego pipeline'u danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ae724e7-9abf-4a35-80a5-08e9c8e9b55c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Izolacja per użytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d33558-b424-4e25-ab88-4da72280f040",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "497f6109-4d20-48b3-b7be-dd8f207a5109",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b15728b9-230e-458c-b7fe-84a7c8b27798",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Wyświetl kontekst użytkownika (zmienne z 00_setup)\n",
    "print(\"=== Kontekst użytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"Użytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog i schemat jako domyślne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "print(\"\\n=== Konfiguracja zakończona pomyślnie ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95c0f73f-744a-4914-8988-c57a06823df8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Kontekst użytkownika\n",
    "\n",
    "Po uruchomieniu setupu sprawdzamy zmienne środowiskowe:\n",
    "\n",
    "**Używamy:**\n",
    "- **CATALOG**: Izolowany katalog per użytkownik\n",
    "- **BRONZE_SCHEMA**: Warstwa surowych danych  \n",
    "- **SILVER_SCHEMA**: Warstwa oczyszczonych danych\n",
    "- **GOLD_SCHEMA**: Warstwa zagregowanych danych\n",
    "- **raw_user**: Identyfikator użytkownika\n",
    "- **DATASET_BASE_PATH**: Ścieżka do folderu z danymi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "576533ef-4937-4ef4-aa89-0f6cbfa378cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Import bibliotek:**\n",
    "- `functions as F` - funkcje PySpark do transformacji danych\n",
    "- `types` - definicje typów danych (StructType, StringType)\n",
    "- `Window` - funkcje okienne do deduplikacji i rankingu\n",
    "- `re` - wyrażenia regularne do walidacji\n",
    "- `datetime` - operacje na datach i czasie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f574b6ad-edb2-4462-be3e-96778847bf7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ustaw katalog i schemat jako domyślne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0ee599f-d703-480b-8e92-55aa33f85e0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Konfigurujemy domyślny katalog i schemat dla sesji Spark, używając zmiennych środowiskowych z `00_setup.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d4cac51-600d-41d8-a12a-ba9c5c5cfc60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wyświetl kontekst użytkownika (zmienne z 00_setup)\n",
    "display(f\"Katalog: {CATALOG}\")\n",
    "display(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "display(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "display(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "display(f\"Użytkownik: {raw_user}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "953131c5-5590-47ac-83aa-560182b5d159",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Weryfikujemy poprawność konfiguracji środowiska, wyświetlając zmienne ustawione przez skrypt `00_setup.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92c8aaa9-b9c7-43fe-a718-21cd1f96d858",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 1: Wczytanie danych z dataset\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "W tym notebooku wykorzystujemy pliki z lokalnego folderu `dataset/` (Dzień 1-2 szkolenia). Pliki CSV, JSON i Parquet są ładowane bezpośrednio z systemu plików używając ścieżki `DATASET_BASE_PATH` z `00_setup.ipynb`.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **DATASET_BASE_PATH**: Ścieżka do folderu dataset/ zdefiniowana w 00_setup.ipynb\n",
    "- **CSV Reader**: spark.read.format(\"csv\") z opcjami (header, inferSchema)\n",
    "- **Schema inference**: Automatyczne wykrywanie typów vs jawna definicja schematu\n",
    "- **File paths**: Absolutne ścieżki do plików w lokalnym systemie\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Wczytywanie raw files z folderu dataset/\n",
    "- Eksploracja i profilowanie danych przed czyszczeniem\n",
    "- Przygotowanie do załadowania do Delta tables (Bronze layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2ee3ace-8a8b-4fa9-bad8-04b0f4e2418d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.1: Wczytanie danych customers z dataset\n",
    "\n",
    "**Cel:** Załadowanie danych klientów z pliku CSV przechowywanego w folderze dataset/\n",
    "\n",
    "**Podejście:**\n",
    "1. Zdefiniowanie ścieżki używając DATASET_BASE_PATH z 00_setup.ipynb\n",
    "2. Wczytanie CSV z opcjami (header, inferSchema)\n",
    "3. Podstawowa eksploracja załadowanych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84389d0f-05ab-4c08-a3a7-099069cb1c18",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: CSV file: {DATASET_BASE_PATH}/customers/customers.csv\n",
    "# VARIABLE: df_customers - DataFrame z danymi klientów\n",
    "\n",
    "# Ścieżka do pliku w dataset\n",
    "customers_path = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "\n",
    "# Wczytanie danych\n",
    "df_customers = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(customers_path)\n",
    "\n",
    "# Podstawowa eksploracja\n",
    "print(\"=== Dane klientów załadowane ===\")\n",
    "print(f\"Liczba rekordów: {df_customers.count()}\")\n",
    "print(f\"Liczba kolumn: {len(df_customers.columns)}\")\n",
    "print(f\"\\nKolumny: {df_customers.columns}\")\n",
    "\n",
    "# Schemat danych\n",
    "print(\"\\n=== Schemat danych ===\")\n",
    "df_customers.printSchema()\n",
    "\n",
    "# Podgląd pierwszych rekordów\n",
    "print(\"\\n=== Pierwsze 10 rekordów ===\")\n",
    "display(df_customers.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d450a25c-9d45-46e7-92a7-5857ffa4738e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Wczytujemy dane klientów z pliku CSV używając ścieżki `DATASET_BASE_PATH` zdefiniowanej w `00_setup.ipynb`. Opcja `inferSchema=true` automatycznie wykrywa typy danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72bc36fd-d623-49e5-8e31-0133294cc9f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Podstawowe statystyki załadowanych danych\n",
    "total_rows = df_customers.count()\n",
    "total_columns = len(df_customers.columns)\n",
    "\n",
    "display(f\"Liczba rekordów: {total_rows}\")\n",
    "display(f\"Liczba kolumn: {total_columns}\")\n",
    "display(f\"Kolumny: {df_customers.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "251af436-db05-4869-98ee-0fc198de2e8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Wyświetlamy podstawowe informacje o załadowanym DataFramie - liczbę rekordów, kolumn i nazwy kolumn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd144bd1-72e3-43f6-b2bf-53473b6703e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Schemat danych - typy kolumn\n",
    "df_customers.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4e55f7d-37e6-4df9-963a-e22acb549592",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sprawdzamy schemat danych - typy kolumn wykryte automatycznie przez Spark. W produkcji zaleca się jawne definiowanie schematów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4937e134-78ff-4213-b15f-36ca89b17458",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Podgląd pierwszych rekordów\n",
    "display(df_customers.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f8f2ce9-cd15-4539-a94c-300f68f6d0fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Wyświetlamy pierwsze 10 rekordów, aby zobaczyć rzeczywiste dane i zidentyfikować potencjalne problemy jakości."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fcc0ef1-e096-4e35-95fe-92df5681abc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.2: Data Profiling - analiza jakości danych\n",
    "\n",
    "**Cel:** Identyfikacja problemów jakości w załadowanych danych przed rozpoczęciem czyszczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c72c6a73-48b0-4712-b96c-fce6312339e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: profiling_report - dict ze statystykami jakości\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY PROFILING REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Completeness - Analiza wartości null\n",
    "print(\"\\n1. COMPLETENESS - Wartości null per kolumna:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "null_analysis = []\n",
    "for col_name in df_customers.columns:\n",
    "    null_count = df_customers.filter(F.col(col_name).isNull()).count()\n",
    "    total_count = df_customers.count()\n",
    "    null_pct = (null_count / total_count) * 100\n",
    "    null_analysis.append((col_name, null_count, null_pct))\n",
    "\n",
    "# Wyświetl statystyki null values\n",
    "for col_name, null_count, null_pct in null_analysis:\n",
    "    display(f\"{col_name:20s}: {null_count:4d} nulls ({null_pct:5.1f}%)\")\n",
    "\n",
    "# 2. Uniqueness - Analiza duplikatów\n",
    "print(\"\\n2. UNIQUENESS - Analiza duplikatów:\")\n",
    "print(\"-\" * 80)\n",
    "total_rows = df_customers.count()\n",
    "unique_rows = df_customers.distinct().count()\n",
    "duplicate_rows = total_rows - unique_rows\n",
    "print(f\"  Total rows: {total_rows}\")\n",
    "print(f\"  Unique rows: {unique_rows}\")\n",
    "print(f\"  Duplicate rows: {duplicate_rows}\")\n",
    "print(f\"  Duplication rate: {(duplicate_rows/total_rows)*100:.1f}%\")\n",
    "\n",
    "# 3. Consistency - Analiza wartości unikalnych w kluczowych kolumnach\n",
    "print(\"\\n3. CONSISTENCY - Wartości unikalne:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for col_name in df_customers.columns:\n",
    "    distinct_count = df_customers.select(col_name).distinct().count()\n",
    "    print(f\"  {col_name:20s}: {distinct_count:4d} unikalnych wartości\")\n",
    "\n",
    "# 4. Accuracy - Przykładowe wartości\n",
    "print(\"\\n4. ACCURACY - Przykładowe wartości (pierwsze 5):\")\n",
    "print(\"-\" * 80)\n",
    "display(df_customers.limit(5))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PROFILING COMPLETED\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "43ec9d3d-e247-4d4b-9944-c0742fbbfa63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Completeness Analysis:** Sprawdzamy procent brakujących wartości w każdej kolumnie. Wysoki procent null values może wskazywać na problemy w systemie źródłowym lub potrzebę alternatywnych strategii wypełniania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e20d3ff1-6862-4872-9f55-9f98d51259e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2. Uniqueness - Analiza duplikatów\n",
    "total_rows = df_customers.count()\n",
    "unique_rows = df_customers.distinct().count()\n",
    "duplicate_rows = total_rows - unique_rows\n",
    "duplication_rate = (duplicate_rows/total_rows)*100\n",
    "\n",
    "display(f\"Total rows: {total_rows}\")\n",
    "display(f\"Unique rows: {unique_rows}\")\n",
    "display(f\"Duplicate rows: {duplicate_rows}\")\n",
    "display(f\"Duplication rate: {duplication_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "703c519e-f8a9-468f-bcda-b141421e911a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Uniqueness Analysis:** Identyfikujemy duplikaty - rekordy całkowicie identyczne we wszystkich kolumnach. Duplikaty mogą wynikać z błędów w ETL lub wielokrotnego ładowania tych samych danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "725b86f2-2e91-435f-880d-67b9a203b0f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 3. Consistency - Analiza wartości unikalnych\n",
    "consistency_analysis = []\n",
    "for col_name in df_customers.columns:\n",
    "    distinct_count = df_customers.select(col_name).distinct().count()\n",
    "    consistency_analysis.append((col_name, distinct_count))\n",
    "\n",
    "# Wyświetl statystyki wartości unikalnych\n",
    "for col_name, distinct_count in consistency_analysis:\n",
    "    display(f\"{col_name:20s}: {distinct_count:4d} unikalnych wartości\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e1f3b2a-6028-4d57-88a1-4cc792442072",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Consistency Analysis:** Sprawdzamy liczbę unikalnych wartości w każdej kolumnie. Pomaga to zidentyfikować kolumny kategoryczne, potencjalne klucze i problemy z formatowaniem danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbd453f2-0035-4f4c-9c3f-57268c901b64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 4. Accuracy - Podgląd przykładowych wartości\n",
    "display(df_customers.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000c6320-981d-434e-8dec-2ecbbc0e03fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Accuracy Analysis:** Podgląd rzeczywistych wartości danych pozwala na manualną weryfikację poprawności - czy dane wyglądają realistycznie i są zgodne z oczekiwaniami biznesowymi."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c334e71-0521-44f6-a728-8306e58fddff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 2: Obsługa wartości pustych\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Brakujące wartości są jednym z najczęstszych problemów jakości danych. Strategia obsługi null values zależy od kontekstu biznesowego i charakteru danych. Niepoprawna obsługa może prowadzić do błędów w analizach i modelach ML.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **fillna()**: Wypełnienie wartości null określoną wartością lub strategią\n",
    "- **dropna()**: Usunięcie rekordów zawierających wartości null\n",
    "- **coalesce()**: Wybór pierwszej niepustej wartości z wielu kolumn\n",
    "- **Imputation**: Statystyczne metody wypełniania (mean, median, mode)\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Uzupełnianie brakujących wartości sensownymi defaultami\n",
    "- Usuwanie rekordów z krytycznymi brakującymi danymi\n",
    "- Fallback do alternatywnych źródeł danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b239f4ac-bd75-41a3-85f6-e6c8bdcbeda0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.1: Wypełnianie wartości null (fillna)\n",
    "\n",
    "**Cel:** Uzupełnienie brakujących wartości w kolumnach różnymi strategiami\n",
    "\n",
    "**Podejście:**\n",
    "1. Identyfikacja kolumn z null values\n",
    "2. Wybór odpowiedniej strategii per kolumna\n",
    "3. Zastosowanie fillna() z słownikiem wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f0e7095-14bf-4600-8e87-7d5280d444b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_filled - DataFrame z wypełnionymi wartościami null\n",
    "\n",
    "# Strategia wypełniania wartości null - tylko dla kolumn które rzeczywiście mogą mieć nulls\n",
    "fill_values = {\n",
    "    \"phone\": \"brak telefonu\",\n",
    "    \"city\": \"Unknown\", \n",
    "    \"state\": \"Unknown\",\n",
    "    \"country\": \"Unknown\"\n",
    "}\n",
    "\n",
    "# Wypełnienie wartości null\n",
    "df_filled = df_customers.fillna(fill_values)\n",
    "\n",
    "# Weryfikacja zmian\n",
    "print(\"=== Porównanie przed i po fillna ===\")\n",
    "for col_name in fill_values.keys():\n",
    "    before_nulls = df_customers.filter(F.col(col_name).isNull()).count()\n",
    "    after_nulls = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "    print(f\"{col_name:15s}: {before_nulls:3d} nulls → {after_nulls:3d} nulls\")\n",
    "\n",
    "# Przykładowe rekordy po wypełnieniu\n",
    "print(\"\\n=== Przykładowe wypełnione rekordy ===\")\n",
    "display(df_filled.filter(\n",
    "    df_customers[\"phone\"].isNull() | \n",
    "    df_customers[\"city\"].isNull() |\n",
    "    df_customers[\"state\"].isNull()\n",
    ").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d3395356-1ec9-46ba-ba6b-2bfd7e68c5dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Definiujemy strategię wypełniania wartości null używając słownika z wartościami domyślnymi dla każdej kolumny. Wybieramy sensowne defaulty zgodne z kontekstem biznesowym."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c944d10-9a7f-49a7-b555-bcfce70b2800",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weryfikacja zmian po fillna\n",
    "for col_name in fill_values.keys():\n",
    "    before_nulls = df_customers.filter(F.col(col_name).isNull()).count()\n",
    "    after_nulls = df_filled.filter(F.col(col_name).isNull()).count()\n",
    "    display(f\"{col_name:15s}: {before_nulls:3d} nulls → {after_nulls:3d} nulls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1351b22-e50e-405d-8022-ba644843ec8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Weryfikujemy skuteczność operacji fillna, porównując liczbę wartości null przed i po transformacji. Wszystkie wartości w wybranych kolumnach powinny być wypełnione."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cc32e771-0a4d-48e3-9bd2-349161677f77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykładowe rekordy po wypełnieniu - rekordy które miały null values\n",
    "display(df_filled.filter(\n",
    "    df_customers[\"phone\"].isNull() | \n",
    "    df_customers[\"city\"].isNull() |\n",
    "    df_customers[\"state\"].isNull()\n",
    ").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d1a13ab-d37e-4a12-a436-32f8c62c0017",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sprawdzamy rekordy, które pierwotnie miały wartości null - teraz powinny być wypełnione wartościami domyślnymi zgodnie z naszą strategią."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40e1ca53-8820-4535-85cf-4fc3239f11f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.2: Usuwanie rekordów z wartościami null (dropna)\n",
    "\n",
    "**Cel:** Usunięcie rekordów z brakującymi wartościami w kluczowych kolumnach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b54ac61-9b65-43ca-9594-c351604ac707",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_valid - DataFrame z usuniętymi rekordami z null w kluczowych kolumnach\n",
    "\n",
    "# Usuń rekordy bez kluczowych informacji (customer_id jest wymagane)\n",
    "df_valid = df_customers.dropna(subset=[\"customer_id\"])\n",
    "\n",
    "# Weryfikacja zmian\n",
    "print(\"=== Porównanie przed i po dropna ===\")\n",
    "print(f\"Liczba rekordów PRZED: {df_customers.count()}\")\n",
    "print(f\"Liczba rekordów PO: {df_valid.count()}\")\n",
    "print(f\"Usunięto rekordów: {df_customers.count() - df_valid.count()}\")\n",
    "\n",
    "# Sprawdzenie, czy są jeszcze null w customer_id\n",
    "null_ids = df_valid.filter(F.col(\"customer_id\").isNull()).count()\n",
    "print(f\"\\nNull w customer_id po dropna: {null_ids}\")\n",
    "\n",
    "# Alternatywne użycie: dropna z how='all' (usuwa tylko jeśli wszystkie kolumny są null)\n",
    "df_any_data = df_customers.dropna(how='all')\n",
    "print(f\"\\nRekordy z jakimikolwiek danymi: {df_any_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1bbca206-5e96-4ad3-8692-982921b99831",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Usuwamy rekordy, które nie mają wartości w kluczowych kolumnach. `customer_id` jest obowiązkowe - rekordy bez niego są bezużyteczne w analizach biznesowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ecfb226-12bb-4219-8e94-c36993a315de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weryfikacja zmian po dropna\n",
    "records_before = df_customers.count()\n",
    "records_after = df_valid.count()\n",
    "records_removed = records_before - records_after\n",
    "\n",
    "display(f\"Liczba rekordów PRZED: {records_before}\")\n",
    "display(f\"Liczba rekordów PO: {records_after}\")\n",
    "display(f\"Usunięto rekordów: {records_removed}\")\n",
    "\n",
    "# Sprawdzenie, czy są jeszcze null w customer_id\n",
    "null_ids = df_valid.filter(F.col(\"customer_id\").isNull()).count()\n",
    "display(f\"Null w customer_id po dropna: {null_ids}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a52ada58-7436-4839-bedb-21242d2a5750",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Weryfikujemy skuteczność operacji dropna - sprawdzamy ile rekordów zostało usuniętych i czy w kolumnie `customer_id` nie ma już wartości null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60590856-30be-49ec-b0eb-82c1933a5fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Alternatywne użycie: dropna z how='all' (usuwa tylko jeśli wszystkie kolumny są null)\n",
    "df_any_data = df_customers.dropna(how='all')\n",
    "display(f\"Rekordy z jakimikolwiek danymi: {df_any_data.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b127574-6ed5-49a6-8eb6-f14c217ef935",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Alternatywna strategia: `how='all'` usuwa tylko rekordy, gdzie wszystkie kolumny są null. Jest mniej restrykcyjna i zachowuje rekordy z przynajmniej jedną niepustą wartością."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "771829fd-0deb-480a-8612-22d6e0681da7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.3: Wybór pierwszej niepustej wartości (coalesce)\n",
    "\n",
    "**Cel:** Użycie coalesce() do fallback między alternatywnymi źródłami danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5561f47d-aae3-46f0-88b9-d0228a706693",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_with_contact - DataFrame z nową kolumną primary_contact\n",
    "\n",
    "from pyspark.sql.functions import coalesce, lit\n",
    "\n",
    "# Przykład: Stwórz primary_contact wybierając pierwszą niepustą wartość\n",
    "df_with_contact = df_customers.withColumn(\n",
    "    \"primary_contact\",\n",
    "    coalesce(F.col(\"email\"), F.col(\"phone\"), lit(\"brak kontaktu\"))\n",
    ")\n",
    "\n",
    "# Stwórz pełny adres z dostępnych pól\n",
    "df_with_contact = df_with_contact.withColumn(\n",
    "    \"full_address\",\n",
    "    coalesce(\n",
    "        F.concat_ws(\", \", F.col(\"city\"), F.col(\"state\"), F.col(\"country\")),\n",
    "        F.concat_ws(\", \", F.col(\"city\"), F.col(\"country\")),\n",
    "        F.col(\"country\"),\n",
    "        lit(\"Address Unknown\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Weryfikacja\n",
    "print(\"=== Analiza primary_contact ===\")\n",
    "contact_stats = df_with_contact.groupBy(\"primary_contact\").count().orderBy(F.desc(\"count\"))\n",
    "display(contact_stats.limit(10))\n",
    "\n",
    "# Przykłady\n",
    "print(\"\\n=== Przykładowe rekordy z primary_contact i full_address ===\")\n",
    "display(df_with_contact.select(\"customer_id\", \"email\", \"phone\", \"primary_contact\", \"city\", \"state\", \"country\", \"full_address\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "678969ba-a5d1-4524-bb42-3606bdb51a00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Używamy `coalesce()` do utworzenia kolumny `primary_contact` - wybieramy pierwszą niepustą wartość z `email`, `phone` lub wartość domyślną. To implementuje logikę fallback w przypadku brakujących danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bae87360-ce13-4f8c-a703-0b6e8c9477ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Stwórz pełny adres z dostępnych pól\n",
    "df_with_contact = df_with_contact.withColumn(\n",
    "    \"full_address\",\n",
    "    coalesce(\n",
    "        F.concat_ws(\", \", F.col(\"city\"), F.col(\"state\"), F.col(\"country\")),\n",
    "        F.concat_ws(\", \", F.col(\"city\"), F.col(\"country\")),\n",
    "        F.col(\"country\"),\n",
    "        lit(\"Address Unknown\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba292a9b-402e-4015-9a19-9b226de1390d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Tworzymy `full_address` używając zaawansowanego coalesce z `concat_ws()` - próbujemy różne kombinacje pól adresowych, wybierając pierwszą niepustą. Strategie fallback: pełny → miasto+kraj → sam kraj → default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58aab105-710a-4244-8235-9b3779553378",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Analiza primary_contact - statystyki używania różnych typów kontaktu\n",
    "contact_stats = df_with_contact.groupBy(\"primary_contact\").count().orderBy(F.desc(\"count\"))\n",
    "display(contact_stats.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c3b7f6c-30b5-40bb-b074-2d594fd02938",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Analizujemy efektywność strategii coalesce - sprawdzamy, ile rekordów używa email, telefon lub wartość domyślną jako primary_contact. Pomaga to ocenić jakość danych kontaktowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7aa6c92c-0712-4420-bd99-c68b8aeff4a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykłady rekordów z primary_contact i full_address\n",
    "display(df_with_contact.select(\n",
    "    \"customer_id\", \"email\", \"phone\", \"primary_contact\", \n",
    "    \"city\", \"state\", \"country\", \"full_address\"\n",
    ").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2edf35d5-1318-4548-8a6e-72c73fdbc618",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Podgląd rekordów pokazuje, jak coalesce wypełnił nowe kolumny `primary_contact` i `full_address` używając dostępnych danych źródłowych lub wartości domyślnych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d50d87ed-2e88-41e8-a281-7aadc227e06a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 3: Walidacja i konwersja typów danych\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Niepoprawne typy danych są częstym problemem podczas ładowania danych z plików tekstowych (CSV, JSON). Konwersje typu muszą być wykonane bezpiecznie z obsługą błędów, aby uniknąć utraty danych lub niepoprawnych wyników analiz.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **cast()**: Konwersja typu danych (string → int, date, timestamp)\n",
    "- **to_date()**: Parsowanie stringów na DateType z określeniem formatu\n",
    "- **to_timestamp()**: Parsowanie stringów na TimestampType\n",
    "- **try_cast()**: Bezpieczna konwersja zwracająca null przy błędzie (Spark 3.4+)\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Walidacja typów po załadowaniu CSV z inferSchema\n",
    "- Parsowanie dat w niestandardowych formatach\n",
    "- Konwersja typów przed złączeniami i agregacjami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e057d71-27de-46e4-8351-bc0043852b65",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.1: Konwersja typów numerycznych (cast)\n",
    "\n",
    "**Cel:** Bezpieczna konwersja stringów na typy numeryczne z walidacją\n",
    "\n",
    "**Podejście:**\n",
    "1. Czyszczenie wartości przed konwersją (usunięcie znaków specjalnych)\n",
    "2. Konwersja typu używając cast()\n",
    "3. Walidacja zakresu wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "133bf1e7-20ba-4c5d-bf39-96d20bc625a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_typed - DataFrame z poprawnie skonwertowanymi typami\n",
    "\n",
    "# Przykład: Walidacja dat rejestracji i dodanie flag jakości\n",
    "df_typed = df_customers.withColumn(\n",
    "    \"registration_date_parsed\",\n",
    "    F.to_date(F.col(\"registration_date\"), \"yyyy-MM-dd\")\n",
    ")\n",
    "\n",
    "# Walidacja zakresu dat (2020-2026)\n",
    "df_typed = df_typed.withColumn(\n",
    "    \"registration_date_valid\",\n",
    "    (F.col(\"registration_date_parsed\").isNotNull()) & \n",
    "    (F.col(\"registration_date_parsed\") >= \"2020-01-01\") & \n",
    "    (F.col(\"registration_date_parsed\") <= \"2026-12-31\")\n",
    ")\n",
    "\n",
    "# Dodaj wiek konta w dniach\n",
    "df_typed = df_typed.withColumn(\n",
    "    \"account_age_days\",\n",
    "    F.datediff(F.current_date(), F.col(\"registration_date_parsed\"))\n",
    ")\n",
    "\n",
    "# Walidacja email format\n",
    "df_typed = df_typed.withColumn(\n",
    "    \"email_valid\",\n",
    "    (F.col(\"email\").isNotNull()) & \n",
    "    F.col(\"email\").rlike(\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\")\n",
    ")\n",
    "\n",
    "# Statystyki konwersji\n",
    "total = df_typed.count()\n",
    "valid_dates = df_typed.filter(F.col(\"registration_date_valid\") == True).count()\n",
    "invalid_dates = df_typed.filter(F.col(\"registration_date_valid\") == False).count()\n",
    "valid_emails = df_typed.filter(F.col(\"email_valid\") == True).count()\n",
    "\n",
    "print(\"=== Statystyki walidacji ===\")\n",
    "print(f\"Total: {total}\")\n",
    "print(f\"Valid registration dates: {valid_dates} ({(valid_dates/total)*100:.1f}%)\")\n",
    "print(f\"Invalid registration dates: {invalid_dates} ({(invalid_dates/total)*100:.1f}%)\")\n",
    "print(f\"Valid emails: {valid_emails} ({(valid_emails/total)*100:.1f}%)\")\n",
    "\n",
    "# Przykłady nieprawidłowych wartości\n",
    "if invalid_dates > 0:\n",
    "    print(\"\\n=== Przykłady nieprawidłowych dat rejestracji ===\")\n",
    "    display(df_typed.filter(F.col(\"registration_date_valid\") == False).select(\"customer_id\", \"registration_date\", \"registration_date_parsed\", \"registration_date_valid\").limit(5))\n",
    "\n",
    "# Schemat po konwersji\n",
    "print(\"\\n=== Schemat po konwersji ===\")\n",
    "df_typed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8f7df7cc-ee25-4111-96c1-36bd0f91279b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Obliczamy wiek konta w dniach używając `datediff()` między datą bieżącą a datą rejestracji. To przydatna metryka dla analiz behawioralnych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66d9c4f5-f806-465c-b025-baaa5dad3d85",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Walidacja formatu email używając wyrażenia regularnego\n",
    "df_typed = df_typed.withColumn(\n",
    "    \"email_valid\",\n",
    "    (F.col(\"email\").isNotNull()) & \n",
    "    F.col(\"email\").rlike(\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}$\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f7be03-1445-46a9-858d-759d629fc351",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Walidujemy format email używając wyrażenia regularnego `rlike()`. Sprawdzamy podstawową strukturę: lokalna_część@domena.rozszerzenie. Flaga `email_valid` może być używana do filtrowania i raportowania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d18aa0d-9e01-49b0-8ca9-541407efb059",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Oblicz statystyki walidacji\n",
    "total = df_typed.count()\n",
    "valid_dates = df_typed.filter(F.col(\"registration_date_valid\") == True).count()\n",
    "invalid_dates = df_typed.filter(F.col(\"registration_date_valid\") == False).count()\n",
    "valid_emails = df_typed.filter(F.col(\"email_valid\") == True).count()\n",
    "\n",
    "# Wyświetl statystyki używając display()\n",
    "display(f\"Total records: {total}\")\n",
    "display(f\"Valid registration dates: {valid_dates} ({(valid_dates/total)*100:.1f}%)\")\n",
    "display(f\"Invalid registration dates: {invalid_dates} ({(invalid_dates/total)*100:.1f}%)\")\n",
    "display(f\"Valid emails: {valid_emails} ({(valid_emails/total)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16fc1445-e421-4248-85c1-4a9f67692d71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Obliczamy i wyświetlamy statystyki skuteczności walidacji - procent poprawnych dat i emaili. Te metryki są kluczowe dla oceny jakości danych źródłowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea23d401-b928-4e00-a459-7355e521aabc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykłady nieprawidłowych dat rejestracji (jeśli istnieją)\n",
    "invalid_dates_sample = df_typed.filter(F.col(\"registration_date_valid\") == False).select(\n",
    "    \"customer_id\", \"registration_date\", \"registration_date_parsed\", \"registration_date_valid\"\n",
    ").limit(5)\n",
    "\n",
    "if invalid_dates_sample.count() > 0:\n",
    "    display(invalid_dates_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58b77bad-1b79-40f4-9a8e-a4cd8045df67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Podgląd rekordów z nieprawidłowymi datami pomaga zrozumieć charakter problemów w danych źródłowych. Może to wskazywać na potrzebę dodatkowych reguł walidacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "97a1f1fc-0213-4e46-a317-575979c95f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdź schemat danych po dodaniu nowych kolumn\n",
    "df_typed.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd6ff183-43bc-45ab-9797-0c460b73ea17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sprawdzamy zaktualizowany schemat DataFrame po dodaniu nowych kolumn z walidacją i obliczeniami. Nowe kolumny typu DateType, BooleanType i LongType rozszerzają oryginalny schemat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1205430e-bf83-43e3-83da-1674639a6f2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Parsujemy stringi dat na typ DateType używając `to_date()` z jawnie określonym formatem. Format `yyyy-MM-dd` jest standardem ISO 8601."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22e1e5de-4a9b-4f97-84c4-7941a953a533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Walidacja zakresu dat (2020-2026) i dodanie flagi jakości\n",
    "df_typed = df_typed.withColumn(\n",
    "    \"registration_date_valid\",\n",
    "    (F.col(\"registration_date_parsed\").isNotNull()) & \n",
    "    (F.col(\"registration_date_parsed\") >= \"2020-01-01\") & \n",
    "    (F.col(\"registration_date_parsed\") <= \"2026-12-31\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77e880d4-9dc0-43c8-ae20-33aa8be2b8fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Dodajemy flagę `registration_date_valid` sprawdzającą logiczny zakres dat (2020-2026). Flagi jakości są kluczowe dla auditability i monitoringu pipeline'ów danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "993abebe-2a10-4750-94e4-9edd132fccc0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.2: Konwersja dat (to_date)\n",
    "\n",
    "**Cel:** Parsowanie stringów na DateType z obsługą wielu formatów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2c49468-ea46-4f0d-9d28-690643fa3248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_with_dates - DataFrame z poprawnie sparsowanymi datami\n",
    "\n",
    "# Konwersja registration_date z obsługą wielu formatów\n",
    "df_with_dates = df_customers.withColumn(\n",
    "    \"registration_date_parsed\",\n",
    "    coalesce(\n",
    "        F.to_date(F.col(\"registration_date\"), \"yyyy-MM-dd\"),     # Format: 2024-01-15\n",
    "        F.to_date(F.col(\"registration_date\"), \"dd/MM/yyyy\"),     # Format: 15/01/2024\n",
    "        F.to_date(F.col(\"registration_date\"), \"MM-dd-yyyy\"),     # Format: 01-15-2024\n",
    "        F.to_date(F.col(\"registration_date\"), \"yyyy.MM.dd\"),     # Format: 2024.01.15\n",
    "        F.to_date(F.col(\"registration_date\"))                    # Automatyczne wykrywanie\n",
    "    )\n",
    ")\n",
    "\n",
    "# Walidacja konwersji\n",
    "total = df_with_dates.count()\n",
    "parsed = df_with_dates.filter(F.col(\"registration_date_parsed\").isNotNull()).count()\n",
    "failed = df_with_dates.filter(\n",
    "    F.col(\"registration_date\").isNotNull() & \n",
    "    F.col(\"registration_date_parsed\").isNull()\n",
    ").count()\n",
    "\n",
    "print(\"=== Statystyki konwersji dat ===\")\n",
    "print(f\"Total: {total}\")\n",
    "print(f\"Poprawnie sparsowane: {parsed} ({(parsed/total)*100:.1f}%)\")\n",
    "print(f\"Nie udało się sparsować: {failed} ({(failed/total)*100:.1f}%)\")\n",
    "\n",
    "# Przykłady konwersji\n",
    "print(\"\\n=== Przykładowe konwersje dat ===\")\n",
    "display(df_with_dates.select(\"customer_id\", \"registration_date\", \"registration_date_parsed\").limit(10))\n",
    "\n",
    "# Rekordy z błędami parsowania\n",
    "if failed > 0:\n",
    "    print(\"\\n=== Rekordy z błędami parsowania ===\")\n",
    "    display(df_with_dates.filter(\n",
    "        F.col(\"registration_date\").isNotNull() & \n",
    "        F.col(\"registration_date_parsed\").isNull()\n",
    "    ).select(\"customer_id\", \"registration_date\", \"registration_date_parsed\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6722dd3f-1e41-4613-a539-ce116e0c2a53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Używamy zaawansowanej strategii konwersji dat z `coalesce()` - próbujemy różne popularne formaty dat, wybierając pierwszy który się powiedzie. To zapewnia większą elastyczność przy niejednolitych formatach w danych źródłowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa0dbc1b-6a37-4532-a253-2b097f869ead",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Oblicz statystyki skuteczności konwersji dat\n",
    "total = df_with_dates.count()\n",
    "parsed = df_with_dates.filter(F.col(\"registration_date_parsed\").isNotNull()).count()\n",
    "failed = df_with_dates.filter(\n",
    "    F.col(\"registration_date\").isNotNull() & \n",
    "    F.col(\"registration_date_parsed\").isNull()\n",
    ").count()\n",
    "\n",
    "# Wyświetl statystyki\n",
    "display(f\"Total records: {total}\")\n",
    "display(f\"Successfully parsed: {parsed} ({(parsed/total)*100:.1f}%)\")\n",
    "display(f\"Failed to parse: {failed} ({(failed/total)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c22dcf5-0d02-4423-b5e3-b51261b45991",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Obliczamy skuteczność strategii coalesce dla konwersji dat - jaki procent rekordów udało się sparsować. Wysoki procent niepowodzeń może wskazywać na potrzebę dodania innych formatów do coalesce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "033d41b0-cce0-4f6a-9d71-68b5feca2a39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Podgląd przykładowych konwersji dat\n",
    "display(df_with_dates.select(\"customer_id\", \"registration_date\", \"registration_date_parsed\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf10623-3980-4ef5-96bb-484b5e1dffa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Podgląd pokazuje oryginalne wartości dat i ich sparsowane odpowiedniki. Pozwala to na weryfikację poprawności konwersji i identyfikację problemów z formatami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0684e15-1bfb-443f-a3b5-34440dba84a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Podgląd rekordów z błędami parsowania (jeśli istnieją)\n",
    "parsing_errors = df_with_dates.filter(\n",
    "    F.col(\"registration_date\").isNotNull() & \n",
    "    F.col(\"registration_date_parsed\").isNull()\n",
    ").select(\"customer_id\", \"registration_date\", \"registration_date_parsed\").limit(5)\n",
    "\n",
    "if parsing_errors.count() > 0:\n",
    "    display(parsing_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d6c6a51-fd7b-4e99-88a7-e7ce00d54898",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Analiza rekordów z błędami parsowania pomaga zidentyfikować niezaplanowane formaty dat w danych źródłowych. Te informacje można wykorzystać do rozszerzenia strategii coalesce o dodatkowe formaty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09bd7ce6-fbf1-44d0-8f29-d1835d3bbfa2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.3: Konwersja timestamp i obliczenia czasowe\n",
    "\n",
    "**Cel:** Konwersja na timestamp i wykonanie obliczeń czasowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f3b5d8-50c6-46fe-b0cf-b95d13fde87a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_with_dates\n",
    "# VARIABLE: df_with_timestamp - DataFrame z timestamp i obliczeniami\n",
    "\n",
    "from pyspark.sql.functions import to_timestamp, current_timestamp, current_date, datediff\n",
    "\n",
    "# Konwersja registration_date_parsed na timestamp (dodaje czas 00:00:00)\n",
    "df_with_timestamp = df_with_dates.withColumn(\n",
    "    \"registration_timestamp\",\n",
    "    F.to_timestamp(F.col(\"registration_date_parsed\"))\n",
    ")\n",
    "\n",
    "# Obliczenia czasowe\n",
    "df_with_timestamp = df_with_timestamp \\\n",
    "    .withColumn(\"current_date\", current_date()) \\\n",
    "    .withColumn(\"days_since_registration\", \n",
    "        datediff(F.col(\"current_date\"), F.col(\"registration_date_parsed\"))\n",
    "    )\n",
    "\n",
    "# Statystyki\n",
    "print(\"=== Statystyki czasowe ===\")\n",
    "df_with_timestamp.select(\n",
    "    F.min(\"days_since_registration\").alias(\"min_days\"),\n",
    "    F.max(\"days_since_registration\").alias(\"max_days\"),\n",
    "    F.avg(\"days_since_registration\").alias(\"avg_days\")\n",
    ").show()\n",
    "\n",
    "# Przykłady\n",
    "print(\"\\n=== Przykładowe obliczenia czasowe ===\")\n",
    "display(df_with_timestamp.select(\n",
    "    \"customer_id\",\n",
    "    \"registration_date\",\n",
    "    \"registration_date_parsed\",\n",
    "    \"registration_timestamp\",\n",
    "    \"days_since_registration\"\n",
    ").orderBy(F.desc(\"days_since_registration\")).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a50f68-1d2d-4bc6-b7d2-43cf5fb0a916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Konwertujemy DateType na TimestampType używając `to_timestamp()`. Timestamp zawiera informację o dacie i czasie (domyślnie 00:00:00 dla samych dat), co jest przydatne do precyzyjnych obliczeń czasowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c3dbb1d-0f84-4a5f-b712-92d20f104d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Obliczenia czasowe - dodanie bieżącej daty i liczby dni od rejestracji\n",
    "df_with_timestamp = df_with_timestamp \\\n",
    "    .withColumn(\"current_date\", current_date()) \\\n",
    "    .withColumn(\"days_since_registration\", \n",
    "        datediff(F.col(\"current_date\"), F.col(\"registration_date_parsed\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86a2df2c-0ae8-4d31-9706-50a881c63bd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Wykonujemy obliczenia czasowe używając `current_date()` i `datediff()`. Kolumna `days_since_registration` pokazuje \"wiek\" każdego konta klienta, co jest przydatne do segmentacji i analizy kohort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c26269e9-69d6-4c4b-b39d-f7701f55d43b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Statystyki czasowe - min, max, średnia liczba dni od rejestracji\n",
    "time_stats = df_with_timestamp.select(\n",
    "    F.min(\"days_since_registration\").alias(\"min_days\"),\n",
    "    F.max(\"days_since_registration\").alias(\"max_days\"),\n",
    "    F.avg(\"days_since_registration\").alias(\"avg_days\")\n",
    ")\n",
    "\n",
    "display(time_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "460efef6-fa03-40ad-9b8d-455c91268fc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Obliczamy statystyki opisowe dla wieku kont - minimum, maksimum i średnią liczbę dni od rejestracji. Te metryki pomagają zrozumieć profil demograficzny bazy klientów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1ef11d4-ccf6-49b9-b469-a206484c375c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Podgląd przykładowych obliczeń czasowych\n",
    "display(df_with_timestamp.select(\n",
    "    \"customer_id\",\n",
    "    \"registration_date\",\n",
    "    \"registration_date_parsed\",\n",
    "    \"registration_timestamp\",\n",
    "    \"days_since_registration\"\n",
    ").orderBy(F.desc(\"days_since_registration\")).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66e32fd0-1b51-4ed6-8171-856ee1a3af6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Podgląd rekordów posortowanych według wieku konta (najstarsze pierwsze) pokazuje wszystkie etapy transformacji daty - od oryginalnego stringa, przez parsed date, timestamp, aż po obliczoną liczbę dni."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d796273-f020-436e-bb3c-9ce06ae27354",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 4: Deduplikacja danych\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Duplikaty są powszechnym problemem jakości danych wynikającym z błędów w systemach źródłowych, wielokrotnego ładowania tych samych danych lub błędów w procesach ETL. Strategia deduplikacji zależy od kontekstu biznesowego.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **dropDuplicates()**: Usunięcie duplikatów na podstawie wszystkich lub wybranych kolumn\n",
    "- **Exact duplicates**: Rekordy identyczne we wszystkich kolumnach\n",
    "- **Key duplicates**: Rekordy z tym samym kluczem biznesowym\n",
    "- **Deduplication strategy**: Którą wersję rekordu zachować (first, last, newest)\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Usuwanie duplikatów po key columns (customer_id)\n",
    "- Zachowanie najnowszej wersji rekordu (last insert wins)\n",
    "- Identyfikacja potencjalnych duplikatów (fuzzy matching)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "338e49bb-24f7-4612-a0e5-5320428ee8e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.1: Deduplikacja - wszystkie kolumny\n",
    "\n",
    "**Cel:** Usunięcie rekordów całkowicie identycznych (exact duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc07a589-27b8-4ccf-8bf3-e2ed1e0094b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_distinct - DataFrame bez dokładnych duplikatów\n",
    "\n",
    "# Usunięcie exact duplicates (wszystkie kolumny identyczne)\n",
    "df_distinct = df_customers.distinct()\n",
    "\n",
    "# Statystyki\n",
    "total = df_customers.count()\n",
    "distinct = df_distinct.count()\n",
    "duplicates = total - distinct\n",
    "\n",
    "print(\"=== Deduplikacja - wszystkie kolumny ===\")\n",
    "print(f\"Total rekordów: {total}\")\n",
    "print(f\"Unikalne rekordy: {distinct}\")\n",
    "print(f\"Usunięte duplikaty: {duplicates}\")\n",
    "print(f\"Duplication rate: {(duplicates/total)*100:.1f}%\")\n",
    "\n",
    "# Identyfikacja duplikatów przed usunięciem\n",
    "from pyspark.sql.functions import count as spark_count\n",
    "\n",
    "duplicated_records = df_customers \\\n",
    "    .groupBy(df_customers.columns) \\\n",
    "    .agg(spark_count(\"*\").alias(\"count\")) \\\n",
    "    .filter(F.col(\"count\") > 1) \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "if duplicated_records.count() > 0:\n",
    "    print(\"\\n=== Przykłady zduplikowanych rekordów ===\")\n",
    "    display(duplicated_records.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e8258f7-c833-49eb-8cb9-cb1c057d5677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Usuwamy duplikaty całkowicie identyczne (exact duplicates) używając `distinct()`. Ta operacja porównuje wszystkie kolumny i zachowuje tylko unikalne rekordy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76fbe0ed-707b-480b-be4c-f9f4881e5cb7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Oblicz statystyki deduplikacji\n",
    "total = df_customers.count()\n",
    "distinct = df_distinct.count()\n",
    "duplicates = total - distinct\n",
    "duplication_rate = (duplicates/total)*100\n",
    "\n",
    "display(f\"Total records: {total}\")\n",
    "display(f\"Unique records: {distinct}\")\n",
    "display(f\"Removed duplicates: {duplicates}\")\n",
    "display(f\"Duplication rate: {duplication_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbde5368-71c8-4939-8993-a4c22b2eb701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Obliczamy statystyki deduplikacji - liczbę usuniętych duplikatów i współczynnik duplikacji. Te metryki pomagają ocenić jakość danych źródłowych i skuteczność procesu czyszczenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8682a9ba-fcf4-4689-89ae-792623392cad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Identyfikacja duplikatów przed usunięciem (jeśli istnieją)\n",
    "from pyspark.sql.functions import count as spark_count\n",
    "\n",
    "duplicated_records = df_customers \\\n",
    "    .groupBy(df_customers.columns) \\\n",
    "    .agg(spark_count(\"*\").alias(\"count\")) \\\n",
    "    .filter(F.col(\"count\") > 1) \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "if duplicated_records.count() > 0:\n",
    "    display(duplicated_records.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d21bc62-a3b8-4092-93c0-1aa9dff4daff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Identyfikujemy konkretne rekordy, które są duplikatami - grupujemy po wszystkich kolumnach i szukamy grup z więcej niż jednym rekordem. To pomaga zrozumieć charakter duplikatów w danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1711638e-b2b3-4c65-987a-925ff3176546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.2: Deduplikacja per key columns\n",
    "\n",
    "**Cel:** Usunięcie duplikatów na podstawie klucza biznesowego (customer_id), zachowując najnowszy rekord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "697fb1a6-1703-483e-b4d9-355f8fc2e1a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_deduped - DataFrame z usuniętymi duplikatami per customer_id\n",
    "\n",
    "# Strategia 1: dropDuplicates() per customer_id - zachowuje pierwszy napotkany rekord\n",
    "df_deduped_simple = df_customers.dropDuplicates([\"customer_id\"])\n",
    "\n",
    "print(\"=== Deduplikacja per customer_id (strategia simple) ===\")\n",
    "print(f\"Przed: {df_customers.count()} rekordów\")\n",
    "print(f\"Po: {df_deduped_simple.count()} rekordów\")\n",
    "print(f\"Usunięto: {df_customers.count() - df_deduped_simple.count()} duplikatów\")\n",
    "\n",
    "# Strategia 2: Window function - zachowaj najnowszy rekord (jeśli mamy timestamp)\n",
    "# Zakładamy, że mamy kolumnę created_at lub inny timestamp\n",
    "\n",
    "if \"created_at\" in df_customers.columns or \"last_updated\" in df_customers.columns:\n",
    "    from pyspark.sql.window import Window\n",
    "    \n",
    "    timestamp_col = \"created_at\" if \"created_at\" in df_customers.columns else \"last_updated\"\n",
    "    \n",
    "    # Okno partycjonowane po customer_id, sortowane po timestamp desc\n",
    "    window_spec = Window.partitionBy(\"customer_id\").orderBy(F.desc(timestamp_col))\n",
    "    \n",
    "    df_deduped = df_customers \\\n",
    "        .withColumn(\"row_num\", F.row_number().over(window_spec)) \\\n",
    "        .filter(F.col(\"row_num\") == 1) \\\n",
    "        .drop(\"row_num\")\n",
    "    \n",
    "    print(f\"\\n=== Deduplikacja per customer_id (strategia: najnowszy rekord) ===\")\n",
    "    print(f\"Przed: {df_customers.count()} rekordów\")\n",
    "    print(f\"Po: {df_deduped.count()} rekordów\")\n",
    "    print(f\"Usunięto: {df_customers.count() - df_deduped.count()} duplikatów\")\n",
    "else:\n",
    "    # Jeśli brak timestamp, użyj prostej strategii\n",
    "    df_deduped = df_deduped_simple\n",
    "    print(\"\\n(Brak kolumny timestamp - użyto prostej strategii)\")\n",
    "\n",
    "# Identyfikacja duplikatów przed usunięciem\n",
    "duplicate_ids = df_customers \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(spark_count(\"*\").alias(\"count\")) \\\n",
    "    .filter(F.col(\"count\") > 1) \\\n",
    "    .orderBy(F.desc(\"count\"))\n",
    "\n",
    "if duplicate_ids.count() > 0:\n",
    "    print(f\"\\n=== {duplicate_ids.count()} customer_id z duplikatami ===\")\n",
    "    display(duplicate_ids.limit(10))\n",
    "    \n",
    "    # Przykłady duplikatów\n",
    "    sample_duplicate_id = duplicate_ids.first()[\"customer_id\"]\n",
    "    print(f\"\\n=== Przykład duplikatów dla customer_id={sample_duplicate_id} ===\")\n",
    "    display(df_customers.filter(F.col(\"customer_id\") == sample_duplicate_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a3b1a21-88d2-4cff-9a25-d8721de74922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Stosujemy prostą strategię deduplikacji per klucz biznesowy (`customer_id`) używając `dropDuplicates()`. Zachowuje pierwszy napotkany rekord dla każdego customer_id, co jest szybkie ale nie pozwala na wybór \"najlepszego\" rekordu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35025bf0-e70f-4ff6-a72a-b402ed6779af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Statystyki deduplikacji per customer_id\n",
    "records_before = df_customers.count()\n",
    "records_after = df_deduped_simple.count()\n",
    "records_removed = records_before - records_after\n",
    "\n",
    "display(f\"Records before: {records_before}\")\n",
    "display(f\"Records after: {records_after}\")\n",
    "display(f\"Removed duplicates: {records_removed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4afa9b74-4c77-4d6f-941e-c69977e0341c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Obliczamy skuteczność deduplikacji per klucz biznesowy - ile rekordów zostało usuniętych z powodu duplikatów customer_id. To kluczowa metryka dla oceny jakości danych i unikalności kluczy biznesowych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a564b0b-a641-440e-87b7-39e634d32224",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 5: Standardyzacja danych\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Standaryzacja polega na ujednoliceniu formatów i reprezentacji danych zgodnie z ustalonymi regułami biznesowymi. Niestandardowe dane (różne case, whitespace, formaty) utrudniają analizy, złączenia i agregacje.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **Text standardization**: trim(), lower(), upper(), initcap()\n",
    "- **Pattern standardization**: regexp_replace() dla kodów, telefonów\n",
    "- **Format standardization**: Ujednolicenie formatów dat, adresów\n",
    "- **Categorical standardization**: Mapowanie wariantów na standardowe wartości\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Ujednolicenie wielkości liter w polach tekstowych\n",
    "- Usunięcie whitespace z początku i końca\n",
    "- Standaryzacja kodów krajów, telefonów, kodów pocztowych\n",
    "- Konsolidacja wariantów kategorii (Active/active/ACTIVE → Active)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f80f07b2-120c-4d91-a4af-cfcb6a264dbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 5.1: Standaryzacja tekstu\n",
    "\n",
    "**Cel:** Oczyszczenie i standaryzacja pól tekstowych (trim, case, whitespace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3edc8e5-f85a-4676-af98-ef0411ba67ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_customers\n",
    "# VARIABLE: df_standardized - DataFrame ze standaryzowanymi polami tekstowymi\n",
    "\n",
    "# Standaryzacja pól tekstowych - usunięcie whitespace\n",
    "df_standardized = df_customers\n",
    "\n",
    "# Trim whitespace z wszystkich string columns\n",
    "for col_name in [\"first_name\", \"last_name\", \"email\", \"phone\", \"city\", \"state\", \"country\"]:\n",
    "    if col_name in df_standardized.columns:\n",
    "        df_standardized = df_standardized.withColumn(\n",
    "            col_name,\n",
    "            F.trim(F.col(col_name))\n",
    "        )\n",
    "\n",
    "# 2. Standaryzacja konkretnych kolumn\n",
    "# first_name, last_name: Title Case\n",
    "df_standardized = df_standardized.withColumn(\n",
    "    \"first_name\",\n",
    "    F.initcap(F.col(\"first_name\"))\n",
    ").withColumn(\n",
    "    \"last_name\", \n",
    "    F.initcap(F.col(\"last_name\"))\n",
    ")\n",
    "\n",
    "# email: lowercase (standard dla emaili)\n",
    "df_standardized = df_standardized.withColumn(\n",
    "    \"email\",\n",
    "    F.lower(F.col(\"email\"))\n",
    ")\n",
    "\n",
    "# country: uppercase (standard ISO dla kodów krajów)\n",
    "df_standardized = df_standardized.withColumn(\n",
    "    \"country\",\n",
    "    F.upper(F.col(\"country\"))\n",
    ")\n",
    "\n",
    "# city: Title Case\n",
    "df_standardized = df_standardized.withColumn(\n",
    "    \"city\",\n",
    "    F.initcap(F.col(\"city\"))\n",
    ")\n",
    "\n",
    "# Porównanie przed i po\n",
    "print(\"=== Porównanie standaryzacji ===\")\n",
    "display(df_customers.select(\"first_name\", \"last_name\", \"email\", \"city\", \"country\").limit(5))\n",
    "print(\"\\n↓↓↓ PO STANDARYZACJI ↓↓↓\\n\")\n",
    "display(df_standardized.select(\"first_name\", \"last_name\", \"email\", \"city\", \"country\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "913937df-ee00-4bce-8920-5c76a0503551",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Usuwamy whitespace (spacje, tabulatory) z początku i końca stringów używając `trim()`. To podstawowy krok standaryzacji, który eliminuje przypadkowe spacje mogące zakłócać analizy i złączenia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0d61108-eb27-4c63-9054-9e421beb9953",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standaryzacja nazwisk - Title Case\n",
    "df_standardized = df_standardized.withColumn(\n",
    "    \"first_name\",\n",
    "    F.initcap(F.col(\"first_name\"))\n",
    ").withColumn(\n",
    "    \"last_name\", \n",
    "    F.initcap(F.col(\"last_name\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c748332a-58f8-4b81-aecc-2dad1721ed4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Standaryzujemy imiona i nazwiska do Title Case używając `initcap()` - pierwsza litera wielka, pozostałe małe. To standard dla pól z nazwami własnymi, zapewniający jednolite formatowanie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afcd7829-c779-4b64-ae60-b3a2c6d4365f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standaryzacja emaili (lowercase) i krajów (uppercase)\n",
    "df_standardized = df_standardized.withColumn(\n",
    "    \"email\",\n",
    "    F.lower(F.col(\"email\"))\n",
    ").withColumn(\n",
    "    \"country\",\n",
    "    F.upper(F.col(\"country\"))\n",
    ").withColumn(\n",
    "    \"city\",\n",
    "    F.initcap(F.col(\"city\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b626829-34db-4210-9d1d-841f51f2ebe3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Stosujemy różne konwencje case'u dla różnych typów danych: email `lowercase` (standard internetowy), country `uppercase` (kody ISO), city `Title Case` (nazwy miejscowe). Każdy typ ma swoje uzasadnione konwencje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "15fc2c7b-0540-48f9-9625-2a127f105e5a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Porównanie przed standaryzacją\n",
    "display(\"BEFORE Standardization:\")\n",
    "display(df_customers.select(\"first_name\", \"last_name\", \"email\", \"city\", \"country\").limit(5))\n",
    "\n",
    "display(\"AFTER Standardization:\")\n",
    "display(df_standardized.select(\"first_name\", \"last_name\", \"email\", \"city\", \"country\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63b0c0b9-d208-48f5-a115-ae2004577e88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Porównanie przed i po standaryzacji pokazuje efekt transformacji - ujednolicone formatowanie, usunięte whitespace i spójne konwencje case'u. To kluczowe dla zapewnienia jakości danych analitycznych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68584674-f31c-4315-b93f-6027094167c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 5.2: Standaryzacja kodów i kategorii\n",
    "\n",
    "**Cel:** Ujednolicenie formatów kodów i mapowanie wariantów kategorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "168546b9-4a72-4645-a8a3-20dfda790e0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# RESOURCE: DataFrame df_standardized\n",
    "# VARIABLE: df_codes_standardized - DataFrame ze standaryzowanymi kodami\n",
    "\n",
    "df_codes_standardized = df_standardized\n",
    "\n",
    "# 1. Standaryzacja phone numbers (usunięcie wszystkich non-digit, format międzynarodowy)\n",
    "if \"phone\" in df_codes_standardized.columns:\n",
    "    df_codes_standardized = df_codes_standardized.withColumn(\n",
    "        \"phone_standardized\",\n",
    "        F.when(F.col(\"phone\").isNotNull(),\n",
    "            F.concat(\n",
    "                F.when(F.col(\"phone\").startswith(\"+\"), \"\")\n",
    "                 .otherwise(\"+1-\"),  # Domyślny prefiks dla USA\n",
    "                F.regexp_replace(F.col(\"phone\"), \"[^0-9]\", \"\")\n",
    "            )\n",
    "        ).otherwise(F.col(\"phone\"))\n",
    "    )\n",
    "\n",
    "# 2. Standaryzacja customer_segment (konsystentne nazewnictwo)\n",
    "if \"customer_segment\" in df_codes_standardized.columns:\n",
    "    df_codes_standardized = df_codes_standardized.withColumn(\n",
    "        \"customer_segment_standardized\",\n",
    "        F.when(F.upper(F.trim(F.col(\"customer_segment\"))) == \"PREMIUM\", \"Premium\")\n",
    "         .when(F.upper(F.trim(F.col(\"customer_segment\"))) == \"STANDARD\", \"Standard\")\n",
    "         .when(F.upper(F.trim(F.col(\"customer_segment\"))) == \"BASIC\", \"Basic\")\n",
    "         .otherwise(\"Unknown\")\n",
    "    )\n",
    "\n",
    "# 3. Standaryzacja country codes (3-letter ISO codes jako przykład)\n",
    "df_codes_standardized = df_codes_standardized.withColumn(\n",
    "    \"country_iso\",\n",
    "    F.when(F.upper(F.col(\"country\")) == \"USA\", \"USA\")\n",
    "     .when(F.upper(F.col(\"country\")) == \"POLAND\", \"POL\")\n",
    "     .when(F.upper(F.col(\"country\")) == \"GERMANY\", \"DEU\")\n",
    "     .when(F.upper(F.col(\"country\")) == \"FRANCE\", \"FRA\")\n",
    "     .otherwise(F.upper(F.col(\"country\")))\n",
    ")\n",
    "\n",
    "# Weryfikacja standaryzacji\n",
    "print(\"=== Standaryzacja kodów i kategorii ===\")\n",
    "\n",
    "if \"phone\" in df_codes_standardized.columns:\n",
    "    print(\"\\n--- Phone numbers ---\")\n",
    "    display(df_codes_standardized.select(\"phone\", \"phone_standardized\").limit(5))\n",
    "\n",
    "if \"customer_segment\" in df_codes_standardized.columns:\n",
    "    print(\"\\n--- Customer segments ---\")\n",
    "    display(df_codes_standardized.groupBy(\"customer_segment\", \"customer_segment_standardized\").count().orderBy(\"customer_segment\"))\n",
    "\n",
    "print(\"\\n--- Country codes ---\")\n",
    "display(df_codes_standardized.groupBy(\"country\", \"country_iso\").count().orderBy(\"country\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23750ae6-0a82-46fe-826e-5c6c44877b2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Porównanie PySpark vs SQL\n",
    "\n",
    "**DataFrame API (PySpark):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19b7ebef-9ca4-4017-8dd5-38e345a53866",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład: Kompleksowe czyszczenie danych w PySpark\n",
    "\n",
    "df_cleaned_pyspark = df_customers \\\n",
    "    .fillna({\"city\": \"Unknown\", \"country\": \"USA\"}) \\\n",
    "    .dropna(subset=[\"customer_id\"]) \\\n",
    "    .withColumn(\"first_name\", F.trim(F.initcap(F.col(\"first_name\")))) \\\n",
    "    .withColumn(\"last_name\", F.trim(F.initcap(F.col(\"last_name\")))) \\\n",
    "    .withColumn(\"email\", F.lower(F.trim(F.col(\"email\")))) \\\n",
    "    .dropDuplicates([\"customer_id\"])\n",
    "\n",
    "print(f\"Rekordy po czyszczeniu (PySpark): {df_cleaned_pyspark.count()}\")\n",
    "display(df_cleaned_pyspark.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dcb295ae-7f38-4e70-9c94-ffd19771a918",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**SQL Equivalent:**\n",
    "\n",
    "```sql\n",
    "# Najpierw utwórz temporary view\n",
    "df_customers.createOrReplaceTempView(\"customers_raw\")\n",
    "\n",
    "# SQL approach\n",
    "df_cleaned_sql = spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "    customer_id,\n",
    "    TRIM(INITCAP(first_name)) as first_name,\n",
    "    TRIM(INITCAP(last_name)) as last_name,\n",
    "    LOWER(TRIM(email)) as email,\n",
    "    COALESCE(city, 'Unknown') as city,\n",
    "    COALESCE(country, 'Unknown') as country,\n",
    "    phone,\n",
    "    registration_date,\n",
    "    customer_segment\n",
    "FROM customers_raw\n",
    "WHERE customer_id IS NOT NULL\n",
    "    AND email IS NOT NULL\n",
    "    AND email LIKE '%@%'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Rekordy po czyszczeniu (SQL): {df_cleaned_sql.count()}\")\n",
    "display(df_cleaned_sql.limit(5))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5349481-bf50-4ecb-b13f-e4577f325e25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Najpierw utwórz temporary view\n",
    "df_customers.createOrReplaceTempView(\"customers_raw\")\n",
    "\n",
    "# SQL approach\n",
    "df_cleaned_sql = spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "    customer_id,\n",
    "    TRIM(INITCAP(first_name)) as first_name,\n",
    "    TRIM(INITCAP(last_name)) as last_name,\n",
    "    LOWER(TRIM(email)) as email,\n",
    "    COALESCE(city, 'Unknown') as city,\n",
    "    COALESCE(country, 'USA') as country,\n",
    "    phone,\n",
    "    registration_date,\n",
    "    customer_segment\n",
    "FROM customers_raw\n",
    "WHERE customer_id IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Rekordy po czyszczeniu (SQL): {df_cleaned_sql.count()}\")\n",
    "display(df_cleaned_sql.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "754c4117-f1f6-439e-8e17-534fdc0f8f8b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Porównanie:**\n",
    "- **Wydajność**: Identyczna - Catalyst optimizer kompiluje oba do tego samego execution plan\n",
    "- **Kiedy używać PySpark**: Łańcuchowe transformacje, dynamiczne pipeline'y, integracja z Python libraries\n",
    "- **Kiedy używać SQL**: Złożone joiny, window functions, analitycy preferujący SQL syntax\n",
    "- **Best practice**: Używaj tego, co jest bardziej czytelne dla zespołu i przypadku użycia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afad2cee-3fc2-431c-81ae-6afa560d3560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist - Co powinieneś uzyskać:\n",
    "- [ ] Dane załadowane z dataset/ (customers.csv używając DATASET_BASE_PATH)\n",
    "- [ ] Data profiling report wykonany\n",
    "- [ ] Null values obsłużone (fillna/dropna/coalesce)\n",
    "- [ ] Typy danych skonwertowane i zwalidowane (cast, to_date)\n",
    "- [ ] Duplikaty usunięte (dropDuplicates)\n",
    "- [ ] Pola tekstowe standaryzowane (trim, case)\n",
    "- [ ] Kody i kategorie zunifikowane\n",
    "- [ ] Brak błędów w execution\n",
    "\n",
    "### Komendy weryfikacyjne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfde7220-5244-4cc2-9d2d-ef793d4cb65d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weryfikacja wyników - użyj df_codes_standardized jako final\n",
    "\n",
    "df_final = df_codes_standardized\n",
    "\n",
    "print(\"=== WERYFIKACJA WYNIKÓW ===\\n\")\n",
    "\n",
    "# 1. Podstawowe statystyki\n",
    "print(\"1. Podstawowe statystyki:\")\n",
    "print(f\"   Liczba rekordów: {df_final.count()}\")\n",
    "print(f\"   Liczba kolumn: {len(df_final.columns)}\")\n",
    "\n",
    "# 2. Null values per kolumna (sprawdź tylko pierwsze 10 kolumn)\n",
    "print(\"\\n2. Null values per kolumna (próbka):\")\n",
    "for col_name in df_final.columns[:10]:  # Pokaż tylko pierwsze 10\n",
    "    null_count = df_final.filter(F.col(col_name).isNull()).count()\n",
    "    print(f\"   {col_name:20s}: {null_count:4d} nulls\")\n",
    "\n",
    "# 3. Duplikaty\n",
    "print(\"\\n3. Analiza duplikatów:\")\n",
    "total = df_final.count()\n",
    "distinct = df_final.distinct().count()\n",
    "print(f\"   Total: {total}, Distinct: {distinct}, Duplicates: {total - distinct}\")\n",
    "\n",
    "# 4. Schemat danych (tylko typy)\n",
    "print(\"\\n4. Schemat danych:\")\n",
    "for field in df_final.schema.fields[:10]:  # Pokaż tylko pierwsze 10\n",
    "    print(f\"   {field.name:20s}: {field.dataType}\")\n",
    "\n",
    "# 5. Podgląd danych\n",
    "print(\"\\n5. Podgląd wyczyszczonych danych:\")\n",
    "display(df_final.select(\"customer_id\", \"first_name\", \"last_name\", \"email\", \"customer_segment\", \"country\").limit(10))\n",
    "\n",
    "# 6. Testy asercyjne\n",
    "try:\n",
    "    assert df_final.count() > 0, \"BŁĄD: DataFrame jest pusty\"\n",
    "    assert df_final.filter(F.col(\"customer_id\").isNull()).count() == 0, \"BŁĄD: Null values w customer_id\"\n",
    "    assert df_final.count() == df_final.dropDuplicates([\"customer_id\"]).count(), \"BŁĄD: Duplikaty w customer_id\"\n",
    "    print(\"\\n✓ Wszystkie testy przeszły pomyślnie!\")\n",
    "except AssertionError as e:\n",
    "    print(f\"\\n✗ Test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ddcecda-332b-4036-a6ed-038d134bafb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Problem 1: Błąd przy wczytywaniu z Volume\n",
    "**Objawy:**\n",
    "- FileNotFoundException\n",
    "- \"Path does not exist\"\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Sprawdź zawartość DATASET_BASE_PATH\n",
    "import os\n",
    "if os.path.exists(DATASET_BASE_PATH):\n",
    "    print(f\"✓ DATASET_BASE_PATH exists: {DATASET_BASE_PATH}\")\n",
    "    for folder in os.listdir(DATASET_BASE_PATH):\n",
    "        print(f\"  - {folder}/\")\n",
    "else:\n",
    "    print(f\"✗ DATASET_BASE_PATH not found: {DATASET_BASE_PATH}\")\n",
    "\n",
    "# Lista plików w customers/\n",
    "customers_dir = f\"{DATASET_BASE_PATH}/customers\"\n",
    "if os.path.exists(customers_dir):\n",
    "    for file in os.listdir(customers_dir):\n",
    "        print(f\"  - {file}\")\n",
    "```\n",
    "\n",
    "### Problem 2: InferSchema daje niepoprawne typy\n",
    "**Objawy:** \n",
    "- Kolumny numeryczne jako StringType\n",
    "- Daty nie są rozpoznawane\n",
    "\n",
    "**Rozwiązanie:**\n",
    "Zdefiniuj schemat jawnie:\n",
    "```python\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"customer_id\", StringType(), False),\n",
    "    StructField(\"first_name\", StringType(), True),\n",
    "    StructField(\"last_name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"phone\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"registration_date\", StringType(), True),\n",
    "    StructField(\"customer_segment\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").schema(schema).load(customers_path)\n",
    "```\n",
    "\n",
    "### Problem 3: Konwersja dat zwraca null\n",
    "**Objawy:**\n",
    "- to_date() zwraca null dla wszystkich wartości\n",
    "- Niepoprawny format daty\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Sprawdź przykładowe wartości\n",
    "df.select(\"registration_date\").show(5, truncate=False)\n",
    "\n",
    "# Wypróbuj różne formaty\n",
    "df.select(\n",
    "    F.col(\"registration_date\"),\n",
    "    F.to_date(F.col(\"registration_date\"), \"yyyy-MM-dd\").alias(\"format1\"),\n",
    "    F.to_date(F.col(\"registration_date\"), \"dd/MM/yyyy\").alias(\"format2\"),\n",
    "    F.to_date(F.col(\"registration_date\"), \"MM-dd-yyyy\").alias(\"format3\")\n",
    ").show(5)\n",
    "```\n",
    "\n",
    "### Debugging tips:\n",
    "- Użyj `.explain()` aby zobaczyć plan wykonania\n",
    "- Sprawdź typy: `df.printSchema()`\n",
    "- Podgląd danych: `display(df.limit(10))`\n",
    "- Monitoruj null counts po każdej transformacji\n",
    "- Użyj `.show(truncate=False)` dla pełnych wartości"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d767b746-5b28-4c79-915c-0ecdfae8b815",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "### Wydajność:\n",
    "- Użyj jawnych schematów zamiast inferSchema w produkcji (szybsze, bardziej przewidywalne)\n",
    "- dropDuplicates() jest operacją shuffle - użyj ostrożnie na dużych danych\n",
    "- Partycjonuj dane przed deduplikacją per klucz biznesowy\n",
    "- Cache() DataFrames używanych wielokrotnie\n",
    "\n",
    "### Jakość kodu:\n",
    "- Zawsze waliduj dane po konwersji typów (null counts, value ranges)\n",
    "- Dodawaj flagi jakości (email_valid, date_parsed_success) dla auditability\n",
    "- Loguj statystyki czyszczenia (ile rekordów usunięto, wypełniono)\n",
    "- Zachowaj oryginalne wartości w osobnych kolumnach dla troubleshootingu\n",
    "\n",
    "### Data Quality:\n",
    "- Definiuj jasne reguły biznesowe dla każdego pola (allowed values, ranges)\n",
    "- Implementuj quality checks jako assertions w pipeline\n",
    "- Używaj coalesce() dla fallback strategies zamiast prostego fillna()\n",
    "- Dokumentuj decyzje o czyszczeniu (dlaczego usunięto, dlaczego fillna)\n",
    "\n",
    "### Governance:\n",
    "- Wszystkie transformacje muszą być deterministyczne (powtarzalne)\n",
    "- Audit trail: timestamp, user, operation w metadata columns\n",
    "- Zachowaj raw data w Bronze layer (nigdy nie nadpisuj źródła)\n",
    "- Używaj DATASET_BASE_PATH z 00_setup.ipynb dla spójnych ścieżek do danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "629e9ca1-ea24-493e-82ad-133cbce3fb72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Podsumowanie\n",
    "\n",
    "### Co zostało osiągnięte:\n",
    "- Załadowanie danych z folderu dataset/ (używając DATASET_BASE_PATH)\n",
    "- Data profiling i identyfikacja problemów jakości\n",
    "- Obsługa wartości null (fillna, dropna, coalesce)\n",
    "- Walidacja i konwersja typów (cast, to_date, to_timestamp)\n",
    "- Deduplikacja rekordów (distinct, dropDuplicates, window functions)\n",
    "- Standaryzacja tekstu i kodów (trim, case, regexp_replace)\n",
    "- Porównanie PySpark vs SQL approaches\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "1. **Data profiling first**: Zawsze analizuj dane przed rozpoczęciem czyszczenia\n",
    "2. **Context matters**: Strategia czyszczenia zależy od kontekstu biznesowego\n",
    "3. **Validation is critical**: Zawsze waliduj wyniki konwersji i transformacji\n",
    "4. **Document decisions**: Loguj statystyki i decyzje dla auditability\n",
    "\n",
    "### Quick Reference - Najważniejsze komendy:\n",
    "\n",
    "| Operacja | PySpark | SQL |\n",
    "|----------|---------|-----|\n",
    "| Fill nulls | `df.fillna({\"col\": \"value\"})` | `COALESCE(col, 'value')` |\n",
    "| Drop nulls | `df.dropna(subset=[\"col\"])` | `WHERE col IS NOT NULL` |\n",
    "| Parse date | `to_date(col(\"date\"), \"yyyy-MM-dd\")` | `TO_DATE(date, 'yyyy-MM-dd')` |\n",
    "| Remove duplicates | `df.dropDuplicates([\"id\"])` | `SELECT DISTINCT` |\n",
    "| Trim whitespace | `trim(col(\"name\"))` | `TRIM(name)` |\n",
    "| Lowercase | `lower(col(\"email\"))` | `LOWER(email)` |\n",
    "| Uppercase | `upper(col(\"country\"))` | `UPPER(country)` |\n",
    "| Title case | `initcap(col(\"city\"))` | `INITCAP(city)` |\n",
    "\n",
    "### Następne kroki:\n",
    "- **Kolejny notebook**: 05_views_workflows.ipynb - Persistent views i basic workflows\n",
    "- **Warsztat praktyczny**: 02_transformations_cleaning_workshop.ipynb\n",
    "- **Materiały dodatkowe**: \n",
    "  - Databricks Data Quality Guide\n",
    "  - Delta Lake best practices documentation\n",
    "- **Zadanie domowe**: Zastosuj poznane techniki czyszczenia na dataset orders (orders_batch.json z dataset/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6da07bba-6574-4094-955d-5e3b2e1f2347",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Czyszczenie zasobów\n",
    "\n",
    "Posprzątaj zasoby utworzone podczas notebooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a480f43-f0b1-4b84-a13a-fab63379f2a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Opcjonalne czyszczenie zasobów testowych\n",
    "# UWAGA: Uruchom tylko jeśli chcesz usunąć wszystkie utworzone dane\n",
    "\n",
    "# Usuń temporary views\n",
    "spark.catalog.dropTempView(\"customers_raw\")\n",
    "\n",
    "# Wyczyść cache\n",
    "spark.catalog.clearCache()\n",
    "\n",
    "print(\"Temporary views i cache zostały wyczyszczone\")\n",
    "print(\"Dane źródłowe w Volume pozostają nienaruszone\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_data_cleaning_quality",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
