{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8060371b-0939-4f55-a509-89cd691f2a22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Workshop: Workspace Setup, Data Import & Exploration\n",
    "\n",
    "**Cel szkoleniowy:** Praktyczne opanowanie konfiguracji workspace, importowania danych z rÃ³Å¼nych formatÃ³w oraz podstawowych operacji eksploracyjnych.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Konfiguracja workspace i klastrÃ³w\n",
    "- Wczytywanie rÃ³Å¼nych formatÃ³w danych (CSV, JSON, Parquet)\n",
    "- Podstawowa eksploracja danych\n",
    "- Konstruowanie schematÃ³w manualnie\n",
    "- Analiza brakÃ³w danych i wartoÅ›ci unikalnych\n",
    "\n",
    "**Czas trwania:** 90 minut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b0c559c-962d-4c80-bed9-adf06ab250e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **DzieÅ„ szkolenia**: DzieÅ„ 1 - Fundamentals & Exploration\n",
    "- **Typ notebooka**: Workshop\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog wÅ‚Ä…czony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df0cb0e5-4641-4bea-b869-58c4944f69cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wprowadzenie do warsztatu\n",
    "\n",
    "W tym warsztacie bÄ™dziesz pracowaÄ‡ z rzeczywistymi danymi KION:\n",
    "- **Customers** (customers.csv) - dane klientÃ³w\n",
    "- **Orders** (orders_batch.json) - zamÃ³wienia\n",
    "- **Products** (products.parquet) - produkty\n",
    "\n",
    "### Zadania do wykonania:\n",
    "1. Konfiguracja Å›rodowiska i zmiennych\n",
    "2. Import danych z CSV, JSON, Parquet\n",
    "3. Konstrukcja schematÃ³w manualnie\n",
    "4. Eksploracja danych: statystyki, braki, unikalne wartoÅ›ci\n",
    "5. Analiza jakoÅ›ci danych\n",
    "\n",
    "### Kryteria sukcesu:\n",
    "- Wszystkie 3 datasety poprawnie wczytane\n",
    "- Schematy zdefiniowane manualnie i zastosowane\n",
    "- Kompletna analiza eksploracyjna przeprowadzona\n",
    "- Zidentyfikowane problemy jakoÅ›ci danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f20c2fc5-c9ff-435f-8204-00027193aa9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## WstÄ™p teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie podstaw pracy z danymi w Databricks Lakehouse\n",
    "\n",
    "**Podstawowe pojÄ™cia:**\n",
    "- **Workspace**: Åšrodowisko Databricks zawierajÄ…ce notebooks, klastry i dane\n",
    "- **Cluster**: ZbiÃ³r maszyn wirtualnych przetwarzajÄ…cych dane\n",
    "- **DataFrame**: Rozproszona kolekcja danych zorganizowana w kolumny\n",
    "- **Schema**: Struktura danych definiujÄ…ca nazwy kolumn i typy danych\n",
    "- **Format danych**: CSV (text), JSON (semi-structured), Parquet (columnar binary)\n",
    "\n",
    "**Dlaczego to waÅ¼ne?**\n",
    "Poprawne wczytanie i eksploracja danych to fundament kaÅ¼dego pipeline'u ETL. Zrozumienie schematÃ³w, formatÃ³w i metod eksploracji pozwala na wczesne wykrycie problemÃ³w jakoÅ›ci danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da5ac3d6-0897-4c7a-bc46-abd7edfc3d8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inicjalizacja Å›rodowiska\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogÃ³w i schematÃ³w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e035ba-e5ef-4d12-8de4-f01acd72ee06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "903908c9-c847-46a1-8fca-49842d81b0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Zdefiniuj zmienne specyficzne dla warsztatu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d29c33a-ff2b-48d2-92b3-d26cb2062f1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ÅšcieÅ¼ki do plikÃ³w danych (juÅ¼ zdefiniowane)\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "# WyÅ›wietl kontekst uÅ¼ytkownika\n",
    "print(\"=== Kontekst uÅ¼ytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"UÅ¼ytkownik: {raw_user}\")\n",
    "print(f\"\\n=== ÅšcieÅ¼ki do danych ===\")\n",
    "print(f\"Customers CSV: {CUSTOMERS_CSV}\")\n",
    "print(f\"Orders JSON: {ORDERS_JSON}\")\n",
    "print(f\"Products Parquet: {PRODUCTS_PARQUET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2adab892-b60e-4d80-9cab-b762263f5c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Zadanie 1: Import danych CSV (20 min)\n",
    "\n",
    "### Cel:\n",
    "Wczytaj dane klientÃ³w z pliku CSV, najpierw z automatycznym wykrywaniem schematu, potem z rÄ™cznie zdefiniowanym schematem.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Wczytaj `customers.csv` z opcjÄ… `inferSchema=True`\n",
    "2. WyÅ›wietl schemat i 5 pierwszych rekordÃ³w\n",
    "3. Policz liczbÄ™ rekordÃ³w\n",
    "4. Zdefiniuj schemat manualnie (StructType)\n",
    "5. Wczytaj ponownie uÅ¼ywajÄ…c rÄ™cznego schematu\n",
    "6. PorÃ³wnaj schematy\n",
    "\n",
    "### Oczekiwany rezultat:\n",
    "- DataFrame z danymi klientÃ³w\n",
    "- Schemat zawierajÄ…cy kolumny: customer_id (int), customer_name (string), email (string), country (string), registration_date (timestamp)\n",
    "\n",
    "### WskazÃ³wki:\n",
    "- UÅ¼yj `spark.read.format(\"csv\").option(\"header\", \"true\")`\n",
    "- Do rÄ™cznego schematu uÅ¼yj: `StructType`, `StructField`, `IntegerType`, `StringType`, `TimestampType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68e06abc-0be9-4adc-8a55-60b7bf487f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PrzykÅ‚ad: Wczytanie pliku CSV z klientami\n",
    "# Ta komÃ³rka jest juÅ¼ gotowa - przeanalizuj kod i uruchom jÄ…\n",
    "\n",
    "customers_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "# WyÅ›wietl schemat i pierwsze wiersze\n",
    "print(\"=== Schema ===\")\n",
    "customers_df.printSchema()\n",
    "\n",
    "print(\"\\n=== PrzykÅ‚adowe dane (5 wierszy) ===\")\n",
    "display(customers_df.limit(5))\n",
    "\n",
    "# Policz rekordy\n",
    "count = customers_df.count()\n",
    "print(f\"\\n[INFO] Liczba klientÃ³w: {count}\")\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f666671-9e58-4956-ab71-15fa1ae84d9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zdefiniuj schemat manualnie\n",
    "# UzupeÅ‚nij typy danych dla kaÅ¼dego pola\n",
    "customers_schema = StructType([\n",
    "    StructField(\"customer_id\", ____, False),  # IntegerType\n",
    "    StructField(\"customer_name\", ____, True),  # StringType\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"country\", StringType(), True),\n",
    "    StructField(\"registration_date\", ____, True)  # TimestampType\n",
    "])\n",
    "\n",
    "# Wczytaj ponownie z rÄ™cznym schematem\n",
    "customers_df_manual = (\n",
    "    spark.read\n",
    "    .format(\"____\")  # csv\n",
    "    .option(\"header\", \"____\")  # true\n",
    "    .schema(____)  # customers_schema\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "# WyÅ›wietl schemat\n",
    "print(\"[INFO] Schemat z rÄ™cznÄ… definicjÄ…:\")\n",
    "customers_df_manual.printSchema()\n",
    "\n",
    "# WyÅ›wietl przykÅ‚adowe dane\n",
    "display(customers_df_manual.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c4bd244-51b5-4f4a-8dc7-16f3eeb4555e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Zadanie 2: Import danych JSON (15 min)\n",
    "\n",
    "### Cel:\n",
    "Wczytaj dane zamÃ³wieÅ„ z pliku JSON i zdefiniuj schemat manualnie.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Wczytaj `orders_batch.json` z `inferSchema=True`\n",
    "2. Zbadaj strukturÄ™ danych (schemat, typy)\n",
    "3. Zdefiniuj schemat manualnie\n",
    "4. Wczytaj ponownie z rÄ™cznym schematem\n",
    "\n",
    "### Oczekiwany rezultat:\n",
    "- DataFrame z zamÃ³wieniami\n",
    "- Schemat: order_id (int), customer_id (int), order_date (timestamp), total_amount (double), status (string)\n",
    "\n",
    "### WskazÃ³wki:\n",
    "- JSON nie wymaga opcji `header`\n",
    "- UÅ¼yj `DoubleType` dla kwot pieniÄ™Å¼nych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "412ccc42-659c-44bc-a8fe-2ccd8c314803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PrzykÅ‚ad: Wczytanie pliku JSON z zamÃ³wieniami\n",
    "# Ta komÃ³rka jest juÅ¼ gotowa - przeanalizuj kod i uruchom jÄ…\n",
    "\n",
    "orders_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "print(\"=== Schema ===\")\n",
    "orders_df.printSchema()\n",
    "\n",
    "print(\"\\n=== PrzykÅ‚adowe dane (5 wierszy) ===\")\n",
    "display(orders_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ed2904e-9dd6-4ed2-8180-3c95e36708f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zdefiniuj schemat dla orders\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", ____, False),  # IntegerType\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", ____, True),  # TimestampType\n",
    "    StructField(\"total_amount\", ____, True),  # DoubleType\n",
    "    StructField(\"status\", ____, True)  # StringType\n",
    "])\n",
    "\n",
    "# Wczytaj z rÄ™cznym schematem\n",
    "orders_df_manual = (\n",
    "    spark.read\n",
    "    .format(\"____\")  # json\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .schema(____)  # orders_schema\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "# WyÅ›wietl schemat\n",
    "print(\"[INFO] Schemat orders z rÄ™cznÄ… definicjÄ…:\")\n",
    "orders_df_manual.printSchema()\n",
    "\n",
    "# WyÅ›wietl przykÅ‚adowe dane\n",
    "display(orders_df_manual.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5ca31f7-ccbc-4d79-9807-8aa8c7c4c572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Zadanie 3: Import danych Parquet (10 min)\n",
    "\n",
    "### Cel:\n",
    "Wczytaj dane produktÃ³w z pliku Parquet (schemat jest wbudowany).\n",
    "\n",
    "### Instrukcje:\n",
    "1. Wczytaj `products.parquet`\n",
    "2. SprawdÅº schemat (Parquet zawiera schemat wbudowany)\n",
    "3. WyÅ›wietl dane\n",
    "4. Policz rekordy\n",
    "\n",
    "### Oczekiwany rezultat:\n",
    "- DataFrame z produktami\n",
    "- Schemat automatycznie zaÅ‚adowany z pliku Parquet\n",
    "\n",
    "### WskazÃ³wki:\n",
    "- Parquet nie wymaga `inferSchema` ani rÄ™cznego schematu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80d4bf49-867d-47e7-a473-3afbb7e6a968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wczytaj products.parquet\n",
    "products_df = (\n",
    "    spark.read\n",
    "    .format(\"____\")  # parquet\n",
    "    .load(PRODUCTS_PARQUET)\n",
    ")\n",
    "\n",
    "# WyÅ›wietl schemat (Parquet zawiera wbudowany schemat)\n",
    "print(\"[INFO] Schemat products (wbudowany w Parquet):\")\n",
    "products_df.printSchema()\n",
    "\n",
    "# WyÅ›wietl dane\n",
    "print(\"\\n[INFO] PrzykÅ‚adowe dane produktÃ³w:\")\n",
    "display(products_df.limit(5))\n",
    "\n",
    "# Policz rekordy\n",
    "product_count = products_df.count()\n",
    "print(f\"\\n[INFO] Liczba produktÃ³w: {product_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95f537b1-0441-4828-9046-54077bda7711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Zapis danych do Delta Lake\n",
    "\n",
    "### Cel:\n",
    "Zapisz wczytane DataFrames do tabel Delta Lake dla dalszego uÅ¼ycia.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Zapisz customers do tabeli `bronze.customers_workshop`\n",
    "2. Zapisz orders do tabeli `bronze.orders_workshop`\n",
    "3. Zapisz products do tabeli `bronze.products_workshop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7694b3d9-1a3d-4d1c-b5f2-a894f49d3a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# PrzykÅ‚ad: Zapis customers do Delta Lake\n",
    "# Ta komÃ³rka jest juÅ¼ gotowa - przeanalizuj kod i uruchom jÄ…\n",
    "\n",
    "customers_table = f\"{BRONZE_SCHEMA}.customers_workshop\"\n",
    "\n",
    "(\n",
    "    customers_df_manual\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(customers_table)\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Zapisano tabelÄ™: {customers_table}\")\n",
    "spark.sql(f\"DESCRIBE TABLE {customers_table}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2fbeb31-be1f-40d6-9045-e14d3789d8c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zapisz orders do Delta Lake\n",
    "orders_table = f\"{BRONZE_SCHEMA}.orders_workshop\"\n",
    "\n",
    "(\n",
    "    orders_df_manual\n",
    "    .write\n",
    "    .format(\"____\")  # delta\n",
    "    .mode(\"____\")  # overwrite\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(____)  # orders_table\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Zapisano tabelÄ™: {orders_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e887243-2829-4e78-aa88-31d3e179a9c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zapisz products do Delta Lake\n",
    "products_table = f\"{BRONZE_SCHEMA}.products_workshop\"\n",
    "\n",
    "(\n",
    "    ____  # products_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(products_table)\n",
    ")\n",
    "\n",
    "print(f\"âœ“ Zapisano tabelÄ™: {products_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f43982a5-ed74-452a-9315-7853905c3c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Zadanie 4: Eksploracja danych - Customers (15 min)\n",
    "\n",
    "### Cel:\n",
    "PrzeprowadÅº szczegÃ³Å‚owÄ… eksploracjÄ™ danych klientÃ³w.\n",
    "\n",
    "### Instrukcje:\n",
    "1. WyÅ›wietl listÄ™ kolumn i ich typy\n",
    "2. Policz liczbÄ™ unikalnych klientÃ³w\n",
    "3. ZnajdÅº liczbÄ™ klientÃ³w wedÅ‚ug krajÃ³w\n",
    "4. SprawdÅº, czy sÄ… wartoÅ›ci NULL w kolumnach\n",
    "5. Wygeneruj statystyki opisowe (`describe()`)\n",
    "\n",
    "### Oczekiwany rezultat:\n",
    "- Kompletna analiza eksploracyjna\n",
    "- Zidentyfikowane braki danych\n",
    "- RozkÅ‚ad geograficzny klientÃ³w\n",
    "\n",
    "### WskazÃ³wki:\n",
    "- UÅ¼yj: `columns`, `dtypes`, `count()`, `distinct()`, `groupBy()`, `describe()`\n",
    "- Do sprawdzenia NULL: `.filter(col(\"column_name\").isNull())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0287bf6e-9cdf-487f-9800-e0269a65d455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WyÅ›wietl kolumny i typy\n",
    "print(\"=== Kolumny i typy danych ===\")\n",
    "print(\"Kolumny:\", customers_df_manual.columns)\n",
    "print(\"\\nTypy danych:\", customers_df_manual.dtypes)\n",
    "\n",
    "# TODO: Policz unikalne wartoÅ›ci customer_id\n",
    "unique_customers = customers_df_manual.select(\"____\").distinct().count()  # customer_id\n",
    "total_customers = customers_df_manual.count()\n",
    "\n",
    "print(f\"\\n=== Statystyki podstawowe ===\")\n",
    "print(f\"Liczba wszystkich rekordÃ³w: {____}\")  # total_customers\n",
    "print(f\"Liczba unikalnych klientÃ³w: {unique_customers}\")\n",
    "print(f\"Duplikaty: {total_customers - unique_customers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9de8a409-133f-4cd7-bfea-ecf1cf82c966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Policz klientÃ³w wedÅ‚ug krajÃ³w\n",
    "customers_by_country = (\n",
    "    customers_df_manual\n",
    "    .groupBy(\"____\")  # country\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=____)  # False\n",
    ")\n",
    "\n",
    "print(\"=== RozkÅ‚ad klientÃ³w wedÅ‚ug krajÃ³w ===\")\n",
    "display(customers_by_country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13256e61-af3e-476e-befa-c377c60138f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SprawdÅº wartoÅ›ci NULL w kaÅ¼dej kolumnie\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "null_counts = customers_df_manual.select([\n",
    "    spark_sum(col(c).____().____(\"int\")).alias(c)  # isNull, cast\n",
    "    for c in customers_df_manual.columns\n",
    "])\n",
    "\n",
    "print(\"=== Liczba wartoÅ›ci NULL w kaÅ¼dej kolumnie ===\")\n",
    "display(null_counts)\n",
    "\n",
    "# Oblicz procent brakujÄ…cych danych\n",
    "total_rows = customers_df_manual.count()\n",
    "print(f\"\\nCaÅ‚kowita liczba rekordÃ³w: {total_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d66afe98-3c5e-4ace-8188-4f3468446e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wygeneruj statystyki opisowe\n",
    "print(\"=== Statystyki opisowe ===\")\n",
    "display(customers_df_manual.____())  # describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52f4c1d3-d675-4290-aa9f-05c95a3df3b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Zadanie 5: Eksploracja danych - Orders (15 min)\n",
    "\n",
    "### Cel:\n",
    "PrzeprowadÅº analizÄ™ zamÃ³wieÅ„.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Policz liczbÄ™ zamÃ³wieÅ„ wedÅ‚ug statusu\n",
    "2. Oblicz caÅ‚kowitÄ… wartoÅ›Ä‡ zamÃ³wieÅ„\n",
    "3. ZnajdÅº Å›redniÄ…, min, max wartoÅ›Ä‡ zamÃ³wienia\n",
    "4. SprawdÅº braki danych\n",
    "5. ZnajdÅº 10 najdroÅ¼szych zamÃ³wieÅ„\n",
    "\n",
    "### Oczekiwany rezultat:\n",
    "- Statystyki biznesowe zamÃ³wieÅ„\n",
    "- Identyfikacja problemÃ³w z danymi\n",
    "- Top 10 zamÃ³wieÅ„\n",
    "\n",
    "### WskazÃ³wki:\n",
    "- UÅ¼yj `.agg()` z funkcjami: `sum`, `avg`, `min`, `max`\n",
    "- Do sortowania: `.orderBy(col(\"column\").desc())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef1f9d64-6c54-428b-b579-e78d0d41b922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Policz zamÃ³wienia wedÅ‚ug statusu\n",
    "orders_by_status = (\n",
    "    orders_df_manual\n",
    "    .groupBy(\"____\")  # status\n",
    "    .____()  # count\n",
    "    .orderBy(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"=== ZamÃ³wienia wedÅ‚ug statusu ===\")\n",
    "display(orders_by_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "498faee6-df10-4523-a21f-6b0fa8c12e78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Oblicz statystyki dla total_amount\n",
    "from pyspark.sql.functions import sum, avg, min, max, count, round as spark_round\n",
    "\n",
    "orders_stats = orders_df_manual.select(\n",
    "    count(\"*\").alias(\"total_orders\"),\n",
    "    spark_round(____(\"total_amount\"), 2).alias(\"total_revenue\"),  # sum\n",
    "    spark_round(____(\"total_amount\"), 2).alias(\"avg_order_value\"),  # avg\n",
    "    ____(\"total_amount\").alias(\"min_order\"),  # min\n",
    "    ____(\"total_amount\").alias(\"max_order\")  # max\n",
    ")\n",
    "\n",
    "print(\"=== Statystyki zamÃ³wieÅ„ ===\")\n",
    "display(orders_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e26c835-d516-487d-bc0d-cd7a63418fec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ZnajdÅº 10 najdroÅ¼szych zamÃ³wieÅ„\n",
    "top_orders = (\n",
    "    orders_df_manual\n",
    "    .orderBy(col(\"total_amount\").____())  # desc\n",
    "    .limit(____)  # 10\n",
    ")\n",
    "\n",
    "print(\"=== Top 10 najdroÅ¼szych zamÃ³wieÅ„ ===\")\n",
    "display(top_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56e445e5-34b0-43ee-aef2-a5f07769edb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SprawdÅº braki danych w orders\n",
    "null_counts_orders = orders_df_manual.select([\n",
    "    spark_sum(col(c).isNull().cast(\"int\")).alias(c) \n",
    "    for c in orders_df_manual.columns\n",
    "])\n",
    "\n",
    "print(\"=== WartoÅ›ci NULL w zamÃ³wieniach ===\")\n",
    "display(null_counts_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20c4faf5-cce1-4715-91e9-c2738fbc69ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Zadanie 6: Eksploracja danych - Products (10 min)\n",
    "\n",
    "### Cel:\n",
    "PrzeprowadÅº analizÄ™ produktÃ³w.\n",
    "\n",
    "### Instrukcje:\n",
    "1. SprawdÅº schemat i kolumny\n",
    "2. Policz produkty wedÅ‚ug kategorii (jeÅ›li kolumna istnieje)\n",
    "3. ZnajdÅº statystyki cenowe (jeÅ›li kolumna price istnieje)\n",
    "4. WyÅ›wietl 10 najdroÅ¼szych produktÃ³w\n",
    "\n",
    "### Oczekiwany rezultat:\n",
    "- Kompletna analiza produktÃ³w\n",
    "- RozkÅ‚ad kategorii\n",
    "- Statystyki cenowe\n",
    "\n",
    "### WskazÃ³wki:\n",
    "- SprawdÅº dostÄ™pne kolumny przed analizÄ…: `products_df.columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c077cc02-299d-4167-acc8-50c216f56b3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SprawdÅº schemat i kolumny\n",
    "print(\"=== Kolumny produktÃ³w ===\")\n",
    "print(\"Kolumny:\", products_df.columns)\n",
    "\n",
    "print(\"\\n=== Schemat produktÃ³w ===\")\n",
    "products_df.printSchema()\n",
    "\n",
    "# WyÅ›wietl przykÅ‚adowe dane\n",
    "print(\"\\n=== PrzykÅ‚adowe dane ===\")\n",
    "display(products_df.limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9007e77a-d26e-4117-a93a-406058a9fc4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# JeÅ›li kolumna 'category' istnieje, policz produkty wedÅ‚ug kategorii\n",
    "if \"category\" in products_df.columns:\n",
    "    products_by_category = (\n",
    "        products_df\n",
    "        .groupBy(\"____\")  # category\n",
    "        .count()\n",
    "        .orderBy(\"____\", ascending=False)  # count\n",
    "    )\n",
    "    \n",
    "    print(\"=== Produkty wedÅ‚ug kategorii ===\")\n",
    "    display(products_by_category)\n",
    "else:\n",
    "    print(\"[INFO] Kolumna 'category' nie istnieje w danych\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81d007c8-86ac-4856-9782-a0a584a569be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# JeÅ›li kolumna 'price' istnieje, oblicz statystyki\n",
    "if \"price\" in products_df.columns:\n",
    "    products_stats = products_df.select(\n",
    "        ____(\"*\").alias(\"total_products\"),  # count\n",
    "        spark_round(avg(\"____\"), 2).alias(\"avg_price\"),  # price\n",
    "        min(\"price\").alias(\"min_price\"),\n",
    "        ____(\"price\").alias(\"max_price\")  # max\n",
    "    )\n",
    "    \n",
    "    print(\"=== Statystyki cenowe produktÃ³w ===\")\n",
    "    display(products_stats)\n",
    "else:\n",
    "    print(\"[INFO] Kolumna 'price' nie istnieje w danych\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9294cc72-10d1-4c6d-8a61-86c8a9b58507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Zadanie 7: Analiza jakoÅ›ci danych (15 min)\n",
    "\n",
    "### Cel:\n",
    "StwÃ³rz raport jakoÅ›ci danych dla wszystkich trzech datasetÃ³w.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Dla kaÅ¼dego datasetu policz:\n",
    "   - LiczbÄ™ rekordÃ³w\n",
    "   - LiczbÄ™ kolumn\n",
    "   - LiczbÄ™ wartoÅ›ci NULL w kaÅ¼dej kolumnie\n",
    "   - Procent kompletnoÅ›ci danych\n",
    "2. Zidentyfikuj duplikaty w kaÅ¼dym datasecie\n",
    "3. ZnajdÅº potencjalne problemy (np. negatywne wartoÅ›ci, nietypowe daty)\n",
    "\n",
    "### Oczekiwany rezultat:\n",
    "- Kompletny raport jakoÅ›ci danych\n",
    "- Lista zidentyfikowanych problemÃ³w\n",
    "- Rekomendacje dziaÅ‚aÅ„ naprawczych\n",
    "\n",
    "### WskazÃ³wki:\n",
    "- UÅ¼yj `.count()`, `.distinct().count()` do wykrywania duplikatÃ³w\n",
    "- UÅ¼yj filtrÃ³w do znajdowania nietypowych wartoÅ›ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bbccab3-b82d-414a-9bae-dc7fc80432d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Funkcja do analizy jakoÅ›ci danych\n",
    "def data_quality_report(df, dataset_name):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"=== Raport jakoÅ›ci danych: {dataset_name} ===\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Podstawowe statystyki\n",
    "    total_rows = df.count()\n",
    "    total_cols = len(df.columns)\n",
    "    print(f\"\\nðŸ“Š Podstawowe statystyki:\")\n",
    "    print(f\"  Liczba rekordÃ³w: {total_rows}\")\n",
    "    print(f\"  Liczba kolumn: {total_cols}\")\n",
    "    \n",
    "    # SprawdÅº duplikaty\n",
    "    distinct_rows = df.____().count()  # distinct\n",
    "    duplicates = total_rows - distinct_rows\n",
    "    dup_pct = (duplicates / total_rows * 100) if total_rows > 0 else 0\n",
    "    print(f\"\\nðŸ” Duplikaty:\")\n",
    "    print(f\"  Duplikaty: {____} ({dup_pct:.2f}%)\")  # duplicates\n",
    "    \n",
    "    # SprawdÅº wartoÅ›ci NULL\n",
    "    print(f\"\\nâŒ WartoÅ›ci NULL w kolumnach:\")\n",
    "    null_counts = df.select([\n",
    "        spark_sum(col(c).isNull().cast(\"int\")).alias(c) \n",
    "        for c in df.columns\n",
    "    ]).collect()[0].asDict()\n",
    "    \n",
    "    for col_name, null_count in null_counts.items():\n",
    "        null_pct = (null_count / total_rows * 100) if total_rows > 0 else 0\n",
    "        status = \"âœ“\" if null_count == 0 else \"âš \"\n",
    "        print(f\"  {status} {col_name}: {null_count} ({null_pct:.2f}%)\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\\n\")\n",
    "\n",
    "# Uruchom raport dla kaÅ¼dego datasetu\n",
    "data_quality_report(____, \"Customers\")  # customers_df_manual\n",
    "data_quality_report(orders_df_manual, \"Orders\")\n",
    "data_quality_report(____, \"Products\")  # products_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6be7903-2d46-43ec-8573-0b8b1be9404b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# SprawdÅº negatywne wartoÅ›ci w orders (total_amount < 0)\n",
    "negative_orders = orders_df_manual.filter(col(\"____\") < 0).count()  # total_amount\n",
    "zero_orders = orders_df_manual.filter(col(\"total_amount\") == ____).count()  # 0\n",
    "\n",
    "print(\"=== Problemy z wartoÅ›ciami zamÃ³wieÅ„ ===\")\n",
    "print(f\"âš  ZamÃ³wienia z negatywnÄ… wartoÅ›ciÄ…: {negative_orders}\")\n",
    "print(f\"âš  ZamÃ³wienia z wartoÅ›ciÄ… zero: {zero_orders}\")\n",
    "\n",
    "# SprawdÅº nietypowe daty\n",
    "from pyspark.sql.functions import year, current_date, datediff\n",
    "\n",
    "future_orders = orders_df_manual.filter(col(\"order_date\") > ____()).count()  # current_date\n",
    "old_orders = orders_df_manual.filter(year(col(\"____\")) < 2020).count()  # order_date\n",
    "\n",
    "print(f\"\\n=== Problemy z datami ===\")\n",
    "print(f\"âš  ZamÃ³wienia z przyszÅ‚Ä… datÄ…: {future_orders}\")\n",
    "print(f\"âš  ZamÃ³wienia starsze niÅ¼ 2020: {old_orders}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "318df488-5a43-4d34-8feb-e0424d1aabfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist - Co powinieneÅ› uzyskaÄ‡:\n",
    "- [ ] Wszystkie 3 datasety wczytane (customers, orders, products)\n",
    "- [ ] Schematy zdefiniowane manualnie dla CSV i JSON\n",
    "- [ ] Kompletna eksploracja kaÅ¼dego datasetu\n",
    "- [ ] Raport jakoÅ›ci danych wygenerowany\n",
    "- [ ] Zidentyfikowane problemy z danymi\n",
    "\n",
    "### Komendy weryfikacyjne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45503cb5-da22-48cf-8a63-331cdc4c19a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"=== WERYFIKACJA WYNIKÃ“W WARSZTATU ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# SprawdÅº, czy DataFrames sÄ… zdefiniowane\n",
    "try:\n",
    "    assert 'customers_df_manual' in locals(), \"customers_df_manual nie zostaÅ‚ utworzony\"\n",
    "    assert 'orders_df_manual' in locals(), \"orders_df_manual nie zostaÅ‚ utworzony\"\n",
    "    assert 'products_df' in locals(), \"products_df nie zostaÅ‚ utworzony\"\n",
    "    \n",
    "    print(\"\\nâœ“ Wszystkie DataFrames utworzone\")\n",
    "    \n",
    "    # SprawdÅº liczby rekordÃ³w\n",
    "    print(f\"\\nðŸ“Š Liczba rekordÃ³w:\")\n",
    "    print(f\"  âœ“ Customers: {customers_df_manual.count()}\")\n",
    "    print(f\"  âœ“ Orders: {orders_df_manual.count()}\")\n",
    "    print(f\"  âœ“ Products: {products_df.count()}\")\n",
    "    \n",
    "    # SprawdÅº tabele Delta Lake\n",
    "    print(f\"\\nðŸ’¾ Zapisane tabele Delta Lake:\")\n",
    "    tables_to_check = [\n",
    "        f\"{BRONZE_SCHEMA}.customers_workshop\",\n",
    "        f\"{BRONZE_SCHEMA}.orders_workshop\",\n",
    "        f\"{BRONZE_SCHEMA}.products_workshop\"\n",
    "    ]\n",
    "    \n",
    "    for table in tables_to_check:\n",
    "        try:\n",
    "            spark.table(table)\n",
    "            print(f\"  âœ“ {table}\")\n",
    "        except:\n",
    "            print(f\"  âœ— {table} - nie istnieje\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ“âœ“âœ“ WARSZTAT ZAKOÅƒCZONY POMYÅšLNIE! âœ“âœ“âœ“\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except AssertionError as e:\n",
    "    print(f\"\\nâœ— BÅ‚Ä…d: {e}\")\n",
    "except NameError as e:\n",
    "    print(f\"\\nâœ— Zmienna nie zostaÅ‚a zdefiniowana: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f2c075b-c674-4610-8edb-86168842449a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Problem 1: BÅ‚Ä…d \"Path does not exist\"\n",
    "**Objawy:**\n",
    "- BÅ‚Ä…d podczas wczytywania plikÃ³w CSV/JSON/Parquet\n",
    "- Komunikat: `Path does not exist: /Volumes/...`\n",
    "\n",
    "**RozwiÄ…zanie:**\n",
    "```python\n",
    "# SprawdÅº, czy Å›cieÅ¼ka istnieje\n",
    "dbutils.fs.ls(DATASET_BASE_PATH)\n",
    "\n",
    "# Upewnij siÄ™, Å¼e zmienne sÄ… poprawnie zdefiniowane\n",
    "print(f\"DATASET_BASE_PATH: {DATASET_BASE_PATH}\")\n",
    "print(f\"CUSTOMERS_CSV: {CUSTOMERS_CSV}\")\n",
    "```\n",
    "\n",
    "### Problem 2: BÅ‚Ä…d typu danych podczas wczytywania\n",
    "**Objawy:**\n",
    "- WartoÅ›ci NULL w miejscach, gdzie powinny byÄ‡ dane\n",
    "- Niepoprawne typy kolumn (wszystko jako string)\n",
    "\n",
    "**RozwiÄ…zanie:**\n",
    "- Zawsze definiuj schemat manualnie zamiast uÅ¼ywaÄ‡ `inferSchema=True`\n",
    "- UÅ¼yj odpowiednich typÃ³w: `IntegerType()`, `StringType()`, `TimestampType()`, `DoubleType()`\n",
    "\n",
    "### Problem 3: Tabela juÅ¼ istnieje\n",
    "**Objawy:**\n",
    "- BÅ‚Ä…d: `Table already exists`\n",
    "\n",
    "**RozwiÄ…zanie:**\n",
    "```python\n",
    "# UÅ¼yj mode=\"overwrite\"\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "# Lub usuÅ„ tabelÄ™ przed zapisem\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "```\n",
    "\n",
    "### Debugging tips:\n",
    "- UÅ¼yj `.printSchema()` aby sprawdziÄ‡ strukturÄ™ DataFrame\n",
    "- UÅ¼yj `.explain()` aby zobaczyÄ‡ plan wykonania\n",
    "- SprawdÅº liczbÄ™ rekordÃ³w: `df.count()`\n",
    "- WyÅ›wietl przykÅ‚adowe dane: `display(df.limit(10))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef18a348-4b54-4463-b0b5-2dcd93f3519f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "### WydajnoÅ›Ä‡:\n",
    "- **Zawsze definiuj schemat manualnie** - `inferSchema=True` jest wolne i moÅ¼e dawaÄ‡ bÅ‚Ä™dne typy\n",
    "- **UÅ¼ywaj Parquet dla duÅ¼ych datasetÃ³w** - najlepsza kompresja i wydajnoÅ›Ä‡\n",
    "- **Partycjonuj duÅ¼e tabele** wedÅ‚ug czÄ™sto uÅ¼ywanych kolumn filtrujÄ…cych\n",
    "- **UÅ¼ywaj `.cache()` tylko gdy DataFrame jest wielokrotnie uÅ¼yty**\n",
    "\n",
    "### JakoÅ›Ä‡ kodu:\n",
    "- **Nazywaj DataFrames opisowo**: `customers_df`, `orders_clean_df`, `products_enriched_df`\n",
    "- **Dodawaj komentarze** wyjaÅ›niajÄ…ce logikÄ™ biznesowÄ…\n",
    "- **Sprawdzaj dane po kaÅ¼dym kroku** transformacji\n",
    "- **UÅ¼ywaj zmiennych dla nazw tabel i schematÃ³w** (nie hardcoduj)\n",
    "\n",
    "### Data Quality:\n",
    "- **Zawsze sprawdzaj wartoÅ›ci NULL** przed transformacjami\n",
    "- **Wykrywaj duplikaty**: `df.distinct().count()` vs `df.count()`\n",
    "- **Waliduj zakresy wartoÅ›ci**: negatywne ceny, przyszÅ‚e daty\n",
    "- **Dokumentuj znalezione problemy** w komentarzach lub osobnym notebooku\n",
    "\n",
    "### Governance:\n",
    "- **UÅ¼ywaj Unity Catalog** dla zarzÄ…dzania tabelami i uprawnieniami\n",
    "- **Stosuj konwencjÄ™ nazewnictwa**: `bronze.customers`, `silver.customers_clean`, `gold.customers_agg`\n",
    "- **Separuj Å›rodowiska**: dev/staging/prod uÅ¼ywajÄ…c rÃ³Å¼nych katalogÃ³w\n",
    "- **Loguj operacje**: zapisuj metadane o przetwarzanych danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8528d360-9d75-4a8a-9792-b3de7b0a6bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "### Co zostaÅ‚o osiÄ…gniÄ™te:\n",
    "- âœ… SkonfigurowaÅ‚eÅ› Å›rodowisko Databricks i zmienne per-user\n",
    "- âœ… WczytaÅ‚eÅ› dane z trzech rÃ³Å¼nych formatÃ³w (CSV, JSON, Parquet)\n",
    "- âœ… ZdefiniowaÅ‚eÅ› schematy manualnie (best practice)\n",
    "- âœ… PrzeprowadziÅ‚eÅ› szczegÃ³Å‚owÄ… eksploracjÄ™ wszystkich datasetÃ³w\n",
    "- âœ… ZidentyfikowaÅ‚eÅ› problemy jakoÅ›ci danych\n",
    "- âœ… WygenerowaÅ‚eÅ› kompletny raport jakoÅ›ci danych\n",
    "- âœ… ZapisaÅ‚eÅ› dane do tabel Delta Lake\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "1. **RÄ™czne schematy > inferSchema**: Szybsze, bezpieczniejsze i przewidywalne typy danych\n",
    "2. **Eksploracja przed transformacjÄ…**: Zawsze analizuj dane przed rozpoczÄ™ciem przetwarzania\n",
    "3. **JakoÅ›Ä‡ wymaga monitorowania**: NULL, duplikaty, outliers muszÄ… byÄ‡ wykrywane systematycznie\n",
    "4. **Parquet to standard lakehouse**: Wbudowany schemat, najlepsza kompresja i wydajnoÅ›Ä‡\n",
    "\n",
    "### Quick Reference - NajwaÅ¼niejsze komendy:\n",
    "\n",
    "| Operacja | PySpark | Notatki |\n",
    "|----------|---------|---------|\n",
    "| Wczytaj CSV | `spark.read.format(\"csv\").option(\"header\",\"true\").schema(schema).load(path)` | Zawsze uÅ¼ywaj rÄ™cznego schematu |\n",
    "| Wczytaj JSON | `spark.read.format(\"json\").schema(schema).load(path)` | Opcja `multiLine=true` dla JSON arrays |\n",
    "| Wczytaj Parquet | `spark.read.format(\"parquet\").load(path)` | Schemat wbudowany |\n",
    "| Zapisz do Delta | `df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table)` | mode: overwrite/append |\n",
    "| SprawdÅº NULL | `df.select([sum(col(c).isNull()).alias(c) for c in df.columns])` | Dla kaÅ¼dej kolumny |\n",
    "| ZnajdÅº duplikaty | `df.count() - df.distinct().count()` | RÃ³Å¼nica = liczba duplikatÃ³w |\n",
    "| Statystyki | `df.describe()` | Dla kolumn numerycznych |\n",
    "| Grupowanie | `df.groupBy(\"col\").count()` | Agregacje |\n",
    "\n",
    "### NastÄ™pne kroki:\n",
    "- **Kolejny warsztat**: `02_transformations_cleaning_workshop.ipynb` - Transformacje i czyszczenie danych\n",
    "- **Praktyka**: PowtÃ³rz eksploracjÄ™ na wÅ‚asnych datasetach\n",
    "- **Dokumentacja**: [Databricks Data Import Best Practices](https://docs.databricks.com/ingestion/index.html)\n",
    "- **Zadanie domowe**: Wykonaj analizÄ™ eksploracyjnÄ… dodatkowego datasetu (np. transactions, events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9945374c-c1b4-4351-9111-1a61c765baa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Czyszczenie zasobÃ³w\n",
    "\n",
    "Opcjonalne - usuÅ„ tabele utworzone podczas warsztatu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56bee403-2364-4a3f-9894-a6e551f3a7a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UWAGA: Uruchom tylko jeÅ›li chcesz usunÄ…Ä‡ wszystkie utworzone tabele\n",
    "# Te tabele mogÄ… byÄ‡ potrzebne w kolejnych warsztatach!\n",
    "\n",
    "# Odkomentuj poniÅ¼sze linie aby usunÄ…Ä‡ tabele:\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {BRONZE_SCHEMA}.customers_workshop\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {BRONZE_SCHEMA}.orders_workshop\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {BRONZE_SCHEMA}.products_workshop\")\n",
    "\n",
    "# # WyczyÅ›Ä‡ cache\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "# print(\"âœ“ Zasoby zostaÅ‚y wyczyszczone\")\n",
    "\n",
    "print(\"[INFO] Czyszczenie zasobÃ³w jest zakomentowane - odkomentuj kod aby usunÄ…Ä‡ tabele\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_workspace_data_exploration_workshop",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
