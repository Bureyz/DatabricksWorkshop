{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8060371b-0939-4f55-a509-89cd691f2a22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Workshop: Workspace Setup, Data Import & Exploration\n",
    "\n",
    "**Cel szkoleniowy:** Praktyczne opanowanie konfiguracji workspace, importowania danych z różnych formatów oraz podstawowych operacji eksploracyjnych.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Konfiguracja workspace i klastrów\n",
    "- Wczytywanie różnych formatów danych (CSV, JSON, Parquet)\n",
    "- Podstawowa eksploracja danych\n",
    "- Konstruowanie schematów manualnie\n",
    "- Analiza braków danych i wartości unikalnych\n",
    "\n",
    "**Czas trwania:** 90 minut"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b0c559c-962d-4c80-bed9-adf06ab250e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 1 - Fundamentals & Exploration\n",
    "- **Typ notebooka**: Workshop\n",
    "- **Wymagania techniczne**:\n",
    " - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    " - Unity Catalog włączony\n",
    " - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    " - Klaster: Standard z minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df0cb0e5-4641-4bea-b869-58c4944f69cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wprowadzenie do warsztatu\n",
    "\n",
    "W tym warsztacie będziesz pracować z rzeczywistymi danymi KION:\n",
    "- **Customers** (customers.csv) - dane klientów\n",
    "- **Orders** (orders_batch.json) - zamówienia\n",
    "- **Products** (products.parquet) - produkty\n",
    "\n",
    "### Zadania do wykonania:\n",
    "1. Konfiguracja środowiska i zmiennych\n",
    "2. Import danych z CSV, JSON, Parquet\n",
    "3. Konstrukcja schematów manualnie\n",
    "4. Eksploracja danych: statystyki, braki, unikalne wartości\n",
    "5. Analiza jakości danych\n",
    "\n",
    "### Kryteria sukcesu:\n",
    "- Wszystkie 3 datasety poprawnie wczytane\n",
    "- Schematy zdefiniowane manualnie i zastosowane\n",
    "- Kompletna analiza eksploracyjna przeprowadzona\n",
    "- Zidentyfikowane problemy jakości danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f20c2fc5-c9ff-435f-8204-00027193aa9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie podstaw pracy z danymi w Databricks Lakehouse\n",
    "\n",
    "**Podstawowe pojęcia:**\n",
    "- **Workspace**: Środowisko Databricks zawierające notebooks, klastry i dane\n",
    "- **Cluster**: Zbiór maszyn wirtualnych przetwarzających dane\n",
    "- **DataFrame**: Rozproszona kolekcja danych zorganizowana w kolumny\n",
    "- **Schema**: Struktura danych definiująca nazwy kolumn i typy danych\n",
    "- **Format danych**: CSV (text), JSON (semi-structured), Parquet (columnar binary)\n",
    "\n",
    "**Dlaczego to ważne?**\n",
    "Poprawne wczytanie i eksploracja danych to fundament każdego pipeline'u ETL. Zrozumienie schematów, formatów i metod eksploracji pozwala na wczesne wykrycie problemów jakości danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da5ac3d6-0897-4c7a-bc46-abd7edfc3d8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Inicjalizacja środowiska\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0e035ba-e5ef-4d12-8de4-f01acd72ee06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "903908c9-c847-46a1-8fca-49842d81b0bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Zdefiniuj zmienne specyficzne dla warsztatu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d29c33a-ff2b-48d2-92b3-d26cb2062f1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Ścieżki do plików danych (już zdefiniowane w 00_setup)\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kontekst konfiguracji**\n",
    "\n",
    "Ścieżki do rzeczywistych plików danych zostały skonfigurowane:\n",
    "- **Customers CSV**: customers.csv (~10,000 rekordów klientów)\n",
    "- **Orders JSON**: orders_batch.json (~100,000 zamówień) \n",
    "- **Products Parquet**: products.parquet (katalog produktów)\n",
    "\n",
    "**Rzeczywista struktura danych:**\n",
    "- **Customers**: `customer_id`, `first_name`, `last_name`, `email`, `phone`, `city`, `state`, `country`, `registration_date`, `customer_segment`\n",
    "- **Orders**: `order_id`, `customer_id`, `product_id`, `store_id`, `order_datetime`, `quantity`, `unit_price`, `discount_percent`, `total_amount`, `payment_method`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2adab892-b60e-4d80-9cab-b762263f5c75",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Zadanie 1: Import danych CSV (20 min)\n",
    "\n",
    "### Cel:\n",
    "Wczytaj dane klientów z pliku CSV, najpierw z automatycznym wykrywaniem schematu, potem z ręcznie zdefiniowanym schematem.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Wczytaj `customers.csv` z opcją `inferSchema=True`\n",
    "2. Wyświetl schemat i 5 pierwszych rekordów\n",
    "3. Policz liczbę rekordów\n",
    "4. Zdefiniuj schemat manualnie (StructType)\n",
    "5. Wczytaj ponownie używając ręcznego schematu\n",
    "6. Porównaj schematy\n",
    "\n",
    "### Oczekiwany rezultat:\n",
    "- DataFrame z danymi klientów\n",
    "- Schemat zawierający kolumny: customer_id (int), first_name (string), last_name (string), email (string), city (string), country (string), registration_date (timestamp)\n",
    "\n",
    "### Wskazówki:\n",
    "- Użyj `spark.read.format(\"csv\").option(\"header\", \"true\")`\n",
    "- Do ręcznego schematu użyj: `StructType`, `StructField`, `IntegerType`, `StringType`, `TimestampType`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68e06abc-0be9-4adc-8a55-60b7bf487f6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wczytanie pliku CSV z klientami (automatyczne wykrywanie schematu)\n",
    "customers_df = (\n",
    " spark.read\n",
    " .format(\"csv\")\n",
    " .option(\"header\", \"true\")\n",
    " .option(\"inferSchema\", \"true\")\n",
    " .load(CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "# Wyświetl schemat i przykładowe dane\n",
    "customers_df.printSchema()\n",
    "display(customers_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analiza danych customers po wczytaniu**\n",
    "\n",
    "Spark automatycznie wykrył schemat. Zauważ rzeczywiste kolumny w pliku CSV:\n",
    "- **Identyfikacja**: `customer_id` (CUST000001...)\n",
    "- **Dane osobowe**: `first_name`, `last_name`, `email`, `phone`\n",
    "- **Lokalizacja**: `city`, `state`, `country`\n",
    "- **Metadane**: `registration_date`, `customer_segment`\n",
    "\n",
    "**Następny krok**: Zdefiniuj schemat manualnie dla lepszej kontroli typów danych i wydajności."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f666671-9e58-4956-ab71-15fa1ae84d9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zdefiniuj schemat manualnie na podstawie rzeczywistych danych\n",
    "# Uzupełnij typy danych dla każdego pola\n",
    "customers_schema = StructType([\n",
    " StructField(\"customer_id\", ____, False), # StringType (CUST000001)\n",
    " StructField(\"first_name\", ____, True), # StringType\n",
    " StructField(\"last_name\", StringType(), True),\n",
    " StructField(\"email\", ____, True), # StringType\n",
    " StructField(\"phone\", StringType(), True),\n",
    " StructField(\"city\", ____, True), # StringType\n",
    " StructField(\"state\", StringType(), True),\n",
    " StructField(\"country\", ____, True), # StringType\n",
    " StructField(\"registration_date\", ____, True), # TimestampType\n",
    " StructField(\"customer_segment\", ____, True) # StringType\n",
    "])\n",
    "\n",
    "# Wczytaj ponownie z ręcznym schematem\n",
    "customers_df_manual = (\n",
    " spark.read\n",
    " .format(\"____\") # csv\n",
    " .option(\"header\", \"____\") # true\n",
    " .schema(____) # customers_schema\n",
    " .load(CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "# Wyświetl schemat\n",
    "print(\"[INFO] Schemat z ręczną definicją:\")\n",
    "customers_df_manual.printSchema()\n",
    "\n",
    "# Wyświetl przykładowe dane\n",
    "display(customers_df_manual.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schemat customers zdefiniowany ręcznie**\n",
    "\n",
    " **Best practice**: Zawsze definiuj schemat manualnie zamiast używać `inferSchema=True`\n",
    "\n",
    "**Korzyści ręcznego schematu:**\n",
    "- **Wydajność**: Brak potrzeby skanowania całego pliku do określenia typów\n",
    "- **Przewidywalność**: Kontrola nad typami danych (String vs Integer)\n",
    "- **Bezpieczeństwo**: Walidacja zgodności danych ze schematem\n",
    "- **Dokumentacja**: Schemat służy jako dokumentacja struktury danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3c4bd244-51b5-4f4a-8dc7-16f3eeb4555e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Zadanie 2: Import danych JSON (15 min)\n",
    "\n",
    "### Cel:\n",
    "Wczytaj dane zamówień z pliku JSON i zdefiniuj schemat manualnie.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Wczytaj `orders_batch.json` z `inferSchema=True`\n",
    "2. Zbadaj strukturę danych (schemat, typy)\n",
    "3. Zdefiniuj schemat manualnie\n",
    "4. Wczytaj ponownie z ręcznym schematem\n",
    "\n",
    "### Oczekiwany rezultat:\n",
    "- DataFrame z zamówieniami\n",
    "- Schemat: order_id (int), customer_id (int), order_date (timestamp), total_amount (double), status (string)\n",
    "\n",
    "### Wskazówki:\n",
    "- JSON nie wymaga opcji `header`\n",
    "- Użyj `DoubleType` dla kwot pieniężnych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "412ccc42-659c-44bc-a8fe-2ccd8c314803",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wczytanie pliku JSON z zamówieniami (automatyczne wykrywanie schematu)\n",
    "orders_df = (\n",
    " spark.read\n",
    " .format(\"json\")\n",
    " .option(\"multiLine\", \"true\")\n",
    " .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "display(orders_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analiza danych orders po wczytaniu z JSON**\n",
    "\n",
    "Zauważ rzeczywistą strukturę danych zamówień:\n",
    "- **Identyfikatory**: `order_id`, `customer_id`, `product_id`, `store_id`\n",
    "- **Szczegóły transakcji**: `order_datetime`, `quantity`, `unit_price`, `discount_percent`\n",
    "- **Podsumowanie**: `total_amount`, `payment_method`\n",
    "\n",
    "** Problemy jakości danych:**\n",
    "- Niektóre rekordy mają `NULL` w `order_id` lub `order_datetime`\n",
    "- Przyszłe daty w `order_datetime` (2026)\n",
    "- Wymagane sprawdzenie spójności danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ed2904e-9dd6-4ed2-8180-3c95e36708f5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zdefiniuj schemat orders na podstawie rzeczywistych danych\n",
    "orders_schema = StructType([\n",
    " StructField(\"order_id\", ____, True), # StringType (może być NULL)\n",
    " StructField(\"customer_id\", ____, True), # StringType\n",
    " StructField(\"product_id\", StringType(), True),\n",
    " StructField(\"store_id\", ____, True), # StringType\n",
    " StructField(\"order_datetime\", ____, True), # TimestampType (może być NULL)\n",
    " StructField(\"quantity\", ____, True), # IntegerType\n",
    " StructField(\"unit_price\", DoubleType(), True),\n",
    " StructField(\"discount_percent\", ____, True), # IntegerType\n",
    " StructField(\"total_amount\", ____, True), # DoubleType\n",
    " StructField(\"payment_method\", ____, True) # StringType\n",
    "])\n",
    "\n",
    "# Wczytaj z ręcznym schematem\n",
    "orders_df_manual = (\n",
    " spark.read\n",
    " .format(\"____\") # json\n",
    " .option(\"multiLine\", \"____\") # true\n",
    " .schema(____) # orders_schema\n",
    " .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "orders_df_manual.printSchema()\n",
    "display(orders_df_manual.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5ca31f7-ccbc-4d79-9807-8aa8c7c4c572",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Zadanie 3: Import danych Parquet (10 min)\n",
    "\n",
    "### Cel:\n",
    "Wczytaj dane produktów z pliku Parquet (schemat jest wbudowany).\n",
    "\n",
    "### Instrukcje:\n",
    "1. Wczytaj `products.parquet`\n",
    "2. Sprawdź schemat (Parquet zawiera schemat wbudowany)\n",
    "3. Wyświetl dane\n",
    "4. Policz rekordy\n",
    "\n",
    "### Oczekiwany rezultat:\n",
    "- DataFrame z produktami\n",
    "- Schemat automatycznie załadowany z pliku Parquet\n",
    "\n",
    "### Wskazówki:\n",
    "- Parquet nie wymaga `inferSchema` ani ręcznego schematu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80d4bf49-867d-47e7-a473-3afbb7e6a968",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wczytaj products.parquet\n",
    "products_df = (\n",
    " spark.read\n",
    " .format(\"____\") # parquet\n",
    " .load(PRODUCTS_PARQUET)\n",
    ")\n",
    "\n",
    "# Wyświetl schemat (Parquet zawiera wbudowany schemat)\n",
    "products_df.printSchema()\n",
    "\n",
    "# Wyświetl dane i policz rekordy\n",
    "display(products_df.limit(5))\n",
    "product_count = products_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Parquet - format z wbudowanym schematem**\n",
    "\n",
    " **Korzyści formatu Parquet:**\n",
    "- **Schemat wbudowany**: Nie wymaga definiowania schematu manualnie\n",
    "- **Kompresja kolumnowa**: Oszczędność miejsca i szybsze zapytania analityczne\n",
    "- **Wydajność**: Najlepszy format do Big Data i analytics w lakehouse\n",
    "- **Kompatybilność**: Uniwersalny standard dla systemów analitycznych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95f537b1-0441-4828-9046-54077bda7711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Zapis danych do Delta Lake\n",
    "\n",
    "### Cel:\n",
    "Zapisz wczytane DataFrames do tabel Delta Lake dla dalszego użycia.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Zapisz customers do tabeli `bronze.customers_workshop`\n",
    "2. Zapisz orders do tabeli `bronze.orders_workshop`\n",
    "3. Zapisz products do tabeli `bronze.products_workshop`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7694b3d9-1a3d-4d1c-b5f2-a894f49d3a80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład: Zapis customers do Delta Lake\n",
    "# Ta komórka jest już gotowa - przeanalizuj kod i uruchom ją\n",
    "\n",
    "customers_table = f\"{BRONZE_SCHEMA}.customers_workshop\"\n",
    "\n",
    "(\n",
    " customers_df_manual\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(customers_table)\n",
    ")\n",
    "\n",
    "# Sprawdź strukturę zapisanej tabeli\n",
    "spark.sql(f\"DESCRIBE TABLE {customers_table}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zapis customers do Delta Lake wykonany**\n",
    "\n",
    " **Delta Lake - format ACID dla lakehouse:**\n",
    "- **ACID transactions**: Atomowe operacje na danych\n",
    "- **Schema evolution**: Możliwość zmiany schematu w czasie\n",
    "- **Time travel**: Dostęp do historycznych wersji danych\n",
    "- **Optimize & Vacuum**: Optymalizacja wydajności\n",
    "\n",
    "Tabela zapisana: `{customers_table}`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2fbeb31-be1f-40d6-9045-e14d3789d8c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zapisz orders do Delta Lake\n",
    "orders_table = f\"{BRONZE_SCHEMA}.orders_workshop\"\n",
    "\n",
    "(\n",
    " orders_df_manual\n",
    " .write\n",
    " .format(\"____\") # delta\n",
    " .mode(\"____\") # overwrite\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(____) # orders_table\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Orders zapisane do Delta Lake**\n",
    "\n",
    "Po uzupełnieniu braków powyżej, tabela orders zostanie zapisana w formacie Delta Lake. \n",
    "Zauważ użycie parametrów:\n",
    "- **format(\"delta\")**: Specyfikuje format Delta Lake\n",
    "- **mode(\"overwrite\")**: Zastępuje istniejące dane\n",
    "- **overwriteSchema**: Pozwala na zmianę schematu tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e887243-2829-4e78-aa88-31d3e179a9c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zapisz products do Delta Lake\n",
    "products_table = f\"{BRONZE_SCHEMA}.products_workshop\"\n",
    "\n",
    "(\n",
    " ____ # products_df\n",
    " .write\n",
    " .format(\"____\") # delta\n",
    " .mode(\"____\") # overwrite\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(____) # products_table\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Products zapisane do Delta Lake**\n",
    "\n",
    "Uzupełnij braki powyżej, aby zapisać tabelę produktów. Wszystkie trzy tabele (customers, orders, products) będą dostępne w schemacie bronze do dalszego przetwarzania."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f43982a5-ed74-452a-9315-7853905c3c0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Zadanie 4: Eksploracja danych - Customers (15 min)\n",
    "\n",
    "### Cel:\n",
    "Przeprowadź szczegółową eksplorację danych klientów.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Wyświetl listę kolumn i ich typy\n",
    "2. Policz liczbę unikalnych klientów\n",
    "3. Znajdź liczbę klientów według krajów\n",
    "4. Sprawdź, czy są wartości NULL w kolumnach\n",
    "5. Wygeneruj statystyki opisowe (`describe()`)\n",
    "\n",
    "### Oczekiwany rezultat:\n",
    "- Kompletna analiza eksploracyjna\n",
    "- Zidentyfikowane braki danych\n",
    "- Rozkład geograficzny klientów\n",
    "\n",
    "### Wskazówki:\n",
    "- Użyj: `columns`, `dtypes`, `count()`, `distinct()`, `groupBy()`, `describe()`\n",
    "- Do sprawdzenia NULL: `.filter(col(\"column_name\").isNull())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0287bf6e-9cdf-487f-9800-e0269a65d455",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Podstawowe informacje o strukturze danych\n",
    "customers_df_manual.columns\n",
    "customers_df_manual.dtypes\n",
    "\n",
    "# Policz unikalne wartości customer_id\n",
    "unique_customers = customers_df_manual.select(\"____\").distinct().count() # customer_id\n",
    "total_customers = customers_df_manual.count()\n",
    "\n",
    "# Wyświetl statystyki\n",
    "display(spark.createDataFrame([\n",
    " (total_customers, unique_customers, total_customers - unique_customers)\n",
    "], [\"total_records\", \"unique_customers\", \"duplicates\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Podstawowe statystyki customers**\n",
    "\n",
    "Sprawdziliśmy:\n",
    "- **Kolumny i typy**: Struktura danych oraz typy każdej kolumny\n",
    "- **Duplikaty**: Czy `customer_id` jest unikalny (klucz główny)\n",
    "- **Kompletność**: Całkowita liczba rekordów vs unikalne identyfikatory\n",
    "\n",
    "**Następny krok**: Sprawdź rozkład geograficzny klientów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9de8a409-133f-4cd7-bfea-ecf1cf82c966",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Policz klientów według krajów\n",
    "customers_by_country = (\n",
    " customers_df_manual\n",
    " .groupBy(\"____\") # country\n",
    " .count()\n",
    " .orderBy(\"count\", ascending=____) # False\n",
    ")\n",
    "\n",
    "display(customers_by_country)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rozkład geograficzny klientów**\n",
    "\n",
    "Analiza pokazuje dystrybucję klientów według krajów, posortowaną malejąco. \n",
    "To pozwala zidentyfikować:\n",
    "- **Główne rynki**: Kraje z największą bazą klientów \n",
    "- **Potencjał ekspansji**: Kraje z małą liczbą klientów\n",
    "- **Koncentrację geograficzną**: Czy firma ma globalny zasięg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13256e61-af3e-476e-befa-c377c60138f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdź wartości NULL w każdej kolumnie\n",
    "from pyspark.sql.functions import col, sum as spark_sum\n",
    "\n",
    "null_counts = customers_df_manual.select([\n",
    " spark_sum(col(c).____().____(\"int\")).alias(c) # isNull, cast\n",
    " for c in customers_df_manual.columns\n",
    "])\n",
    "\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analiza wartości NULL**\n",
    "\n",
    "Sprawdzenie brakujących danych w każdej kolumnie:\n",
    "- **0 = Kompletne dane** w kolumnie\n",
    "- **>0 = Braki danych** wymagające obsługi\n",
    "\n",
    "**Istotne dla jakości danych**: Braki w kluczowych polach (`customer_id`, `email`) są krytyczne dla biznesu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d66afe98-3c5e-4ace-8188-4f3468446e32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wygeneruj statystyki opisowe\n",
    "display(customers_df_manual.____()) # describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statystyki opisowe customers**\n",
    "\n",
    "Metoda `describe()` pokazuje:\n",
    "- **count**: Liczba niepustych wartości\n",
    "- **mean/stddev**: Średnia i odchylenie standardowe (dla kolumn numerycznych)\n",
    "- **min/max**: Wartości skrajne\n",
    "- Dla kolumn string: najczęstsze wartości\n",
    "\n",
    "**Użycie**: Identyfikacja odstających wartości i ogólnego rozkładu danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52f4c1d3-d675-4290-aa9f-05c95a3df3b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Zadanie 5: Eksploracja danych - Orders (15 min)\n",
    "\n",
    "### Cel:\n",
    "Przeprowadź analizę zamówień.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Policz liczbę zamówień według statusu\n",
    "2. Oblicz całkowitą wartość zamówień\n",
    "3. Znajdź średnią, min, max wartość zamówienia\n",
    "4. Sprawdź braki danych\n",
    "5. Znajdź 10 najdroższych zamówień\n",
    "\n",
    "### Oczekiwany rezultat:\n",
    "- Statystyki biznesowe zamówień\n",
    "- Identyfikacja problemów z danymi\n",
    "- Top 10 zamówień\n",
    "\n",
    "### Wskazówki:\n",
    "- Użyj `.agg()` z funkcjami: `sum`, `avg`, `min`, `max`\n",
    "- Do sortowania: `.orderBy(col(\"column\").desc())`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef1f9d64-6c54-428b-b579-e78d0d41b922",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Policz zamówienia według metod płatności \n",
    "orders_by_payment = (\n",
    " orders_df_manual\n",
    " .groupBy(\"____\") # payment_method\n",
    " .____() # count\n",
    " .orderBy(\"count\", ascending=False)\n",
    ")\n",
    "\n",
    "display(orders_by_payment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rozkład metod płatności**\n",
    "\n",
    "Analiza pokazuje preferencje płatnicze klientów:\n",
    "- **Cash, Credit Card, Debit Card, PayPal**: Główne metody\n",
    "- **Biznesowe wnioski**: Które metody są najpopularniejsze?\n",
    "- **Planowanie**: Czy potrzebne są dodatkowe metody płatności?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "498faee6-df10-4523-a21f-6b0fa8c12e78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Oblicz statystyki dla total_amount\n",
    "from pyspark.sql.functions import sum, avg, min, max, count, round as spark_round\n",
    "\n",
    "orders_stats = orders_df_manual.select(\n",
    " count(\"*\").alias(\"total_orders\"),\n",
    " spark_round(____(\"total_amount\"), 2).alias(\"total_revenue\"), # sum\n",
    " spark_round(____(\"total_amount\"), 2).alias(\"avg_order_value\"), # avg\n",
    " ____(\"total_amount\").alias(\"min_order\"), # min\n",
    " ____(\"total_amount\").alias(\"max_order\") # max\n",
    ")\n",
    "\n",
    "display(orders_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kluczowe metryki biznesowe orders**\n",
    "\n",
    "Obliczone statystyki pokazują:\n",
    "- **Total Orders**: Całkowita liczba zamówień\n",
    "- **Total Revenue**: Suma wszystkich zamówień (przychód)\n",
    "- **Average Order Value (AOV)**: Średnia wartość zamówienia\n",
    "- **Min/Max Order**: Zakres wartości zamówień\n",
    "\n",
    "**Biznesowe zastosowanie**: Benchmarking, planowanie budżetu, analiza trendów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e26c835-d516-487d-bc0d-cd7a63418fec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Znajdź 10 najdroższych zamówień\n",
    "top_orders = (\n",
    " orders_df_manual\n",
    " .orderBy(col(\"total_amount\").____()) # desc\n",
    " .limit(____) # 10\n",
    ")\n",
    "\n",
    "display(top_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top 10 najdroższych zamówień**\n",
    "\n",
    "Analiza największych transakcji pokazuje:\n",
    "- **VIP customers**: Kto generuje najwyższe przychody?\n",
    "- **Produkty premium**: Które produkty w najdroższych zamówieniach?\n",
    "- **Wzorce**: Czy wysokie zamówienia mają wspólne cechy?\n",
    "\n",
    "**Zastosowanie biznesowe**: Segmentacja klientów, strategie retention, cross-selling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56e445e5-34b0-43ee-aef2-a5f07769edb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdź braki danych w orders\n",
    "null_counts_orders = orders_df_manual.select([\n",
    " spark_sum(col(c).isNull().cast(\"int\")).alias(c) \n",
    " for c in orders_df_manual.columns\n",
    "])\n",
    "\n",
    "display(null_counts_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analiza braków danych w orders**\n",
    "\n",
    "Kluczowe braki do sprawdzenia:\n",
    "- **order_id NULL**: Zamówienia bez identyfikatora (problem systemu?)\n",
    "- **order_datetime NULL**: Brak czasu transakcji (wpływ na reporting)\n",
    "- **customer_id NULL**: Niemożliwość powiązania z klientem\n",
    "\n",
    "**Akcje naprawcze**: Usunięcie lub wypełnienie brakujących wartości w kolejnych krokach ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20c4faf5-cce1-4715-91e9-c2738fbc69ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Zadanie 6: Eksploracja danych - Products (10 min)\n",
    "\n",
    "### Cel:\n",
    "Przeprowadź analizę produktów.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Sprawdź schemat i kolumny\n",
    "2. Policz produkty według kategorii (jeśli kolumna istnieje)\n",
    "3. Znajdź statystyki cenowe (jeśli kolumna price istnieje)\n",
    "4. Wyświetl 10 najdroższych produktów\n",
    "\n",
    "### Oczekiwany rezultat:\n",
    "- Kompletna analiza produktów\n",
    "- Rozkład kategorii\n",
    "- Statystyki cenowe\n",
    "\n",
    "### Wskazówki:\n",
    "- Sprawdź dostępne kolumny przed analizą: `products_df.columns`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c077cc02-299d-4167-acc8-50c216f56b3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdź schemat i kolumny\n",
    "products_df.columns\n",
    "products_df.printSchema()\n",
    "\n",
    "# Wyświetl przykładowe dane\n",
    "display(products_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analiza struktury products**\n",
    "\n",
    "Sprawdzenie schematu pozwala zrozumieć:\n",
    "- **Dostępne kolumny**: Jakie informacje o produktach są dostępne?\n",
    "- **Typy danych**: Parquet automatycznie zachowuje właściwe typy\n",
    "- **Przykładowe dane**: Zawartość i format wartości\n",
    "\n",
    "**Następne kroki**: Sprawdź czy istnieją kolumny `category` i `price` do dalszej analizy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9007e77a-d26e-4117-a93a-406058a9fc4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdź czy kolumna 'category' istnieje, policz produkty według kategorii\n",
    "if \"category\" in products_df.columns:\n",
    " products_by_category = (\n",
    " products_df\n",
    " .groupBy(\"____\") # category\n",
    " .count()\n",
    " .orderBy(\"____\", ascending=False) # count\n",
    " )\n",
    " \n",
    " display(products_by_category)\n",
    "else:\n",
    " display(spark.createDataFrame([(\"Kolumna 'category' nie istnieje w danych\",)], [\"info\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rozkład kategorii produktów**\n",
    "\n",
    "Jeśli kolumna `category` istnieje:\n",
    "- **Portfel produktowy**: Jakie kategorie są dostępne?\n",
    "- **Koncentracja**: Które kategorie dominują w ofercie?\n",
    "- **Możliwości cross-selling**: Powiązane kategorie\n",
    "\n",
    "Jeśli nie istnieje - może być potrzebne wzbogacenie danych o kategorizację produktów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "81d007c8-86ac-4856-9782-a0a584a569be",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdź czy kolumna 'price' istnieje, oblicz statystyki\n",
    "if \"price\" in products_df.columns:\n",
    " products_stats = products_df.select(\n",
    " ____(\"*\").alias(\"total_products\"), # count\n",
    " spark_round(avg(\"____\"), 2).alias(\"avg_price\"), # price\n",
    " min(\"price\").alias(\"min_price\"),\n",
    " ____(\"price\").alias(\"max_price\") # max\n",
    " )\n",
    " \n",
    " display(products_stats)\n",
    "else:\n",
    " display(spark.createDataFrame([(\"Kolumna 'price' nie istnieje w danych\",)], [\"info\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statystyki cenowe produktów**\n",
    "\n",
    "Jeśli kolumna `price` istnieje, analizujemy:\n",
    "- **Średnia cena**: Ogólny poziom cenowy\n",
    "- **Zakres cen**: min/max dla zrozumienia portfela\n",
    "- **Segmentacja**: Produkty tanie vs premium\n",
    "\n",
    "**Zastosowanie**: Pricing strategy, analiza konkurencyjności, segmentacja produktów"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9294cc72-10d1-4c6d-8a61-86c8a9b58507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Zadanie 7: Analiza jakości danych (15 min)\n",
    "\n",
    "### Cel:\n",
    "Stwórz raport jakości danych dla wszystkich trzech datasetów.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Dla każdego datasetu policz:\n",
    " - Liczbę rekordów\n",
    " - Liczbę kolumn\n",
    " - Liczbę wartości NULL w każdej kolumnie\n",
    " - Procent kompletności danych\n",
    "2. Zidentyfikuj duplikaty w każdym datasecie\n",
    "3. Znajdź potencjalne problemy (np. negatywne wartości, nietypowe daty)\n",
    "\n",
    "### Oczekiwany rezultat:\n",
    "- Kompletny raport jakości danych\n",
    "- Lista zidentyfikowanych problemów\n",
    "- Rekomendacje działań naprawczych\n",
    "\n",
    "### Wskazówki:\n",
    "- Użyj `.count()`, `.distinct().count()` do wykrywania duplikatów\n",
    "- Użyj filtrów do znajdowania nietypowych wartości"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bbccab3-b82d-414a-9bae-dc7fc80432d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Funkcja do analizy jakości danych\n",
    "def data_quality_report(df, dataset_name):\n",
    " total_rows = df.count()\n",
    " total_cols = len(df.columns)\n",
    " \n",
    " # Sprawdź duplikaty\n",
    " distinct_rows = df.____().count() # distinct\n",
    " duplicates = total_rows - distinct_rows\n",
    " dup_pct = (duplicates / total_rows * 100) if total_rows > 0 else 0\n",
    " \n",
    " # Sprawdź wartości NULL\n",
    " null_counts = df.select([\n",
    " spark_sum(col(c).isNull().cast(\"int\")).alias(c) \n",
    " for c in df.columns\n",
    " ]).collect()[0].asDict()\n",
    " \n",
    " # Stwórz podsumowanie\n",
    " summary_data = [\n",
    " (dataset_name, total_rows, total_cols, ____, # duplicates\n",
    " f\"{dup_pct:.2f}%\", sum(null_counts.values()))\n",
    " ]\n",
    " \n",
    " summary_df = spark.createDataFrame(summary_data, \n",
    " [\"Dataset\", \"Total_Rows\", \"Total_Cols\", \"Duplicates\", \"Dup_Percent\", \"Total_NULLs\"])\n",
    " \n",
    " return summary_df, null_counts\n",
    "\n",
    "# Uruchom raport dla każdego datasetu\n",
    "customers_summary, customers_nulls = data_quality_report(____, \"Customers\") # customers_df_manual\n",
    "orders_summary, orders_nulls = data_quality_report(orders_df_manual, \"Orders\")\n",
    "products_summary, products_nulls = data_quality_report(____, \"Products\") # products_df\n",
    "\n",
    "# Połącz wszystkie podsumowania\n",
    "combined_summary = customers_summary.union(orders_summary).union(products_summary)\n",
    "display(combined_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skonsolidowany raport jakości danych**\n",
    "\n",
    "Podsumowanie pokazuje dla każdego datasetu:\n",
    "- **Total_Rows**: Liczba rekordów\n",
    "- **Total_Cols**: Liczba kolumn \n",
    "- **Duplicates**: Liczba duplikatów\n",
    "- **Dup_Percent**: Procent duplikatów\n",
    "- **Total_NULLs**: Suma wszystkich wartości NULL\n",
    "\n",
    "**Kluczowe pytania:**\n",
    "- Które datasety mają najwięcej problemów jakości?\n",
    "- Czy duplikaty są akceptowalne biznesowo?\n",
    "- Które kolumny wymagają wypełnienia braków?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d6be7903-2d46-43ec-8573-0b8b1be9404b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdź problematyczne wartości w orders\n",
    "negative_orders = orders_df_manual.filter(col(\"____\") < 0).count() # total_amount\n",
    "zero_orders = orders_df_manual.filter(col(\"total_amount\") == ____).count() # 0\n",
    "\n",
    "# Sprawdź nietypowe daty\n",
    "from pyspark.sql.functions import year, current_date, datediff\n",
    "\n",
    "future_orders = orders_df_manual.filter(col(\"order_datetime\") > ____()).count() # current_date\n",
    "old_orders = orders_df_manual.filter(year(col(\"____\")) < 2020).count() # order_datetime\n",
    "\n",
    "# Stwórz podsumowanie problemów\n",
    "problems_data = [\n",
    " (\"Negatywne wartości zamówień\", negative_orders),\n",
    " (\"Zamówienia z wartością zero\", zero_orders),\n",
    " (\"Zamówienia z przyszłą datą\", future_orders),\n",
    " (\"Zamówienia starsze niż 2020\", old_orders)\n",
    "]\n",
    "\n",
    "problems_df = spark.createDataFrame(problems_data, [\"Problem\", \"Count\"])\n",
    "display(problems_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Identyfikacja problemów jakości danych**\n",
    "\n",
    "Znalezione anomalie w danych:\n",
    "- **Negatywne wartości**: Błędy systemowe lub zwroty?\n",
    "- **Wartości zero**: Promocje czy błędy?\n",
    "- **Przyszłe daty**: Błędy wprowadzania danych\n",
    "- **Bardzo stare daty**: Dane testowe czy prawdziwe?\n",
    "\n",
    "**Następne kroki**: Decyzja biznesowa - usunąć, poprawić czy oznacza jako odstające?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "318df488-5a43-4d34-8feb-e0424d1aabfc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist - Co powinieneś uzyskać:\n",
    "- [ ] Wszystkie 3 datasety wczytane (customers, orders, products)\n",
    "- [ ] Schematy zdefiniowane manualnie dla CSV i JSON\n",
    "- [ ] Kompletna eksploracja każdego datasetu\n",
    "- [ ] Raport jakości danych wygenerowany\n",
    "- [ ] Zidentyfikowane problemy z danymi\n",
    "\n",
    "### Komendy weryfikacyjne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45503cb5-da22-48cf-8a63-331cdc4c19a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"=== WERYFIKACJA WYNIKÓW WARSZTATU ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Weryfikacja wyników warsztatu\n",
    "try:\n",
    " assert 'customers_df_manual' in locals(), \"customers_df_manual nie został utworzony\"\n",
    " assert 'orders_df_manual' in locals(), \"orders_df_manual nie został utworzony\"\n",
    " assert 'products_df' in locals(), \"products_df nie został utworzony\"\n",
    " \n",
    " # Sprawdź liczby rekordów\n",
    " verification_data = [\n",
    " (\"Customers\", customers_df_manual.count()),\n",
    " (\"Orders\", orders_df_manual.count()),\n",
    " (\"Products\", products_df.count())\n",
    " ]\n",
    " \n",
    " verification_df = spark.createDataFrame(verification_data, [\"Dataset\", \"Record_Count\"])\n",
    " display(verification_df)\n",
    " \n",
    " # Sprawdź tabele Delta Lake\n",
    " tables_to_check = [\n",
    " f\"{BRONZE_SCHEMA}.customers_workshop\",\n",
    " f\"{BRONZE_SCHEMA}.orders_workshop\", \n",
    " f\"{BRONZE_SCHEMA}.products_workshop\"\n",
    " ]\n",
    " \n",
    " table_status = []\n",
    " for table in tables_to_check:\n",
    " try:\n",
    " spark.table(table)\n",
    " table_status.append((table, \" Exists\"))\n",
    " except:\n",
    " table_status.append((table, \"✗ Missing\"))\n",
    " \n",
    " tables_df = spark.createDataFrame(table_status, [\"Table\", \"Status\"])\n",
    " display(tables_df)\n",
    " \n",
    " print(\" WARSZTAT ZAKOŃCZONY POMYŚLNIE! \")\n",
    " \n",
    "except (AssertionError, NameError) as e:\n",
    " print(f\"✗ Błąd: {e}\")\n",
    " print(\"Sprawdź czy wszystkie komórki zostały wykonane poprawnie.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wyniki weryfikacji warsztatu**\n",
    "\n",
    "Tabela weryfikacji pokazuje:\n",
    "- **Record_Count**: Liczba rekordów w każdym DataFrame\n",
    "- **Table Status**: Czy tabele Delta Lake zostały utworzone\n",
    "\n",
    "**Warunki sukcesu:**\n",
    "- Wszystkie DataFrames utworzone\n",
    "- Liczba rekordów > 0\n",
    "- Wszystkie tabele zapisane w Delta Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f2c075b-c674-4610-8edb-86168842449a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "### Problem 1: Błąd \"Path does not exist\"\n",
    "**Objawy:**\n",
    "- Błąd podczas wczytywania plików CSV/JSON/Parquet\n",
    "- Komunikat: `Path does not exist: /Volumes/...`\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Sprawdź, czy ścieżka istnieje\n",
    "dbutils.fs.ls(DATASET_BASE_PATH)\n",
    "\n",
    "# Upewnij się, że zmienne są poprawnie zdefiniowane\n",
    "print(f\"DATASET_BASE_PATH: {DATASET_BASE_PATH}\")\n",
    "print(f\"CUSTOMERS_CSV: {CUSTOMERS_CSV}\")\n",
    "```\n",
    "\n",
    "### Problem 2: Błąd typu danych podczas wczytywania\n",
    "**Objawy:**\n",
    "- Wartości NULL w miejscach, gdzie powinny być dane\n",
    "- Niepoprawne typy kolumn (wszystko jako string)\n",
    "\n",
    "**Rozwiązanie:**\n",
    "- Zawsze definiuj schemat manualnie zamiast używać `inferSchema=True`\n",
    "- Użyj odpowiednich typów: `IntegerType()`, `StringType()`, `TimestampType()`, `DoubleType()`\n",
    "\n",
    "### Problem 3: Tabela już istnieje\n",
    "**Objawy:**\n",
    "- Błąd: `Table already exists`\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Użyj mode=\"overwrite\"\n",
    "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "# Lub usuń tabelę przed zapisem\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "```\n",
    "\n",
    "### Debugging tips:\n",
    "- Użyj `.printSchema()` aby sprawdzić strukturę DataFrame\n",
    "- Użyj `.explain()` aby zobaczyć plan wykonania\n",
    "- Sprawdź liczbę rekordów: `df.count()`\n",
    "- Wyświetl przykładowe dane: `display(df.limit(10))`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ef18a348-4b54-4463-b0b5-2dcd93f3519f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices\n",
    "\n",
    "### Wydajność:\n",
    "- **Zawsze definiuj schemat manualnie** - `inferSchema=True` jest wolne i może dawać błędne typy\n",
    "- **Używaj Parquet dla dużych datasetów** - najlepsza kompresja i wydajność\n",
    "- **Partycjonuj duże tabele** według często używanych kolumn filtrujących\n",
    "- **Używaj `.cache()` tylko gdy DataFrame jest wielokrotnie użyty**\n",
    "\n",
    "### Jakość kodu:\n",
    "- **Nazywaj DataFrames opisowo**: `customers_df`, `orders_clean_df`, `products_enriched_df`\n",
    "- **Dodawaj komentarze** wyjaśniające logikę biznesową\n",
    "- **Sprawdzaj dane po każdym kroku** transformacji\n",
    "- **Używaj zmiennych dla nazw tabel i schematów** (nie hardcoduj)\n",
    "\n",
    "### Data Quality:\n",
    "- **Zawsze sprawdzaj wartości NULL** przed transformacjami\n",
    "- **Wykrywaj duplikaty**: `df.distinct().count()` vs `df.count()`\n",
    "- **Waliduj zakresy wartości**: negatywne ceny, przyszłe daty\n",
    "- **Dokumentuj znalezione problemy** w komentarzach lub osobnym notebooku\n",
    "\n",
    "### Governance:\n",
    "- **Używaj Unity Catalog** dla zarządzania tabelami i uprawnieniami\n",
    "- **Stosuj konwencję nazewnictwa**: `bronze.customers`, `silver.customers_clean`, `gold.customers_agg`\n",
    "- **Separuj środowiska**: dev/staging/prod używając różnych katalogów\n",
    "- **Loguj operacje**: zapisuj metadane o przetwarzanych danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8528d360-9d75-4a8a-9792-b3de7b0a6bfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "### Co zostało osiągnięte:\n",
    "- Skonfigurowałeś środowisko Databricks i zmienne per-user\n",
    "- Wczytałeś dane z trzech różnych formatów (CSV, JSON, Parquet)\n",
    "- Zdefiniowałeś schematy manualnie (best practice)\n",
    "- Przeprowadziłeś szczegółową eksplorację wszystkich datasetów\n",
    "- Zidentyfikowałeś problemy jakości danych\n",
    "- Wygenerowałeś kompletny raport jakości danych\n",
    "- Zapisałeś dane do tabel Delta Lake\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "1. **Ręczne schematy > inferSchema**: Szybsze, bezpieczniejsze i przewidywalne typy danych\n",
    "2. **Eksploracja przed transformacją**: Zawsze analizuj dane przed rozpoczęciem przetwarzania\n",
    "3. **Jakość wymaga monitorowania**: NULL, duplikaty, outliers muszą być wykrywane systematycznie\n",
    "4. **Parquet to standard lakehouse**: Wbudowany schemat, najlepsza kompresja i wydajność\n",
    "\n",
    "### Quick Reference - Najważniejsze komendy:\n",
    "\n",
    "| Operacja | PySpark | Notatki |\n",
    "|----------|---------|---------|\n",
    "| Wczytaj CSV | `spark.read.format(\"csv\").option(\"header\",\"true\").schema(schema).load(path)` | Zawsze używaj ręcznego schematu |\n",
    "| Wczytaj JSON | `spark.read.format(\"json\").schema(schema).load(path)` | Opcja `multiLine=true` dla JSON arrays |\n",
    "| Wczytaj Parquet | `spark.read.format(\"parquet\").load(path)` | Schemat wbudowany |\n",
    "| Zapisz do Delta | `df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table)` | mode: overwrite/append |\n",
    "| Sprawdź NULL | `df.select([sum(col(c).isNull()).alias(c) for c in df.columns])` | Dla każdej kolumny |\n",
    "| Znajdź duplikaty | `df.count() - df.distinct().count()` | Różnica = liczba duplikatów |\n",
    "| Statystyki | `df.describe()` | Dla kolumn numerycznych |\n",
    "| Grupowanie | `df.groupBy(\"col\").count()` | Agregacje |\n",
    "\n",
    "### Następne kroki:\n",
    "- **Kolejny warsztat**: `02_transformations_cleaning_workshop.ipynb` - Transformacje i czyszczenie danych\n",
    "- **Praktyka**: Powtórz eksplorację na własnych datasetach\n",
    "- **Dokumentacja**: [Databricks Data Import Best Practices](https://docs.databricks.com/ingestion/index.html)\n",
    "- **Zadanie domowe**: Wykonaj analizę eksploracyjną dodatkowego datasetu (np. transactions, events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9945374c-c1b4-4351-9111-1a61c765baa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Czyszczenie zasobów\n",
    "\n",
    "Opcjonalne - usuń tabele utworzone podczas warsztatu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "56bee403-2364-4a3f-9894-a6e551f3a7a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# UWAGA: Uruchom tylko jeśli chcesz usunąć wszystkie utworzone tabele\n",
    "# Te tabele mogą być potrzebne w kolejnych warsztatach!\n",
    "\n",
    "# Odkomentuj poniższe linie aby usunąć tabele:\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {BRONZE_SCHEMA}.customers_workshop\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {BRONZE_SCHEMA}.orders_workshop\") \n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {BRONZE_SCHEMA}.products_workshop\")\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "display(spark.createDataFrame([\n",
    " (\"Czyszczenie zasobów jest zakomentowane\",),\n",
    " (\"Odkomentuj kod powyżej aby usunąć tabele\",),\n",
    " (\"UWAGA: Tabele mogą być potrzebne w kolejnych warsztatach!\",)\n",
    "], [\"Info\"]))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_workspace_data_exploration_workshop",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}