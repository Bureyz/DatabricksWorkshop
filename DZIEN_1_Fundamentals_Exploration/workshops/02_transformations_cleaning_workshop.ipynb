{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46846e6b",
   "metadata": {},
   "source": [
    "# Workshop: Transformations & Data Cleaning\n",
    "\n",
    "**Cel szkoleniowy:** Praktyczne opanowanie transformacji danych w PySpark i SQL oraz technik czyszczenia danych.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Transformacje kolumnowe i warunkowe\n",
    "- Filtry, sortowania, agregacje\n",
    "- Por√≥wnanie PySpark vs SQL\n",
    "- Czyszczenie: nulls, duplikaty, walidacja typ√≥w\n",
    "- Quality checks i flagowanie problem√≥w\n",
    "\n",
    "**Czas trwania:** 90 minut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad6e36f",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzie≈Ñ szkolenia**: Dzie≈Ñ 1 - Fundamentals & Exploration\n",
    "- **Typ notebooka**: Workshop\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog w≈ÇƒÖczony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b062cd0d",
   "metadata": {},
   "source": [
    "## Wprowadzenie do warsztatu\n",
    "\n",
    "W tym warsztacie bƒôdziesz:\n",
    "1. Transformowaƒá dane za pomocƒÖ PySpark DataFrame API\n",
    "2. Pisaƒá r√≥wnowa≈ºne transformacje w SQL\n",
    "3. Czy≈õciƒá dane (nulls, duplikaty, nieprawid≈Çowe warto≈õci)\n",
    "4. Walidowaƒá typy danych i konwertowaƒá kolumny\n",
    "5. Implementowaƒá quality checks\n",
    "\n",
    "### Kryteria sukcesu:\n",
    "- Wszystkie transformacje dzia≈ÇajƒÖ poprawnie w PySpark i SQL\n",
    "- Dane sƒÖ oczyszczone z duplikat√≥w i null values\n",
    "- Quality checks zidentyfikowa≈Çy wszystkie problemy\n",
    "- Finalna tabela spe≈Çnia standardy jako≈õci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3c827",
   "metadata": {},
   "source": [
    "## Izolacja per u≈ºytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalog√≥w i schemat√≥w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f50fa",
   "metadata": {},
   "source": [
    "## Konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58bb532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when, trim, lower, upper, regexp_replace, to_date, coalesce, lit\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ≈öcie≈ºki do danych\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema: {BRONZE_SCHEMA}\")\n",
    "\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941d0499",
   "metadata": {},
   "source": [
    "## Przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72875bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytaj dane\n",
    "customers_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(CUSTOMERS_CSV)\n",
    "orders_df = spark.read.format(\"json\").option(\"inferSchema\", \"true\").load(ORDERS_JSON)\n",
    "\n",
    "print(f\"Customers: {customers_df.count()} rekord√≥w\")\n",
    "print(f\"Orders: {orders_df.count()} rekord√≥w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3967d82a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 1: Transformacje kolumnowe - PySpark (15 min)\n",
    "\n",
    "### Cel:\n",
    "Zastosuj transformacje kolumnowe na danych klient√≥w.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Dodaj kolumnƒô `email_domain` wyodrƒôbniajƒÖc domenƒô z email\n",
    "2. Dodaj kolumnƒô `country_code` z pierwszymi 2 literami kraju (uppercase)\n",
    "3. Dodaj kolumnƒô `customer_full_name` - trim i lowercase\n",
    "4. Usu≈Ñ kolumnƒô `registration_date` (je≈õli istnieje)\n",
    "5. Zmie≈Ñ nazwƒô kolumny `first_name` na `customer_full_name`\n",
    "\n",
    "### Wskaz√≥wki:\n",
    "- U≈ºyj: `withColumn()`, `regexp_extract()`, `substring()`, `trim()`, `lower()`, `drop()`, `withColumnRenamed()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0382566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dodaj kolumnƒô email_domain\n",
    "customers_transformed = customers_df.withColumn(\n",
    "    \"email_domain\",\n",
    "    F.____(____(____), \"@(.+)$\", 1)  # regexp_extract, col, email\n",
    ")\n",
    "\n",
    "# TODO: Dodaj kolumnƒô country_code (pierwsze 2 litery, uppercase)\n",
    "customers_transformed = customers_transformed.withColumn(\n",
    "    \"country_code\",\n",
    "    ____(col(\"____\").substr(1, ____))  # upper, country, 2\n",
    ")\n",
    "\n",
    "# TODO: Dodaj kolumnƒô customer_full_name (trim i lowercase)\n",
    "customers_transformed = customers_transformed.withColumn(\n",
    "    \"customer_full_name\",\n",
    "    ____(____(col(\"first_name\")))  # lower, trim\n",
    ")\n",
    "\n",
    "# TODO: Usu≈Ñ kolumnƒô registration_date (je≈õli istnieje)\n",
    "if \"registration_date\" in customers_transformed.columns:\n",
    "    customers_transformed = customers_transformed.____(\"____\")  # drop, registration_date\n",
    "\n",
    "# TODO: Zmie≈Ñ nazwƒô first_name na customer_full_name\n",
    "customers_transformed = customers_transformed.____(\"first_name\", \"____\")  # withColumnRenamed, customer_full_name\n",
    "\n",
    "print(\"=== Przyk≈Çadowe dane po transformacjach ===\")\n",
    "display(customers_transformed.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d1717d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 2: Transformacje warunkowe (15 min)\n",
    "\n",
    "### Cel:\n",
    "Dodaj kolumny z logikƒÖ warunkowƒÖ.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Dodaj kolumnƒô `customer_segment` (VIP dla PL/DE, Regular dla reszty)\n",
    "2. Dodaj kolumnƒô `email_valid` (True je≈õli email zawiera '@', False w przeciwnym razie)\n",
    "3. Dodaj kolumnƒô `country_region` (Europe dla PL/DE/FR, Other dla reszty)\n",
    "\n",
    "### Wskaz√≥wki:\n",
    "- U≈ºyj: `when().otherwise()`\n",
    "- Dla wielu warunk√≥w: `.when().when().otherwise()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d9e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dodaj kolumnƒô customer_segment (VIP dla PL/DE, Regular dla reszty)\n",
    "customers_transformed = customers_transformed.withColumn(\n",
    "    \"customer_segment\",\n",
    "    ____(col(\"country\").isin(\"Poland\", \"____\"), \"VIP\")  # when, Germany\n",
    "    .otherwise(\"____\")  # Regular\n",
    ")\n",
    "\n",
    "# TODO: Dodaj kolumnƒô email_valid (True je≈õli zawiera @)\n",
    "customers_transformed = customers_transformed.withColumn(\n",
    "    \"email_valid\",\n",
    "    when(col(\"____\").____(\"@\"), ____)  # email, contains, True\n",
    "    .otherwise(False)\n",
    ")\n",
    "\n",
    "# TODO: Dodaj kolumnƒô country_region (Europe dla PL/DE/FR, Other dla reszty)\n",
    "customers_transformed = customers_transformed.withColumn(\n",
    "    \"country_region\",\n",
    "    when(col(\"country\").____(\"Poland\", \"Germany\", \"France\"), \"____\")  # isin, Europe\n",
    "    .____(\"Other\")  # otherwise\n",
    ")\n",
    "\n",
    "print(\"=== Przyk≈Çadowe dane z transformacjami warunkowymi ===\")\n",
    "display(\n",
    "    customers_transformed\n",
    "    .select(\"customer_id\", \"country\", \"customer_segment\", \"email_valid\", \"country_region\")\n",
    "    .limit(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1f45ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 3: Filtry i sortowanie (10 min)\n",
    "\n",
    "### Cel:\n",
    "Filtruj i sortuj dane.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Filtruj klient√≥w z Polski\n",
    "2. Filtruj zam√≥wienia o warto≈õci > 100\n",
    "3. Sortuj zam√≥wienia wed≈Çug total_amount malejƒÖco\n",
    "4. Znajd≈∫ top 10 najdro≈ºszych zam√≥wie≈Ñ\n",
    "\n",
    "### Wskaz√≥wki:\n",
    "- U≈ºyj: `.filter()` lub `.where()`, `.orderBy()`, `.limit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc780106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Filtruj klient√≥w z Polski\n",
    "polish_customers = customers_transformed.filter(col(\"____\") == \"Poland\")  # country\n",
    "print(f\"Klienci z Polski: {polish_customers.count()}\")\n",
    "\n",
    "# TODO: Filtruj zam√≥wienia o warto≈õci > 100\n",
    "high_value_orders = orders_df.filter(col(\"____\") > ____)  # total_amount, 100\n",
    "print(f\"Zam√≥wienia > 100: {high_value_orders.count()}\")\n",
    "\n",
    "# TODO: Top 10 najdro≈ºszych zam√≥wie≈Ñ (sortowanie malejƒÖce)\n",
    "top_orders = (\n",
    "    orders_df\n",
    "    .orderBy(col(\"total_amount\").____())  # desc\n",
    "    .limit(____)  # 10\n",
    ")\n",
    "\n",
    "print(\"\\n=== Top 10 najdro≈ºszych zam√≥wie≈Ñ ===\")\n",
    "display(top_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0718977a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 4: Agregacje - PySpark (15 min)\n",
    "\n",
    "### Cel:\n",
    "Przeprowad≈∫ agregacje na zam√≥wieniach.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Policz zam√≥wienia wed≈Çug statusu\n",
    "2. Oblicz ca≈ÇkowitƒÖ warto≈õƒá zam√≥wie≈Ñ wed≈Çug statusu\n",
    "3. Znajd≈∫ ≈õredniƒÖ warto≈õƒá zam√≥wienia wed≈Çug customer_id\n",
    "4. Policz zam√≥wienia per klient\n",
    "\n",
    "### Wskaz√≥wki:\n",
    "- U≈ºyj: `.groupBy()`, `.agg()`, `count()`, `sum()`, `avg()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6424a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Policz zam√≥wienia wed≈Çug statusu\n",
    "orders_by_status = (\n",
    "    orders_df\n",
    "    .groupBy(\"____\")  # status\n",
    "    .count()\n",
    "    .orderBy(\"count\", ascending=____)  # False\n",
    ")\n",
    "\n",
    "print(\"=== Zam√≥wienia wed≈Çug statusu ===\")\n",
    "display(orders_by_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7642a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ca≈Çkowita warto≈õƒá i ≈õrednia wed≈Çug statusu\n",
    "revenue_by_status = (\n",
    "    orders_df\n",
    "    .groupBy(\"status\")\n",
    "    .agg(\n",
    "        F.____(\"total_amount\").alias(\"total_revenue\"),  # sum\n",
    "        F.____(\"total_amount\").alias(\"avg_order_value\")  # avg\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"=== Revenue wed≈Çug statusu ===\")\n",
    "display(revenue_by_status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a18ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Statystyki per klient\n",
    "customer_stats = (\n",
    "    orders_df\n",
    "    .groupBy(\"____\")  # customer_id\n",
    "    .agg(\n",
    "        F.____(\"*\").alias(\"total_orders\"),  # count\n",
    "        F.sum(\"____\").alias(\"total_spent\"),  # total_amount\n",
    "        F.avg(\"total_amount\").alias(\"avg_order_value\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"=== Top 10 klient√≥w wed≈Çug wydatk√≥w ===\")\n",
    "display(customer_stats.orderBy(\"____\", ascending=False).limit(10))  # total_spent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5851e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 5: SQL Equivalent (10 min)\n",
    "\n",
    "### Cel:\n",
    "Napisz te same transformacje w SQL.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Utw√≥rz temp view dla customers i orders\n",
    "2. Napisz SQL query agregujƒÖcy zam√≥wienia wed≈Çug statusu\n",
    "3. Napisz SQL query z JOINem customers i orders\n",
    "\n",
    "### Wskaz√≥wki:\n",
    "- U≈ºyj: `createOrReplaceTempView()`, `spark.sql()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utw√≥rz temp views dla SQL queries\n",
    "customers_df.____(\"customers\")  # createOrReplaceTempView\n",
    "orders_df.createOrReplaceTempView(\"____\")  # orders\n",
    "\n",
    "print(\"‚úì Temp views utworzone: customers, orders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9bcf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: SQL query - agregacja zam√≥wie≈Ñ wed≈Çug statusu\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        status,\n",
    "        ____(*) as order_count,  -- COUNT\n",
    "        ____(total_amount) as total_revenue,  -- SUM\n",
    "        ____(total_amount) as avg_order_value  -- AVG\n",
    "    FROM ____  -- orders\n",
    "    GROUP BY ____  -- status\n",
    "    ORDER BY total_revenue ____  -- DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== Agregacja zam√≥wie≈Ñ (SQL) ===\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d19882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: SQL query - JOIN customers i orders\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.first_name,\n",
    "        c.country,\n",
    "        ____(____.order_id) as total_orders,  -- COUNT, o\n",
    "        SUM(o.____) as total_spent  -- total_amount\n",
    "    FROM ____ c  -- customers\n",
    "    ____ JOIN orders o ON c.customer_id = o.customer_id  -- LEFT\n",
    "    GROUP BY c.customer_id, c.first_name, c.country\n",
    "    ORDER BY ____ DESC  -- total_spent\n",
    "    LIMIT ____  -- 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== Top 10 klient√≥w z zam√≥wieniami (SQL JOIN) ===\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38e9a30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 6: Data Cleaning - Nulls (15 min)\n",
    "\n",
    "### Cel:\n",
    "Obs≈Çu≈º warto≈õci NULL w danych.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Policz warto≈õci NULL w ka≈ºdej kolumnie customers\n",
    "2. ZastƒÖp NULL w `email` warto≈õciƒÖ \"unknown@example.com\"\n",
    "3. ZastƒÖp NULL w `country` warto≈õciƒÖ \"Unknown\"\n",
    "4. Usu≈Ñ rekordy gdzie `customer_id` jest NULL\n",
    "\n",
    "### Wskaz√≥wki:\n",
    "- U≈ºyj: `.fillna()`, `.dropna()`, `coalesce()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5977ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Policz warto≈õci NULL w ka≈ºdej kolumnie\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "null_counts = customers_df.select([\n",
    "    spark_sum(col(c).____().cast(\"int\")).alias(c)  # isNull\n",
    "    for c in customers_df.columns\n",
    "])\n",
    "\n",
    "print(\"=== Warto≈õci NULL przed czyszczeniem ===\")\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a741a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ZastƒÖp NULL w email warto≈õciƒÖ domy≈õlnƒÖ\n",
    "customers_clean = customers_df.____({  # fillna\n",
    "    \"email\": \"____\"  # unknown@example.com\n",
    "})\n",
    "\n",
    "# TODO: ZastƒÖp NULL w country\n",
    "customers_clean = customers_clean.fillna({\n",
    "    \"____\": \"Unknown\"  # country\n",
    "})\n",
    "\n",
    "# TODO: Usu≈Ñ rekordy gdzie customer_id jest NULL\n",
    "customers_clean = customers_clean.____(____ =[\"customer_id\"])  # dropna, subset\n",
    "\n",
    "print(f\"\\n‚úì Rekordy po czyszczeniu NULL: {customers_clean.count()}\")\n",
    "print(f\"  Usuniƒôto: {customers_df.count() - customers_clean.count()} rekord√≥w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f5e46d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 7: Deduplikacja (10 min)\n",
    "\n",
    "### Cel:\n",
    "Usu≈Ñ duplikaty z danych.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Znajd≈∫ liczbƒô duplikat√≥w w customers (wszystkie kolumny)\n",
    "2. Usu≈Ñ duplikaty oparte na customer_id\n",
    "3. Usu≈Ñ duplikaty oparte na email\n",
    "\n",
    "### Wskaz√≥wki:\n",
    "- U≈ºyj: `.dropDuplicates()`, `.distinct()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ecfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Znajd≈∫ liczbƒô duplikat√≥w (wszystkie kolumny)\n",
    "total_rows = customers_clean.count()\n",
    "distinct_rows = customers_clean.____().count()  # distinct\n",
    "duplicates = total_rows - distinct_rows\n",
    "\n",
    "print(\"=== Analiza duplikat√≥w ===\")\n",
    "print(f\"Ca≈Çkowita liczba rekord√≥w: {total_rows}\")\n",
    "print(f\"Unikalne rekordy: {distinct_rows}\")\n",
    "print(f\"Duplikaty (wszystkie kolumny): {____}\")  # duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f2d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Usu≈Ñ duplikaty na podstawie customer_id\n",
    "customers_dedup = customers_clean.____([____])  # dropDuplicates, \"customer_id\"\n",
    "\n",
    "print(f\"\\n‚úì Rekordy po deduplikacji (customer_id): {customers_dedup.count()}\")\n",
    "print(f\"  Usuniƒôto: {customers_clean.count() - customers_dedup.count()} duplikat√≥w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bc287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sprawd≈∫ duplikaty w kolumnie email\n",
    "email_duplicates = (\n",
    "    customers_clean\n",
    "    .groupBy(\"____\")  # email\n",
    "    .count()\n",
    "    .filter(\"count > ____\")  # 1\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Duplikaty email ===\")\n",
    "print(f\"Liczba duplikat√≥w email: {email_duplicates.count()}\")\n",
    "\n",
    "if email_duplicates.count() > 0:\n",
    "    print(\"\\nPrzyk≈Çady duplikat√≥w email:\")\n",
    "    display(email_duplicates.orderBy(\"count\", ascending=False).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957c7fed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 8: Quality Checks (10 min)\n",
    "\n",
    "### Cel:\n",
    "Zaimplementuj quality checks i flaguj problemy.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Dodaj kolumnƒô `quality_flag` (0 = OK, 1 = problem)\n",
    "2. Flaguj rekordy gdzie email nie zawiera '@'\n",
    "3. Flaguj rekordy gdzie country = \"Unknown\"\n",
    "4. Policz rekordy z problemami\n",
    "\n",
    "### Wskaz√≥wki:\n",
    "- U≈ºyj: `when().otherwise()` dla flagowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad812c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dodaj quality_flag (0 = OK, 1 = problem)\n",
    "customers_with_flags = customers_dedup.withColumn(\n",
    "    \"quality_flag\",\n",
    "    when(~col(\"email\").____(\"@\"), ____)  # contains, 1\n",
    "    .when(col(\"____\") == \"Unknown\", 1)  # country\n",
    "    .otherwise(____)  # 0\n",
    ")\n",
    "\n",
    "# TODO: Policz rekordy z problemami\n",
    "problem_count = customers_with_flags.filter(col(\"quality_flag\") == ____).count()  # 1\n",
    "ok_count = customers_with_flags.filter(col(\"quality_flag\") == 0).count()\n",
    "\n",
    "print(\"=== Quality Checks ===\")\n",
    "print(f\"‚úì Rekordy OK: {ok_count}\")\n",
    "print(f\"‚ö† Rekordy z problemami: {____}\")  # problem_count\n",
    "print(f\"  Procent problem√≥w: {(problem_count / customers_with_flags.count() * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523dcc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wy≈õwietl przyk≈Çadowe rekordy z problemami\n",
    "if problem_count > 0:\n",
    "    print(\"\\n=== Przyk≈Çadowe rekordy z problemami ===\")\n",
    "    display(\n",
    "        customers_with_flags\n",
    "        .filter(col(\"____\") == 1)  # quality_flag\n",
    "        .select(\"customer_id\", \"customer_full_name\", \"email\", \"country\", \"quality_flag\")\n",
    "        .limit(5)\n",
    "    )\n",
    "else:\n",
    "    print(\"\\n‚úì Brak rekord√≥w z problemami!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e703fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist:\n",
    "- [ ] Transformacje kolumnowe zastosowane\n",
    "- [ ] Transformacje warunkowe dzia≈ÇajƒÖ\n",
    "- [ ] Agregacje poprawne (PySpark i SQL)\n",
    "- [ ] NULL values obs≈Çu≈ºone\n",
    "- [ ] Duplikaty usuniƒôte\n",
    "- [ ] Quality checks zaimplementowane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"=== WERYFIKACJA WYNIK√ìW WARSZTATU ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    "    # Sprawd≈∫ czy wszystkie DataFrames zosta≈Çy utworzone\n",
    "    assert 'customers_transformed' in locals(), \"customers_transformed nie zosta≈Ç utworzony\"\n",
    "    assert 'customers_clean' in locals(), \"customers_clean nie zosta≈Ç utworzony\"\n",
    "    assert 'customers_dedup' in locals(), \"customers_dedup nie zosta≈Ç utworzony\"\n",
    "    assert 'customers_with_flags' in locals(), \"customers_with_flags nie zosta≈Ç utworzony\"\n",
    "    \n",
    "    print(\"\\n‚úì Wszystkie DataFrames utworzone\")\n",
    "    \n",
    "    # Statystyki finalne\n",
    "    print(f\"\\nüìä Statystyki ko≈Ñcowe:\")\n",
    "    print(f\"  Oryginalna liczba rekord√≥w: {customers_df.count()}\")\n",
    "    print(f\"  Po transformacjach: {customers_transformed.count()}\")\n",
    "    print(f\"  Po czyszczeniu NULL: {customers_clean.count()}\")\n",
    "    print(f\"  Po deduplikacji: {customers_dedup.count()}\")\n",
    "    print(f\"  Finalna liczba rekord√≥w: {customers_with_flags.count()}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ Quality Checks:\")\n",
    "    ok_records = customers_with_flags.filter(col('quality_flag') == 0).count()\n",
    "    problem_records = customers_with_flags.filter(col('quality_flag') == 1).count()\n",
    "    print(f\"  ‚úì Rekordy bez problem√≥w: {ok_records}\")\n",
    "    print(f\"  ‚ö† Rekordy z problemami: {problem_records}\")\n",
    "    \n",
    "    # Zapisz do tabeli (opcjonalnie)\n",
    "    output_table = f\"{BRONZE_SCHEMA}.customers_clean_workshop\"\n",
    "    print(f\"\\nüíæ Zapisywanie do tabeli: {output_table}\")\n",
    "    \n",
    "    customers_with_flags.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(output_table)\n",
    "    \n",
    "    print(f\"  ‚úì Zapisano tabelƒô: {output_table}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"‚úì‚úì‚úì WARSZTAT ZAKO≈ÉCZONY POMY≈öLNIE! ‚úì‚úì‚úì\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "except AssertionError as e:\n",
    "    print(f\"\\n‚úó B≈ÇƒÖd: {e}\")\n",
    "    print(\"  Upewnij siƒô, ≈ºe wszystkie zadania zosta≈Çy wykonane!\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚úó Nieoczekiwany b≈ÇƒÖd: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c4e25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie warsztatu\n",
    "\n",
    "### ‚úÖ Co osiƒÖgnƒÖ≈Çe≈õ:\n",
    "\n",
    "**Transformacje:**\n",
    "- ‚úÖ Transformacje kolumnowe: `withColumn()`, `drop()`, `withColumnRenamed()`\n",
    "- ‚úÖ Ekstrakcja danych: `regexp_extract()`, `substring()`\n",
    "- ‚úÖ Formatowanie: `trim()`, `lower()`, `upper()`\n",
    "- ‚úÖ Logika warunkowa: `when().otherwise()`\n",
    "- ‚úÖ Filtry i sortowanie: `filter()`, `orderBy()`, `limit()`\n",
    "\n",
    "**Agregacje:**\n",
    "- ‚úÖ Grupowanie: `groupBy()`, `agg()`\n",
    "- ‚úÖ Funkcje agregujƒÖce: `count()`, `sum()`, `avg()`, `min()`, `max()`\n",
    "- ‚úÖ R√≥wnowa≈ºne zapytania SQL\n",
    "\n",
    "**Data Cleaning:**\n",
    "- ‚úÖ Obs≈Çuga warto≈õci NULL: `fillna()`, `dropna()`, `coalesce()`\n",
    "- ‚úÖ Deduplikacja: `dropDuplicates()`, `distinct()`\n",
    "- ‚úÖ Quality checks i flagowanie problem√≥w\n",
    "- ‚úÖ Analiza jako≈õci danych\n",
    "\n",
    "### üéØ Kluczowe wnioski:\n",
    "\n",
    "1. **PySpark vs SQL** - Oba podej≈õcia dajƒÖ te same rezultaty, wyb√≥r zale≈ºy od preferencji\n",
    "2. **Czyszczenie danych jest kluczowe** - NULL i duplikaty mogƒÖ zniekszta≈Çciƒá analizy\n",
    "3. **Quality flags** - PozwalajƒÖ na monitorowanie jako≈õci bez usuwania danych\n",
    "4. **Transformacje ≈Ça≈Ñcuchowe** - PySpark pozwala na eleganckie ≈ÇƒÖczenie operacji\n",
    "\n",
    "### üìö Best Practices zastosowane:\n",
    "\n",
    "- ‚úì Dodawanie nowych kolumn zamiast modyfikowania istniejƒÖcych\n",
    "- ‚úì Logowanie statystyk przed i po czyszczeniu\n",
    "- ‚úì U≈ºywanie flag jako≈õci zamiast usuwania problematycznych danych\n",
    "- ‚úì Komentowanie i dokumentowanie transformacji\n",
    "- ‚úì Weryfikacja wynik√≥w na ka≈ºdym etapie\n",
    "\n",
    "### ‚û°Ô∏è Nastƒôpne kroki:\n",
    "- **Kolejny warsztat**: `03_views_basic_jobs_workshop.ipynb`\n",
    "- **Praktyka**: Zastosuj te techniki na w≈Çasnych danych\n",
    "- **Dokumentacja**: [PySpark SQL Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
