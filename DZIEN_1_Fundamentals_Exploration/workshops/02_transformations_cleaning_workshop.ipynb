{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46846e6b",
   "metadata": {},
   "source": [
    "# Workshop: Transformations & Data Cleaning\n",
    "\n",
    "**Cel szkoleniowy:** Praktyczne opanowanie transformacji danych w PySpark i SQL oraz technik czyszczenia danych.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Transformacje kolumnowe i warunkowe\n",
    "- Filtry, sortowania, agregacje\n",
    "- Por√≥wnanie PySpark vs SQL\n",
    "- Czyszczenie: nulls, duplikaty, walidacja typ√≥w\n",
    "- Quality checks i flagowanie problem√≥w\n",
    "\n",
    "**Czas trwania:** 90 minut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad6e36f",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzie≈Ñ szkolenia**: Dzie≈Ñ 1 - Fundamentals & Exploration\n",
    "- **Typ notebooka**: Workshop\n",
    "- **Wymagania techniczne**:\n",
    " - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    " - Unity Catalog w≈ÇƒÖczony\n",
    " - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    " - Klaster: Standard z minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b062cd0d",
   "metadata": {},
   "source": [
    "## Wprowadzenie do warsztatu\n",
    "\n",
    "W tym warsztacie bƒôdziesz:\n",
    "1. Transformowaƒá dane za pomocƒÖ PySpark DataFrame API\n",
    "2. Pisaƒá r√≥wnowa≈ºne transformacje w SQL\n",
    "3. Czy≈õciƒá dane (nulls, duplikaty, nieprawid≈Çowe warto≈õci)\n",
    "4. Walidowaƒá typy danych i konwertowaƒá kolumny\n",
    "5. Implementowaƒá quality checks\n",
    "\n",
    "### Kryteria sukcesu:\n",
    "- Wszystkie transformacje dzia≈ÇajƒÖ poprawnie w PySpark i SQL\n",
    "- Dane sƒÖ oczyszczone z duplikat√≥w i null values\n",
    "- Quality checks zidentyfikowa≈Çy wszystkie problemy\n",
    "- Finalna tabela spe≈Çnia standardy jako≈õci"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce3c827",
   "metadata": {},
   "source": [
    "## Izolacja per u≈ºytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalog√≥w i schemat√≥w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175c900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80f50fa",
   "metadata": {},
   "source": [
    "## Konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58bb532",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, when, trim, lower, upper, regexp_replace, to_date, coalesce, lit\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ≈öcie≈ºki do danych\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2f217",
   "metadata": {},
   "source": [
    "**Konfiguracja ≈õrodowiska wykonana**\n",
    "\n",
    "Biblioteki i ≈õcie≈ºki zosta≈Çy skonfigurowane:\n",
    "- **Funkcje PySpark**: Transformacje, filtry, agregacje \n",
    "- **Katalog**: `{CATALOG}`\n",
    "- **Schema**: `{BRONZE_SCHEMA}` (izolacja per u≈ºytkownik)\n",
    "- **≈πr√≥d≈Ça danych**: customers.csv, orders_batch.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941d0499",
   "metadata": {},
   "source": [
    "## Przygotowanie danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72875bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytaj dane customers\n",
    "customers_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(CUSTOMERS_CSV)\n",
    "customers_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9d7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytaj dane orders\n",
    "orders_df = spark.read.format(\"json\").option(\"inferSchema\", \"true\").load(ORDERS_JSON)\n",
    "orders_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6502de",
   "metadata": {},
   "source": [
    "**Dane wczytane pomy≈õlnie**\n",
    "\n",
    "Oba datasety zosta≈Çy wczytane:\n",
    "- **Customers**: ~10,000 rekord√≥w z pliku CSV\n",
    "- **Orders**: ~100,000 rekord√≥w z pliku JSON\n",
    "\n",
    "**Rzeczywiste kolumny:**\n",
    "- **Customers**: `customer_id`, `first_name`, `last_name`, `email`, `phone`, `city`, `state`, `country`, `registration_date`, `customer_segment`\n",
    "- **Orders**: `order_id`, `customer_id`, `product_id`, `store_id`, `order_datetime`, `quantity`, `unit_price`, `discount_percent`, `total_amount`, `payment_method`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3967d82a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 1: Transformacje kolumnowe - PySpark (15 min)\n",
    "\n",
    "### Cel:\n",
    "Zastosuj transformacje kolumnowe na danych klient√≥w.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Dodaj kolumnƒô `email_domain` wyodrƒôbniajƒÖc domenƒô z email\n",
    "2. Dodaj kolumnƒô `country_code` z pierwszymi 2 literami kraju (uppercase)\n",
    "3. Dodaj kolumnƒô `customer_full_name` - trim i lowercase\n",
    "4. Usu≈Ñ kolumnƒô `registration_date` (je≈õli istnieje)\n",
    "5. Zmie≈Ñ nazwƒô kolumny `first_name` na `customer_full_name`\n",
    "\n",
    "### Wskaz√≥wki:\n",
    "- U≈ºyj: `withColumn()`, `regexp_extract()`, `substring()`, `trim()`, `lower()`, `drop()`, `withColumnRenamed()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0382566e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dodaj kolumnƒô email_domain\n",
    "customers_transformed = customers_df.withColumn(\n",
    " \"email_domain\",\n",
    " F.____(____(____), \"@(.+)$\", 1) # regexp_extract, col, \"email\"\n",
    ")\n",
    "\n",
    "display(customers_transformed.select(\"customer_id\", \"email\", \"email_domain\").limit(5))\n",
    "\n",
    "# TODO: Dodaj kolumnƒô country_code (pierwsze 2 litery, uppercase)\n",
    "customers_transformed = customers_transformed.withColumn(\n",
    " \"country_code\",\n",
    " ____(col(\"____\").substr(1, ____)) # upper, country, 2\n",
    ")\n",
    "\n",
    "# TODO: Dodaj kolumnƒô customer_full_name (trim i lowercase)\n",
    "customers_transformed = customers_transformed.withColumn(\n",
    " \"customer_full_name\",\n",
    " ____(____(col(\"first_name\"))) # lower, trim\n",
    ")\n",
    "\n",
    "# TODO: Usu≈Ñ kolumnƒô registration_date (je≈õli istnieje)\n",
    "if \"registration_date\" in customers_transformed.columns:\n",
    " customers_transformed = customers_transformed.____(\"____\") # drop, registration_date\n",
    "\n",
    "# TODO: Zmie≈Ñ nazwƒô first_name na customer_full_name\n",
    "customers_transformed = customers_transformed.____(\"first_name\", \"____\") # withColumnRenamed, customer_full_name\n",
    "\n",
    "print(\"=== Przyk≈Çadowe dane po transformacjach ===\")\n",
    "display(customers_transformed.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f8d42d",
   "metadata": {},
   "source": [
    "**Transformacja email_domain wykonana**\n",
    "\n",
    "Za pomocƒÖ `regexp_extract()` wyodrƒôbniamy domenƒô email po znaku `@`.\n",
    "- **Regex pattern**: `@(.+)$` - wszystko po `@` do ko≈Ñca stringa\n",
    "- **Group 1**: Pierwsza grupa w nawiasach `(.+)`\n",
    "\n",
    "**Rezultat**: Nowa kolumna `email_domain` z domenami email (gmail.com, interia.pl, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371da883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dodaj kolumnƒô country_code (pierwsze 2 litery, uppercase)\n",
    "customers_transformed = customers_transformed.withColumn(\n",
    " \"country_code\",\n",
    " ____(col(\"____\").substr(1, ____)) # upper, \"country\", 2\n",
    ")\n",
    "\n",
    "display(customers_transformed.select(\"customer_id\", \"country\", \"country_code\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38ad5375",
   "metadata": {},
   "source": [
    "**Transformacja country_code wykonana**\n",
    "\n",
    "Tworzenie kodu kraju z pierwszych 2 liter:\n",
    "- **substr(1, 2)**: Pierwsze 2 znaki z nazwy kraju\n",
    "- **upper()**: Konwersja na wielkie litery\n",
    "\n",
    "**Rezultat**: \"USA\" -> \"US\", \"Poland\" -> \"PO\", \"Germany\" -> \"GE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb471c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dodaj kolumnƒô full_name (≈ÇƒÖczƒÖc imiƒô i nazwisko)\n",
    "customers_transformed = customers_transformed.withColumn(\n",
    " \"full_name\",\n",
    " ____(____(col(\"____\") + \" \" + col(\"____\"))) # trim, lower, \"first_name\", \"last_name\"\n",
    ")\n",
    "\n",
    "display(customers_transformed.select(\"customer_id\", \"first_name\", \"last_name\", \"full_name\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4598fe7f",
   "metadata": {},
   "source": [
    "**Transformacja full_name wykonana**\n",
    "\n",
    "≈ÅƒÖczenie imienia i nazwiska w jednƒÖ kolumnƒô:\n",
    "- **Konkatenacja**: `first_name + \" \" + last_name`\n",
    "- **trim()**: Usuwa zbƒôdne spacje\n",
    "- **lower()**: Konwersja na ma≈Çe litery dla sp√≥jno≈õci\n",
    "\n",
    "**Rezultat**: \"Jesse Hoffman\" -> \"jesse hoffman\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d1717d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 2: Transformacje warunkowe (15 min)\n",
    "\n",
    "### Cel:\n",
    "Dodaj kolumny z logikƒÖ warunkowƒÖ.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Dodaj kolumnƒô `customer_segment` (VIP dla PL/DE, Regular dla reszty)\n",
    "2. Dodaj kolumnƒô `email_valid` (True je≈õli email zawiera '@', False w przeciwnym razie)\n",
    "3. Dodaj kolumnƒô `country_region` (Europe dla PL/DE/FR, Other dla reszty)\n",
    "\n",
    "### Wskaz√≥wki:\n",
    "- U≈ºyj: `when().otherwise()`\n",
    "- Dla wielu warunk√≥w: `.when().when().otherwise()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d9e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dodaj kolumnƒô customer_tier (VIP dla USA, Regular dla reszty)\n",
    "customers_transformed = customers_transformed.withColumn(\n",
    " \"customer_tier\",\n",
    " when(col(\"country\") == \"USA\", \"VIP\") # when, \"USA\"\n",
    " .otherwise(\"Regular\") # \"Regular\"\n",
    ")\n",
    "\n",
    "display(customers_transformed.select(\"customer_id\", \"country\", \"customer_tier\").limit(10))\n",
    "\n",
    "# TODO: Dodaj kolumnƒô customer_segment (VIP dla PL/DE, Regular dla reszty)\n",
    "customers_transformed = customers_transformed.withColumn(\n",
    " \"customer_segment\",\n",
    " when(col(\"country\").isin(\"Poland\", \"Germany\"), \"VIP\") # when, Germany\n",
    " .otherwise(\"Regular\") # Regular\n",
    ")\n",
    "\n",
    "# TODO: Dodaj kolumnƒô email_valid (True je≈õli zawiera @)\n",
    "customers_transformed = customers_transformed.withColumn(\n",
    " \"email_valid\",\n",
    " when(col(\"email\").contains(\"@\"), True) # email, contains, True\n",
    " .otherwise(False)\n",
    ")\n",
    "\n",
    "# TODO: Dodaj kolumnƒô country_region (Europe dla PL/DE/FR, Other dla reszty)\n",
    "customers_transformed = customers_transformed.withColumn(\n",
    " \"country_region\",\n",
    " when(col(\"country\").isin(\"Poland\", \"Germany\", \"France\"), \"Europe\") # isin, Europe\n",
    " .otherwise(\"Other\") # otherwise\n",
    ")\n",
    "\n",
    "print(\"=== Przyk≈Çadowe dane z transformacjami warunkowymi ===\")\n",
    "display(\n",
    " customers_transformed\n",
    " .select(\"customer_id\", \"country\", \"customer_segment\", \"email_valid\", \"country_region\")\n",
    " .limit(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3cbd3f",
   "metadata": {},
   "source": [
    "**Segmentacja klient√≥w customer_tier**\n",
    "\n",
    "Logika biznesowa segmentacji:\n",
    "- **VIP**: Klienci z USA (g≈Ç√≥wny rynek)\n",
    "- **Regular**: Wszyscy pozostali klienci\n",
    "\n",
    "**when().otherwise()**: Struktura warunkowa PySpark podobna do CASE WHEN w SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777d7aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dodaj kolumnƒô email_valid (walidacja obecno≈õci @)\n",
    "customers_transformed = customers_transformed.withColumn(\n",
    " \"email_valid\",\n",
    " when(col(\"____\").____(\"@\"), ____) # \"email\", contains, True\n",
    " .otherwise(____) # False\n",
    ")\n",
    "\n",
    "display(customers_transformed.select(\"customer_id\", \"email\", \"email_valid\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1c84de",
   "metadata": {},
   "source": [
    "**Walidacja email_valid**\n",
    "\n",
    "Prosta walidacja formatu email:\n",
    "- **True**: Email zawiera znak `@`\n",
    "- **False**: Brak znaku `@` (nieprawid≈Çowy format)\n",
    "\n",
    "**Zastosowanie biznesowe**: Flagowanie niepoprawnych adres√≥w email do rƒôcznej weryfikacji lub czyszczenia"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1f45ab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 3: Filtry i sortowanie (10 min)\n",
    "\n",
    "### Cel:\n",
    "Filtruj i sortuj dane.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Filtruj klient√≥w z Polski\n",
    "2. Filtruj zam√≥wienia o warto≈õci > 100\n",
    "3. Sortuj zam√≥wienia wed≈Çug total_amount malejƒÖco\n",
    "4. Znajd≈∫ top 10 najdro≈ºszych zam√≥wie≈Ñ\n",
    "\n",
    "### Wskaz√≥wki:\n",
    "- U≈ºyj: `.filter()` lub `.where()`, `.orderBy()`, `.limit()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc780106",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtruj klient√≥w z USA\n",
    "usa_customers = customers_transformed.filter(col(\"____\") == \"USA\") # \"country\"\n",
    "usa_customers.count()\n",
    "\n",
    "# TODO: Filtruj klient√≥w z Polski\n",
    "polish_customers = customers_transformed.filter(col(\"____\") == \"Poland\") # country\n",
    "print(f\"Klienci z Polski: {polish_customers.count()}\")\n",
    "\n",
    "# TODO: Filtruj zam√≥wienia o warto≈õci > 100\n",
    "high_value_orders = orders_df.filter(col(\"____\") > ____) # total_amount, 100\n",
    "print(f\"Zam√≥wienia > 100: {high_value_orders.count()}\")\n",
    "\n",
    "# TODO: Top 10 najdro≈ºszych zam√≥wie≈Ñ (sortowanie malejƒÖce)\n",
    "top_orders = (\n",
    " orders_df\n",
    " .orderBy(col(\"total_amount\").____()) # desc\n",
    " .limit(____) # 10\n",
    ")\n",
    "\n",
    "print(\"\\n=== Top 10 najdro≈ºszych zam√≥wie≈Ñ ===\")\n",
    "display(top_orders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6fd9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtruj zam√≥wienia o warto≈õci > 500\n",
    "high_value_orders = orders_df.filter(col(\"____\") > ____) # \"total_amount\", 500\n",
    "high_value_orders.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4c788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 10 najdro≈ºszych zam√≥wie≈Ñ (sortowanie malejƒÖce)\n",
    "top_orders = (\n",
    " orders_df\n",
    " .orderBy(col(\"total_amount\").____()) # desc\n",
    " .limit(____) # 10\n",
    ")\n",
    "\n",
    "display(top_orders.select(\"order_id\", \"customer_id\", \"total_amount\", \"payment_method\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841c5220",
   "metadata": {},
   "source": [
    "**Operacje filtrowania i sortowania wykonane**\n",
    "\n",
    "Zastosowane filtry:\n",
    "- **USA customers**: Filtrowanie po kraju r√≥wnym \"USA\"\n",
    "- **High-value orders**: Zam√≥wienia > 500 (pr√≥g premium)\n",
    "- **Top 10 orders**: Sortowanie malejƒÖce po `total_amount`\n",
    "\n",
    "**Metody**: `filter()`, `orderBy()`, `limit()` - podstawowe operacje analityczne w PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0718977a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 4: Agregacje - PySpark (15 min)\n",
    "\n",
    "### Cel:\n",
    "Przeprowad≈∫ agregacje na zam√≥wieniach.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Policz zam√≥wienia wed≈Çug statusu\n",
    "2. Oblicz ca≈ÇkowitƒÖ warto≈õƒá zam√≥wie≈Ñ wed≈Çug statusu\n",
    "3. Znajd≈∫ ≈õredniƒÖ warto≈õƒá zam√≥wienia wed≈Çug customer_id\n",
    "4. Policz zam√≥wienia per klient\n",
    "\n",
    "### Wskaz√≥wki:\n",
    "- U≈ºyj: `.groupBy()`, `.agg()`, `count()`, `sum()`, `avg()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6424a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Policz zam√≥wienia wed≈Çug metod p≈Çatno≈õci\n",
    "orders_by_payment = (\n",
    " orders_df\n",
    " .groupBy(\"____\") # \"payment_method\"\n",
    " .count()\n",
    " .orderBy(\"count\", ascending=____) # False\n",
    ")\n",
    "\n",
    "display(orders_by_payment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3123eb",
   "metadata": {},
   "source": [
    "**Agregacja wed≈Çug metod p≈Çatno≈õci**\n",
    "\n",
    "Analiza preferencji p≈Çatniczych klient√≥w:\n",
    "- **groupBy(\"payment_method\")**: Grupowanie wed≈Çug sposobu p≈Çatno≈õci\n",
    "- **count()**: Liczba zam√≥wie≈Ñ w ka≈ºdej grupie\n",
    "- **orderBy(desc)**: Sortowanie od najczƒôstszej metody\n",
    "\n",
    "**Biznesowa warto≈õƒá**: Identyfikacja popularnych metod p≈Çatno≈õci dla optymalizacji procesu checkout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7642a53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ca≈Çkowita warto≈õƒá i ≈õrednia wed≈Çug metod p≈Çatno≈õci\n",
    "revenue_by_payment = (\n",
    " orders_df\n",
    " .groupBy(\"payment_method\")\n",
    " .agg(\n",
    " F.____(\"total_amount\").alias(\"total_revenue\"), # sum\n",
    " F.____(\"total_amount\").alias(\"avg_order_value\") # avg\n",
    " )\n",
    " .orderBy(\"total_revenue\", ascending=False)\n",
    ")\n",
    "\n",
    "display(revenue_by_payment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bc9349",
   "metadata": {},
   "source": [
    "**Analiza przychod√≥w wed≈Çug metod p≈Çatno≈õci**\n",
    "\n",
    "Zaawansowana agregacja z wieloma miarami:\n",
    "- **F.sum()**: Ca≈Çkowity przych√≥d per metoda p≈Çatno≈õci\n",
    "- **F.avg()**: ≈örednia warto≈õƒá zam√≥wienia (AOV)\n",
    "- **alias()**: Nadanie czytelnych nazw kolumnom wynikowym\n",
    "\n",
    "**Insight**: Kt√≥ra metoda p≈Çatno≈õci generuje najwiƒôkszy przych√≥d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a18ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statystyki per klient\n",
    "customer_stats = (\n",
    " orders_df\n",
    " .groupBy(\"____\") # \"customer_id\"\n",
    " .agg(\n",
    " F.____(\"*\").alias(\"total_orders\"), # count\n",
    " F.sum(\"____\").alias(\"total_spent\"), # \"total_amount\"\n",
    " F.avg(\"total_amount\").alias(\"avg_order_value\")\n",
    " )\n",
    " .orderBy(\"____\", ascending=False) # \"total_spent\"\n",
    " .limit(10)\n",
    ")\n",
    "\n",
    "display(customer_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072f959d",
   "metadata": {},
   "source": [
    "**Top 10 klient√≥w - analiza RFM**\n",
    "\n",
    "Kluczowe metryki klient√≥w:\n",
    "- **total_orders**: Frequency - jak czƒôsto kupuje\n",
    "- **total_spent**: Monetary - ile wydaje ≈ÇƒÖcznie \n",
    "- **avg_order_value**: ≈örednia warto≈õƒá koszyka\n",
    "\n",
    "**Zastosowanie**: Identyfikacja VIP customers, programy lojalno≈õciowe, personalizacja ofert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5851e0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 5: SQL Equivalent (10 min)\n",
    "\n",
    "### Cel:\n",
    "Napisz te same transformacje w SQL.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Utw√≥rz temp view dla customers i orders\n",
    "2. Napisz SQL query agregujƒÖcy zam√≥wienia wed≈Çug statusu\n",
    "3. Napisz SQL query z JOINem customers i orders\n",
    "\n",
    "### Wskaz√≥wki:\n",
    "- U≈ºyj: `createOrReplaceTempView()`, `spark.sql()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098c825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utw√≥rz temp views dla SQL queries\n",
    "customers_df.____(\"customers\") # createOrReplaceTempView\n",
    "orders_df.createOrReplaceTempView(\"____\") # \"orders\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d7a3ef9",
   "metadata": {},
   "source": [
    "**Temporary Views utworzone**\n",
    "\n",
    "Views pozwalajƒÖ na u≈ºywanie SQL na DataFrames:\n",
    "- **customers**: DataFrame customers_df jako tabela SQL\n",
    "- **orders**: DataFrame orders_df jako tabela SQL\n",
    "\n",
    "**Zasiƒôg**: Sesja Spark (znikajƒÖ po restarcie klastra)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9bcf20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query - agregacja zam√≥wie≈Ñ wed≈Çug metod p≈Çatno≈õci\n",
    "result = spark.sql(\"\"\"\n",
    " SELECT \n",
    " payment_method,\n",
    " ____(*) as order_count, -- COUNT\n",
    " ____(total_amount) as total_revenue, -- SUM\n",
    " ____(total_amount) as avg_order_value -- AVG\n",
    " FROM ____ -- orders\n",
    " GROUP BY ____ -- payment_method\n",
    " ORDER BY total_revenue ____ -- DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== Agregacja zam√≥wie≈Ñ (SQL) ===\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8cd441",
   "metadata": {},
   "source": [
    "**SQL Aggregation - identyczna logika jak PySpark**\n",
    "\n",
    "SQL wersja poprzedniej agregacji PySpark:\n",
    "- **SELECT**: Wyb√≥r kolumn i funkcji agregujƒÖcych\n",
    "- **GROUP BY**: Grupowanie po payment_method\n",
    "- **ORDER BY**: Sortowanie wynik√≥w\n",
    "\n",
    "**Rezultat**: Identyczne wyniki jak `groupBy().agg()` w PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d19882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL query - JOIN customers i orders\n",
    "result = spark.sql(\"\"\"\n",
    " SELECT \n",
    " c.customer_id,\n",
    " c.first_name,\n",
    " c.last_name,\n",
    " c.country,\n",
    " ____(____.order_id) as total_orders, -- COUNT, o\n",
    " SUM(o.____) as total_spent -- total_amount\n",
    " FROM ____ c -- customers\n",
    " ____ JOIN orders o ON c.customer_id = o.customer_id -- LEFT\n",
    " GROUP BY c.customer_id, c.first_name, c.last_name, c.country\n",
    " ORDER BY ____ DESC -- total_spent\n",
    " LIMIT ____ -- 10\n",
    "\"\"\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fa05c2",
   "metadata": {},
   "source": [
    "**SQL JOIN - ≈ÇƒÖczenie tabel customers i orders**\n",
    "\n",
    "Zaawansowane SQL z JOINem:\n",
    "- **LEFT JOIN**: Wszyscy klienci + ich zam√≥wienia (je≈õli istniejƒÖ)\n",
    "- **GROUP BY**: Agregacja per klient\n",
    "- **ORDER BY + LIMIT**: Top 10 klient√≥w wed≈Çug wydatk√≥w\n",
    "\n",
    "**Equivalent PySpark**: `customers_df.join(orders_df, \"customer_id\", \"left\").groupBy(...)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38e9a30",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 6: Data Cleaning - Nulls (15 min)\n",
    "\n",
    "### Cel:\n",
    "Obs≈Çu≈º warto≈õci NULL w danych.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Policz warto≈õci NULL w ka≈ºdej kolumnie customers\n",
    "2. ZastƒÖp NULL w `email` warto≈õciƒÖ \"unknown@example.com\"\n",
    "3. ZastƒÖp NULL w `country` warto≈õciƒÖ \"Unknown\"\n",
    "4. Usu≈Ñ rekordy gdzie `customer_id` jest NULL\n",
    "\n",
    "### Wskaz√≥wki:\n",
    "- U≈ºyj: `.fillna()`, `.dropna()`, `coalesce()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5977ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Policz warto≈õci NULL w ka≈ºdej kolumnie\n",
    "from pyspark.sql.functions import sum as spark_sum\n",
    "\n",
    "null_counts = customers_df.select([\n",
    " spark_sum(col(c).____().cast(\"int\")).alias(c) # isNull\n",
    " for c in customers_df.columns\n",
    "])\n",
    "\n",
    "print(\"=== Warto≈õci NULL przed czyszczeniem ===\")\n",
    "display(null_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a741a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: ZastƒÖp NULL w email warto≈õciƒÖ domy≈õlnƒÖ\n",
    "customers_clean = customers_df.____({ # fillna\n",
    " \"email\": \"____\" # unknown@example.com\n",
    "})\n",
    "\n",
    "# TODO: ZastƒÖp NULL w country\n",
    "customers_clean = customers_clean.fillna({\n",
    " \"____\": \"Unknown\" # country\n",
    "})\n",
    "\n",
    "# TODO: Usu≈Ñ rekordy gdzie customer_id jest NULL\n",
    "customers_clean = customers_clean.____(____ =[\"customer_id\"]) # dropna, subset\n",
    "\n",
    "print(f\"\\n Rekordy po czyszczeniu NULL: {customers_clean.count()}\")\n",
    "print(f\" Usuniƒôto: {customers_df.count() - customers_clean.count()} rekord√≥w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f5e46d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 7: Deduplikacja (10 min)\n",
    "\n",
    "### Cel:\n",
    "Usu≈Ñ duplikaty z danych.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Znajd≈∫ liczbƒô duplikat√≥w w customers (wszystkie kolumny)\n",
    "2. Usu≈Ñ duplikaty oparte na customer_id\n",
    "3. Usu≈Ñ duplikaty oparte na email\n",
    "\n",
    "### Wskaz√≥wki:\n",
    "- U≈ºyj: `.dropDuplicates()`, `.distinct()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1ecfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Znajd≈∫ liczbƒô duplikat√≥w (wszystkie kolumny)\n",
    "total_rows = customers_clean.count()\n",
    "distinct_rows = customers_clean.____().count() # distinct\n",
    "duplicates = total_rows - distinct_rows\n",
    "\n",
    "print(\"=== Analiza duplikat√≥w ===\")\n",
    "print(f\"Ca≈Çkowita liczba rekord√≥w: {total_rows}\")\n",
    "print(f\"Unikalne rekordy: {distinct_rows}\")\n",
    "print(f\"Duplikaty (wszystkie kolumny): {____}\") # duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f2d7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Usu≈Ñ duplikaty na podstawie customer_id\n",
    "customers_dedup = customers_clean.____([____]) # dropDuplicates, \"customer_id\"\n",
    "\n",
    "print(f\"\\n Rekordy po deduplikacji (customer_id): {customers_dedup.count()}\")\n",
    "print(f\" Usuniƒôto: {customers_clean.count() - customers_dedup.count()} duplikat√≥w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693bc287",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sprawd≈∫ duplikaty w kolumnie email\n",
    "email_duplicates = (\n",
    " customers_clean\n",
    " .groupBy(\"____\") # email\n",
    " .count()\n",
    " .filter(\"count > ____\") # 1\n",
    ")\n",
    "\n",
    "print(f\"\\n=== Duplikaty email ===\")\n",
    "print(f\"Liczba duplikat√≥w email: {email_duplicates.count()}\")\n",
    "\n",
    "if email_duplicates.count() > 0:\n",
    " print(\"\\nPrzyk≈Çady duplikat√≥w email:\")\n",
    " display(email_duplicates.orderBy(\"count\", ascending=False).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957c7fed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 8: Quality Checks (10 min)\n",
    "\n",
    "### Cel:\n",
    "Zaimplementuj quality checks i flaguj problemy.\n",
    "\n",
    "### Instrukcje:\n",
    "1. Dodaj kolumnƒô `quality_flag` (0 = OK, 1 = problem)\n",
    "2. Flaguj rekordy gdzie email nie zawiera '@'\n",
    "3. Flaguj rekordy gdzie country = \"Unknown\"\n",
    "4. Policz rekordy z problemami\n",
    "\n",
    "### Wskaz√≥wki:\n",
    "- U≈ºyj: `when().otherwise()` dla flagowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad812c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dodaj quality_flag (0 = OK, 1 = problem)\n",
    "customers_with_flags = customers_dedup.withColumn(\n",
    " \"quality_flag\",\n",
    " when(~col(\"email\").____(\"@\"), ____) # contains, 1\n",
    " .when(col(\"____\") == \"Unknown\", 1) # country\n",
    " .otherwise(____) # 0\n",
    ")\n",
    "\n",
    "# TODO: Policz rekordy z problemami\n",
    "problem_count = customers_with_flags.filter(col(\"quality_flag\") == ____).count() # 1\n",
    "ok_count = customers_with_flags.filter(col(\"quality_flag\") == 0).count()\n",
    "\n",
    "print(\"=== Quality Checks ===\")\n",
    "print(f\" Rekordy OK: {ok_count}\")\n",
    "print(f\" Rekordy z problemami: {____}\") # problem_count\n",
    "print(f\" Procent problem√≥w: {(problem_count / customers_with_flags.count() * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523dcc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wy≈õwietl przyk≈Çadowe rekordy z problemami\n",
    "if problem_count > 0:\n",
    " print(\"\\n=== Przyk≈Çadowe rekordy z problemami ===\")\n",
    " display(\n",
    " customers_with_flags\n",
    " .filter(col(\"____\") == 1) # quality_flag\n",
    " .select(\"customer_id\", \"customer_full_name\", \"email\", \"country\", \"quality_flag\")\n",
    " .limit(5)\n",
    " )\n",
    "else:\n",
    " print(\"\\n Brak rekord√≥w z problemami!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e703fd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist:\n",
    "- [ ] Transformacje kolumnowe zastosowane\n",
    "- [ ] Transformacje warunkowe dzia≈ÇajƒÖ\n",
    "- [ ] Agregacje poprawne (PySpark i SQL)\n",
    "- [ ] NULL values obs≈Çu≈ºone\n",
    "- [ ] Duplikaty usuniƒôte\n",
    "- [ ] Quality checks zaimplementowane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b99ab9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"=== WERYFIKACJA WYNIK√ìW WARSZTATU ===\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "try:\n",
    " # Sprawd≈∫ czy wszystkie DataFrames zosta≈Çy utworzone\n",
    " assert 'customers_transformed' in locals(), \"customers_transformed nie zosta≈Ç utworzony\"\n",
    " assert 'customers_clean' in locals(), \"customers_clean nie zosta≈Ç utworzony\"\n",
    " assert 'customers_dedup' in locals(), \"customers_dedup nie zosta≈Ç utworzony\"\n",
    " assert 'customers_with_flags' in locals(), \"customers_with_flags nie zosta≈Ç utworzony\"\n",
    " \n",
    " print(\"\\n Wszystkie DataFrames utworzone\")\n",
    " \n",
    " # Statystyki finalne\n",
    " print(f\"\\nüìä Statystyki ko≈Ñcowe:\")\n",
    " print(f\" Oryginalna liczba rekord√≥w: {customers_df.count()}\")\n",
    " print(f\" Po transformacjach: {customers_transformed.count()}\")\n",
    " print(f\" Po czyszczeniu NULL: {customers_clean.count()}\")\n",
    " print(f\" Po deduplikacji: {customers_dedup.count()}\")\n",
    " print(f\" Finalna liczba rekord√≥w: {customers_with_flags.count()}\")\n",
    " \n",
    " print(f\"\\n Quality Checks:\")\n",
    " ok_records = customers_with_flags.filter(col('quality_flag') == 0).count()\n",
    " problem_records = customers_with_flags.filter(col('quality_flag') == 1).count()\n",
    " print(f\" Rekordy bez problem√≥w: {ok_records}\")\n",
    " print(f\" Rekordy z problemami: {problem_records}\")\n",
    " \n",
    " # Zapisz do tabeli (opcjonalnie)\n",
    " output_table = f\"{BRONZE_SCHEMA}.customers_clean_workshop\"\n",
    " print(f\"\\nüíæ Zapisywanie do tabeli: {output_table}\")\n",
    " \n",
    " customers_with_flags.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(output_table)\n",
    " \n",
    " print(f\" Zapisano tabelƒô: {output_table}\")\n",
    " \n",
    " print(\"\\n\" + \"=\"*60)\n",
    " print(\" WARSZTAT ZAKO≈ÉCZONY POMY≈öLNIE! \")\n",
    " print(\"=\"*60)\n",
    " \n",
    "except AssertionError as e:\n",
    " print(f\"\\n‚úó B≈ÇƒÖd: {e}\")\n",
    " print(\" Upewnij siƒô, ≈ºe wszystkie zadania zosta≈Çy wykonane!\")\n",
    "except Exception as e:\n",
    " print(f\"\\n‚úó Nieoczekiwany b≈ÇƒÖd: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825c4e25",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie warsztatu\n",
    "\n",
    "### Co osiƒÖgnƒÖ≈Çe≈õ:\n",
    "\n",
    "**Transformacje:**\n",
    "- Transformacje kolumnowe: `withColumn()`, `drop()`, `withColumnRenamed()`\n",
    "- Ekstrakcja danych: `regexp_extract()`, `substring()`\n",
    "- Formatowanie: `trim()`, `lower()`, `upper()`\n",
    "- Logika warunkowa: `when().otherwise()`\n",
    "- Filtry i sortowanie: `filter()`, `orderBy()`, `limit()`\n",
    "\n",
    "**Agregacje:**\n",
    "- Grupowanie: `groupBy()`, `agg()`\n",
    "- Funkcje agregujƒÖce: `count()`, `sum()`, `avg()`, `min()`, `max()`\n",
    "- R√≥wnowa≈ºne zapytania SQL\n",
    "\n",
    "**Data Cleaning:**\n",
    "- Obs≈Çuga warto≈õci NULL: `fillna()`, `dropna()`, `coalesce()`\n",
    "- Deduplikacja: `dropDuplicates()`, `distinct()`\n",
    "- Quality checks i flagowanie problem√≥w\n",
    "- Analiza jako≈õci danych\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "\n",
    "1. **PySpark vs SQL** - Oba podej≈õcia dajƒÖ te same rezultaty, wyb√≥r zale≈ºy od preferencji\n",
    "2. **Czyszczenie danych jest kluczowe** - NULL i duplikaty mogƒÖ zniekszta≈Çciƒá analizy\n",
    "3. **Quality flags** - PozwalajƒÖ na monitorowanie jako≈õci bez usuwania danych\n",
    "4. **Transformacje ≈Ça≈Ñcuchowe** - PySpark pozwala na eleganckie ≈ÇƒÖczenie operacji\n",
    "\n",
    "### üìö Best Practices zastosowane:\n",
    "\n",
    "- Dodawanie nowych kolumn zamiast modyfikowania istniejƒÖcych\n",
    "- Logowanie statystyk przed i po czyszczeniu\n",
    "- U≈ºywanie flag jako≈õci zamiast usuwania problematycznych danych\n",
    "- Komentowanie i dokumentowanie transformacji\n",
    "- Weryfikacja wynik√≥w na ka≈ºdym etapie\n",
    "\n",
    "### ‚û° Nastƒôpne kroki:\n",
    "- **Kolejny warsztat**: `03_views_basic_jobs_workshop.ipynb`\n",
    "- **Praktyka**: Zastosuj te techniki na w≈Çasnych danych\n",
    "- **Dokumentacja**: [PySpark SQL Functions](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}