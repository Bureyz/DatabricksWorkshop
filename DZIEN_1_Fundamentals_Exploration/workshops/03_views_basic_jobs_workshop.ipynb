{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6004e677",
   "metadata": {},
   "source": [
    "# Warsztat 3: Widoki i Podstawowe Jobsy\n",
    "\n",
    "**Czas trwania**: 90 minut  \n",
    "**Poziom**: Podstawowy/Średniozaawansowany  \n",
    "**Ostatnia aktualizacja**: 2025-01-31\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b897bd5b",
   "metadata": {},
   "source": [
    "## 1. Kontekst i Cele Warsztatu\n",
    "\n",
    "### Czego się nauczysz?\n",
    "- Tworzyć widoki tymczasowe i globalne\n",
    "- Tworzyć widoki materialized w Delta Lake\n",
    "- Konfigurować podstawowe joby w Databricks\n",
    "- Monitorować wykonanie jobów\n",
    "\n",
    "### Wymagania wstępne\n",
    "- Ukończenie Warsztatu 1 i 2\n",
    "- Podstawowa znajomość SQL i PySpark\n",
    "- Wyczyszczone dane z poprzedniego warsztatu\n",
    "\n",
    "### Co będziesz implementować?\n",
    "Stworzysz zestaw widoków biznesowych i skonfigurujesz job ETL, który automatycznie przetwarza dane.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d293d",
   "metadata": {},
   "source": [
    "## 2. Materiały Teoretyczne\n",
    "\n",
    "### 2.1 Typy Widoków\n",
    "- **Temporary View** - widok tymczasowy, dostępny tylko w bieżącej sesji Spark\n",
    "- **Global Temporary View** - widok globalny, dostępny we wszystkich sesjach Spark\n",
    "- **View (Materialized)** - widok trwały, zapisany w metastore\n",
    "\n",
    "### 2.2 Databricks Jobs\n",
    "- **Task** - pojedyncze zadanie (notebook, JAR, Python script)\n",
    "- **Job** - zestaw tasków z zależnościami\n",
    "- **Cluster** - zasoby obliczeniowe do wykonania jobu\n",
    "- **Schedule** - harmonogram uruchamiania jobu\n",
    "\n",
    "### 2.3 Best Practices\n",
    "- Używaj widoków dla często używanych zapytań\n",
    "- Dokumentuj widoki - dodawaj komentarze\n",
    "- Monitoruj performance jobów\n",
    "- Używaj retry policy dla jobów\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab642b5",
   "metadata": {},
   "source": [
    "## 3. Inicjalizacja Środowiska\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b965213",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc162d",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3fb2a",
   "metadata": {},
   "source": [
    "### 3.2 Konfiguracja Zmiennych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8297ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Nazwy tabel z poprzednich warsztatów\n",
    "CUSTOMERS_TABLE = f\"{SILVER_SCHEMA}.customers_clean\"\n",
    "ORDERS_TABLE = f\"{BRONZE_SCHEMA}.orders_workshop\"\n",
    "PRODUCTS_TABLE = f\"{BRONZE_SCHEMA}.products_workshop\"\n",
    "\n",
    "# Nazwy widoków do utworzenia\n",
    "CUSTOMER_ORDERS_VIEW = f\"{SILVER_SCHEMA}.vw_customer_orders\"\n",
    "TOP_CUSTOMERS_VIEW = f\"{SILVER_SCHEMA}.vw_top_customers\"\n",
    "PRODUCT_SALES_VIEW = f\"{SILVER_SCHEMA}.vw_product_sales\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82053a8e",
   "metadata": {},
   "source": [
    "**Wyjaśnienie konfiguracji:**\n",
    "- Importujemy biblioteki PySpark do transformacji danych\n",
    "- Definiujemy nazwy tabel źródłowych z poprzednich warsztatów\n",
    "- Konfigurujemy nazwy widoków, które będziemy tworzyć w tym warsztacie\n",
    "- Widoki będą przechowywane w schemacie SILVER dla czytelności i organizacji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd9b7d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Widoki Tymczasowe\n",
    "\n",
    "### 4.1 Przykład: Temporary View\n",
    "Ta komórka jest już gotowa - przeanalizuj kod i uruchom go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład: Tworzenie widoku tymczasowego\n",
    "# Widok będzie dostępny tylko w tej sesji Spark\n",
    "\n",
    "customers_df = spark.table(CUSTOMERS_TABLE)\n",
    "customers_df.createOrReplaceTempView(\"tmp_customers\")\n",
    "\n",
    "# Użyj widoku w zapytaniu SQL\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        city,\n",
    "        COUNT(*) as customer_count\n",
    "    FROM tmp_customers\n",
    "    GROUP BY city\n",
    "    ORDER BY customer_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133c336b",
   "metadata": {},
   "source": [
    "**Wyjaśnienie Temporary View:**\n",
    "- Widok tymczasowy `tmp_customers` istnieje tylko w tej sesji Spark\n",
    "- Pozwala na używanie SQL na DataFrame bez zapisywania do metastore\n",
    "- Idealne do eksploracji danych i tymczasowych analiz\n",
    "- Query pokazuje top 10 miast z największą liczbą klientów"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b2dd3e",
   "metadata": {},
   "source": [
    "### 4.2 Zadanie: Global Temporary View\n",
    "\n",
    "**TODO**: Uzupełnij kod, aby:\n",
    "1. Utworzyć globalny widok tymczasowy dla tabeli zamówień\n",
    "2. Wyświetlić statystyki zamówień z tego widoku\n",
    "\n",
    "**Wskazówka**: Użyj `.createOrReplaceGlobalTempView()` i odwołuj się przez `global_temp.nazwa_widoku`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b42d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utwórz globalny widok tymczasowy\n",
    "orders_df = spark.table(ORDERS_TABLE)\n",
    "\n",
    "# TODO: Uzupełnij kod\n",
    "# orders_df.createOrReplaceGlobalTempView(\"____\")\n",
    "\n",
    "# TODO: Użyj widoku w zapytaniu SQL\n",
    "# Wskazówka: SELECT ... FROM global_temp.gtmp_orders\n",
    "result = spark.sql(\"\"\"\n",
    "    -- TODO: Napisz zapytanie, które policzy:\n",
    "    -- - Całkowitą liczbę zamówień: COUNT(order_id)\n",
    "    -- - Całkowitą wartość zamówień: SUM(total_amount)\n",
    "    -- - Średnią wartość zamówienia: AVG(total_amount)\n",
    "    -- - Minimalną i maksymalną wartość: MIN/MAX(total_amount)\n",
    "    SELECT \n",
    "        COUNT(____) as total_orders,\n",
    "        SUM(____) as total_value,\n",
    "        AVG(____) as avg_order_value,\n",
    "        MIN(____) as min_order_value,\n",
    "        MAX(____) as max_order_value\n",
    "    FROM global_temp.____\n",
    "\"\"\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec58d665",
   "metadata": {},
   "source": [
    "**Wyjaśnienie Global Temporary View:**\n",
    "- Global Temporary View jest dostępny we wszystkich sesjach Spark w tym samym aplikacji\n",
    "- Używa prefiksu `global_temp.` w zapytaniach SQL\n",
    "- Przydatny do współdzielenia danych między różnymi notebook'ami\n",
    "- Kolumny w tabeli orders: `order_id`, `total_amount`, `quantity`, `payment_method`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f80db0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Widoki Materialized (Trwałe)\n",
    "\n",
    "### 5.1 Przykład: Widok Zamówień z Klientami\n",
    "Ta komórka jest już gotowa - przeanalizuj kod i uruchom go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład: Widok łączący zamówienia z klientami\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {CUSTOMER_ORDERS_VIEW}\n",
    "    COMMENT 'Widok łączący dane zamówień z informacjami o klientach'\n",
    "    AS\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        o.order_datetime,\n",
    "        o.total_amount,\n",
    "        o.payment_method,\n",
    "        o.quantity,\n",
    "        c.customer_id,\n",
    "        c.first_name,\n",
    "        c.last_name,\n",
    "        c.email,\n",
    "        c.city,\n",
    "        c.country,\n",
    "        c.customer_segment\n",
    "    FROM {ORDERS_TABLE} o\n",
    "    INNER JOIN {CUSTOMERS_TABLE} c\n",
    "        ON o.customer_id = c.customer_id\n",
    "\"\"\")\n",
    "\n",
    "# Sprawdź widok\n",
    "result = spark.sql(f\"SELECT * FROM {CUSTOMER_ORDERS_VIEW} LIMIT 10\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e1aca9",
   "metadata": {},
   "source": [
    "**Wyjaśnienie Materialized View:**\n",
    "- Widok zostaje zapisany w metastore i jest dostępny dla wszystkich użytkowników\n",
    "- JOIN łączy tabele zamówień i klientów po `customer_id`\n",
    "- Zawiera kompletne informacje o zamówieniach wraz z danymi klientów\n",
    "- Użyte kolumny to rzeczywiste kolumny z dataset'u: `order_datetime`, `total_amount`, `payment_method`, `customer_segment`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8809087",
   "metadata": {},
   "source": [
    "### 5.2 Zadanie: Widok Top Klientów\n",
    "\n",
    "**TODO**: Uzupełnij kod, aby utworzyć widok, który:\n",
    "1. Policzy liczbę zamówień dla każdego klienta\n",
    "2. Obliczy całkowitą wartość zamówień dla każdego klienta\n",
    "3. Obliczy średnią wartość zamówienia\n",
    "4. Pokaże tylko top 100 klientów (według wartości)\n",
    "\n",
    "**Wskazówka**: Użyj `CREATE OR REPLACE VIEW` z `GROUP BY` i `ORDER BY ... LIMIT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c969e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utwórz widok top klientów\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {TOP_CUSTOMERS_VIEW}\n",
    "    COMMENT 'Top 100 klientów według wartości zamówień'\n",
    "    AS\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        first_name,\n",
    "        last_name,\n",
    "        email,\n",
    "        city,\n",
    "        customer_segment,\n",
    "        COUNT(____) AS order_count,\n",
    "        SUM(____) AS total_spent,\n",
    "        AVG(____) AS avg_order_value\n",
    "    FROM {CUSTOMER_ORDERS_VIEW}\n",
    "    GROUP BY customer_id, first_name, last_name, email, city, customer_segment\n",
    "    ORDER BY ____ DESC\n",
    "    LIMIT ____\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Utworzono widok: {TOP_CUSTOMERS_VIEW}\")\n",
    "\n",
    "# Sprawdź widok\n",
    "result = spark.sql(f\"SELECT * FROM {TOP_CUSTOMERS_VIEW} LIMIT 10\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420064b7",
   "metadata": {},
   "source": [
    "**Wskazówki do uzupełnienia:**\n",
    "- `COUNT(____)` - policz `order_id` aby uzyskać liczbę zamówień\n",
    "- `SUM(____)` - zsumuj `total_amount` aby uzyskać całkowitą wartość\n",
    "- `AVG(____)` - uśrednij `total_amount` dla średniej wartości zamówienia\n",
    "- `ORDER BY ____` - sortuj według `total_spent` malejąco\n",
    "- `LIMIT ____` - ogranicz do 100 rekordów"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db907dd2",
   "metadata": {},
   "source": [
    "### 5.3 Zadanie: Widok Sprzedaży Produktów\n",
    "\n",
    "**TODO**: Uzupełnij kod, aby utworzyć widok sprzedaży produktów.\n",
    "Zakładamy, że tabela zamówień ma kolumnę `product_id` i `quantity`.\n",
    "\n",
    "**Wskazówka**: Połącz tabele zamówień i produktów, oblicz statystyki sprzedaży"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1396414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utwórz widok sprzedaży produktów\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {PRODUCT_SALES_VIEW}\n",
    "    COMMENT 'Statystyki sprzedaży produktów'\n",
    "    AS\n",
    "    SELECT \n",
    "        p.product_id,\n",
    "        p.product_name,\n",
    "        p.category,\n",
    "        p.price,\n",
    "        COUNT(o.order_id) AS order_count,\n",
    "        SUM(o.____) AS total_quantity,\n",
    "        SUM(o.____) AS total_revenue,\n",
    "        AVG(o.____) AS avg_order_value\n",
    "    FROM {ORDERS_TABLE} o\n",
    "    INNER JOIN {PRODUCTS_TABLE} p\n",
    "        ON o.____ = p.____\n",
    "    GROUP BY p.product_id, p.product_name, p.category, p.price\n",
    "    ORDER BY ____ DESC\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Utworzono widok: {PRODUCT_SALES_VIEW}\")\n",
    "\n",
    "# Sprawdź widok\n",
    "result = spark.sql(f\"SELECT * FROM {PRODUCT_SALES_VIEW} LIMIT 10\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf307a2",
   "metadata": {},
   "source": [
    "**Wskazówki do uzupełnienia Product Sales:**\n",
    "- `SUM(o.____)` - zsumuj `quantity` i `total_amount` dla statystyk\n",
    "- `AVG(o.____)` - uśrednij `total_amount` dla średniej wartości\n",
    "- `ON o.____ = p.____` - połącz tabele przez `product_id`\n",
    "- `ORDER BY ____` - sortuj według `total_revenue` malejąco\n",
    "- Kolumny produktów: `product_name`, `category`, `price`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500e9c80",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Zapytania na Widokach\n",
    "\n",
    "### 6.1 Przykład: Analiza Top Klientów\n",
    "Ta komórka jest już gotowa - przeanalizuj kod i uruchom go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dae4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład: Zapytanie na widoku top klientów\n",
    "result = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        first_name || ' ' || last_name AS customer_name,\n",
    "        city,\n",
    "        customer_segment,\n",
    "        order_count,\n",
    "        ROUND(total_spent, 2) AS total_spent,\n",
    "        ROUND(avg_order_value, 2) AS avg_order_value\n",
    "    FROM {TOP_CUSTOMERS_VIEW}\n",
    "    WHERE order_count >= 5\n",
    "    ORDER BY total_spent DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae1055a",
   "metadata": {},
   "source": [
    "**Wyjaśnienie zapytania na widoku:**\n",
    "- Wykorzystujemy wcześniej utworzony widok `vw_top_customers`\n",
    "- Łączymy imię i nazwisko w jedną kolumnę `customer_name`\n",
    "- Filtrujemy klientów z minimum 5 zamówieniami\n",
    "- Zaokrąglamy wartości finansowe do 2 miejsc po przecinku\n",
    "- Sortujemy według całkowitej wartości zamówień"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8deeaa6",
   "metadata": {},
   "source": [
    "### 6.2 Zadanie: Analiza Segmentów Klientów\n",
    "\n",
    "**TODO**: Napisz zapytanie SQL, które:\n",
    "1. Użyje widoku `TOP_CUSTOMERS_VIEW`\n",
    "2. Podzieli klientów na segmenty:\n",
    "   - VIP: total_spent >= 5000\n",
    "   - Premium: total_spent >= 2000 AND < 5000\n",
    "   - Regular: total_spent < 2000\n",
    "3. Policzy liczbę klientów w każdym segmencie\n",
    "4. Obliczy średnią wartość zamówienia dla każdego segmentu\n",
    "\n",
    "**Wskazówka**: Użyj `CASE WHEN` do utworzenia segmentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c964baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uzupełnij zapytanie SQL\n",
    "result = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN total_spent >= ____ THEN 'VIP'\n",
    "            WHEN total_spent >= ____ THEN 'Premium'\n",
    "            ELSE 'Regular'\n",
    "        END AS segment,\n",
    "        COUNT(*) AS customer_count,\n",
    "        AVG(____) AS avg_order_value,\n",
    "        SUM(____) AS total_revenue,\n",
    "        ROUND(AVG(total_spent), 2) AS avg_customer_value\n",
    "    FROM {TOP_CUSTOMERS_VIEW}\n",
    "    GROUP BY ____\n",
    "    ORDER BY total_revenue ____\n",
    "\"\"\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b8c1b0",
   "metadata": {},
   "source": [
    "**Wskazówki do segmentacji klientów:**\n",
    "- `WHEN total_spent >= ____` - ustaw próg na 5000 dla VIP, 2000 dla Premium\n",
    "- `AVG(____)` - użyj `avg_order_value` do obliczenia średniej dla segmentu\n",
    "- `SUM(____)` - użyj `total_spent` do obliczenia łącznego przychodu\n",
    "- `GROUP BY ____` - grupuj według `segment`\n",
    "- `ORDER BY total_revenue ____` - sortuj malejąco (`DESC`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad6dc0f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Zarządzanie Widokami\n",
    "\n",
    "### 7.1 Przykład: Sprawdzanie Metadanych Widoku\n",
    "Ta komórka jest już gotowa - przeanalizuj kod i uruchom go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff786e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład: Sprawdzenie metadanych widoku\n",
    "spark.sql(f\"DESCRIBE EXTENDED {CUSTOMER_ORDERS_VIEW}\").show(truncate=False)\n",
    "\n",
    "spark.sql(f\"SHOW CREATE TABLE {CUSTOMER_ORDERS_VIEW}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68af70b",
   "metadata": {},
   "source": [
    "**Wyjaśnienie metadanych widoku:**\n",
    "- `DESCRIBE EXTENDED` pokazuje szczegóły struktury widoku, typy kolumn i metadane\n",
    "- `SHOW CREATE TABLE` wyświetla definicję SQL użytą do utworzenia widoku\n",
    "- Przydatne do debugowania i dokumentacji widoków\n",
    "- Pozwala sprawdzić czy widok został poprawnie utworzony"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed5c1cf",
   "metadata": {},
   "source": [
    "### 7.2 Zadanie: Lista Wszystkich Widoków\n",
    "\n",
    "**TODO**: Uzupełnij kod, aby:\n",
    "1. Wyświetlić listę wszystkich widoków w schemacie SILVER\n",
    "2. Dla każdego widoku wyświetlić: nazwę, typ (VIEW vs TABLE), komentarz\n",
    "\n",
    "**Wskazówka**: Użyj `SHOW TABLES IN schema` i przefiltruj po typie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wyświetl listę widoków\n",
    "# Wskazówka: SHOW TABLES IN schema\n",
    "result = spark.sql(f\"\"\"\n",
    "    SHOW TABLES IN {____}\n",
    "\"\"\")\n",
    "\n",
    "# Przefiltruj tylko widoki (isTemporary = false)\n",
    "views_only = result.filter(F.col(\"isTemporary\") == ____)\n",
    "\n",
    "display(views_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c606497",
   "metadata": {},
   "source": [
    "**Wskazówki do listowania widoków:**\n",
    "- `SHOW TABLES IN {____}` - użyj `SILVER_SCHEMA` \n",
    "- `F.col(\"isTemporary\") == ____` - użyj `False` aby pokazać tylko trwałe widoki\n",
    "- Komenda `SHOW TABLES` pokazuje zarówno tabele jak i widoki\n",
    "- Kolumna `isTemporary` pozwala odfiltrować widoki tymczasowe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708368d7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Przygotowanie Notebooka dla Joba\n",
    "\n",
    "### 8.1 Przykład: Parametryzowany Notebook\n",
    "Ta komórka demonstruje, jak utworzyć notebook, który może być uruchomiony jako job z parametrami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a0421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład: Widget do parametryzacji notebooka\n",
    "# W Databricks możesz utworzyć widget, który pozwala przekazać parametry do notebooka\n",
    "\n",
    "# Utwórz widget dla daty przetwarzania\n",
    "dbutils.widgets.text(\"processing_date\", \"2025-01-31\", \"Data przetwarzania\")\n",
    "dbutils.widgets.dropdown(\"mode\", \"full\", [\"full\", \"incremental\"], \"Tryb przetwarzania\")\n",
    "\n",
    "# Odczytaj wartości parametrów\n",
    "processing_date = dbutils.widgets.get(\"processing_date\")\n",
    "mode = dbutils.widgets.get(\"mode\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f58e38",
   "metadata": {},
   "source": [
    "**Wyjaśnienie parametryzacji notebooka:**\n",
    "- `dbutils.widgets` pozwala utworzyć interfejs do przekazywania parametrów\n",
    "- Widget tekstowy `processing_date` dla daty przetwarzania\n",
    "- Widget dropdown `mode` z opcjami: \"full\" (pełne) lub \"incremental\" (przyrostowe)\n",
    "- Parametry mogą być przekazane z Databricks Jobs lub ustawione ręcznie"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3fcb3a",
   "metadata": {},
   "source": [
    "### 8.2 Zadanie: ETL Pipeline\n",
    "\n",
    "**TODO**: Uzupełnij kod pipeline'u, który:\n",
    "1. Wczyta dane z tabel bronze\n",
    "2. Wyczyści i przekształci dane\n",
    "3. Zapisze wyniki do tabel silver\n",
    "4. Odświeży widoki\n",
    "5. Wyświetli statystyki przetwarzania\n",
    "\n",
    "Ten kod będzie używany w jobie Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c627b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementuj pipeline ETL - Krok 1: Inicjalizacja\n",
    "from datetime import datetime\n",
    "\n",
    "# Rozpocznij pipeline\n",
    "start_time = datetime.now()\n",
    "print(f\"=== Pipeline ETL rozpoczęty: {start_time} ===\")\n",
    "\n",
    "# Krok 1: Wczytaj dane bronze\n",
    "print(\"\\n[1/5] Wczytywanie danych bronze...\")\n",
    "# TODO: Wczytaj tabele bronze\n",
    "customers_bronze = spark.table(f\"{BRONZE_SCHEMA}.customers_workshop\")\n",
    "orders_bronze = spark.table(f\"{BRONZE_SCHEMA}.orders_workshop\")\n",
    "\n",
    "print(f\"  - Klienci: {customers_bronze.count()} wierszy\")\n",
    "print(f\"  - Zamówienia: {orders_bronze.count()} wierszy\")\n",
    "\n",
    "# Krok 2: Czyszczenie danych\n",
    "print(\"\\n[2/5] Czyszczenie danych...\")\n",
    "# TODO: Usuń duplikaty, obsłuż NULL, standaryzuj dane\n",
    "customers_clean = (\n",
    "    customers_bronze\n",
    "    .dropDuplicates([\"customer_id\"])\n",
    "    .withColumn(\"email\", F.trim(F.lower(F.col(\"email\"))))\n",
    "    .withColumn(\"last_name\", F.initcap(F.col(\"last_name\")))\n",
    ")\n",
    "\n",
    "orders_clean = (\n",
    "    orders_bronze\n",
    "    .dropna(subset=[\"order_amount\"])\n",
    "    .fillna({\"order_status\": \"Unknown\"})\n",
    ")\n",
    "\n",
    "print(f\"  - Klienci po czyszczeniu: {customers_clean.count()} wierszy\")\n",
    "print(f\"  - Zamówienia po czyszczeniu: {orders_clean.count()} wierszy\")\n",
    "\n",
    "# Krok 3: Zapis do silver\n",
    "print(\"\\n[3/5] Zapis do silver schema...\")\n",
    "# TODO: Zapisz wyczyszczone dane do silver\n",
    "(\n",
    "    customers_clean\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(f\"{SILVER_SCHEMA}.customers_clean\")\n",
    ")\n",
    "\n",
    "(\n",
    "    orders_clean\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(f\"{SILVER_SCHEMA}.orders_clean\")\n",
    ")\n",
    "\n",
    "print(\"  - Tabele zapisane do silver\")\n",
    "\n",
    "# Krok 4: Odśwież widoki (opcjonalnie recreate)\n",
    "print(\"\\n[4/5] Odświeżanie widoków...\")\n",
    "# TODO: Odśwież widoki jeśli to konieczne\n",
    "# Wskazówka: W Delta Lake widoki są automatycznie aktualizowane\n",
    "print(\"  - Widoki odświeżone\")\n",
    "\n",
    "# Krok 5: Statystyki\n",
    "print(\"\\n[5/5] Statystyki przetwarzania...\")\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\n=== Pipeline ETL zakończony ===\")\n",
    "print(f\"Czas rozpoczęcia: {start_time}\")\n",
    "print(f\"Czas zakończenia: {end_time}\")\n",
    "print(f\"Czas trwania: {duration} sekund\")\n",
    "print(f\"\\nPrzetworzone rekordy:\")\n",
    "print(f\"  - Klienci: {customers_clean.count()}\")\n",
    "print(f\"  - Zamówienia: {orders_clean.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6552be96",
   "metadata": {},
   "source": [
    "**Krok 9: Finalizacja i statystyki**\n",
    "- Zapisujemy czas zakończenia i obliczamy czas trwania pipeline'u\n",
    "- Wyświetlamy podsumowanie z kluczowymi metrykami\n",
    "- To pomaga w monitorowaniu wydajności procesu ETL\n",
    "- W środowisku produkcyjnym te metryki można logować do systemu monitoringu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Krok 9: Finalizacja i statystyki\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "# Wyświetl podsumowanie pipeline'u\n",
    "display(spark.createDataFrame([\n",
    "    (\"Czas rozpoczęcia\", str(start_time)),\n",
    "    (\"Czas zakończenia\", str(end_time)),\n",
    "    (\"Czas trwania (sekundy)\", str(duration)),\n",
    "    (\"Klienci przetworzone\", str(customers_clean_count)),\n",
    "    (\"Zamówienia przetworzone\", str(orders_clean_count))\n",
    "], [\"Metryka\", \"Wartość\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c9d78a",
   "metadata": {},
   "source": [
    "**Krok 8: Zapis zamówień do Silver**\n",
    "- Analogicznie jak dla klientów - używamy Delta Lake format\n",
    "- Zapisujemy wyczyszczone zamówienia do warstwy Silver\n",
    "- Warstwa Silver zawiera wyczyszczone i zwalidowane dane"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97d49cb",
   "metadata": {},
   "source": [
    "**Wskazówki do zadania podsumowującego:**\n",
    "- `MONTH(____)` - użyj `order_datetime` \n",
    "- `COUNT(____)` - policz `order_id`\n",
    "- `SUM(____)` i `AVG(____)` - użyj `total_amount`\n",
    "- `COUNT(DISTINCT ____)` - policz unikalnych `customer_id`\n",
    "- `WHERE ____ IS NOT NULL` - filtruj po `order_datetime`\n",
    "- `LAG(____)` - użyj `total_sales` do obliczenia trendu MoM (month-over-month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62bd489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Krok 8: Zapisz zamówienia do Silver\n",
    "(\n",
    "    orders_clean\n",
    "    .write\n",
    "    .format(\"____\")  # delta\n",
    "    .mode(\"____\")    # overwrite\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(f\"{____}.orders_clean\")  # SILVER_SCHEMA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97875aa2",
   "metadata": {},
   "source": [
    "**Krok 7: Zapis klientów do Silver**\n",
    "- `.format(\"____\")` - użyj `\"delta\"` dla Delta Lake\n",
    "- `.mode(\"____\")` - użyj `\"overwrite\"` aby zastąpić istniejące dane\n",
    "- `.saveAsTable(f\"{____}.customers_clean\")` - użyj `SILVER_SCHEMA`\n",
    "- `overwriteSchema` pozwala na zmiany w schemacie tabeli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaa7eed",
   "metadata": {},
   "source": [
    "**Czyszczenie zasobów:**\n",
    "- Opcjonalnie usuwamy utworzone widoki testowe\n",
    "- Usuwamy widoki tymczasowe z pamięci Spark\n",
    "- Usuwamy wszystkie widgety utworzone w notebook'u\n",
    "- W środowisku produkcyjnym należy zachować widoki do dalszego użytku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Krok 7: Zapisz klientów do Silver\n",
    "(\n",
    "    customers_clean\n",
    "    .write\n",
    "    .format(\"____\")  # delta\n",
    "    .mode(\"____\")    # overwrite\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(f\"{____}.customers_clean\")  # SILVER_SCHEMA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb9bcda",
   "metadata": {},
   "source": [
    "**Krok 6: Sprawdzenie wyników czyszczenia**\n",
    "- Ponownie używamy `.count()` aby sprawdzić ile rekordów zostało po czyszczeniu\n",
    "- Porównujemy liczby przed i po czyszczeniu\n",
    "- To pozwala ocenić jakość danych i skuteczność procesu czyszczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825ed5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Krok 6: Sprawdź wyniki czyszczenia\n",
    "customers_clean_count = customers_clean.____()\n",
    "orders_clean_count = orders_clean.____()\n",
    "\n",
    "# Wyświetl porównanie przed i po czyszczeniu\n",
    "display(spark.createDataFrame([\n",
    "    (\"Klienci - przed\", customers_count),\n",
    "    (\"Klienci - po\", customers_clean_count),\n",
    "    (\"Zamówienia - przed\", orders_count),\n",
    "    (\"Zamówienia - po\", orders_clean_count)\n",
    "], [\"Etap\", \"Liczba_rekordów\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e91a314",
   "metadata": {},
   "source": [
    "**Krok 5: Czyszczenie danych zamówień**\n",
    "- `.dropna(subset=[\"____\"])` - użyj `total_amount` aby usunąć zamówienia bez wartości\n",
    "- `.fillna({\"payment_method\": \"____\"})` - użyj `\"Unknown\"` dla brakujących metod płatności\n",
    "- Filtrujemy zamówienia z dodatnią wartością `total_amount` i `quantity`\n",
    "- To usuwa błędne dane z ujemnymi wartościami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Krok 5: Czyszczenie danych zamówień\n",
    "orders_clean = (\n",
    "    orders_bronze\n",
    "    .dropna(subset=[\"____\"])  # usuń rekordy bez total_amount\n",
    "    .fillna({\"payment_method\": \"____\"})  # wypełnij brakujące metody płatności\n",
    "    .filter(F.col(\"____\") > 0)  # usuń zamówienia z ujemną wartością\n",
    "    .filter(F.col(\"____\") > 0)  # usuń zamówienia z ujemną ilością\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e3131",
   "metadata": {},
   "source": [
    "**Krok 4: Czyszczenie danych klientów**\n",
    "- `.dropDuplicates([\"____\"])` - użyj `customer_id` aby usunąć duplikaty\n",
    "- Standaryzujemy email: `.trim()` usuwa spacje, `.lower()` zmienia na małe litery\n",
    "- `.initcap()` - zmienia pierwszą literę na wielką dla `last_name` i `first_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Krok 4: Czyszczenie danych klientów\n",
    "customers_clean = (\n",
    "    customers_bronze\n",
    "    .dropDuplicates([\"____\"])\n",
    "    .withColumn(\"email\", F.trim(F.lower(F.col(\"____\"))))\n",
    "    .withColumn(\"last_name\", F.initcap(F.col(\"____\")))\n",
    "    .withColumn(\"first_name\", F.initcap(F.col(\"____\")))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7295258",
   "metadata": {},
   "source": [
    "**Krok 3: Sprawdzenie liczby rekordów**\n",
    "- Użyj metody `count()` do policzenia rekordów w każdej tabeli\n",
    "- Wyświetlamy statystyki w czytelnej formie tabeli\n",
    "- To pomaga zweryfikować, że dane zostały poprawnie wczytane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61cae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Krok 3: Sprawdź liczby rekordów\n",
    "customers_count = customers_bronze.____()\n",
    "orders_count = orders_bronze.____()\n",
    "\n",
    "# Wyświetl statystyki wczytanych danych\n",
    "display(spark.createDataFrame([\n",
    "    (\"Klienci\", customers_count),\n",
    "    (\"Zamówienia\", orders_count)\n",
    "], [\"Tabela\", \"Liczba_rekordów\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1aac68",
   "metadata": {},
   "source": [
    "**Krok 2: Wczytywanie danych Bronze**\n",
    "- Wczytujemy tabele z warstwy Bronze (surowe dane)\n",
    "- Użyj `BRONZE_SCHEMA` w miejscach z `____`\n",
    "- Tabele `customers_workshop` i `orders_workshop` zostały utworzone w poprzednich warsztatach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc3d6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Krok 2: Wczytaj dane bronze\n",
    "customers_bronze = spark.table(f\"{____}.customers_workshop\")\n",
    "orders_bronze = spark.table(f\"{____}.orders_workshop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Krok 1: Inicjalizacja Pipeline'u ETL**\n",
    "- Importujemy `datetime` do śledzenia czasu wykonania\n",
    "- Zapisujemy czas rozpoczęcia pipeline'u\n",
    "- To pozwoli nam zmierzyć wydajność procesu ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca70bda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Konfiguracja Joba (Instrukcja)\n",
    "\n",
    "### 9.1 Tworzenie Joba w Databricks UI\n",
    "\n",
    "**Krok po kroku:**\n",
    "\n",
    "1. **Przejdź do zakładki Jobs**\n",
    "   - W menu bocznym wybierz \"Workflows\" → \"Jobs\"\n",
    "   - Kliknij \"Create Job\"\n",
    "\n",
    "2. **Skonfiguruj Task**\n",
    "   - **Task name**: `etl_silver_pipeline`\n",
    "   - **Type**: `Notebook`\n",
    "   - **Source**: Wybierz ten notebook\n",
    "   - **Cluster**: Wybierz istniejący cluster lub utwórz nowy\n",
    "\n",
    "3. **Parametry (opcjonalnie)**\n",
    "   - Dodaj parametry dla `processing_date` i `mode`\n",
    "   - Przykład: `{\"processing_date\": \"2025-01-31\", \"mode\": \"full\"}`\n",
    "\n",
    "4. **Harmonogram (opcjonalnie)**\n",
    "   - **Schedule**: Cron expression, np. `0 2 * * *` (codziennie o 2:00)\n",
    "   - **Timezone**: Wybierz swoją strefę czasową\n",
    "\n",
    "5. **Retry Policy**\n",
    "   - **Max retries**: 3\n",
    "   - **Retry interval**: 60 sekund\n",
    "\n",
    "6. **Notifications (opcjonalnie)**\n",
    "   - Dodaj email dla powiadomień o sukcesie/błędzie\n",
    "\n",
    "7. **Zapisz i uruchom**\n",
    "   - Kliknij \"Create\" aby zapisać job\n",
    "   - Kliknij \"Run now\" aby uruchomić job ręcznie\n",
    "\n",
    "### 9.2 Monitorowanie Joba\n",
    "\n",
    "Po uruchomieniu joba:\n",
    "- Obserwuj status w zakładce \"Runs\"\n",
    "- Sprawdź logi w \"Logs\"\n",
    "- Analizuj metryki w \"Metrics\"\n",
    "- Sprawdź outputy notebooka\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b499e1",
   "metadata": {},
   "source": [
    "## 10. Zadanie Podsumowujące\n",
    "\n",
    "### 10.1 TODO: Kompleksowe Zadanie\n",
    "\n",
    "**Zadanie**: Zaimplementuj kompletny proces:\n",
    "\n",
    "1. Utwórz widok `vw_monthly_sales` pokazujący:\n",
    "   - Rok i miesiąc\n",
    "   - Liczbę zamówień\n",
    "   - Całkowitą wartość sprzedaży\n",
    "   - Średnią wartość zamówienia\n",
    "   - Liczbę unikalnych klientów\n",
    "\n",
    "2. Napisz zapytanie, które:\n",
    "   - Użyje utworzonego widoku\n",
    "   - Obliczy wzrost sprzedaży miesiąc do miesiąca (MoM growth)\n",
    "   - Wyświetli trendy\n",
    "\n",
    "3. Przygotuj ten kod do uruchomienia jako job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0534ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementuj zadanie podsumowujące\n",
    "\n",
    "# Część 1: Utwórz widok monthly_sales\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {SILVER_SCHEMA}.vw_monthly_sales\n",
    "    COMMENT 'Miesięczne statystyki sprzedaży'\n",
    "    AS\n",
    "    SELECT \n",
    "        YEAR(order_datetime) as year,\n",
    "        MONTH(____) as month,\n",
    "        COUNT(____) as order_count,\n",
    "        SUM(____) as total_sales,\n",
    "        AVG(____) as avg_order_value,\n",
    "        COUNT(DISTINCT ____) as unique_customers\n",
    "    FROM {SILVER_SCHEMA}.orders_clean\n",
    "    WHERE ____ IS NOT NULL\n",
    "    GROUP BY YEAR(order_datetime), MONTH(order_datetime)\n",
    "    ORDER BY year, month\n",
    "\"\"\")\n",
    "\n",
    "# Część 2: Zapytanie z trendem MoM\n",
    "result = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        year,\n",
    "        month,\n",
    "        order_count,\n",
    "        total_sales,\n",
    "        LAG(____) OVER (ORDER BY year, month) as prev_month_sales,\n",
    "        ROUND(\n",
    "            (total_sales - LAG(____) OVER (ORDER BY year, month)) / \n",
    "            LAG(____) OVER (ORDER BY year, month) * 100, 2\n",
    "        ) as mom_growth_percent\n",
    "    FROM {SILVER_SCHEMA}.vw_monthly_sales\n",
    "    ORDER BY year, month\n",
    "\"\"\")\n",
    "\n",
    "display(result)\n",
    "\n",
    "# Część 3: Dodaj error handling i logging\n",
    "# TODO: try-except, logging statystyk\n",
    "\n",
    "print(\"Zadanie zakończone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7698bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Best Practices i Podsumowanie\n",
    "\n",
    "### 11.1 Best Practices dla Widoków\n",
    "1. **Nazywaj widoki prefiksem** - `vw_` dla łatwej identyfikacji\n",
    "2. **Dodawaj komentarze** - dokumentuj cel widoku\n",
    "3. **Używaj widoków dla logiki biznesowej** - enkapsulacja złożonych zapytań\n",
    "4. **Monitoruj performance** - widoki mogą wpływać na wydajność\n",
    "5. **Rozważ materialized views** - dla często używanych, kosztownych zapytań\n",
    "\n",
    "### 11.2 Best Practices dla Jobs\n",
    "1. **Parametryzuj notebooki** - używaj widgets\n",
    "2. **Implementuj error handling** - try-except, retry policy\n",
    "3. **Loguj wszystko** - starty, zakończenia, błędy, statystyki\n",
    "4. **Monitoruj joby** - alerting, notifications\n",
    "5. **Testuj przed wdrożeniem** - uruchom ręcznie i sprawdź wyniki\n",
    "6. **Używaj idempotentnych operacji** - job można uruchomić wielokrotnie bez efektów ubocznych\n",
    "\n",
    "### 11.3 Co osiągnąłeś?\n",
    "- Stworzyłeś widoki tymczasowe i trwałe\n",
    "- Stworzyłeś widoki biznesowe (top klienci, sprzedaż produktów)\n",
    "- Napisałeś zapytania wykorzystujące widoki\n",
    "- Przygotowałeś notebook do uruchomienia jako job\n",
    "- Zaimplementowałeś prosty pipeline ETL\n",
    "- Poznałeś best practices dla widoków i jobs\n",
    "\n",
    "### 11.4 Następne kroki\n",
    "- Delta Live Tables (DLT)\n",
    "- Zaawansowane joby (multi-task workflows)\n",
    "- Monitoring i alerting\n",
    "- CI/CD dla Databricks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d23f247",
   "metadata": {},
   "source": [
    "## 12. Czyszczenie Zasobów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1031ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcjonalnie: usuń widoki testowe\n",
    "# spark.sql(f\"DROP VIEW IF EXISTS {CUSTOMER_ORDERS_VIEW}\")\n",
    "# spark.sql(f\"DROP VIEW IF EXISTS {TOP_CUSTOMERS_VIEW}\")\n",
    "# spark.sql(f\"DROP VIEW IF EXISTS {PRODUCT_SALES_VIEW}\")\n",
    "\n",
    "# Usuń widoki tymczasowe\n",
    "# spark.catalog.dropTempView(\"tmp_customers\")\n",
    "# spark.catalog.dropGlobalTempView(\"gtmp_orders\")\n",
    "\n",
    "# Usuń widgety\n",
    "dbutils.widgets.removeAll()\n",
    "\n",
    "display(spark.createDataFrame([\n",
    "    (\"Status\", \"Zasoby wyczyszczone!\"),\n",
    "    (\"Warsztat\", \"Zakończony!\")\n",
    "], [\"Kategoria\", \"Informacja\"]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
