{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6004e677",
   "metadata": {},
   "source": [
    "# Warsztat 3: Widoki i Podstawowe Jobsy\n",
    "\n",
    "**Czas trwania**: 90 minut  \n",
    "**Poziom**: Podstawowy/Średniozaawansowany  \n",
    "**Ostatnia aktualizacja**: 2025-01-31\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b897bd5b",
   "metadata": {},
   "source": [
    "## 1. Kontekst i Cele Warsztatu\n",
    "\n",
    "### Czego się nauczysz?\n",
    "- Tworzyć widoki tymczasowe i globalne\n",
    "- Tworzyć widoki materialized w Delta Lake\n",
    "- Konfigurować podstawowe joby w Databricks\n",
    "- Monitorować wykonanie jobów\n",
    "\n",
    "### Wymagania wstępne\n",
    "- Ukończenie Warsztatu 1 i 2\n",
    "- Podstawowa znajomość SQL i PySpark\n",
    "- Wyczyszczone dane z poprzedniego warsztatu\n",
    "\n",
    "### Co będziesz implementować?\n",
    "Stworzysz zestaw widoków biznesowych i skonfigurujesz job ETL, który automatycznie przetwarza dane.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681d293d",
   "metadata": {},
   "source": [
    "## 2. Materiały Teoretyczne\n",
    "\n",
    "### 2.1 Typy Widoków\n",
    "- **Temporary View** - widok tymczasowy, dostępny tylko w bieżącej sesji Spark\n",
    "- **Global Temporary View** - widok globalny, dostępny we wszystkich sesjach Spark\n",
    "- **View (Materialized)** - widok trwały, zapisany w metastore\n",
    "\n",
    "### 2.2 Databricks Jobs\n",
    "- **Task** - pojedyncze zadanie (notebook, JAR, Python script)\n",
    "- **Job** - zestaw tasków z zależnościami\n",
    "- **Cluster** - zasoby obliczeniowe do wykonania jobu\n",
    "- **Schedule** - harmonogram uruchamiania jobu\n",
    "\n",
    "### 2.3 Best Practices\n",
    "- Używaj widoków dla często używanych zapytań\n",
    "- Dokumentuj widoki - dodawaj komentarze\n",
    "- Monitoruj performance jobów\n",
    "- Używaj retry policy dla jobów\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab642b5",
   "metadata": {},
   "source": [
    "## 3. Inicjalizacja Środowiska\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b965213",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dbc162d",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf3fb2a",
   "metadata": {},
   "source": [
    "### 3.2 Konfiguracja Zmiennych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8297ae7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Nazwy tabel z poprzednich warsztatów\n",
    "CUSTOMERS_TABLE = f\"{SILVER_SCHEMA}.customers_clean\"\n",
    "ORDERS_TABLE = f\"{BRONZE_SCHEMA}.orders_workshop\"\n",
    "PRODUCTS_TABLE = f\"{BRONZE_SCHEMA}.products_workshop\"\n",
    "\n",
    "# Nazwy widoków do utworzenia\n",
    "CUSTOMER_ORDERS_VIEW = f\"{SILVER_SCHEMA}.vw_customer_orders\"\n",
    "TOP_CUSTOMERS_VIEW = f\"{SILVER_SCHEMA}.vw_top_customers\"\n",
    "PRODUCT_SALES_VIEW = f\"{SILVER_SCHEMA}.vw_product_sales\"\n",
    "\n",
    "print(\"=== Konfiguracja ===\" )\n",
    "print(f\"Silver Schema: {SILVER_SCHEMA}\")\n",
    "print(f\"Tabela klientów: {CUSTOMERS_TABLE}\")\n",
    "print(f\"Widoki do utworzenia:\")\n",
    "print(f\"  - {CUSTOMER_ORDERS_VIEW}\")\n",
    "print(f\"  - {TOP_CUSTOMERS_VIEW}\")\n",
    "print(f\"  - {PRODUCT_SALES_VIEW}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd9b7d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Widoki Tymczasowe\n",
    "\n",
    "### 4.1 Przykład: Temporary View\n",
    "Ta komórka jest już gotowa - przeanalizuj kod i uruchom go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950d24b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład: Tworzenie widoku tymczasowego\n",
    "# Widok będzie dostępny tylko w tej sesji Spark\n",
    "\n",
    "customers_df = spark.table(CUSTOMERS_TABLE)\n",
    "customers_df.createOrReplaceTempView(\"tmp_customers\")\n",
    "\n",
    "# Użyj widoku w zapytaniu SQL\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        city,\n",
    "        COUNT(*) as customer_count\n",
    "    FROM tmp_customers\n",
    "    GROUP BY city\n",
    "    ORDER BY customer_count DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== Top 10 miast z najwięcej klientami ===\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b2dd3e",
   "metadata": {},
   "source": [
    "### 4.2 Zadanie: Global Temporary View\n",
    "\n",
    "**TODO**: Uzupełnij kod, aby:\n",
    "1. Utworzyć globalny widok tymczasowy dla tabeli zamówień\n",
    "2. Wyświetlić statystyki zamówień z tego widoku\n",
    "\n",
    "**Wskazówka**: Użyj `.createOrReplaceGlobalTempView()` i odwołuj się przez `global_temp.nazwa_widoku`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62b42d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utwórz globalny widok tymczasowy\n",
    "orders_df = spark.table(ORDERS_TABLE)\n",
    "\n",
    "# TODO: Uzupełnij kod\n",
    "# orders_df.createOrReplaceGlobalTempView(\"gtmp_orders\")\n",
    "\n",
    "# TODO: Użyj widoku w zapytaniu SQL\n",
    "# Wskazówka: SELECT ... FROM global_temp.gtmp_orders\n",
    "result = spark.sql(\"\"\"\n",
    "    -- TODO: Napisz zapytanie, które policzy:\n",
    "    -- - Całkowitą liczbę zamówień\n",
    "    -- - Całkowitą wartość zamówień\n",
    "    -- - Średnią wartość zamówienia\n",
    "    -- - Minimalną i maksymalną wartość zamówienia\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== Statystyki zamówień ===\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f80db0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Widoki Materialized (Trwałe)\n",
    "\n",
    "### 5.1 Przykład: Widok Zamówień z Klientami\n",
    "Ta komórka jest już gotowa - przeanalizuj kod i uruchom go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0340444b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład: Widok łączący zamówienia z klientami\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {CUSTOMER_ORDERS_VIEW}\n",
    "    COMMENT 'Widok łączący dane zamówień z informacjami o klientach'\n",
    "    AS\n",
    "    SELECT \n",
    "        o.order_id,\n",
    "        o.order_date,\n",
    "        o.order_amount,\n",
    "        o.order_status,\n",
    "        c.customer_id,\n",
    "        c.first_name,\n",
    "        c.last_name,\n",
    "        c.email,\n",
    "        c.city,\n",
    "        c.country\n",
    "    FROM {ORDERS_TABLE} o\n",
    "    INNER JOIN {CUSTOMERS_TABLE} c\n",
    "        ON o.customer_id = c.customer_id\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Utworzono widok: {CUSTOMER_ORDERS_VIEW}\")\n",
    "\n",
    "# Sprawdź widok\n",
    "result = spark.sql(f\"SELECT * FROM {CUSTOMER_ORDERS_VIEW} LIMIT 10\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8809087",
   "metadata": {},
   "source": [
    "### 5.2 Zadanie: Widok Top Klientów\n",
    "\n",
    "**TODO**: Uzupełnij kod, aby utworzyć widok, który:\n",
    "1. Policzy liczbę zamówień dla każdego klienta\n",
    "2. Obliczy całkowitą wartość zamówień dla każdego klienta\n",
    "3. Obliczy średnią wartość zamówienia\n",
    "4. Pokaże tylko top 100 klientów (według wartości)\n",
    "\n",
    "**Wskazówka**: Użyj `CREATE OR REPLACE VIEW` z `GROUP BY` i `ORDER BY ... LIMIT`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c969e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utwórz widok top klientów\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {TOP_CUSTOMERS_VIEW}\n",
    "    COMMENT 'Top 100 klientów według wartości zamówień'\n",
    "    AS\n",
    "    SELECT \n",
    "        -- TODO: Dodaj kolumny:\n",
    "        -- customer_id\n",
    "        -- first_name, last_name\n",
    "        -- COUNT(order_id) AS order_count\n",
    "        -- SUM(order_amount) AS total_spent\n",
    "        -- AVG(order_amount) AS avg_order_value\n",
    "    FROM {CUSTOMER_ORDERS_VIEW}\n",
    "    -- TODO: Dodaj GROUP BY\n",
    "    -- TODO: Dodaj ORDER BY total_spent DESC\n",
    "    -- TODO: Dodaj LIMIT 100\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Utworzono widok: {TOP_CUSTOMERS_VIEW}\")\n",
    "\n",
    "# Sprawdź widok\n",
    "result = spark.sql(f\"SELECT * FROM {TOP_CUSTOMERS_VIEW} LIMIT 10\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db907dd2",
   "metadata": {},
   "source": [
    "### 5.3 Zadanie: Widok Sprzedaży Produktów\n",
    "\n",
    "**TODO**: Uzupełnij kod, aby utworzyć widok sprzedaży produktów.\n",
    "Zakładamy, że tabela zamówień ma kolumnę `product_id` i `quantity`.\n",
    "\n",
    "**Wskazówka**: Połącz tabele zamówień i produktów, oblicz statystyki sprzedaży"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1396414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utwórz widok sprzedaży produktów\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW {PRODUCT_SALES_VIEW}\n",
    "    COMMENT 'Statystyki sprzedaży produktów'\n",
    "    AS\n",
    "    SELECT \n",
    "        -- TODO: Dodaj kolumny:\n",
    "        -- p.product_id, p.product_name, p.category\n",
    "        -- COUNT(o.order_id) AS order_count\n",
    "        -- SUM(o.quantity) AS total_quantity\n",
    "        -- SUM(o.order_amount) AS total_revenue\n",
    "    FROM {ORDERS_TABLE} o\n",
    "    -- TODO: Dodaj JOIN z tabelą produktów\n",
    "    -- TODO: Dodaj GROUP BY\n",
    "    -- TODO: Dodaj ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Utworzono widok: {PRODUCT_SALES_VIEW}\")\n",
    "\n",
    "# Sprawdź widok\n",
    "result = spark.sql(f\"SELECT * FROM {PRODUCT_SALES_VIEW} LIMIT 10\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500e9c80",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Zapytania na Widokach\n",
    "\n",
    "### 6.1 Przykład: Analiza Top Klientów\n",
    "Ta komórka jest już gotowa - przeanalizuj kod i uruchom go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dae4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład: Zapytanie na widoku top klientów\n",
    "result = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        first_name || ' ' || last_name AS customer_name,\n",
    "        order_count,\n",
    "        ROUND(total_spent, 2) AS total_spent,\n",
    "        ROUND(avg_order_value, 2) AS avg_order_value\n",
    "    FROM {TOP_CUSTOMERS_VIEW}\n",
    "    WHERE order_count >= 5\n",
    "    ORDER BY total_spent DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== Top 20 klientów z min. 5 zamówieniami ===\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8deeaa6",
   "metadata": {},
   "source": [
    "### 6.2 Zadanie: Analiza Segmentów Klientów\n",
    "\n",
    "**TODO**: Napisz zapytanie SQL, które:\n",
    "1. Użyje widoku `TOP_CUSTOMERS_VIEW`\n",
    "2. Podzieli klientów na segmenty:\n",
    "   - VIP: total_spent >= 5000\n",
    "   - Premium: total_spent >= 2000 AND < 5000\n",
    "   - Regular: total_spent < 2000\n",
    "3. Policzy liczbę klientów w każdym segmencie\n",
    "4. Obliczy średnią wartość zamówienia dla każdego segmentu\n",
    "\n",
    "**Wskazówka**: Użyj `CASE WHEN` do utworzenia segmentów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c964baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Uzupełnij zapytanie SQL\n",
    "result = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        -- TODO: Dodaj kolumnę segment używając CASE WHEN\n",
    "        CASE \n",
    "            WHEN total_spent >= 5000 THEN 'VIP'\n",
    "            WHEN total_spent >= 2000 THEN 'Premium'\n",
    "            ELSE 'Regular'\n",
    "        END AS segment,\n",
    "        -- TODO: Dodaj COUNT(*) AS customer_count\n",
    "        -- TODO: Dodaj AVG(avg_order_value) AS avg_order_value\n",
    "        -- TODO: Dodaj SUM(total_spent) AS total_revenue\n",
    "    FROM {TOP_CUSTOMERS_VIEW}\n",
    "    -- TODO: Dodaj GROUP BY segment\n",
    "    -- TODO: Dodaj ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== Segmenty klientów ===\")\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad6dc0f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Zarządzanie Widokami\n",
    "\n",
    "### 7.1 Przykład: Sprawdzanie Metadanych Widoku\n",
    "Ta komórka jest już gotowa - przeanalizuj kod i uruchom go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff786e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład: Sprawdzenie metadanych widoku\n",
    "print(f\"=== Metadane widoku: {CUSTOMER_ORDERS_VIEW} ===\")\n",
    "spark.sql(f\"DESCRIBE EXTENDED {CUSTOMER_ORDERS_VIEW}\").show(truncate=False)\n",
    "\n",
    "print(f\"\\n=== Definicja widoku ===\")\n",
    "spark.sql(f\"SHOW CREATE TABLE {CUSTOMER_ORDERS_VIEW}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed5c1cf",
   "metadata": {},
   "source": [
    "### 7.2 Zadanie: Lista Wszystkich Widoków\n",
    "\n",
    "**TODO**: Uzupełnij kod, aby:\n",
    "1. Wyświetlić listę wszystkich widoków w schemacie SILVER\n",
    "2. Dla każdego widoku wyświetlić: nazwę, typ (VIEW vs TABLE), komentarz\n",
    "\n",
    "**Wskazówka**: Użyj `SHOW TABLES IN schema` i przefiltruj po typie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e188050e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wyświetl listę widoków\n",
    "# Wskazówka: SHOW TABLES IN schema\n",
    "result = spark.sql(f\"\"\"\n",
    "    -- TODO: Napisz zapytanie, które wyświetli widoki w SILVER_SCHEMA\n",
    "    -- Wskazówka: SHOW TABLES IN {SILVER_SCHEMA}\n",
    "\"\"\")\n",
    "\n",
    "# Przefiltruj tylko widoki (isTemporary = false)\n",
    "views_only = result.filter(F.col(\"isTemporary\") == False)\n",
    "\n",
    "print(f\"=== Widoki w schemacie: {SILVER_SCHEMA} ===\")\n",
    "display(views_only)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708368d7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Przygotowanie Notebooka dla Joba\n",
    "\n",
    "### 8.1 Przykład: Parametryzowany Notebook\n",
    "Ta komórka demonstruje, jak utworzyć notebook, który może być uruchomiony jako job z parametrami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "080a0421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład: Widget do parametryzacji notebooka\n",
    "# W Databricks możesz utworzyć widget, który pozwala przekazać parametry do notebooka\n",
    "\n",
    "# Utwórz widget dla daty przetwarzania\n",
    "dbutils.widgets.text(\"processing_date\", \"2025-01-31\", \"Data przetwarzania\")\n",
    "dbutils.widgets.dropdown(\"mode\", \"full\", [\"full\", \"incremental\"], \"Tryb przetwarzania\")\n",
    "\n",
    "# Odczytaj wartości parametrów\n",
    "processing_date = dbutils.widgets.get(\"processing_date\")\n",
    "mode = dbutils.widgets.get(\"mode\")\n",
    "\n",
    "print(f\"=== Parametry jobu ===\")\n",
    "print(f\"Data przetwarzania: {processing_date}\")\n",
    "print(f\"Tryb: {mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3fcb3a",
   "metadata": {},
   "source": [
    "### 8.2 Zadanie: ETL Pipeline\n",
    "\n",
    "**TODO**: Uzupełnij kod pipeline'u, który:\n",
    "1. Wczyta dane z tabel bronze\n",
    "2. Wyczyści i przekształci dane\n",
    "3. Zapisze wyniki do tabel silver\n",
    "4. Odświeży widoki\n",
    "5. Wyświetli statystyki przetwarzania\n",
    "\n",
    "Ten kod będzie używany w jobie Databricks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c627b965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementuj pipeline ETL\n",
    "from datetime import datetime\n",
    "\n",
    "# Rozpocznij pipeline\n",
    "start_time = datetime.now()\n",
    "print(f\"=== Pipeline ETL rozpoczęty: {start_time} ===\")\n",
    "\n",
    "# Krok 1: Wczytaj dane bronze\n",
    "print(\"\\n[1/5] Wczytywanie danych bronze...\")\n",
    "# TODO: Wczytaj tabele bronze\n",
    "customers_bronze = spark.table(f\"{BRONZE_SCHEMA}.customers_workshop\")\n",
    "orders_bronze = spark.table(f\"{BRONZE_SCHEMA}.orders_workshop\")\n",
    "\n",
    "print(f\"  - Klienci: {customers_bronze.count()} wierszy\")\n",
    "print(f\"  - Zamówienia: {orders_bronze.count()} wierszy\")\n",
    "\n",
    "# Krok 2: Czyszczenie danych\n",
    "print(\"\\n[2/5] Czyszczenie danych...\")\n",
    "# TODO: Usuń duplikaty, obsłuż NULL, standaryzuj dane\n",
    "customers_clean = (\n",
    "    customers_bronze\n",
    "    .dropDuplicates([\"customer_id\"])\n",
    "    .withColumn(\"email\", F.trim(F.lower(F.col(\"email\"))))\n",
    "    .withColumn(\"last_name\", F.initcap(F.col(\"last_name\")))\n",
    ")\n",
    "\n",
    "orders_clean = (\n",
    "    orders_bronze\n",
    "    .dropna(subset=[\"order_amount\"])\n",
    "    .fillna({\"order_status\": \"Unknown\"})\n",
    ")\n",
    "\n",
    "print(f\"  - Klienci po czyszczeniu: {customers_clean.count()} wierszy\")\n",
    "print(f\"  - Zamówienia po czyszczeniu: {orders_clean.count()} wierszy\")\n",
    "\n",
    "# Krok 3: Zapis do silver\n",
    "print(\"\\n[3/5] Zapis do silver schema...\")\n",
    "# TODO: Zapisz wyczyszczone dane do silver\n",
    "(\n",
    "    customers_clean\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(f\"{SILVER_SCHEMA}.customers_clean\")\n",
    ")\n",
    "\n",
    "(\n",
    "    orders_clean\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(f\"{SILVER_SCHEMA}.orders_clean\")\n",
    ")\n",
    "\n",
    "print(\"  - Tabele zapisane do silver\")\n",
    "\n",
    "# Krok 4: Odśwież widoki (opcjonalnie recreate)\n",
    "print(\"\\n[4/5] Odświeżanie widoków...\")\n",
    "# TODO: Odśwież widoki jeśli to konieczne\n",
    "# Wskazówka: W Delta Lake widoki są automatycznie aktualizowane\n",
    "print(\"  - Widoki odświeżone\")\n",
    "\n",
    "# Krok 5: Statystyki\n",
    "print(\"\\n[5/5] Statystyki przetwarzania...\")\n",
    "end_time = datetime.now()\n",
    "duration = (end_time - start_time).total_seconds()\n",
    "\n",
    "print(f\"\\n=== Pipeline ETL zakończony ===\")\n",
    "print(f\"Czas rozpoczęcia: {start_time}\")\n",
    "print(f\"Czas zakończenia: {end_time}\")\n",
    "print(f\"Czas trwania: {duration} sekund\")\n",
    "print(f\"\\nPrzetworzone rekordy:\")\n",
    "print(f\"  - Klienci: {customers_clean.count()}\")\n",
    "print(f\"  - Zamówienia: {orders_clean.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca70bda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 9. Konfiguracja Joba (Instrukcja)\n",
    "\n",
    "### 9.1 Tworzenie Joba w Databricks UI\n",
    "\n",
    "**Krok po kroku:**\n",
    "\n",
    "1. **Przejdź do zakładki Jobs**\n",
    "   - W menu bocznym wybierz \"Workflows\" → \"Jobs\"\n",
    "   - Kliknij \"Create Job\"\n",
    "\n",
    "2. **Skonfiguruj Task**\n",
    "   - **Task name**: `etl_silver_pipeline`\n",
    "   - **Type**: `Notebook`\n",
    "   - **Source**: Wybierz ten notebook\n",
    "   - **Cluster**: Wybierz istniejący cluster lub utwórz nowy\n",
    "\n",
    "3. **Parametry (opcjonalnie)**\n",
    "   - Dodaj parametry dla `processing_date` i `mode`\n",
    "   - Przykład: `{\"processing_date\": \"2025-01-31\", \"mode\": \"full\"}`\n",
    "\n",
    "4. **Harmonogram (opcjonalnie)**\n",
    "   - **Schedule**: Cron expression, np. `0 2 * * *` (codziennie o 2:00)\n",
    "   - **Timezone**: Wybierz swoją strefę czasową\n",
    "\n",
    "5. **Retry Policy**\n",
    "   - **Max retries**: 3\n",
    "   - **Retry interval**: 60 sekund\n",
    "\n",
    "6. **Notifications (opcjonalnie)**\n",
    "   - Dodaj email dla powiadomień o sukcesie/błędzie\n",
    "\n",
    "7. **Zapisz i uruchom**\n",
    "   - Kliknij \"Create\" aby zapisać job\n",
    "   - Kliknij \"Run now\" aby uruchomić job ręcznie\n",
    "\n",
    "### 9.2 Monitorowanie Joba\n",
    "\n",
    "Po uruchomieniu joba:\n",
    "- Obserwuj status w zakładce \"Runs\"\n",
    "- Sprawdź logi w \"Logs\"\n",
    "- Analizuj metryki w \"Metrics\"\n",
    "- Sprawdź outputy notebooka\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b499e1",
   "metadata": {},
   "source": [
    "## 10. Zadanie Podsumowujące\n",
    "\n",
    "### 10.1 TODO: Kompleksowe Zadanie\n",
    "\n",
    "**Zadanie**: Zaimplementuj kompletny proces:\n",
    "\n",
    "1. Utwórz widok `vw_monthly_sales` pokazujący:\n",
    "   - Rok i miesiąc\n",
    "   - Liczbę zamówień\n",
    "   - Całkowitą wartość sprzedaży\n",
    "   - Średnią wartość zamówienia\n",
    "   - Liczbę unikalnych klientów\n",
    "\n",
    "2. Napisz zapytanie, które:\n",
    "   - Użyje utworzonego widoku\n",
    "   - Obliczy wzrost sprzedaży miesiąc do miesiąca (MoM growth)\n",
    "   - Wyświetli trendy\n",
    "\n",
    "3. Przygotuj ten kod do uruchomienia jako job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0534ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implementuj zadanie podsumowujące\n",
    "\n",
    "# Część 1: Utwórz widok monthly_sales\n",
    "# TODO: Napisz CREATE OR REPLACE VIEW\n",
    "\n",
    "# Część 2: Zapytanie z trendem MoM\n",
    "# TODO: Użyj LAG() lub LEAD() do obliczenia wzrostu\n",
    "\n",
    "# Część 3: Dodaj error handling i logging\n",
    "# TODO: try-except, logging statystyk\n",
    "\n",
    "print(\"Zadanie zakończone!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7698bf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 11. Best Practices i Podsumowanie\n",
    "\n",
    "### 11.1 Best Practices dla Widoków\n",
    "1. **Nazywaj widoki prefiksem** - `vw_` dla łatwej identyfikacji\n",
    "2. **Dodawaj komentarze** - dokumentuj cel widoku\n",
    "3. **Używaj widoków dla logiki biznesowej** - enkapsulacja złożonych zapytań\n",
    "4. **Monitoruj performance** - widoki mogą wpływać na wydajność\n",
    "5. **Rozważ materialized views** - dla często używanych, kosztownych zapytań\n",
    "\n",
    "### 11.2 Best Practices dla Jobs\n",
    "1. **Parametryzuj notebooki** - używaj widgets\n",
    "2. **Implementuj error handling** - try-except, retry policy\n",
    "3. **Loguj wszystko** - starty, zakończenia, błędy, statystyki\n",
    "4. **Monitoruj joby** - alerting, notifications\n",
    "5. **Testuj przed wdrożeniem** - uruchom ręcznie i sprawdź wyniki\n",
    "6. **Używaj idempotentnych operacji** - job można uruchomić wielokrotnie bez efektów ubocznych\n",
    "\n",
    "### 11.3 Co osiągnąłeś?\n",
    "- Stworzyłeś widoki tymczasowe i trwałe\n",
    "- Stworzyłeś widoki biznesowe (top klienci, sprzedaż produktów)\n",
    "- Napisałeś zapytania wykorzystujące widoki\n",
    "- Przygotowałeś notebook do uruchomienia jako job\n",
    "- Zaimplementowałeś prosty pipeline ETL\n",
    "- Poznałeś best practices dla widoków i jobs\n",
    "\n",
    "### 11.4 Następne kroki\n",
    "- Delta Live Tables (DLT)\n",
    "- Zaawansowane joby (multi-task workflows)\n",
    "- Monitoring i alerting\n",
    "- CI/CD dla Databricks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d23f247",
   "metadata": {},
   "source": [
    "## 12. Czyszczenie Zasobów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1031ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcjonalnie: usuń widoki testowe\n",
    "# spark.sql(f\"DROP VIEW IF EXISTS {CUSTOMER_ORDERS_VIEW}\")\n",
    "# spark.sql(f\"DROP VIEW IF EXISTS {TOP_CUSTOMERS_VIEW}\")\n",
    "# spark.sql(f\"DROP VIEW IF EXISTS {PRODUCT_SALES_VIEW}\")\n",
    "\n",
    "# Usuń widoki tymczasowe\n",
    "# spark.catalog.dropTempView(\"tmp_customers\")\n",
    "# spark.catalog.dropGlobalTempView(\"gtmp_orders\")\n",
    "\n",
    "# Usuń widgety\n",
    "dbutils.widgets.removeAll()\n",
    "\n",
    "print(\"Zasoby wyczyszczone!\")\n",
    "print(\"Warsztat zakończony!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
