{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23cffde0-db08-4560-8121-7f59b784a13f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Optymalizacja i najlepsze praktyki\n",
    "\n",
    "**Cel szkoleniowy:** Opanowanie technik optymalizacji performance'u zapyta≈Ñ i tabel Delta w Databricks.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Optymalizacja zapyta≈Ñ: predicate pushdown, file pruning, column pruning\n",
    "- Analiza planu fizycznego (explain())\n",
    "- Optymalizacja tabel: partitioning, small files problem\n",
    "- Auto optimize / auto compaction\n",
    "- Dob√≥r rozmiaru plik√≥w i strategii ZORDER\n",
    "- Liquid Clustering - nowoczesna alternatywa dla partycjonowania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dfc4662-9aaa-4a92-8d26-2ebb05cfa80c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzie≈Ñ szkolenia**: Dzie≈Ñ 2 - Delta Lake & Lakehouse\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 16.4 LTS lub nowszy (zalecane: 17.3 LTS)\n",
    "  - Unity Catalog w≈ÇƒÖczony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers lub **Serverless Compute** (zalecane)\n",
    "- **Zale≈ºno≈õci**: Wykonany notebook `01_delta_lake_operations.ipynb`\n",
    "- **Czas realizacji**: ~45 minut\n",
    "\n",
    "> **Uwaga (2025):** Serverless Compute jest teraz domy≈õlnym trybem dla nowych workload√≥w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9200a175-242b-4dc7-acd4-288781f4e3df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wstƒôp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie kluczowych mechanizm√≥w optymalizacji w Databricks i Delta Lake.\n",
    "\n",
    "**Typy optymalizacji:**\n",
    "\n",
    "**1. Optymalizacja zapyta≈Ñ (Query Optimization):**\n",
    "- **Predicate Pushdown**: Przeniesienie filtr√≥w jak najni≈ºej w planie wykonania\n",
    "- **Column Pruning**: Odczyt tylko wymaganych kolumn (kolumnowy format Parquet)\n",
    "- **File Pruning**: Ominiƒôcie plik√≥w nieistotnych dla zapytania\n",
    "- **Join Optimization**: Broadcast joins, bucket joins, sortmerge joins\n",
    "\n",
    "**2. Optymalizacja tabel (Table Optimization):**\n",
    "- **Partitioning**: Fizyczne rozdzielenie danych wed≈Çug kluczy\n",
    "- **Z-Ordering**: Klasterowanie danych w plikach wed≈Çug wybranych kolumn\n",
    "- **Compaction (OPTIMIZE)**: ≈ÅƒÖczenie ma≈Çych plik√≥w w wiƒôksze\n",
    "- **Auto Compaction**: Automatyczne ≈ÇƒÖczenie podczas zapisu\n",
    "\n",
    "**3. Small Files Problem:**\n",
    "Problem wydajno≈õci wywo≈Çany przez zbyt wiele ma≈Çych plik√≥w w tabeli. Spark preferuje pliki 128MB-1GB dla optymalnej wydajno≈õci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ee9b43-acbf-4216-a499-8bea3d48739b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Izolacja per u≈ºytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalog√≥w i schemat√≥w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a250c520-ae2e-4004-b2d6-f2994e411e6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6163cff-16e5-44b6-9cce-a3bf932fe679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych ≈õrodowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a0cc155-45ac-4679-bf40-0efeb89e7b09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit, count, avg, sum, max, min\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# Ustaw katalog i schemat jako domy≈õlne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kontekst u≈ºytkownika:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"CATALOG\", CATALOG),\n",
    "        (\"BRONZE_SCHEMA\", BRONZE_SCHEMA),\n",
    "        (\"SILVER_SCHEMA\", SILVER_SCHEMA),\n",
    "        (\"GOLD_SCHEMA\", GOLD_SCHEMA),\n",
    "        (\"USER\", raw_user)\n",
    "    ], [\"Variable\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 0: Przygotowanie danych\n",
    "\n",
    "Ten notebook jest **w pe≈Çni niezale≈ºny** - sam ≈Çaduje dane ≈∫r√≥d≈Çowe i tworzy tabele potrzebne do demo optymalizacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ≈öcie≈ºki do danych ≈∫r√≥d≈Çowych\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "# Nazwy tabel (unikalne dla tego notebooka)\n",
    "ORDERS_OPT = f\"{BRONZE_SCHEMA}.orders_optimization\"\n",
    "CUSTOMERS_OPT = f\"{BRONZE_SCHEMA}.customers_optimization\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wczytanie danych ≈∫r√≥d≈Çowych:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytaj customers\n",
    "customers_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(CUSTOMERS_CSV)\n",
    "\n",
    "# Wczytaj orders\n",
    "orders_df = spark.read.json(ORDERS_JSON)\n",
    "\n",
    "# Dodaj kolumnƒô order_date (data bez czasu) dla partycjonowania\n",
    "orders_df = orders_df.withColumn(\n",
    "    \"order_date\", \n",
    "    F.to_date(F.col(\"order_datetime\"))\n",
    ")\n",
    "\n",
    "print(f\"‚úì Customers: {customers_df.count()} rekord√≥w\")\n",
    "print(f\"‚úì Orders: {orders_df.count()} rekord√≥w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zapisz jako tabele Delta do optymalizacji:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapisz jako tabele Delta\n",
    "customers_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(CUSTOMERS_OPT)\n",
    "\n",
    "orders_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(ORDERS_OPT)\n",
    "\n",
    "display(spark.createDataFrame([\n",
    "    (\"CUSTOMERS_OPT\", CUSTOMERS_OPT, str(customers_df.count())),\n",
    "    (\"ORDERS_OPT\", ORDERS_OPT, str(orders_df.count()))\n",
    "], [\"Tabela\", \"Full Name\", \"Rekordy\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd499c1-678e-463f-9081-7ef9b14301c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 1: Analiza planu fizycznego (explain())\n",
    "\n",
    "**Cel sekcji:** Naucz siƒô analizowaƒá plany wykonania zapyta≈Ñ, aby zidentyfikowaƒá bottlenecki wydajno≈õciowe.\n",
    "\n",
    "**Teoria:**\n",
    "Plan fizyczny to szczeg√≥≈Çowa mapa tego, jak Spark wykonuje zapytanie:\n",
    "- **Stages**: Logiczne etapy przetwarzania\n",
    "- **Tasks**: Jednostki pracy wykonywane na partycjach\n",
    "- **Shuffles**: Wymiana danych miƒôdzy executor'ami\n",
    "- **Pushdowns**: Optymalizacje przeniesione do ≈∫r√≥d≈Ça danych\n",
    "\n",
    "**Rodzaje explain():**\n",
    "- `explain()` - podstawowy plan\n",
    "- `explain(True)` - pe≈Çny plan z detalami\n",
    "- `explain('extended')` - rozszerzony plan\n",
    "- `explain('cost')` - plan z kosztem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "918895ad-596e-4f57-bdb7-05d587e80952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 1.1: Analiza planu prostego zapytania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6531acdf-98d5-45d5-a312-5f369f2fa2ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.1 - Analiza planu prostego zapytania\n",
    "\n",
    "simple_query = spark.sql(f\"\"\"\n",
    "    SELECT customer_id, first_name, last_name, customer_segment, city\n",
    "    FROM {CUSTOMERS_OPT}\n",
    "    WHERE customer_segment = 'Premium'\n",
    "    ORDER BY customer_id DESC\n",
    "    LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Podstawowy plan zapytania:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_query.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rozszerzony plan zapytania (z detalami):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_query.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d9af332-8c17-4825-9496-b9029a9421ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 1.2: Predicate Pushdown w praktyce\n",
    "\n",
    "**Teoria:** Predicate pushdown to optymalizacja, gdzie filtry (warunki WHERE) sƒÖ \"przepychane\" jak najni≈ºej w planie wykonania, najlepiej do poziomu czytania plik√≥w. Dziƒôki temu czytamy tylko dane, kt√≥re spe≈ÇniajƒÖ warunki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73394fb4-ff75-4f2d-aae5-1f68db484ec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.2 - Predicate Pushdown\n",
    "\n",
    "filtered_query = spark.sql(f\"\"\"\n",
    "    SELECT order_id, customer_id, total_amount, order_date\n",
    "    FROM {ORDERS_OPT}\n",
    "    WHERE total_amount > 100 \n",
    "    AND order_date >= '2024-01-01'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sprawd≈∫ plan - poszukaj \"PushedFilters\" w planie:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawd≈∫ plan - poszukaj \"PushedFilters\" w planie\n",
    "filtered_query.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° W planie szukaj:**\n",
    "- `PushedFilters` - filtry przepchniƒôte do poziomu czytania\n",
    "- `ReadSchema` - tylko wybrane kolumny (column pruning)  \n",
    "- `PartitionFilters` - filtry na partycjach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a51281f-4f44-4ea6-82f2-996f7edaee07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 2: Strategia partycjonowania\n",
    "\n",
    "**Cel sekcji:** Nauka wyboru optymalnych kluczy partycjonowania dla najlepszej wydajno≈õci.\n",
    "\n",
    "**Teoria partycjonowania:**\n",
    "- **Partitioning**: Fizyczne rozdzielenie tabeli na katalogi wed≈Çug warto≈õci kolumn\n",
    "- **Partition Pruning**: Spark pomija ca≈Çe partycje, kt√≥re nie sƒÖ potrzebne dla zapytania\n",
    "- **Idealne partycje**: 1-10GB danych na partycjƒô, nie wiƒôcej ni≈º 10,000 partycji\n",
    "\n",
    "**Najlepsze praktyki:**\n",
    "- Partycjonuj wed≈Çug kolumn czƒôsto u≈ºywanych w filtrach\n",
    "- Unikaj partycjonowania wed≈Çug kolumn o wysokiej kardinalno≈õci\n",
    "- Preferuj kolumny z naturalnƒÖ hierarchiƒÖ czasowƒÖ (rok/miesiƒÖc/dzie≈Ñ)\n",
    "- Unikaj zbyt wielu ma≈Çych partycji (small files problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0665d14-7df4-4304-b215-ad5531b8a72a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 2.1: Tworzenie tabeli partycjonowanej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00e19ec-cf42-486f-be1f-0082e1e602d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 2.1 - Tworzenie tabeli partycjonowanej\n",
    "\n",
    "# Utw√≥rz tabelƒô partycjonowanƒÖ wed≈Çug roku i miesiƒÖca\n",
    "ORDERS_PARTITIONED = f\"{BRONZE_SCHEMA}.orders_opt_partitioned\"\n",
    "\n",
    "# Dodaj kolumny do partycjonowania\n",
    "orders_with_partitions = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        *,\n",
    "        YEAR(order_date) as year,\n",
    "        MONTH(order_date) as month\n",
    "    FROM {ORDERS_OPT}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zapisz jako tabelƒô partycjonowanƒÖ:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_with_partitions.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .saveAsTable(ORDERS_PARTITIONED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sprawd≈∫ strukturƒô partycji:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.sql(f\"DESCRIBE DETAIL {ORDERS_PARTITIONED}\")\n",
    "    .select(\"name\", \"location\", \"partitionColumns\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ef8762-1a93-41d5-83af-0545643857ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 2.2: Partition Pruning w dzia≈Çaniu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e30be36f-2a13-44f8-85a8-05c76566b45a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 2.2 - Partition Pruning\n",
    "\n",
    "# Zapytanie kt√≥ry wykorzystuje partycje (rok/miesiƒÖc)\n",
    "efficient_query = spark.sql(f\"\"\"\n",
    "    SELECT order_id, customer_id, total_amount, order_date\n",
    "    FROM {ORDERS_PARTITIONED}\n",
    "    WHERE year = 2024 AND month = 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sprawd≈∫ plan - partition pruning w dzia≈Çaniu:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawd≈∫ plan - poszukaj \"PartitionFilters\"\n",
    "efficient_query.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Por√≥wnanie: zapytanie BEZ partition pruning:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapytanie kt√≥re nie wykorzystuje partycji (nie filtruje po roku/miesiƒÖcu)\n",
    "inefficient_query = spark.sql(f\"\"\"\n",
    "    SELECT order_id, customer_id, total_amount, order_date\n",
    "    FROM {ORDERS_PARTITIONED}\n",
    "    WHERE customer_id = 1\n",
    "\"\"\")\n",
    "\n",
    "inefficient_query.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db85bc0-8aa8-4630-9e18-69e5530fea96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 3: Small Files Problem\n",
    "\n",
    "**Cel sekcji:** Zrozumienie i rozwiƒÖzanie problemu ma≈Çych plik√≥w w Delta Lake.\n",
    "\n",
    "**Co to jest Small Files Problem?**\n",
    "- Gdy tabela ma zbyt wiele ma≈Çych plik√≥w (< 128MB ka≈ºdy)\n",
    "- Spark preferuje pliki 128MB-1GB dla optymalnej wydajno≈õci\n",
    "- Ma≈Çe pliki powodujƒÖ overhead w metadanych i zmniejszajƒÖ throughput\n",
    "\n",
    "**Przyczyny ma≈Çych plik√≥w:**\n",
    "- Czƒôste zapisy INSERT w ma≈Çych batch'ach\n",
    "- Wysokie partycjonowanie z ma≈ÇƒÖ ilo≈õciƒÖ danych na partycjƒô\n",
    "- Streaming z kr√≥tkimi trigger intervals\n",
    "\n",
    "**RozwiƒÖzania:**\n",
    "- **OPTIMIZE** - ≈ÇƒÖczy ma≈Çe pliki w wiƒôksze\n",
    "- **Auto Compaction** - automatyczne ≈ÇƒÖczenie podczas zapisu\n",
    "- **Repartition** przed zapisem\n",
    "- **Coalesce** dla zmniejszenia liczby partycji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "965ad450-5f1a-4e33-ac31-ba922037d071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 3.1: Symulacja Small Files Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22a608c1-9ec7-43a3-9b0b-abfd0658aebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 3.1 - Symulacja Small Files Problem\n",
    "\n",
    "SMALL_FILES_TABLE = f\"{BRONZE_SCHEMA}.small_files_demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Symuluj wiele ma≈Çych zapis√≥w (ka≈ºdy tworzy osobny plik):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    small_batch = spark.range(i*100, (i+1)*100).select(\n",
    "        col(\"id\"),\n",
    "        (col(\"id\") * 2).alias(\"value\"),\n",
    "        lit(f\"batch_{i}\").alias(\"batch_name\")\n",
    "    )\n",
    "    \n",
    "    small_batch.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(SMALL_FILES_TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sprawd≈∫ liczbƒô plik√≥w:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail = spark.sql(f\"DESCRIBE DETAIL {SMALL_FILES_TABLE}\").collect()[0]\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Liczba plik√≥w\", str(detail['numFiles'])),\n",
    "        (\"Rozmiar tabeli\", f\"{detail['sizeInBytes']} bytes\"),\n",
    "        (\"≈öredni rozmiar pliku\", f\"{detail['sizeInBytes'] / detail['numFiles']:.0f} bytes\"),\n",
    "        (\"Status\", \"‚ö†Ô∏è Problem: zbyt wiele ma≈Çych plik√≥w!\")\n",
    "    ], [\"Metric\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d568f02-9857-4fa2-bbdd-ef467fa13477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 3.2: RozwiƒÖzanie - OPTIMIZE i Auto Compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d3c587-4607-41fa-b0db-3e1ab0774a60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 3.2 - RozwiƒÖzanie Small Files Problem\n",
    "\n",
    "# Wykonaj OPTIMIZE na tabeli z ma≈Çymi plikami\n",
    "spark.sql(f\"OPTIMIZE {SMALL_FILES_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sprawd≈∫ stan po OPTIMIZE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detail_after = spark.sql(f\"DESCRIBE DETAIL {SMALL_FILES_TABLE}\").collect()[0]\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Liczba plik√≥w (PO OPTIMIZE)\", str(detail_after['numFiles'])),\n",
    "        (\"Rozmiar tabeli\", f\"{detail_after['sizeInBytes']} bytes\"),\n",
    "        (\"≈öredni rozmiar pliku\", f\"{detail_after['sizeInBytes'] / detail_after['numFiles']:.0f} bytes\")\n",
    "    ], [\"Metric\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**W≈ÇƒÖcz Auto Compaction dla przysz≈Çych zapis√≥w:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {SMALL_FILES_TABLE}\n",
    "    SET TBLPROPERTIES (\n",
    "        'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "        'delta.autoOptimize.autoCompact' = 'true'\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sprawd≈∫ w≈ÇƒÖczone w≈Ça≈õciwo≈õci Auto Compaction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "properties = spark.sql(f\"SHOW TBLPROPERTIES {SMALL_FILES_TABLE}\").collect()\n",
    "auto_props = [(p['key'], p['value']) for p in properties if 'autoOptimize' in p['key']]\n",
    "\n",
    "display(spark.createDataFrame(auto_props, [\"Property\", \"Value\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 3.3: VACUUM - Usuwanie starych plik√≥w\n",
    "\n",
    "**Teoria:**\n",
    "VACUUM usuwa stare pliki, kt√≥re nie sƒÖ ju≈º potrzebne (po operacjach DELETE, UPDATE, MERGE, OPTIMIZE). \n",
    "Domy≈õlnie Delta Lake zachowuje pliki przez 7 dni (168 godzin) dla Time Travel.\n",
    "\n",
    "**‚ö†Ô∏è Uwaga:** Po VACUUM nie mo≈ºna u≈ºywaƒá Time Travel do wersji wcze≈õniejszych ni≈º retention period!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawd≈∫ ile plik√≥w mo≈ºna usunƒÖƒá (DRY RUN)\n",
    "spark.sql(f\"VACUUM {SMALL_FILES_TABLE} DRY RUN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wykonaj VACUUM (usu≈Ñ stare pliki):**\n",
    "\n",
    "> **Uwaga:** W produkcji u≈ºywaj domy≈õlnego retention (7 dni). Poni≈ºszy kod z `RETAIN 0 HOURS` jest tylko do demo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wykonaj VACUUM - usu≈Ñ stare pliki\n",
    "# W ≈õrodowisku demo wy≈ÇƒÖczamy sprawdzenie retention\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "# VACUUM z kr√≥tkim retention (TYLKO DO DEMO!)\n",
    "spark.sql(\"\"\"\n",
    "    VACUUM orders_opt RETAIN 1 HOURS\n",
    "\"\"\")\n",
    "\n",
    "# Przywr√≥ƒá domy≈õlne ustawienie\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n",
    "\n",
    "print(\"‚úÖ VACUUM wykonany - stare pliki usuniƒôte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c75d4e-fc27-4cfc-b053-d93b48b5baef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 4: ZORDER BY - Advanced Clustering\n",
    "\n",
    "**Cel sekcji:** Nauka wykorzystania ZORDER BY do optymalizacji zapyta≈Ñ z filtrami i joinami.\n",
    "\n",
    "**Co to jest ZORDER BY?**\n",
    "- Multi-dimensional clustering algorithm w Delta Lake\n",
    "- Organizuje dane w plikach wed≈Çug warto≈õci wybranych kolumn\n",
    "- Poprawia data skipping - pomijanie niepotrzebnych plik√≥w podczas czytania\n",
    "- Szczeg√≥lnie skuteczny dla kolumn czƒôsto u≈ºywanych w filtrach WHERE i JOIN\n",
    "\n",
    "**Kiedy u≈ºywaƒá ZORDER:**\n",
    "- Kolumny czƒôsto filtrowane w zapytaniach\n",
    "- Kolumny u≈ºywane w JOIN operations\n",
    "- High-cardinality columns (wiele unikalnych warto≈õci)\n",
    "- Maksymalnie 3-4 kolumny (wiƒôcej = diminishing returns)\n",
    "\n",
    "**ZORDER vs Partitioning:**\n",
    "- Partitioning: fizyczne rozdzielenie na katalogi\n",
    "- ZORDER: logiczne uporzƒÖdkowanie w plikach (zachowuje pojedynczƒÖ strukturƒô folder√≥w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da4d32df-ebbe-427d-ba8c-720d1006449c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 4.1: ZORDER BY dla czƒôsto filtrowanych kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c5e7c6-4377-40d4-9e18-46ff53c6a9e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wykonaj ZORDER BY na najczƒô≈õciej filtrowanych kolumnach\n",
    "spark.sql(f\"\"\"\n",
    "    OPTIMIZE {ORDERS_OPT}\n",
    "    ZORDER BY (customer_id, order_date)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6cc295-74eb-4af8-b88c-07402191fa62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 4.2: Pomiar skuteczno≈õci ZORDER - Data Skipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc56cf1a-23cd-4ede-aa44-64333b3af744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 4.2 - Pomiar skuteczno≈õci ZORDER\n",
    "\n",
    "import time\n",
    "\n",
    "# Zapytanie wykorzystujƒÖce kolumny z ZORDER\n",
    "# customer_id to STRING (np. CUST000123), order_date to DATE\n",
    "test_query = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as cnt, AVG(total_amount) as avg_amount\n",
    "    FROM {ORDERS_OPT}\n",
    "    WHERE customer_id BETWEEN 'CUST000100' AND 'CUST000500'\n",
    "    AND order_date >= '2024-06-01'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pomiar czasu wykonania:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "result = test_query.collect()\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Wynik\", str(result[0])),\n",
    "        (\"Czas wykonania\", f\"{elapsed:.2f}s\")\n",
    "    ], [\"Metric\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plan zapytania - sprawd≈∫ data skipping:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawd≈∫ plan zapytania - data skipping\n",
    "test_query.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üí° W planie szukaj:**\n",
    "- `numFilesTotal` vs `numFilesSelected` - ile plik√≥w pominiƒôto\n",
    "- `metadata time` - czas parsowania metadanych\n",
    "- `files pruned` - data skipping statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sekcja 5: Liquid Clustering - Przysz≈Ço≈õƒá optymalizacji\n",
    "\n",
    "**Cel sekcji:** Poznanie Liquid Clustering - nowoczesnej techniki zastƒôpujƒÖcej Hive Partitioning i ZORDER.\n",
    "\n",
    "**Co to jest Liquid Clustering?**\n",
    "To elastyczny mechanizm uk≈Çadania danych, kt√≥ry:\n",
    "- Nie wymaga sztywnej struktury katalog√≥w (jak Partitioning)\n",
    "- Pozwala na zmianƒô kluczy klastrowania bez przepisywania ca≈Çej tabeli\n",
    "- Eliminuje problem \"Small Files\" zwiƒÖzany z nadmiernym partycjonowaniem\n",
    "- Dzia≈Ça inkrementalnie (nie trzeba optymalizowaƒá ca≈Çej tabeli na raz)\n",
    "\n",
    "**Kiedy u≈ºywaƒá?**\n",
    "- Zamiast partycjonowania dla wiƒôkszo≈õci nowych tabel\n",
    "- Gdy klucze partycjonowania majƒÖ wysokƒÖ kardynalno≈õƒá\n",
    "- Gdy wzorce zapyta≈Ñ zmieniajƒÖ siƒô w czasie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LIQUID_TABLE = f\"{BRONZE_SCHEMA}.orders_opt_liquid\"\n",
    "\n",
    "# Tworzymy tabelƒô u≈ºywajƒÖc CLUSTER BY zamiast PARTITIONED BY\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {LIQUID_TABLE}\n",
    "CLUSTER BY (customer_id, order_date)\n",
    "AS SELECT * FROM {ORDERS_OPT}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sprawd≈∫ w≈Ça≈õciwo≈õci tabeli:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawd≈∫ w≈Ça≈õciwo≈õci tabeli\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {LIQUID_TABLE}\").select(\"name\", \"clusteringColumns\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 5.2: Inkrementalna optymalizacja\n",
    "\n",
    "**Teoria:**\n",
    "W przeciwie≈Ñstwie do ZORDER, kt√≥ry musi przeliczyƒá ca≈ÇƒÖ partycjƒô/tabelƒô, Liquid Clustering dzia≈Ça inkrementalnie. `OPTIMIZE` uporzƒÖdkuje tylko te dane, kt√≥re tego wymagajƒÖ (np. nowo dodane), co oszczƒôdza czas i zasoby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uruchom OPTIMIZE - Liquid Clustering wie jak uk≈Çadaƒá dane na podstawie definicji tabeli\n",
    "spark.sql(f\"OPTIMIZE {LIQUID_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sprawd≈∫ historiƒô, aby zobaczyƒá operacjƒô CLUSTERING:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.sql(f\"DESCRIBE HISTORY {LIQUID_TABLE}\")\n",
    "    .select(\"version\", \"operation\", \"operationParameters\")\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Por√≥wnanie: Liquid Clustering vs Partitioning + ZORDER\n",
    "\n",
    "| Cecha | Partitioning + ZORDER | Liquid Clustering |\n",
    "|-------|-----------------------|-------------------|\n",
    "| **Konfiguracja** | Wymaga starannego doboru kolumn partycjonowania | Elastyczne `CLUSTER BY` |\n",
    "| **Small Files** | Ryzyko przy nadmiernym partycjonowaniu | Automatycznie zarzƒÖdzane |\n",
    "| **Zmiana klucza** | Trudna (wymaga przepisania tabeli) | ≈Åatwa (`ALTER TABLE CLUSTER BY`) |\n",
    "| **Optymalizacja** | `OPTIMIZE ZORDER BY` (kosztowne) | `OPTIMIZE` (inkrementalne) |\n",
    "| **Skew Data** | Podatne na data skew | Odporne na data skew |\n",
    "\n",
    "**Rekomendacja:** U≈ºywaj Liquid Clustering dla wszystkich nowych tabel w Databricks Runtime 13.3+, chyba ≈ºe masz specyficzny pow√≥d, by u≈ºywaƒá partycjonowania (np. kompatybilno≈õƒá ze starszymi czytnikami)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1ca93de-4605-4f85-8401-a40985237e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices - Przewodnik optymalizacji\n",
    "\n",
    "### üéØ Strategia optymalizacji (w kolejno≈õci priorytet√≥w):\n",
    "\n",
    "**1. Analiza workload:**\n",
    "- Zidentyfikuj najczƒô≈õciej wykonywane zapytania\n",
    "- Znajd≈∫ kolumny najczƒô≈õciej u≈ºywane w filtrach WHERE\n",
    "- Sprawd≈∫ kt√≥re zapytania zajmujƒÖ najwiƒôcej czasu\n",
    "\n",
    "**2. Optymalizacja zapyta≈Ñ:**\n",
    "- U≈ºywaj filtr√≥w WHERE jak najwcze≈õniej w zapytaniu\n",
    "- Wybieraj tylko potrzebne kolumny (SELECT specific columns, nie *)\n",
    "- Preferuj JOIN na zindeksowanych kolumnach\n",
    "- U≈ºywaj LIMIT gdy mo≈ºliwe\n",
    "\n",
    "**3. Optymalizacja tabel:**\n",
    "- **Partitioning**: Tylko dla du≈ºych tabel (>1TB) i czƒôsto filtrowanych kolumn\n",
    "- **ZORDER BY**: Dla 2-4 najczƒô≈õciej filtrowanych kolumn\n",
    "- **OPTIMIZE**: Regularnie (np. daily) dla aktywnych tabel\n",
    "- **Auto Compaction**: W≈ÇƒÖcz dla tabel z czƒôstymi zapisami\n",
    "\n",
    "**4. Monitoring i maintenance:**\n",
    "- Regularnie sprawdzaj `DESCRIBE DETAIL` - liczba plik√≥w, rozmiar\n",
    "- Uruchamiaj `VACUUM` co najmniej raz w tygodniu\n",
    "- Monitoruj Spark UI dla d≈Çugo dzia≈ÇajƒÖcych zapyta≈Ñ\n",
    "- U≈ºywaj `explain()` do analizy problemowych zapyta≈Ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f7365e-c56b-49d8-8b5c-f76a13678b8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Troubleshooting - Diagnoza problem√≥w wydajno≈õciowych\n",
    "\n",
    "### üîç Najczƒôstsze problemy i rozwiƒÖzania:\n",
    "\n",
    "**Problem 1: Zapytanie dzia≈Ça bardzo wolno**\n",
    "```python\n",
    "# Diagnoza:\n",
    "df.explain(True)  # Sprawd≈∫ plan wykonania\n",
    "```\n",
    "**Mo≈ºliwe przyczyny:**\n",
    "- Brak filtr√≥w - czyta ca≈ÇƒÖ tabelƒô\n",
    "- Shuffle operations - du≈ºo ruchu sieciowego  \n",
    "- Skewed data - nier√≥wnomierne roz≈Ço≈ºenie danych\n",
    "- Small files - zbyt wiele ma≈Çych plik√≥w\n",
    "\n",
    "**Problem 2: \"OutOfMemoryError\" podczas JOIN**\n",
    "**RozwiƒÖzanie:**\n",
    "```python\n",
    "# Zwiƒôksz partycje przed JOIN\n",
    "df1 = df1.repartition(200, \"join_key\")\n",
    "df2 = df2.repartition(200, \"join_key\")\n",
    "\n",
    "# Lub u≈ºyj broadcast join dla ma≈Çych tabel\n",
    "from pyspark.sql.functions import broadcast\n",
    "result = large_df.join(broadcast(small_df), \"key\")\n",
    "```\n",
    "\n",
    "**Problem 3: D≈Çugie czasy zapisu do Delta**\n",
    "**RozwiƒÖzanie:**\n",
    "- W≈ÇƒÖcz Auto Compaction\n",
    "- U≈ºyj `coalesce()` przed zapisem\n",
    "- Avoid zbyt wysokie partycjonowanie\n",
    "\n",
    "**Problem 4: OPTIMIZE nie poprawia wydajno≈õci**\n",
    "**Przyczyna:** ZORDER BY jest potrzebny dla specific query patterns\n",
    "```python\n",
    "# Zamiast samego OPTIMIZE:\n",
    "OPTIMIZE table_name\n",
    "\n",
    "# U≈ºyj OPTIMIZE z ZORDER:\n",
    "OPTIMIZE table_name ZORDER BY (frequently_filtered_columns)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ca92322-c50a-41aa-8a24-007833c526ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Podsumowanie\n",
    "\n",
    "### Co osiƒÖgnƒôli≈õmy:\n",
    "\n",
    "- **Analiza wydajno≈õci**: Czytanie i interpretacja plan√≥w fizycznych z `explain()`\n",
    "- **Predicate Pushdown**: Identyfikacja bottleneck√≥w i pushed filters\n",
    "- **Partycjonowanie**: Strategia partycjonowania wed≈Çug czƒôsto filtrowanych kolumn\n",
    "- **ZORDER BY**: Multi-dimensional clustering dla 2-4 kolumn\n",
    "- **Small Files Problem**: RozwiƒÖzywanie przez OPTIMIZE i Auto Compaction\n",
    "- **Liquid Clustering**: Nowoczesna alternatywa dla partycjonowania\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "\n",
    "| # | Zasada |\n",
    "|---|--------|\n",
    "| 1 | **Analiza przed optymalizacjƒÖ** - zawsze najpierw `explain()` |\n",
    "| 2 | **Partitioning ‚â† ZORDER** - r√≥≈ºne techniki dla r√≥≈ºnych przypadk√≥w |\n",
    "| 3 | **Small files = performance killer** - regularne OPTIMIZE |\n",
    "| 4 | **ZORDER BY** - maksymalnie 3-4 kolumny, wybieraj najczƒô≈õciej filtrowane |\n",
    "| 5 | **Liquid Clustering** - preferuj dla nowych tabel w DBR 13.3+ |\n",
    "\n",
    "### Metryki do monitorowania:\n",
    "\n",
    "| Metryka | Dobra warto≈õƒá | Akcja je≈õli przekroczona |\n",
    "|---------|---------------|--------------------------|\n",
    "| Liczba plik√≥w | < 1000/TB | OPTIMIZE |\n",
    "| ≈öredni rozmiar pliku | 128MB-1GB | OPTIMIZE + Auto Compaction |\n",
    "| Skipped files ratio | >80% | ZORDER BY |\n",
    "\n",
    "### Nastƒôpne kroki:\n",
    "\n",
    "üìö **Kolejny dzie≈Ñ:** DZIEN_3 - Transformacje i Governance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d22f78-2445-487b-931a-e6eb94f7209c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Opcjonalnie usu≈Ñ tabele demo utworzone podczas ƒáwicze≈Ñ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b1ec66-98c3-49a5-a03a-8a180a2958a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup - usu≈Ñ tabele demo utworzone w tym notebooku\n",
    "\n",
    "# Odkomentuj poni≈ºsze linie aby usunƒÖƒá tabele demo:\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {ORDERS_OPT}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CUSTOMERS_OPT}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {ORDERS_PARTITIONED}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {SMALL_FILES_TABLE}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {LIQUID_TABLE}\")\n",
    "\n",
    "# print(\"‚úÖ Wszystkie tabele demo usuniƒôte\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è Cleanup wy≈ÇƒÖczony (odkomentuj kod aby usunƒÖƒá tabele demo)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_optimization_best_practices",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
