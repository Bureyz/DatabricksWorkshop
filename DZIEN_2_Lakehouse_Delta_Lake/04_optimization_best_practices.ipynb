{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23cffde0-db08-4560-8121-7f59b784a13f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Optymalizacja i najlepsze praktyki\n",
    "\n",
    "**Cel szkoleniowy:** Opanowanie technik optymalizacji performance'u zapyta≈Ñ i tabel Delta w Databricks.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Optymalizacja zapyta≈Ñ: predicate pushdown, file pruning, column pruning\n",
    "- Analiza planu fizycznego (explain())\n",
    "- Optymalizacja tabel: partitioning, small files problem\n",
    "- Auto optimize / auto compaction\n",
    "- Dob√≥r rozmiaru plik√≥w i strategii ZORDER\n",
    "- Liquid Clustering - nowoczesna alternatywa dla partycjonowania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dfc4662-9aaa-4a92-8d26-2ebb05cfa80c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzie≈Ñ szkolenia**: Dzie≈Ñ 2 - Delta Lake & Lakehouse\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 16.4 LTS lub nowszy (zalecane: 17.3 LTS)\n",
    "  - Unity Catalog w≈ÇƒÖczony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers lub **Serverless Compute** (zalecane)\n",
    "- **Zale≈ºno≈õci**: Wykonany notebook `01_delta_lake_operations.ipynb`\n",
    "- **Czas realizacji**: ~45 minut\n",
    "\n",
    "> **Uwaga (2025):** Serverless Compute jest teraz domy≈õlnym trybem dla nowych workload√≥w."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9200a175-242b-4dc7-acd4-288781f4e3df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wstƒôp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie kluczowych mechanizm√≥w optymalizacji w Databricks i Delta Lake.\n",
    "\n",
    "**Typy optymalizacji:**\n",
    "\n",
    "**1. Optymalizacja zapyta≈Ñ (Query Optimization):**\n",
    "- **Predicate Pushdown**: Przeniesienie filtr√≥w jak najni≈ºej w planie wykonania\n",
    "- **Column Pruning**: Odczyt tylko wymaganych kolumn (kolumnowy format Parquet)\n",
    "- **File Pruning**: Ominiƒôcie plik√≥w nieistotnych dla zapytania\n",
    "- **Join Optimization**: Broadcast joins, bucket joins, sortmerge joins\n",
    "\n",
    "**2. Optymalizacja tabel (Table Optimization):**\n",
    "- **Partitioning**: Fizyczne rozdzielenie danych wed≈Çug kluczy\n",
    "- **Z-Ordering**: Klasterowanie danych w plikach wed≈Çug wybranych kolumn\n",
    "- **Compaction (OPTIMIZE)**: ≈ÅƒÖczenie ma≈Çych plik√≥w w wiƒôksze\n",
    "- **Auto Compaction**: Automatyczne ≈ÇƒÖczenie podczas zapisu\n",
    "\n",
    "**3. Small Files Problem:**\n",
    "Problem wydajno≈õci wywo≈Çany przez zbyt wiele ma≈Çych plik√≥w w tabeli. Spark preferuje pliki 128MB-1GB dla optymalnej wydajno≈õci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ee9b43-acbf-4216-a499-8bea3d48739b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Izolacja per u≈ºytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalog√≥w i schemat√≥w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a250c520-ae2e-4004-b2d6-f2994e411e6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6163cff-16e5-44b6-9cce-a3bf932fe679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych ≈õrodowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a0cc155-45ac-4679-bf40-0efeb89e7b09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit, count, avg, sum, max, min\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# Ustaw katalog i schemat jako domy≈õlne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f62a5d56-fc77-4c1e-8241-7a415ab5c300",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Kontekst u≈ºytkownika:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59a7d3f9-953e-4173-b17e-d19c44d28cc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"CATALOG\", CATALOG),\n",
    "        (\"BRONZE_SCHEMA\", BRONZE_SCHEMA),\n",
    "        (\"SILVER_SCHEMA\", SILVER_SCHEMA),\n",
    "        (\"GOLD_SCHEMA\", GOLD_SCHEMA),\n",
    "        (\"USER\", raw_user)\n",
    "    ], [\"Variable\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b34e9e2-0bd9-43eb-b563-02e892f6494d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 0: Przygotowanie danych\n",
    "\n",
    "Ten notebook jest **w pe≈Çni niezale≈ºny** - sam ≈Çaduje dane ≈∫r√≥d≈Çowe i tworzy tabele potrzebne do demo optymalizacji."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a79c716-353d-451d-bdc9-42f36aafb449",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ≈öcie≈ºki do danych ≈∫r√≥d≈Çowych\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "# Nazwy tabel (unikalne dla tego notebooka)\n",
    "ORDERS_OPT = f\"{BRONZE_SCHEMA}.orders_optimization\"\n",
    "CUSTOMERS_OPT = f\"{BRONZE_SCHEMA}.customers_optimization\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3adb149a-7307-4539-ad3c-4a94df3fc737",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wczytanie danych ≈∫r√≥d≈Çowych:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92f2390e-57c5-4742-bccd-a646584b145f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wczytaj customers\n",
    "customers_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(CUSTOMERS_CSV)\n",
    "\n",
    "# Wczytaj orders\n",
    "orders_df = spark.read.json(ORDERS_JSON)\n",
    "\n",
    "# Dodaj kolumnƒô order_date (data bez czasu) dla partycjonowania\n",
    "orders_df = orders_df.withColumn(\n",
    "    \"order_date\", \n",
    "    F.to_date(F.col(\"order_datetime\"))\n",
    ")\n",
    "\n",
    "print(f\"‚úì Customers: {customers_df.count()} rekord√≥w\")\n",
    "print(f\"‚úì Orders: {orders_df.count()} rekord√≥w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cf6b602-eab0-4ddb-ab01-af38e593fb4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Zapisz jako tabele Delta do optymalizacji:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2614896d-055e-41ad-8e08-0fff2ba70486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zapisz jako tabele Delta\n",
    "customers_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(CUSTOMERS_OPT)\n",
    "\n",
    "orders_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(ORDERS_OPT)\n",
    "\n",
    "display(spark.createDataFrame([\n",
    "    (\"CUSTOMERS_OPT\", CUSTOMERS_OPT, str(customers_df.count())),\n",
    "    (\"ORDERS_OPT\", ORDERS_OPT, str(orders_df.count()))\n",
    "], [\"Tabela\", \"Full Name\", \"Rekordy\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "676205fd-452f-43c1-a452-8072b6513717",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%skip\n",
    "orders_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", f\"{DATASET_BASE_PATH}/delta/orders_optimization\") \\\n",
    "    .saveAsTable(ORDERS_OPT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd499c1-678e-463f-9081-7ef9b14301c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 1: Analiza planu fizycznego (explain())\n",
    "\n",
    "**Cel sekcji:** Naucz siƒô analizowaƒá plany wykonania zapyta≈Ñ, aby zidentyfikowaƒá bottlenecki wydajno≈õciowe.\n",
    "\n",
    "**Teoria:**\n",
    "Plan fizyczny to szczeg√≥≈Çowa mapa tego, jak Spark wykonuje zapytanie:\n",
    "- **Stages**: Logiczne etapy przetwarzania\n",
    "- **Tasks**: Jednostki pracy wykonywane na partycjach\n",
    "- **Shuffles**: Wymiana danych miƒôdzy executor'ami\n",
    "- **Pushdowns**: Optymalizacje przeniesione do ≈∫r√≥d≈Ça danych\n",
    "\n",
    "**Rodzaje explain():**\n",
    "- `explain()` - podstawowy plan\n",
    "- `explain(True)` - pe≈Çny plan z detalami\n",
    "- `explain('extended')` - rozszerzony plan\n",
    "- `explain('cost')` - plan z kosztem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "918895ad-596e-4f57-bdb7-05d587e80952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 1.1: Analiza planu prostego zapytania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6531acdf-98d5-45d5-a312-5f369f2fa2ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.1 - Analiza planu prostego zapytania\n",
    "\n",
    "simple_query = spark.sql(f\"\"\"\n",
    "    SELECT customer_id, first_name, last_name, customer_segment, city\n",
    "    FROM {CUSTOMERS_OPT}\n",
    "    WHERE customer_segment = 'Premium'\n",
    "    ORDER BY customer_id DESC\n",
    "    LIMIT 10\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35f19041-0931-4a08-a8e0-20e32a6124ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Podstawowy plan zapytania:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2181b138-61da-4aee-bf65-68a74549c9af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "simple_query.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e42fa92-dd6b-498e-aeb9-ee451569f053",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Rozszerzony plan zapytania (z detalami):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03aa87a5-a7b2-42a1-b6c3-f8fb0854d2e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "simple_query.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d9af332-8c17-4825-9496-b9029a9421ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 1.2: Predicate Pushdown w praktyce\n",
    "\n",
    "**Teoria:** Predicate pushdown to optymalizacja, gdzie filtry (warunki WHERE) sƒÖ \"przepychane\" jak najni≈ºej w planie wykonania, najlepiej do poziomu czytania plik√≥w. Dziƒôki temu czytamy tylko dane, kt√≥re spe≈ÇniajƒÖ warunki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73394fb4-ff75-4f2d-aae5-1f68db484ec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.2 - Predicate Pushdown\n",
    "\n",
    "filtered_query = spark.sql(f\"\"\"\n",
    "    SELECT order_id, customer_id, total_amount, order_date\n",
    "    FROM {ORDERS_OPT}\n",
    "    WHERE total_amount > 100 \n",
    "    AND order_date >= '2024-01-01'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8e41032-3c95-4168-bd99-a3b1a3c47c1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Sprawd≈∫ plan - poszukaj \"PushedFilters\" w planie:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ead573aa-851f-4ec2-bdc4-413254fe850b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawd≈∫ plan - poszukaj \"PushedFilters\" w planie\n",
    "filtered_query.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41fc6c80-bebb-424b-a76c-b85f633da3bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**üí° W planie szukaj:**\n",
    "- `PushedFilters` - filtry przepchniƒôte do poziomu czytania\n",
    "- `ReadSchema` - tylko wybrane kolumny (column pruning)  \n",
    "- `PartitionFilters` - filtry na partycjach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a51281f-4f44-4ea6-82f2-996f7edaee07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 2: Strategia partycjonowania\n",
    "\n",
    "**Cel sekcji:** Nauka wyboru optymalnych kluczy partycjonowania dla najlepszej wydajno≈õci.\n",
    "\n",
    "**Teoria partycjonowania:**\n",
    "- **Partitioning**: Fizyczne rozdzielenie tabeli na katalogi wed≈Çug warto≈õci kolumn\n",
    "- **Partition Pruning**: Spark pomija ca≈Çe partycje, kt√≥re nie sƒÖ potrzebne dla zapytania\n",
    "- **Idealne partycje**: 1-10GB danych na partycjƒô, nie wiƒôcej ni≈º 10,000 partycji\n",
    "\n",
    "**Najlepsze praktyki:**\n",
    "- Partycjonuj wed≈Çug kolumn czƒôsto u≈ºywanych w filtrach\n",
    "- Unikaj partycjonowania wed≈Çug kolumn o wysokiej kardinalno≈õci\n",
    "- Preferuj kolumny z naturalnƒÖ hierarchiƒÖ czasowƒÖ (rok/miesiƒÖc/dzie≈Ñ)\n",
    "- Unikaj zbyt wielu ma≈Çych partycji (small files problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0665d14-7df4-4304-b215-ad5531b8a72a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 2.1: Tworzenie tabeli partycjonowanej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00e19ec-cf42-486f-be1f-0082e1e602d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 2.1 - Tworzenie tabeli partycjonowanej\n",
    "\n",
    "# Utw√≥rz tabelƒô partycjonowanƒÖ wed≈Çug roku i miesiƒÖca\n",
    "ORDERS_PARTITIONED = f\"{BRONZE_SCHEMA}.orders_opt_partitioned\"\n",
    "\n",
    "# Dodaj kolumny do partycjonowania\n",
    "orders_with_partitions = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        *,\n",
    "        YEAR(order_date) as year,\n",
    "        MONTH(order_date) as month\n",
    "    FROM {ORDERS_OPT}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8ffa99b2-1246-49c4-ac26-0e50398d876a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Zapisz jako tabelƒô partycjonowanƒÖ:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1ae9d87-c4a4-4e3d-820a-e21e4f36a722",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "orders_with_partitions.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .saveAsTable(ORDERS_PARTITIONED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c2f20147-390f-464a-b2c8-2a2860bc0666",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Sprawd≈∫ strukturƒô partycji:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e521c585-00d8-4132-bd73-9e00f315c55c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.sql(f\"DESCRIBE DETAIL {ORDERS_PARTITIONED}\")\n",
    "    .select(\"name\", \"location\", \"partitionColumns\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ef8762-1a93-41d5-83af-0545643857ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 2.2: Partition Pruning w dzia≈Çaniu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e30be36f-2a13-44f8-85a8-05c76566b45a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 2.2 - Partition Pruning\n",
    "\n",
    "# Zapytanie kt√≥ry wykorzystuje partycje (rok/miesiƒÖc)\n",
    "efficient_query = spark.sql(f\"\"\"\n",
    "    SELECT order_id, customer_id, total_amount, order_date\n",
    "    FROM {ORDERS_PARTITIONED}\n",
    "    WHERE year = 2024 AND month = 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87b8b31e-222a-4a5e-b30a-3d852213dd7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Sprawd≈∫ plan - partition pruning w dzia≈Çaniu:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c854427-294e-446c-846b-c8bbe77a8514",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawd≈∫ plan - poszukaj \"PartitionFilters\"\n",
    "efficient_query.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b6b8cdb-1ef8-4bcc-a64c-d1ffb7ba2a13",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Por√≥wnanie: zapytanie BEZ partition pruning:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c8552ae-d3d5-4cf4-8fef-8a97c7df2e21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zapytanie kt√≥re nie wykorzystuje partycji (nie filtruje po roku/miesiƒÖcu)\n",
    "inefficient_query = spark.sql(f\"\"\"\n",
    "    SELECT order_id, customer_id, total_amount, order_date\n",
    "    FROM {ORDERS_PARTITIONED}\n",
    "    WHERE customer_id = 1\n",
    "\"\"\")\n",
    "\n",
    "inefficient_query.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "64af6cdb-4166-4a66-9534-bd992f07f7f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 2b: JOIN Optimization i Shuffle\n",
    "\n",
    "**Cel sekcji:** Optymalizacja operacji JOIN i minimalizacja kosztownych shuffle operations.\n",
    "\n",
    "**Teoria - Typy JOIN w Spark:**\n",
    "\n",
    "| Typ JOIN | Kiedy u≈ºywany | Charakterystyka |\n",
    "|----------|---------------|-----------------|\n",
    "| **Broadcast Hash Join** | Jedna tabela < 10MB (domy≈õlnie) | Najszybszy - ma≈Ça tabela kopiowana do wszystkich executor√≥w |\n",
    "| **Sort Merge Join** | Obie tabele du≈ºe | Wymaga shuffle i sortowania obu stron |\n",
    "| **Shuffle Hash Join** | ≈örednie tabele | Shuffle bez sortowania |\n",
    "\n",
    "**Shuffle - co to jest?**\n",
    "- Wymiana danych miƒôdzy executorami (przez sieƒá)\n",
    "- Najdro≈ºsza operacja w Spark (I/O, network, serialization)\n",
    "- Powoduje \"stage boundaries\" w DAG\n",
    "\n",
    "**Jak minimalizowaƒá shuffle:**\n",
    "1. **Broadcast JOIN** - dla ma≈Çych tabel (< 10MB, mo≈ºna zwiƒôkszyƒá do 100MB)\n",
    "2. **Repartition** przed JOIN - wyr√≥wnanie partycji\n",
    "3. **Colocated JOIN** - dane ju≈º na tych samych partycjach\n",
    "4. **Bucketing** - pre-partycjonowanie tabel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37daf578-66cb-401e-93e7-c1b5865af67e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 2b.1: Broadcast JOIN - optymalizacja dla ma≈Çych tabel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6604c43-b3b0-4c10-844b-58b04582815c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 2b.1 - Broadcast JOIN\n",
    "\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "# Wczytaj tabele do JOIN\n",
    "orders_df = spark.table(ORDERS_OPT)\n",
    "customers_df = spark.table(CUSTOMERS_OPT)\n",
    "\n",
    "print(f\"Orders: {orders_df.count()} rekord√≥w\")\n",
    "print(f\"Customers: {customers_df.count()} rekord√≥w (ma≈Ça tabela - kandydat do broadcast)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f2b6aa9-d1aa-4476-b413-fb286577e4e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Standardowy JOIN (bez broadcast) - wymaga shuffle:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0748f9e-6288-4887-b363-48e36cba8ad8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Standardowy JOIN - Spark sam zdecyduje o strategii\n",
    "standard_join = orders_df.join(\n",
    "    customers_df,\n",
    "    orders_df.customer_id == customers_df.customer_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    orders_df.order_id,\n",
    "    orders_df.total_amount,\n",
    "    customers_df.first_name,\n",
    "    customers_df.customer_segment\n",
    ")\n",
    "\n",
    "# Sprawd≈∫ plan - szukaj \"SortMergeJoin\" lub \"BroadcastHashJoin\"\n",
    "standard_join.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40c43c66-f59e-418d-b5be-004c1583f577",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wymuszony Broadcast JOIN - eliminuje shuffle:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28beffb3-0349-49c6-854a-c0f6bc884344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wymuszony Broadcast JOIN - ma≈Ça tabela (customers) kopiowana do wszystkich executor√≥w\n",
    "broadcast_join = orders_df.join(\n",
    "    broadcast(customers_df),  # Wymuszamy broadcast mniejszej tabeli\n",
    "    orders_df.customer_id == customers_df.customer_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    orders_df.order_id,\n",
    "    orders_df.total_amount,\n",
    "    customers_df.first_name,\n",
    "    customers_df.customer_segment\n",
    ")\n",
    "\n",
    "# Sprawd≈∫ plan - powinien pokazaƒá \"BroadcastHashJoin\"\n",
    "broadcast_join.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c37e46ab-6ce8-46f2-bf5f-7ffd3dafb5ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**üí° W planie szukaj:**\n",
    "- `BroadcastHashJoin` - broadcast join (brak shuffle!)\n",
    "- `SortMergeJoin` - sort merge join (wymaga shuffle obu stron)\n",
    "- `BroadcastExchange` - kopiowanie ma≈Çej tabeli do executor√≥w\n",
    "- `ShuffleExchange` - shuffle (kosztowna operacja!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8aec715b-465a-491a-8a41-6cad52c4170c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 2b.3: Repartition vs Coalesce - kontrola shuffle\n",
    "\n",
    "**Co to oznacza?**\n",
    "\n",
    "- **repartition(n)** ‚Äì powoduje pe≈Çny shuffle danych, rozdzielajƒÖc je r√≥wnomiernie na n partycji. U≈ºywaj, gdy chcesz zwiƒôkszyƒá liczbƒô partycji lub wyr√≥wnaƒá rozk≈Çad danych (np. przed du≈ºym JOIN).\n",
    "- **coalesce(n)** ‚Äì ≈ÇƒÖczy istniejƒÖce partycje bez shuffle (narrow transformation), zmniejszajƒÖc ich liczbƒô. U≈ºywaj, gdy chcesz zmniejszyƒá liczbƒô partycji (np. przed zapisem do pliku), ale nie zale≈ºy Ci na r√≥wnomiernym rozk≈Çadzie.\n",
    "\n",
    "**Podsumowanie:**  \n",
    "Wybierz `repartition`, gdy zale≈ºy Ci na r√≥wnomiernym rozk≈Çadzie i mo≈ºesz zaakceptowaƒá koszt shuffle. Wybierz `coalesce`, gdy chcesz tylko zmniejszyƒá liczbƒô partycji bez kosztownego shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f1d45a9-4afa-4f72-a989-d4ffef704164",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1764310848426}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawd≈∫ liczbƒô partycji i plik√≥w w tabeli Delta\n",
    "df_detail = spark.sql(f\"DESCRIBE DETAIL {ORDERS_PARTITIONED}\")\n",
    "display(\n",
    "    df_detail#.select(\"partitionColumns\", \"numFiles\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17aed8d8-935a-4e52-9243-be7f85ecdf19",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartition vs Coalesce - differences\n",
    "\n",
    "# REPARTITION - full shuffle, even data distribution\n",
    "orders_repartitioned = orders_df.repartition(10)\n",
    "display(\n",
    "    spark.sql(f\"DESCRIBE DETAIL {ORDERS_PARTITIONED}\")\n",
    "    .select(\"numFiles\")\n",
    ")\n",
    "\n",
    "# COALESCE - no shuffle, only merges partitions (narrow transformation)\n",
    "orders_coalesced = orders_df.coalesce(4)\n",
    "display(\n",
    "    spark.sql(f\"DESCRIBE DETAIL {ORDERS_PARTITIONED}\")\n",
    "    .select(\"numFiles\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d062bea-f5b3-4dc7-a565-4d3721946ded",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Repartition BY kolumna - optymalizacja dla JOIN:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d72cbee1-9d79-41b6-9aa8-2772280361cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Repartition BY join key - dane z tym samym kluczem trafiajƒÖ do tej samej partycji\n",
    "# To minimalizuje shuffle podczas JOIN (collocated data)\n",
    "\n",
    "orders_by_customer = orders_df.repartition(10, \"customer_id\")\n",
    "customers_by_id = customers_df.repartition(10, \"customer_id\")\n",
    "\n",
    "# Teraz JOIN bƒôdzie szybszy - dane ju≈º sƒÖ na tych samych partycjach\n",
    "optimized_join = orders_by_customer.join(\n",
    "    customers_by_id,\n",
    "    \"customer_id\",  # Wsp√≥lna kolumna partycjonowania\n",
    "    \"inner\"\n",
    ")\n",
    "\n",
    "print(\"‚úì Dane przepartycjonowane po customer_id - JOIN bƒôdzie szybszy\")\n",
    "optimized_join.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c1c166a-2a5d-4960-89e6-cb93ae31eee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 2b.4: Por√≥wnanie wydajno≈õci JOIN\n",
    "\n",
    "**Teoria - kiedy u≈ºywaƒá kt√≥rego typu JOIN:**\n",
    "\n",
    "| Scenariusz | Rekomendacja |\n",
    "|------------|--------------|\n",
    "| Ma≈Ça tabela lookup (< 10MB) | `broadcast(small_df)` |\n",
    "| Ma≈Ça tabela lookup (10-100MB) | Zwiƒôksz `autoBroadcastJoinThreshold` |\n",
    "| Obie tabele du≈ºe | SortMergeJoin (domy≈õlny) + pre-repartition |\n",
    "| Czƒôste JOIN na tym samym kluczu | Bucketing (przy tworzeniu tabeli) |\n",
    "| Skewed data (nier√≥wnomierny rozk≈Çad) | Salting lub AQE (Adaptive Query Execution) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6eb6608a-7590-4619-a9c5-b54b0da03e00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Por√≥wnanie wydajno≈õci: Broadcast vs SortMerge JOIN\n",
    "\n",
    "import time\n",
    "\n",
    "# Test 1: Broadcast JOIN\n",
    "start_broadcast = time.time()\n",
    "result_broadcast = orders_df.join(\n",
    "    broadcast(customers_df),\n",
    "    orders_df.customer_id == customers_df.customer_id\n",
    ").count()\n",
    "time_broadcast = time.time() - start_broadcast\n",
    "\n",
    "# Test 2: Wy≈ÇƒÖczony broadcast (wymusza SortMergeJoin)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "start_sortmerge = time.time()\n",
    "result_sortmerge = orders_df.join(\n",
    "    customers_df,\n",
    "    orders_df.customer_id == customers_df.customer_id\n",
    ").count()\n",
    "time_sortmerge = time.time() - start_sortmerge\n",
    "\n",
    "# Przywr√≥ƒá domy≈õlne ustawienie\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", 10 * 1024 * 1024)\n",
    "\n",
    "# Wyniki\n",
    "display(spark.createDataFrame([\n",
    "    (\"Broadcast JOIN\", f\"{time_broadcast:.2f}s\", \"‚úÖ Szybki (bez shuffle)\"),\n",
    "    (\"SortMerge JOIN\", f\"{time_sortmerge:.2f}s\", \"‚ö†Ô∏è Wolniejszy (wymaga shuffle)\")\n",
    "], [\"Typ JOIN\", \"Czas\", \"Uwagi\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "490f3f51-7773-4c85-83fd-6cb74c67d1e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 2b.5: Shuffle Partitions - konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e32a9fea-04ed-42ea-a0f8-a50858076efe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Shuffle Partitions - kontrola liczby partycji po shuffle\n",
    "\n",
    "# Sprawd≈∫ aktualnƒÖ warto≈õƒá (domy≈õlnie 200)\n",
    "current_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(f\"Aktualne shuffle.partitions: {current_partitions}\")\n",
    "\n",
    "# Dla ma≈Çych danych (< 1GB) - zmniejsz liczbƒô partycji\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", 10)\n",
    "\n",
    "# Dla du≈ºych danych (> 100GB) - zwiƒôksz\n",
    "# spark.conf.set(\"spark.sql.shuffle.partitions\", 500)\n",
    "\n",
    "# Najlepsza praktyka: Adaptive Query Execution (AQE) automatycznie optymalizuje\n",
    "print(f\"AQE enabled: {spark.conf.get('spark.sql.adaptive.enabled')}\")\n",
    "print(f\"AQE coalesce partitions: {spark.conf.get('spark.sql.adaptive.coalescePartitions.enabled')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2324c1a2-c156-45f8-889e-76d465326035",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**üìä Podsumowanie optymalizacji JOIN i Shuffle:**\n",
    "\n",
    "| Technika | Kiedy u≈ºywaƒá | Korzy≈õƒá |\n",
    "|----------|--------------|---------|\n",
    "| `broadcast(df)` | Ma≈Ça tabela < 100MB | Eliminuje shuffle |\n",
    "| `repartition(n, col)` | Przed JOIN na du≈ºych tabelach | Collocated data |\n",
    "| `coalesce(n)` | Zmniejszenie partycji przed zapisem | Bez shuffle |\n",
    "| `shuffle.partitions` | Dostosowanie do rozmiaru danych | Optymalna parallelizacja |\n",
    "| AQE (Adaptive) | Zawsze w≈ÇƒÖczone (domy≈õlnie) | Auto-optymalizacja |\n",
    "\n",
    "> **üí° Best Practice:** W Databricks Runtime 14+ AQE jest domy≈õlnie w≈ÇƒÖczone i automatycznie optymalizuje shuffle partitions, broadcast thresholds i skewed joins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db85bc0-8aa8-4630-9e18-69e5530fea96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 3: Small Files Problem\n",
    "\n",
    "**Cel sekcji:** Zrozumienie i rozwiƒÖzanie problemu ma≈Çych plik√≥w w Delta Lake.\n",
    "\n",
    "**Co to jest Small Files Problem?**\n",
    "- Gdy tabela ma zbyt wiele ma≈Çych plik√≥w (< 128MB ka≈ºdy)\n",
    "- Spark preferuje pliki 128MB-1GB dla optymalnej wydajno≈õci\n",
    "- Ma≈Çe pliki powodujƒÖ overhead w metadanych i zmniejszajƒÖ throughput\n",
    "\n",
    "**Przyczyny ma≈Çych plik√≥w:**\n",
    "- Czƒôste zapisy INSERT w ma≈Çych batch'ach\n",
    "- Wysokie partycjonowanie z ma≈ÇƒÖ ilo≈õciƒÖ danych na partycjƒô\n",
    "- Streaming z kr√≥tkimi trigger intervals\n",
    "\n",
    "**RozwiƒÖzania:**\n",
    "- **OPTIMIZE** - ≈ÇƒÖczy ma≈Çe pliki w wiƒôksze\n",
    "- **Auto Compaction** - automatyczne ≈ÇƒÖczenie podczas zapisu\n",
    "- **Repartition** przed zapisem\n",
    "- **Coalesce** dla zmniejszenia liczby partycji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "965ad450-5f1a-4e33-ac31-ba922037d071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 3.1: Symulacja Small Files Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22a608c1-9ec7-43a3-9b0b-abfd0658aebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 3.1 - Symulacja Small Files Problem\n",
    "\n",
    "SMALL_FILES_TABLE = f\"{BRONZE_SCHEMA}.small_files_demo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25e86915-41be-4bd1-8c9a-96cad790baeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Symuluj wiele ma≈Çych zapis√≥w (ka≈ºdy tworzy osobny plik):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dccc446-bed7-47b1-92f7-2d46a7acb593",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    small_batch = spark.range(i*100, (i+1)*100).select(\n",
    "        col(\"id\"),\n",
    "        (col(\"id\") * 2).alias(\"value\"),\n",
    "        lit(f\"batch_{i}\").alias(\"batch_name\")\n",
    "    )\n",
    "    \n",
    "    small_batch.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(SMALL_FILES_TABLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3faccb40-d181-48a2-8966-868cf96939c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Sprawd≈∫ liczbƒô plik√≥w:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7199cd96-46ef-408a-9c47-6058b2aa4bd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "detail = spark.sql(f\"DESCRIBE DETAIL {SMALL_FILES_TABLE}\").collect()[0]\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Liczba plik√≥w\", str(detail['numFiles'])),\n",
    "        (\"Rozmiar tabeli\", f\"{detail['sizeInBytes']} bytes\"),\n",
    "        (\"≈öredni rozmiar pliku\", f\"{detail['sizeInBytes'] / detail['numFiles']:.0f} bytes\"),\n",
    "        (\"Status\", \"‚ö†Ô∏è Problem: zbyt wiele ma≈Çych plik√≥w!\")\n",
    "    ], [\"Metric\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d568f02-9857-4fa2-bbdd-ef467fa13477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 3.2: RozwiƒÖzanie - OPTIMIZE i Auto Compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d3c587-4607-41fa-b0db-3e1ab0774a60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 3.2 - RozwiƒÖzanie Small Files Problem\n",
    "\n",
    "# Wykonaj OPTIMIZE na tabeli z ma≈Çymi plikami\n",
    "spark.sql(f\"OPTIMIZE {SMALL_FILES_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bea67be9-12b9-4e95-b8de-a6c2e5fc006f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Sprawd≈∫ stan po OPTIMIZE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4391f06-7cc2-4368-bebc-ba1a4f74ac9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "detail_after = spark.sql(f\"DESCRIBE DETAIL {SMALL_FILES_TABLE}\").collect()[0]\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Liczba plik√≥w (PO OPTIMIZE)\", str(detail_after['numFiles'])),\n",
    "        (\"Rozmiar tabeli\", f\"{detail_after['sizeInBytes']} bytes\"),\n",
    "        (\"≈öredni rozmiar pliku\", f\"{detail_after['sizeInBytes'] / detail_after['numFiles']:.0f} bytes\")\n",
    "    ], [\"Metric\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b812c27-429f-4174-8592-58a8b942e27b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**W≈ÇƒÖcz Auto Compaction dla przysz≈Çych zapis√≥w:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4ff41c0-94fc-4307-8808-2ecf8060f880",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {SMALL_FILES_TABLE}\n",
    "    SET TBLPROPERTIES (\n",
    "        'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "        'delta.autoOptimize.autoCompact' = 'true'\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2469e89e-dd32-4b2e-990a-9faf9a079077",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Sprawd≈∫ w≈ÇƒÖczone w≈Ça≈õciwo≈õci Auto Compaction:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "590ec970-c668-4d60-807c-2b0c70c20d9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "properties = spark.sql(f\"SHOW TBLPROPERTIES {SMALL_FILES_TABLE}\").collect()\n",
    "auto_props = [(p['key'], p['value']) for p in properties if 'autoOptimize' in p['key']]\n",
    "\n",
    "display(spark.createDataFrame(auto_props, [\"Property\", \"Value\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b059245f-e9e4-43e3-b375-db8117460191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 3.3: VACUUM - Usuwanie starych plik√≥w\n",
    "\n",
    "**Teoria:**\n",
    "VACUUM usuwa stare pliki, kt√≥re nie sƒÖ ju≈º potrzebne (po operacjach DELETE, UPDATE, MERGE, OPTIMIZE). \n",
    "Domy≈õlnie Delta Lake zachowuje pliki przez 7 dni (168 godzin) dla Time Travel.\n",
    "\n",
    "**‚ö†Ô∏è Uwaga:** Po VACUUM nie mo≈ºna u≈ºywaƒá Time Travel do wersji wcze≈õniejszych ni≈º retention period!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de4e8797-b430-4edc-ae27-990c54df9e50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawd≈∫ ile plik√≥w mo≈ºna usunƒÖƒá (DRY RUN)\n",
    "spark.sql(f\"VACUUM {SMALL_FILES_TABLE} DRY RUN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9923335b-4cb8-4570-80c9-bc5a73b1c7f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wykonaj VACUUM (usu≈Ñ stare pliki):**\n",
    "\n",
    "> **Uwaga:** W produkcji u≈ºywaj domy≈õlnego retention (7 dni). Poni≈ºszy kod z `RETAIN 0 HOURS` jest tylko do demo!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75d953d2-90c7-4026-8099-de9c6d8bbc39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wykonaj VACUUM - usu≈Ñ stare pliki\n",
    "# W ≈õrodowisku demo wy≈ÇƒÖczamy sprawdzenie retention\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "# VACUUM z kr√≥tkim retention (TYLKO DO DEMO!)\n",
    "spark.sql(f\"\"\"\n",
    "    VACUUM {ORDERS_OPT} RETAIN 1 HOURS\n",
    "\"\"\")\n",
    "\n",
    "# Przywr√≥ƒá domy≈õlne ustawienie\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n",
    "\n",
    "print(\"‚úÖ VACUUM wykonany - stare pliki usuniƒôte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c75d4e-fc27-4cfc-b053-d93b48b5baef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 4: ZORDER BY - Advanced Clustering\n",
    "\n",
    "**Cel sekcji:** Nauka wykorzystania ZORDER BY do optymalizacji zapyta≈Ñ z filtrami i joinami.\n",
    "\n",
    "**Co to jest ZORDER BY?**\n",
    "- Multi-dimensional clustering algorithm w Delta Lake\n",
    "- Organizuje dane w plikach wed≈Çug warto≈õci wybranych kolumn\n",
    "- Poprawia data skipping - pomijanie niepotrzebnych plik√≥w podczas czytania\n",
    "- Szczeg√≥lnie skuteczny dla kolumn czƒôsto u≈ºywanych w filtrach WHERE i JOIN\n",
    "\n",
    "**Kiedy u≈ºywaƒá ZORDER:**\n",
    "- Kolumny czƒôsto filtrowane w zapytaniach\n",
    "- Kolumny u≈ºywane w JOIN operations\n",
    "- High-cardinality columns (wiele unikalnych warto≈õci)\n",
    "- Maksymalnie 3-4 kolumny (wiƒôcej = diminishing returns)\n",
    "\n",
    "**ZORDER vs Partitioning:**\n",
    "- Partitioning: fizyczne rozdzielenie na katalogi\n",
    "- ZORDER: logiczne uporzƒÖdkowanie w plikach (zachowuje pojedynczƒÖ strukturƒô folder√≥w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da4d32df-ebbe-427d-ba8c-720d1006449c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 4.1: ZORDER BY dla czƒôsto filtrowanych kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c5e7c6-4377-40d4-9e18-46ff53c6a9e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wykonaj ZORDER BY na najczƒô≈õciej filtrowanych kolumnach\n",
    "spark.sql(f\"\"\"\n",
    "    OPTIMIZE {ORDERS_OPT}\n",
    "    ZORDER BY (customer_id, order_date)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6cc295-74eb-4af8-b88c-07402191fa62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 4.2: Pomiar skuteczno≈õci ZORDER - Data Skipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc56cf1a-23cd-4ede-aa44-64333b3af744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 4.2 - Pomiar skuteczno≈õci ZORDER\n",
    "\n",
    "import time\n",
    "\n",
    "# Zapytanie wykorzystujƒÖce kolumny z ZORDER\n",
    "# customer_id to STRING (np. CUST000123), order_date to DATE\n",
    "test_query = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as cnt, AVG(total_amount) as avg_amount\n",
    "    FROM {ORDERS_OPT}\n",
    "    WHERE customer_id BETWEEN 'CUST000100' AND 'CUST000500'\n",
    "    AND order_date >= '2024-06-01'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ebb4ddd-45d4-4404-b5bc-4a061dcc9ea4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Pomiar czasu wykonania:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7882b76-14f9-4a29-a4de-acfd6774461b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "result = test_query.collect()\n",
    "elapsed = time.time() - start_time\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Wynik\", str(result[0])),\n",
    "        (\"Czas wykonania\", f\"{elapsed:.2f}s\")\n",
    "    ], [\"Metric\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c01944f2-4333-4d46-8cb1-fb1190ccc1e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Plan zapytania - sprawd≈∫ data skipping:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ff59074-1fd3-453b-a930-80d99ceb6cda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawd≈∫ plan zapytania - data skipping\n",
    "test_query.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7319858f-10d0-4cc1-b7d2-043e5af51a7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**üí° W planie szukaj:**\n",
    "- `numFilesTotal` vs `numFilesSelected` - ile plik√≥w pominiƒôto\n",
    "- `metadata time` - czas parsowania metadanych\n",
    "- `files pruned` - data skipping statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c88267bd-b51d-4c25-8d47-d82a5006a672",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 5: Liquid Clustering - Przysz≈Ço≈õƒá optymalizacji\n",
    "\n",
    "**Cel sekcji:** Poznanie Liquid Clustering - nowoczesnej techniki zastƒôpujƒÖcej Hive Partitioning i ZORDER.\n",
    "\n",
    "**Co to jest Liquid Clustering?**\n",
    "To elastyczny mechanizm uk≈Çadania danych, kt√≥ry:\n",
    "- Nie wymaga sztywnej struktury katalog√≥w (jak Partitioning)\n",
    "- Pozwala na zmianƒô kluczy klastrowania bez przepisywania ca≈Çej tabeli\n",
    "- Eliminuje problem \"Small Files\" zwiƒÖzany z nadmiernym partycjonowaniem\n",
    "- Dzia≈Ça inkrementalnie (nie trzeba optymalizowaƒá ca≈Çej tabeli na raz)\n",
    "\n",
    "**Kiedy u≈ºywaƒá?**\n",
    "- Zamiast partycjonowania dla wiƒôkszo≈õci nowych tabel\n",
    "- Gdy klucze partycjonowania majƒÖ wysokƒÖ kardynalno≈õƒá\n",
    "- Gdy wzorce zapyta≈Ñ zmieniajƒÖ siƒô w czasie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aebdbf22-0336-44c6-bb4e-50cc2ba0b198",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "LIQUID_TABLE = f\"{BRONZE_SCHEMA}.orders_opt_liquid\"\n",
    "\n",
    "# Tworzymy tabelƒô u≈ºywajƒÖc CLUSTER BY zamiast PARTITIONED BY\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {LIQUID_TABLE}\n",
    "CLUSTER BY (customer_id, order_date)\n",
    "AS SELECT * FROM {ORDERS_OPT}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09d932a6-3b8c-47d9-9a43-310dc09902f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Sprawd≈∫ w≈Ça≈õciwo≈õci tabeli:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2044deb7-701d-400f-b3f8-0e08c4077ec7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawd≈∫ w≈Ça≈õciwo≈õci tabeli\n",
    "display(spark.sql(f\"DESCRIBE DETAIL {LIQUID_TABLE}\").select(\"name\", \"clusteringColumns\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca37d586-6ebb-4589-b1a1-6aed6c3e0867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 5.2: Inkrementalna optymalizacja\n",
    "\n",
    "**Teoria:**\n",
    "W przeciwie≈Ñstwie do ZORDER, kt√≥ry musi przeliczyƒá ca≈ÇƒÖ partycjƒô/tabelƒô, Liquid Clustering dzia≈Ça inkrementalnie. `OPTIMIZE` uporzƒÖdkuje tylko te dane, kt√≥re tego wymagajƒÖ (np. nowo dodane), co oszczƒôdza czas i zasoby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d43cae37-4bab-4d8b-8660-d31a354d8ccd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Uruchom OPTIMIZE - Liquid Clustering wie jak uk≈Çadaƒá dane na podstawie definicji tabeli\n",
    "spark.sql(f\"OPTIMIZE {LIQUID_TABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d6fa602-e15f-4bc6-81ec-cb2cae552e3a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Sprawd≈∫ historiƒô, aby zobaczyƒá operacjƒô CLUSTERING:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c41681d-7395-4cce-983e-025c6e04de7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.sql(f\"DESCRIBE HISTORY {LIQUID_TABLE}\")\n",
    "    .select(\"version\", \"operation\", \"operationParameters\")\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1848ae0b-3811-4dfb-943e-f5227331f90f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Por√≥wnanie: Liquid Clustering vs Partitioning + ZORDER\n",
    "\n",
    "| Cecha | Partitioning + ZORDER | Liquid Clustering |\n",
    "|-------|-----------------------|-------------------|\n",
    "| **Konfiguracja** | Wymaga starannego doboru kolumn partycjonowania | Elastyczne `CLUSTER BY` |\n",
    "| **Small Files** | Ryzyko przy nadmiernym partycjonowaniu | Automatycznie zarzƒÖdzane |\n",
    "| **Zmiana klucza** | Trudna (wymaga przepisania tabeli) | ≈Åatwa (`ALTER TABLE CLUSTER BY`) |\n",
    "| **Optymalizacja** | `OPTIMIZE ZORDER BY` (kosztowne) | `OPTIMIZE` (inkrementalne) |\n",
    "| **Skew Data** | Podatne na data skew | Odporne na data skew |\n",
    "\n",
    "**Rekomendacja:** U≈ºywaj Liquid Clustering dla wszystkich nowych tabel w Databricks Runtime 13.3+, chyba ≈ºe masz specyficzny pow√≥d, by u≈ºywaƒá partycjonowania (np. kompatybilno≈õƒá ze starszymi czytnikami)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1ca93de-4605-4f85-8401-a40985237e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices - Przewodnik optymalizacji\n",
    "\n",
    "### üéØ Strategia optymalizacji (w kolejno≈õci priorytet√≥w):\n",
    "\n",
    "**1. Analiza workload:**\n",
    "- Zidentyfikuj najczƒô≈õciej wykonywane zapytania\n",
    "- Znajd≈∫ kolumny najczƒô≈õciej u≈ºywane w filtrach WHERE\n",
    "- Sprawd≈∫ kt√≥re zapytania zajmujƒÖ najwiƒôcej czasu\n",
    "\n",
    "**2. Optymalizacja zapyta≈Ñ:**\n",
    "- U≈ºywaj filtr√≥w WHERE jak najwcze≈õniej w zapytaniu\n",
    "- Wybieraj tylko potrzebne kolumny (SELECT specific columns, nie *)\n",
    "- Preferuj JOIN na zindeksowanych kolumnach\n",
    "- U≈ºywaj LIMIT gdy mo≈ºliwe\n",
    "\n",
    "**3. Optymalizacja tabel:**\n",
    "- **Partitioning**: Tylko dla du≈ºych tabel (>1TB) i czƒôsto filtrowanych kolumn\n",
    "- **ZORDER BY**: Dla 2-4 najczƒô≈õciej filtrowanych kolumn\n",
    "- **OPTIMIZE**: Regularnie (np. daily) dla aktywnych tabel\n",
    "- **Auto Compaction**: W≈ÇƒÖcz dla tabel z czƒôstymi zapisami\n",
    "\n",
    "**4. Monitoring i maintenance:**\n",
    "- Regularnie sprawdzaj `DESCRIBE DETAIL` - liczba plik√≥w, rozmiar\n",
    "- Uruchamiaj `VACUUM` co najmniej raz w tygodniu\n",
    "- Monitoruj Spark UI dla d≈Çugo dzia≈ÇajƒÖcych zapyta≈Ñ\n",
    "- U≈ºywaj `explain()` do analizy problemowych zapyta≈Ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f7365e-c56b-49d8-8b5c-f76a13678b8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Troubleshooting - Diagnoza problem√≥w wydajno≈õciowych\n",
    "\n",
    "### üîç Najczƒôstsze problemy i rozwiƒÖzania:\n",
    "\n",
    "**Problem 1: Zapytanie dzia≈Ça bardzo wolno**\n",
    "```python\n",
    "# Diagnoza:\n",
    "df.explain(True)  # Sprawd≈∫ plan wykonania\n",
    "```\n",
    "**Mo≈ºliwe przyczyny:**\n",
    "- Brak filtr√≥w - czyta ca≈ÇƒÖ tabelƒô\n",
    "- Shuffle operations - du≈ºo ruchu sieciowego  \n",
    "- Skewed data - nier√≥wnomierne roz≈Ço≈ºenie danych\n",
    "- Small files - zbyt wiele ma≈Çych plik√≥w\n",
    "\n",
    "**Problem 2: \"OutOfMemoryError\" podczas JOIN**\n",
    "**RozwiƒÖzanie:**\n",
    "```python\n",
    "# Zwiƒôksz partycje przed JOIN\n",
    "df1 = df1.repartition(200, \"join_key\")\n",
    "df2 = df2.repartition(200, \"join_key\")\n",
    "\n",
    "# Lub u≈ºyj broadcast join dla ma≈Çych tabel\n",
    "from pyspark.sql.functions import broadcast\n",
    "result = large_df.join(broadcast(small_df), \"key\")\n",
    "```\n",
    "\n",
    "**Problem 3: D≈Çugie czasy zapisu do Delta**\n",
    "**RozwiƒÖzanie:**\n",
    "- W≈ÇƒÖcz Auto Compaction\n",
    "- U≈ºyj `coalesce()` przed zapisem\n",
    "- Avoid zbyt wysokie partycjonowanie\n",
    "\n",
    "**Problem 4: OPTIMIZE nie poprawia wydajno≈õci**\n",
    "**Przyczyna:** ZORDER BY jest potrzebny dla specific query patterns\n",
    "```python\n",
    "# Zamiast samego OPTIMIZE:\n",
    "OPTIMIZE table_name\n",
    "\n",
    "# U≈ºyj OPTIMIZE z ZORDER:\n",
    "OPTIMIZE table_name ZORDER BY (frequently_filtered_columns)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ca92322-c50a-41aa-8a24-007833c526ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Podsumowanie\n",
    "\n",
    "### Co osiƒÖgnƒôli≈õmy:\n",
    "\n",
    "- **Analiza wydajno≈õci**: Czytanie i interpretacja plan√≥w fizycznych z `explain()`\n",
    "- **Predicate Pushdown**: Identyfikacja bottleneck√≥w i pushed filters\n",
    "- **Partycjonowanie**: Strategia partycjonowania wed≈Çug czƒôsto filtrowanych kolumn\n",
    "- **ZORDER BY**: Multi-dimensional clustering dla 2-4 kolumn\n",
    "- **Small Files Problem**: RozwiƒÖzywanie przez OPTIMIZE i Auto Compaction\n",
    "- **Liquid Clustering**: Nowoczesna alternatywa dla partycjonowania\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "\n",
    "| # | Zasada |\n",
    "|---|--------|\n",
    "| 1 | **Analiza przed optymalizacjƒÖ** - zawsze najpierw `explain()` |\n",
    "| 2 | **Partitioning ‚â† ZORDER** - r√≥≈ºne techniki dla r√≥≈ºnych przypadk√≥w |\n",
    "| 3 | **Small files = performance killer** - regularne OPTIMIZE |\n",
    "| 4 | **ZORDER BY** - maksymalnie 3-4 kolumny, wybieraj najczƒô≈õciej filtrowane |\n",
    "| 5 | **Liquid Clustering** - preferuj dla nowych tabel w DBR 13.3+ |\n",
    "\n",
    "### Metryki do monitorowania:\n",
    "\n",
    "| Metryka | Dobra warto≈õƒá | Akcja je≈õli przekroczona |\n",
    "|---------|---------------|--------------------------|\n",
    "| Liczba plik√≥w | < 1000/TB | OPTIMIZE |\n",
    "| ≈öredni rozmiar pliku | 128MB-1GB | OPTIMIZE + Auto Compaction |\n",
    "| Skipped files ratio | >80% | ZORDER BY |\n",
    "\n",
    "### Nastƒôpne kroki:\n",
    "\n",
    "üìö **Kolejny dzie≈Ñ:** DZIEN_3 - Transformacje i Governance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d22f78-2445-487b-931a-e6eb94f7209c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Opcjonalnie usu≈Ñ tabele demo utworzone podczas ƒáwicze≈Ñ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b1ec66-98c3-49a5-a03a-8a180a2958a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup - usu≈Ñ tabele demo utworzone w tym notebooku\n",
    "\n",
    "# Odkomentuj poni≈ºsze linie aby usunƒÖƒá tabele demo:\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {ORDERS_OPT}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CUSTOMERS_OPT}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {ORDERS_PARTITIONED}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {SMALL_FILES_TABLE}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {LIQUID_TABLE}\")\n",
    "\n",
    "# print(\"‚úÖ Wszystkie tabele demo usuniƒôte\")\n",
    "\n",
    "print(\"‚ÑπÔ∏è Cleanup wy≈ÇƒÖczony (odkomentuj kod aby usunƒÖƒá tabele demo)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04_optimization_best_practices",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
