{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be568cc0",
   "metadata": {},
   "source": [
    "# Batch & Streaming Load - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Opanowanie metod ładowania danych: COPY INTO, Auto Loader i Structured Streaming.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- COPY INTO: kiedy używać, parametry (FILEFORMAT, VALIDATION_MODE, PATTERN)\n",
    "- Auto Loader (CloudFiles): file notification, checkpointing, schema inference\n",
    "- Schema evolution w praktyce\n",
    "- Structured Streaming: micro-batch architecture\n",
    "- readStream() / writeStream()\n",
    "- Triggering: once vs processingTime\n",
    "- Zarządzanie checkpointami\n",
    "- MERGE na streamingu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98a575e",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 2 - Lakehouse & Delta Lake\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f5837",
   "metadata": {},
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie różnych metod ładowania danych do Delta Lake: batch vs streaming.\n",
    "\n",
    "**Podstawowe pojęcia:**\n",
    "- **COPY INTO**: SQL command dla batch loads z idempotency (incremental batch)\n",
    "- **Auto Loader**: Databricks-managed solution dla incremental file ingestion z automatycznym schema inference\n",
    "- **Structured Streaming**: Spark streaming API z micro-batch processing i exactly-once semantics\n",
    "- **Checkpoint**: Location przechowujący offset/progress dla fault tolerance\n",
    "\n",
    "**Dlaczego to ważne?**\n",
    "Wybór metody ingestion ma wpływ na latency, throughput, cost i operacyjną złożoność. COPY INTO dla batch (hourly/daily), Auto Loader dla near real-time z małymi plikami, Structured Streaming dla pure streaming sources (Kafka, Event Hub)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c590c92b",
   "metadata": {},
   "source": [
    "## Izolacja per użytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c974e2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6425842c",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0819f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Wyświetl kontekst użytkownika\n",
    "print(\"=== Kontekst użytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Użytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog i schemat\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Ścieżki do danych i checkpointów\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "CHECKPOINT_PATH = f\"/tmp/{raw_user}/checkpoints\"\n",
    "\n",
    "print(f\"\\n=== Konfiguracja ===\")\n",
    "print(f\"Orders JSON: {ORDERS_JSON}\")\n",
    "print(f\"Checkpoint path: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1892fcbb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 1: COPY INTO - Batch Ingestion\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "COPY INTO to SQL command dla idempotent batch loads. Automatycznie śledzi załadowane pliki i pomija duplikaty. Idealny dla scheduled batch jobs (hourly, daily).\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **Idempotency**: Wielokrotne wykonanie COPY INTO z tymi samymi plikami nie powoduje duplikatów\n",
    "- **File tracking**: Delta Log przechowuje listę załadowanych plików\n",
    "- **Pattern matching**: Możliwość filtrowania plików po nazwie (PATTERN)\n",
    "- **Validation mode**: Kontrola zachowania przy błędach (PERMISSIVE, FAILFAST)\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Scheduled batch loads z cloud storage (S3, ADLS, GCS)\n",
    "- Incremental data ingestion bez ręcznego trackowania offsetów\n",
    "- ETL pipelines z retry logic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cea57a",
   "metadata": {},
   "source": [
    "### Przykład 1.1: Podstawowy COPY INTO\n",
    "\n",
    "**Cel:** Załadowanie plików JSON za pomocą COPY INTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73df08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 1.1 - COPY INTO basic\n",
    "\n",
    "# Utwórz target table\n",
    "copy_into_table = f\"{BRONZE_SCHEMA}.orders_copy_into\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {copy_into_table} (\n",
    "        order_id INT,\n",
    "        customer_id INT,\n",
    "        order_date STRING,\n",
    "        order_amount DOUBLE,\n",
    "        order_status STRING,\n",
    "        _metadata STRUCT<file_path: STRING, file_name: STRING, file_modification_time: TIMESTAMP>\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"✓ Utworzono target table: {copy_into_table}\")\n",
    "\n",
    "# Wykonaj COPY INTO\n",
    "copy_result = spark.sql(f\"\"\"\n",
    "    COPY INTO {copy_into_table}\n",
    "    FROM (SELECT *, _metadata FROM '{ORDERS_JSON}')\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('multiLine' = 'true')\n",
    "    COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n=== COPY INTO Result ===\")\n",
    "display(copy_result)\n",
    "\n",
    "# Sprawdź załadowane dane\n",
    "loaded_count = spark.table(copy_into_table).count()\n",
    "print(f\"\\nZaładowano {loaded_count} rekordów\")\n",
    "\n",
    "# Wyświetl przykładowe dane z metadata\n",
    "print(\"\\n=== Dane z metadata ===\")\n",
    "display(\n",
    "    spark.table(copy_into_table)\n",
    "    .select(\"order_id\", \"order_amount\", \"_metadata.file_name\")\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5f86d8",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "COPY INTO:\n",
    "- **_metadata column**: Automatycznie dodawana kolumna z file path, name, modification time\n",
    "- **Idempotency**: Ponowne wykonanie tego samego COPY INTO nie załaduje duplikatów\n",
    "- **mergeSchema**: Automatyczne dodawanie nowych kolumn (schema evolution)\n",
    "- **File tracking**: Delta Log przechowuje hash załadowanych plików"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4450590f",
   "metadata": {},
   "source": [
    "### Przykład 1.2: COPY INTO z filtrowaniem (PATTERN)\n",
    "\n",
    "**Cel:** Selective ingestion - tylko pliki spełniające pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9c7f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 1.2 - COPY INTO z PATTERN\n",
    "\n",
    "# Re-run tego samego COPY INTO - demonstracja idempotency\n",
    "print(\"=== Ponowne wykonanie COPY INTO (idempotency test) ===\")\n",
    "copy_result_2 = spark.sql(f\"\"\"\n",
    "    COPY INTO {copy_into_table}\n",
    "    FROM (SELECT *, _metadata FROM '{ORDERS_JSON}')\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('multiLine' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "display(copy_result_2)\n",
    "\n",
    "# Sprawdź czy count się nie zmienił (idempotency)\n",
    "new_count = spark.table(copy_into_table).count()\n",
    "print(f\"\\nLiczba rekordów (po ponownym COPY INTO): {new_count}\")\n",
    "print(f\"Czy idempotentny? {new_count == loaded_count}\")\n",
    "\n",
    "# Historia COPY INTO\n",
    "print(\"\\n=== Historia COPY INTO ===\")\n",
    "history = spark.sql(f\"DESCRIBE HISTORY {copy_into_table}\")\n",
    "display(\n",
    "    history\n",
    "    .filter(F.col(\"operation\") == \"COPY INTO\")\n",
    "    .select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6c25fb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 2: Auto Loader (CloudFiles)\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Auto Loader to Databricks-managed solution dla incremental file ingestion. Używa file notification (Event Grid/SQS) lub directory listing dla automatic discovery nowych plików. Idealny dla near real-time ingestion z małymi plikami.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **cloudFiles format**: Specjalny format Spark dla Auto Loader\n",
    "- **Schema inference**: Automatyczne wykrywanie i ewolucja schematu\n",
    "- **Checkpoint location**: Przechowuje progress i schema history\n",
    "- **File notification**: Event-driven approach dla cloud storage\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Near real-time ingestion (latency: sekundy-minuty)\n",
    "- Małe pliki arriving continuously\n",
    "- Schema evolution bez manual intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e3f5b7",
   "metadata": {},
   "source": [
    "### Przykład 2.1: Auto Loader - Basic Setup\n",
    "\n",
    "**Cel:** Konfiguracja Auto Loader z schema inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd4b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 2.1 - Auto Loader basic\n",
    "\n",
    "autoloader_table = f\"{BRONZE_SCHEMA}.orders_autoloader\"\n",
    "autoloader_checkpoint = f\"{CHECKPOINT_PATH}/autoloader_orders\"\n",
    "\n",
    "# Auto Loader z readStream\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")  # Auto Loader format\n",
    "    .option(\"cloudFiles.format\", \"json\")  # Source format\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{autoloader_checkpoint}/schema\")  # Schema tracking\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")  # Infer types (not just strings)\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "print(\"=== Auto Loader Stream Schema ===\")\n",
    "orders_stream.printSchema()\n",
    "\n",
    "# Zapis z trigger(once) dla demo (batch mode)\n",
    "query = (\n",
    "    orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", autoloader_checkpoint)\n",
    "    .trigger(once=True)  # Process all available data, then stop\n",
    "    .table(autoloader_table)\n",
    ")\n",
    "\n",
    "# Czekaj na zakończenie\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Auto Loader completed\")\n",
    "print(f\"Załadowano {spark.table(autoloader_table).count()} rekordów\")\n",
    "\n",
    "# Wyświetl dane\n",
    "print(\"\\n=== Załadowane dane ===\")\n",
    "display(spark.table(autoloader_table).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ac2228",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Auto Loader:\n",
    "- **cloudFiles format**: Specjalny format dla Structured Streaming\n",
    "- **trigger(once=True)**: Batch mode - process all files, then stop (użyteczne dla testing)\n",
    "- **checkpointLocation**: Obowiązkowe - przechowuje progress i schema\n",
    "- **Schema inference**: Automatyczne wykrywanie typów (inferColumnTypes=true)\n",
    "\n",
    "W produkcji: używamy `trigger(processingTime='5 minutes')` dla continuous processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f20e7a",
   "metadata": {},
   "source": [
    "### Przykład 2.2: Auto Loader - Schema Evolution\n",
    "\n",
    "**Cel:** Demonstracja automatycznej ewolucji schematu przy nowych plikach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287ba41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 2.2 - Schema Evolution\n",
    "\n",
    "# Sprawdź schema location (gdzie Auto Loader przechowuje schema history)\n",
    "schema_location = f\"{autoloader_checkpoint}/schema\"\n",
    "print(f\"Schema location: {schema_location}\")\n",
    "\n",
    "# Lista plików w schema location\n",
    "print(\"\\n=== Schema history files ===\")\n",
    "schema_files = dbutils.fs.ls(schema_location)\n",
    "for file in schema_files:\n",
    "    print(f\"  {file.name}\")\n",
    "\n",
    "# Odczytaj schema history\n",
    "print(\"\\n=== Current Schema ===\")\n",
    "current_schema = spark.table(autoloader_table).schema\n",
    "for field in current_schema.fields:\n",
    "    print(f\"  {field.name}: {field.dataType}\")\n",
    "\n",
    "# W przypadku nowych plików z dodatkowymi kolumnami,\n",
    "# Auto Loader automatycznie zaktualizuje schemat\n",
    "print(\"\\n⚠️ Uwaga: Schema evolution działa automatycznie przy nowych plikach z dodatkowymi kolumnami\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79628954",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 3: Structured Streaming - Continuous Processing\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Structured Streaming to Spark API dla continuous data processing. Traktuje stream jako unbounded table z micro-batch execution. Zapewnia exactly-once semantics i fault tolerance.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **readStream / writeStream**: API dla streaming operations\n",
    "- **Trigger**: Processing interval (once, processingTime, availableNow)\n",
    "- **Output mode**: append, complete, update\n",
    "- **Watermark**: Time-based windowing dla late data handling\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Real-time ETL z Kafka, Event Hub, Kinesis\n",
    "- Continuous aggregations i windowing\n",
    "- Exactly-once processing semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470b6504",
   "metadata": {},
   "source": [
    "### Przykład 3.1: Structured Streaming - Basic Stream\n",
    "\n",
    "**Cel:** Utworzenie basic streaming pipeline z transformacjami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79512634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 3.1 - Structured Streaming basic\n",
    "\n",
    "streaming_table = f\"{BRONZE_SCHEMA}.orders_streaming\"\n",
    "streaming_checkpoint = f\"{CHECKPOINT_PATH}/streaming_orders\"\n",
    "\n",
    "# ReadStream z transformacjami\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{streaming_checkpoint}/schema\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "# Transformacje na streamie (jak na batch DataFrame)\n",
    "orders_transformed = (\n",
    "    orders_stream\n",
    "    .withColumn(\"order_date\", F.to_date(F.col(\"order_date\")))\n",
    "    .withColumn(\"order_status\", F.upper(F.col(\"order_status\")))\n",
    "    .withColumn(\"stream_processed_ts\", F.current_timestamp())\n",
    "    .filter(F.col(\"order_amount\") > 0)  # Quality check\n",
    ")\n",
    "\n",
    "print(\"=== Transformed Stream Schema ===\")\n",
    "orders_transformed.printSchema()\n",
    "\n",
    "# WriteStream z trigger(once) dla demo\n",
    "query = (\n",
    "    orders_transformed\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", streaming_checkpoint)\n",
    "    .trigger(once=True)\n",
    "    .table(streaming_table)\n",
    ")\n",
    "\n",
    "# Czekaj na zakończenie\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Streaming pipeline completed\")\n",
    "print(f\"Przetworzono {spark.table(streaming_table).count()} rekordów\")\n",
    "\n",
    "# Wyświetl dane z transformation\n",
    "print(\"\\n=== Przetworzone dane ===\")\n",
    "display(\n",
    "    spark.table(streaming_table)\n",
    "    .select(\"order_id\", \"order_date\", \"order_status\", \"stream_processed_ts\")\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961206d2",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Structured Streaming:\n",
    "- **Transformacje**: Możemy używać standardowych DataFrame API (withColumn, filter, join)\n",
    "- **trigger(once=True)**: Batch mode - użyteczne dla testing i backfill\n",
    "- **outputMode=\"append\"**: Tylko nowe rekordy zapisywane (domyślne dla streaming)\n",
    "- **checkpointLocation**: Fault tolerance - możliwość recovery po failure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ac4add",
   "metadata": {},
   "source": [
    "### Przykład 3.2: Streaming Aggregations\n",
    "\n",
    "**Cel:** Continuous aggregations na streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10052b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 3.2 - Streaming Aggregations\n",
    "\n",
    "streaming_agg_table = f\"{BRONZE_SCHEMA}.orders_streaming_agg\"\n",
    "agg_checkpoint = f\"{CHECKPOINT_PATH}/streaming_agg\"\n",
    "\n",
    "# ReadStream\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{agg_checkpoint}/schema\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "# Agregacje: count i sum per status\n",
    "orders_agg = (\n",
    "    orders_stream\n",
    "    .withColumn(\"order_status\", F.upper(F.col(\"order_status\")))\n",
    "    .groupBy(\"order_status\")\n",
    "    .agg(\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.sum(\"order_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"order_amount\").alias(\"avg_order_value\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# WriteStream z outputMode=\"complete\" dla agregacji\n",
    "query = (\n",
    "    orders_agg\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"complete\")  # Complete mode dla groupBy bez watermark\n",
    "    .option(\"checkpointLocation\", agg_checkpoint)\n",
    "    .trigger(once=True)\n",
    "    .table(streaming_agg_table)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Streaming aggregation completed\")\n",
    "print(\"\\n=== Wyniki agregacji ===\")\n",
    "display(spark.table(streaming_agg_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6546288",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Streaming Aggregations:\n",
    "- **outputMode=\"complete\"**: Cała tabela wynikowa zapisywana przy każdym micro-batch (wymagane dla groupBy bez watermark)\n",
    "- **outputMode=\"update\"**: Tylko zaktualizowane wiersze (użyteczne z watermark)\n",
    "- **Stateful operations**: GroupBy/Aggregations wymagają state management (przechowywany w checkpoint)\n",
    "\n",
    "W produkcji: używamy watermark dla windowed aggregations i outputMode=\"update\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e6171b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 4: MERGE na Streamingu (Upsert)\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Structured Streaming może pisać do Delta z MERGE logic (upsert). Używamy foreachBatch dla custom write logic w każdym micro-batch.\n",
    "\n",
    "**Zastosowanie:**\n",
    "- CDC (Change Data Capture) streaming\n",
    "- Upsert streaming events do dimension tables\n",
    "- Deduplication w real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c815aaeb",
   "metadata": {},
   "source": [
    "### Przykład 4.1: Streaming MERGE (Upsert)\n",
    "\n",
    "**Cel:** Implementacja streaming upsert z MERGE INTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c4ee24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 4.1 - Streaming MERGE\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Target table dla upsert\n",
    "upsert_table = f\"{BRONZE_SCHEMA}.orders_upsert\"\n",
    "upsert_checkpoint = f\"{CHECKPOINT_PATH}/streaming_upsert\"\n",
    "\n",
    "# Utwórz target table jeśli nie istnieje\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {upsert_table} (\n",
    "        order_id INT,\n",
    "        customer_id INT,\n",
    "        order_date STRING,\n",
    "        order_amount DOUBLE,\n",
    "        order_status STRING,\n",
    "        last_updated TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# ForeachBatch function dla MERGE\n",
    "def upsert_to_delta(microBatchDF, batchId):\n",
    "    # Dodaj last_updated timestamp\n",
    "    microBatchDF = microBatchDF.withColumn(\"last_updated\", F.current_timestamp())\n",
    "    \n",
    "    # DeltaTable dla MERGE\n",
    "    deltaTable = DeltaTable.forName(spark, upsert_table)\n",
    "    \n",
    "    # MERGE INTO logic\n",
    "    (\n",
    "        deltaTable.alias(\"target\")\n",
    "        .merge(\n",
    "            microBatchDF.alias(\"source\"),\n",
    "            \"target.order_id = source.order_id\"\n",
    "        )\n",
    "        .whenMatchedUpdate(set = {\n",
    "            \"order_status\": \"source.order_status\",\n",
    "            \"order_amount\": \"source.order_amount\",\n",
    "            \"last_updated\": \"source.last_updated\"\n",
    "        })\n",
    "        .whenNotMatchedInsert(values = {\n",
    "            \"order_id\": \"source.order_id\",\n",
    "            \"customer_id\": \"source.customer_id\",\n",
    "            \"order_date\": \"source.order_date\",\n",
    "            \"order_amount\": \"source.order_amount\",\n",
    "            \"order_status\": \"source.order_status\",\n",
    "            \"last_updated\": \"source.last_updated\"\n",
    "        })\n",
    "        .execute()\n",
    "    )\n",
    "    \n",
    "    print(f\"Batch {batchId}: Merged {microBatchDF.count()} records\")\n",
    "\n",
    "# ReadStream\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{upsert_checkpoint}/schema\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "# WriteStream z foreachBatch\n",
    "query = (\n",
    "    orders_stream\n",
    "    .writeStream\n",
    "    .foreachBatch(upsert_to_delta)  # Custom MERGE logic\n",
    "    .option(\"checkpointLocation\", upsert_checkpoint)\n",
    "    .trigger(once=True)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Streaming MERGE completed\")\n",
    "print(f\"Final count: {spark.table(upsert_table).count()}\")\n",
    "\n",
    "print(\"\\n=== Upserted data ===\")\n",
    "display(spark.table(upsert_table).orderBy(\"order_id\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ae782c",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Streaming MERGE:\n",
    "- **foreachBatch**: Custom function wykonywana na każdym micro-batch\n",
    "- **MERGE INTO**: Upsert logic - UPDATE jeśli istnieje, INSERT jeśli nie\n",
    "- **Idempotency**: Ponowne procesowanie tego samego batch ID daje ten sam rezultat\n",
    "- **Use case**: CDC streaming, real-time dimension updates, deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e060929d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Porównanie metod ingestion\n",
    "\n",
    "| Feature | COPY INTO | Auto Loader | Structured Streaming |\n",
    "|---------|-----------|-------------|---------------------|\n",
    "| **Latency** | Minuty-godziny | Sekundy-minuty | Sub-sekundy |\n",
    "| **Use case** | Scheduled batch | Near real-time files | Pure streaming (Kafka) |\n",
    "| **Idempotency** | ✅ Built-in | ✅ Built-in | ⚠️ Requires checkpoint |\n",
    "| **Schema evolution** | ⚠️ Manual | ✅ Automatic | ⚠️ Manual |\n",
    "| **Complexity** | Low | Medium | High |\n",
    "| **Cost** | Lowest | Medium | Highest |\n",
    "| **File tracking** | Delta Log | Checkpoint | Checkpoint |\n",
    "\n",
    "**Rekomendacje:**\n",
    "- **COPY INTO**: Batch loads (hourly, daily), duże pliki, niskie koszty\n",
    "- **Auto Loader**: Near real-time, małe pliki, schema evolution\n",
    "- **Structured Streaming**: Pure streaming sources (Kafka), sub-second latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ec5a7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "**COPY INTO:**\n",
    "- Używaj dla scheduled batch jobs (daily, hourly)\n",
    "- Zawsze dodawaj _metadata column dla audytu\n",
    "- Używaj PATTERN dla filtrowania plików\n",
    "- Monitor operationMetrics w DESCRIBE HISTORY\n",
    "\n",
    "**Auto Loader:**\n",
    "- Włącz schema inference (inferColumnTypes=true)\n",
    "- Używaj trigger(availableNow=True) dla backfill\n",
    "- Monitor schema evolution w schemaLocation\n",
    "- Rozważ file notification dla dużej liczby plików (>10k)\n",
    "\n",
    "**Structured Streaming:**\n",
    "- Zawsze ustawiaj checkpointLocation\n",
    "- Używaj trigger(processingTime) dla continuous streams\n",
    "- Implementuj watermark dla windowed aggregations\n",
    "- Monitor stream metrics (numInputRows, inputRowsPerSecond)\n",
    "\n",
    "**Checkpoints:**\n",
    "- Przechowuj w niezależnej lokalizacji (nie w table location)\n",
    "- Backup przed schema changes\n",
    "- Nie usuwaj checkpoint - loss of progress!\n",
    "- Używaj external location (S3, ADLS) dla durability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de02682",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Problem 1: \"Stream stopped unexpectedly\"**\n",
    "**Rozwiązanie:**\n",
    "- Sprawdź checkpoint location - czy istnieje i jest writable\n",
    "- Sprawdź logi streaming query: `query.lastProgress`\n",
    "- Monitor exceptions: `query.exception()`\n",
    "\n",
    "**Problem 2: \"Schema mismatch in Auto Loader\"**\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Włącz schema evolution\n",
    ".option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    ".option(\"mergeSchema\", \"true\")\n",
    "```\n",
    "\n",
    "**Problem 3: COPY INTO nie wykrywa nowych plików**\n",
    "**Rozwiązanie:**\n",
    "- COPY INTO śledzi tylko file path - zmiana zawartości nie jest wykrywana\n",
    "- Użyj COPY_OPTIONS ('force' = 'true') dla re-ingestion\n",
    "\n",
    "**Problem 4: Streaming aggregation state grows indefinitely**\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Dodaj watermark dla time-based cleanup\n",
    ".withWatermark(\"event_time\", \"1 hour\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2806e9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "**W tym notebooku nauczyliśmy się:**\n",
    "\n",
    "✅ **COPY INTO:**\n",
    "- Idempotent batch loads z automatic file tracking\n",
    "- Pattern matching dla selective ingestion\n",
    "- _metadata column dla audytu\n",
    "\n",
    "✅ **Auto Loader:**\n",
    "- Near real-time file ingestion z cloudFiles format\n",
    "- Automatic schema inference i evolution\n",
    "- Checkpoint-based progress tracking\n",
    "\n",
    "✅ **Structured Streaming:**\n",
    "- Continuous processing z micro-batch architecture\n",
    "- Transformacje i agregacje na streaming data\n",
    "- foreachBatch dla custom write logic (MERGE)\n",
    "\n",
    "✅ **Triggering modes:**\n",
    "- trigger(once=True) - batch mode dla testing\n",
    "- trigger(processingTime) - continuous processing\n",
    "- trigger(availableNow=True) - backfill mode\n",
    "\n",
    "**Kluczowe wnioski:**\n",
    "1. Wybór metody ingestion zależy od latency requirements i source type\n",
    "2. COPY INTO dla scheduled batch, Auto Loader dla near real-time files\n",
    "3. Structured Streaming dla pure streaming sources (Kafka)\n",
    "4. Checkpoint location jest krytyczny dla fault tolerance\n",
    "\n",
    "**Następne kroki:**\n",
    "- **Kolejny notebook**: 04_bronze_silver_gold_pipeline.ipynb\n",
    "- **Warsztat praktyczny**: 02_ingestion_pipeline_workshop.ipynb\n",
    "- **Delta Live Tables**: Declarative pipelines z automatic orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5489ba0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Posprzątaj zasoby utworzone podczas notebooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d28db334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcjonalne czyszczenie zasobów testowych\n",
    "# UWAGA: Uruchom tylko jeśli chcesz usunąć wszystkie utworzone dane\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {copy_into_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {autoloader_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {streaming_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {streaming_agg_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {upsert_table}\")\n",
    "\n",
    "# Usuń checkpointy\n",
    "# dbutils.fs.rm(CHECKPOINT_PATH, recurse=True)\n",
    "\n",
    "# spark.catalog.clearCache()\n",
    "# print(\"Zasoby zostały wyczyszczone\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
