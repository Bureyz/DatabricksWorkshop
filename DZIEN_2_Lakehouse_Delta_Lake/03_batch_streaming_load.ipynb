{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50863a14-361e-4077-8fe2-f42bd65a65d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Batch & Streaming Load - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Opanowanie metod ładowania danych: COPY INTO, Auto Loader i Structured Streaming.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- COPY INTO: kiedy używać, parametry (FILEFORMAT, VALIDATION_MODE, PATTERN)\n",
    "- Auto Loader (CloudFiles): file notification, checkpointing, schema inference\n",
    "- Schema evolution w praktyce\n",
    "- Structured Streaming: micro-batch architecture\n",
    "- readStream() / writeStream()\n",
    "- Triggering: once vs processingTime\n",
    "- Zarządzanie checkpointami\n",
    "- MERGE na streamingu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1763efe9-b15a-4c32-ad7e-f899ffc1d5bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 2 - Lakehouse & Delta Lake\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "701be052-0360-4f23-9030-b397f1ae39d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie różnych metod ładowania danych do Delta Lake: batch vs streaming.\n",
    "\n",
    "**Podstawowe pojęcia:**\n",
    "- **COPY INTO**: SQL command dla batch loads z idempotency (incremental batch)\n",
    "- **Auto Loader**: Databricks-managed solution dla incremental file ingestion z automatycznym schema inference\n",
    "- **Structured Streaming**: Spark streaming API z micro-batch processing i exactly-once semantics\n",
    "- **Checkpoint**: Location przechowujący offset/progress dla fault tolerance\n",
    "\n",
    "**Dlaczego to ważne?**\n",
    "Wybór metody ingestion ma wpływ na latency, throughput, cost i operacyjną złożoność. COPY INTO dla batch (hourly/daily), Auto Loader dla near real-time z małymi plikami, Structured Streaming dla pure streaming sources (Kafka, Event Hub)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "17db2306-1000-44f9-b0f5-d3cd2242c56d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Izolacja per użytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "980be326-95f4-4ed8-89d3-0f078eec8873",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db8b7b29-4303-4090-a8ee-4bdfa34dd900",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e348ea39-ea05-4c0a-9570-b6caba44bc6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Wyświetl kontekst użytkownika\n",
    "print(\"=== Kontekst użytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Użytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog i schemat\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Ścieżki do danych i checkpointów\n",
    "ORDERS_BATCH_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "ORDERS_STREAMING_DIR = f\"{DATASET_BASE_PATH}/orders\"  # Folder dla plików streaming\n",
    "CHECKPOINT_PATH = f\"/tmp/{raw_user}/checkpoints\"\n",
    "\n",
    "print(f\"\\n=== Konfiguracja ===\")\n",
    "print(f\"Orders Batch JSON: {ORDERS_BATCH_JSON}\")\n",
    "print(f\"Orders Streaming Directory: {ORDERS_STREAMING_DIR}\")\n",
    "print(f\"Checkpoint path: {CHECKPOINT_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "12d8eef0-64b9-4062-a161-7a977a60d9de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 1: COPY INTO - Batch Ingestion\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "COPY INTO to SQL command dla idempotent batch loads. Automatycznie śledzi załadowane pliki i pomija duplikaty. Idealny dla scheduled batch jobs (hourly, daily).\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **Idempotency**: Wielokrotne wykonanie COPY INTO z tymi samymi plikami nie powoduje duplikatów\n",
    "- **File tracking**: Delta Log przechowuje listę załadowanych plików\n",
    "- **Pattern matching**: Możliwość filtrowania plików po nazwie (PATTERN)\n",
    "- **Validation mode**: Kontrola zachowania przy błędach (PERMISSIVE, FAILFAST)\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Scheduled batch loads z cloud storage (S3, ADLS, GCS)\n",
    "- Incremental data ingestion bez ręcznego trackowania offsetów\n",
    "- ETL pipelines z retry logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2d69ed90-cae8-4857-9374-021a63c6736f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.1: Podstawowy COPY INTO\n",
    "\n",
    "**Cel:** Załadowanie plików JSON za pomocą COPY INTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6e7c29c9-7767-4d28-8c23-6a2d8f4e118a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 1.1 - COPY INTO basic\n",
    "\n",
    "# Utwórz target table z poprawnymi typami danych\n",
    "copy_into_table = f\"{BRONZE_SCHEMA}.orders_copy_into\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {copy_into_table} (\n",
    "        order_id STRING,\n",
    "        customer_id STRING,\n",
    "        product_id STRING,\n",
    "        store_id STRING,\n",
    "        order_datetime TIMESTAMP,\n",
    "        quantity INT,\n",
    "        unit_price DOUBLE,\n",
    "        discount_percent INT,\n",
    "        total_amount DOUBLE,\n",
    "        payment_method STRING,\n",
    "        _metadata STRUCT<file_path: STRING, file_name: STRING, file_modification_time: TIMESTAMP>\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"✓ Utworzono target table: {copy_into_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c0a296a-2e2f-458e-b391-8e0febf71e2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wykonaj COPY INTO\n",
    "copy_result = spark.sql(f\"\"\"\n",
    "    COPY INTO {copy_into_table}\n",
    "    FROM (SELECT *, _metadata FROM '{ORDERS_BATCH_JSON}')\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('multiLine' = 'true')\n",
    "    COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "print(\"=== COPY INTO Result ===\")\n",
    "display(copy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50efcbcb-aedc-4c23-8f80-b18404f44390",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdź załadowane dane\n",
    "loaded_count = spark.table(copy_into_table).count()\n",
    "print(f\"Załadowano {loaded_count} rekordów\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "daf015b9-4011-4abb-bad6-446ddd4e6790",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wyświetl przykładowe dane z metadata\n",
    "print(\"=== Dane z metadata ===\")\n",
    "display(\n",
    "    spark.table(copy_into_table)\n",
    "    .select(\"order_id\", \"customer_id\", \"total_amount\", \"payment_method\", \"_metadata.file_name\")\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca735f16-ada6-4739-be2f-cf445cf39cec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "COPY INTO:\n",
    "- **_metadata column**: Automatycznie dodawana kolumna z file path, name, modification time\n",
    "- **Idempotency**: Ponowne wykonanie tego samego COPY INTO nie załaduje duplikatów\n",
    "- **mergeSchema**: Automatyczne dodawanie nowych kolumn (schema evolution)\n",
    "- **File tracking**: Delta Log przechowuje hash załadowanych plików"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "568467de-7a6d-4421-8a1f-e271aec2b8c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.2: COPY INTO z VALIDATION_MODE\n",
    "\n",
    "**Cel:** Kontrola zachowania przy błędach w danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d05fd094-886d-41b1-a9ba-655157f0e9a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 1.2 - COPY INTO z VALIDATION_MODE\n",
    "\n",
    "# VALIDATION_MODE pozwala testować ingestion bez zapisywania danych\n",
    "# Użyteczne dla weryfikacji schema i quality przed faktycznym załadowaniem\n",
    "\n",
    "validation_table = f\"{BRONZE_SCHEMA}.orders_validation\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {validation_table} (\n",
    "        order_id STRING,\n",
    "        customer_id STRING,\n",
    "        product_id STRING,\n",
    "        store_id STRING,\n",
    "        order_datetime TIMESTAMP,\n",
    "        quantity INT,\n",
    "        unit_price DOUBLE,\n",
    "        discount_percent INT,\n",
    "        total_amount DOUBLE,\n",
    "        payment_method STRING,\n",
    "        _metadata STRUCT<file_path: STRING, file_name: STRING, file_modification_time: TIMESTAMP>\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"✓ Utworzono validation table: {validation_table}\")\n",
    "\n",
    "# Re-run tego samego COPY INTO - demonstracja idempotency\n",
    "print(\"=== Ponowne wykonanie COPY INTO (idempotency test) ===\")\n",
    "copy_result_2 = spark.sql(f\"\"\"\n",
    "    COPY INTO {copy_into_table}\n",
    "    FROM (SELECT *, _metadata FROM '{ORDERS_BATCH_JSON}')\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('multiLine' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "display(copy_result_2)\n",
    "\n",
    "# Sprawdź czy count się nie zmienił (idempotency)\n",
    "new_count = spark.table(copy_into_table).count()\n",
    "print(f\"\\nLiczba rekordów (po ponownym COPY INTO): {new_count}\")\n",
    "print(f\"Czy idempotentny? {new_count == loaded_count}\")\n",
    "\n",
    "# Historia COPY INTO\n",
    "print(\"\\n=== Historia COPY INTO ===\")\n",
    "history = spark.sql(f\"DESCRIBE HISTORY {copy_into_table}\")\n",
    "display(\n",
    "    history\n",
    "    .filter(F.col(\"operation\") == \"COPY INTO\")\n",
    "    .select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9bac845-f388-4f45-a1ec-494941877fcf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# VALIDATION_MODE = N - validate tylko N rekordów (dry run)\n",
    "print(\"=== VALIDATION_MODE - Test 10 rekordów ===\")\n",
    "validation_result = spark.sql(f\"\"\"\n",
    "    COPY INTO {validation_table}\n",
    "    FROM (SELECT *, _metadata FROM '{ORDERS_BATCH_JSON}')\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('multiLine' = 'true')\n",
    "    COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "    VALIDATION_MODE = 10\n",
    "\"\"\")\n",
    "\n",
    "display(validation_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5281c90b-faca-443d-a73e-7eda59fbffbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Po pozytywnej walidacji - faktyczne załadowanie\n",
    "print(\"\\n=== Faktyczne załadowanie danych ===\")\n",
    "load_result = spark.sql(f\"\"\"\n",
    "    COPY INTO {validation_table}\n",
    "    FROM (SELECT *, _metadata FROM '{ORDERS_BATCH_JSON}')\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('multiLine' = 'true')\n",
    "    COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "display(load_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fa9b952-822b-4f07-be62-5296182e94c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.3: COPY INTO z PATTERN (Selective Ingestion)\n",
    "\n",
    "**Cel:** Filtrowanie plików po nazwie - załaduj tylko pliki spełniające pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2e4dd3ec-34cc-44af-b5a4-aef64bbe0a01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 1.3 - COPY INTO z PATTERN\n",
    "\n",
    "# PATTERN pozwala filtrować pliki po nazwie\n",
    "# Przydatne gdy mamy wiele plików, ale chcemy załadować tylko konkretne\n",
    "\n",
    "pattern_table = f\"{BRONZE_SCHEMA}.orders_pattern\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {pattern_table} (\n",
    "        order_id STRING,\n",
    "        customer_id STRING,\n",
    "        product_id STRING,\n",
    "        store_id STRING,\n",
    "        order_datetime TIMESTAMP,\n",
    "        quantity INT,\n",
    "        unit_price DOUBLE,\n",
    "        discount_percent INT,\n",
    "        total_amount DOUBLE,\n",
    "        payment_method STRING,\n",
    "        _metadata STRUCT<file_path: STRING, file_name: STRING, file_modification_time: TIMESTAMP>\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"✓ Utworzono pattern table: {pattern_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef1e22d9-b70e-4b3f-866f-3a321e3e14b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Załaduj tylko pliki streaming 001, 002, 003 (pattern)\n",
    "print(\"=== COPY INTO z PATTERN - tylko pliki 001-003 ===\")\n",
    "pattern_result = spark.sql(f\"\"\"\n",
    "    COPY INTO {pattern_table}\n",
    "    FROM (SELECT *, _metadata FROM '{ORDERS_STREAMING_DIR}')\n",
    "    FILEFORMAT = JSON\n",
    "    PATTERN = 'orders_stream_00[1-3].json'\n",
    "    FORMAT_OPTIONS ('multiLine' = 'true')\n",
    "    COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "display(pattern_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edc7583a-89b8-4eda-a2ba-1dcb0b76f9e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdź które pliki zostały załadowane\n",
    "print(\"\\n=== Załadowane pliki (metadata) ===\")\n",
    "display(\n",
    "    spark.table(pattern_table)\n",
    "    .select(\"_metadata.file_name\")\n",
    "    .distinct()\n",
    "    .orderBy(\"file_name\")\n",
    ")\n",
    "\n",
    "print(f\"\\nŁącznie załadowano {spark.table(pattern_table).count()} rekordów z plików 001-003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ce32dfa-5a29-4fbc-9b91-a00f308c1438",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.4: COPY INTO - Test Idempotency\n",
    "\n",
    "**Cel:** Demonstracja że ponowne wykonanie COPY INTO nie powoduje duplikatów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93444ce9-c35b-4279-996c-4c856fa9836c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 1.4 - Test idempotency\n",
    "\n",
    "# Zapisz obecny count\n",
    "count_before = spark.table(copy_into_table).count()\n",
    "print(f\"Liczba rekordów przed ponownym COPY INTO: {count_before}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "819eff33-9fce-43f0-88f6-44f3af666af2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Ponowne wykonanie COPY INTO\n",
    "print(\"\\n=== Ponowne wykonanie COPY INTO (idempotency test) ===\")\n",
    "copy_result_2 = spark.sql(f\"\"\"\n",
    "    COPY INTO {copy_into_table}\n",
    "    FROM (SELECT *, _metadata FROM '{ORDERS_BATCH_JSON}')\n",
    "    FILEFORMAT = JSON\n",
    "    FORMAT_OPTIONS ('multiLine' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "display(copy_result_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f42793f9-02c3-43de-a1ae-25ae74ce6254",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdź czy count się nie zmienił (idempotency)\n",
    "count_after = spark.table(copy_into_table).count()\n",
    "print(f\"\\nLiczba rekordów po ponownym COPY INTO: {count_after}\")\n",
    "print(f\"Czy idempotentny? {count_after == count_before} (żadne nowe rekordy nie zostały załadowane)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ddd5ede-457b-4dd9-af19-dac07b853daa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Historia COPY INTO\n",
    "print(\"\\n=== Historia COPY INTO ===\")\n",
    "history = spark.sql(f\"DESCRIBE HISTORY {copy_into_table}\")\n",
    "display(\n",
    "    history\n",
    "    .filter(F.col(\"operation\") == \"COPY INTO\")\n",
    "    .select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4aebcfaa-f4b1-467b-8746-6bb23160a620",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 2: Auto Loader (CloudFiles)\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Auto Loader to Databricks-managed solution dla incremental file ingestion. Używa file notification (Event Grid/SQS) lub directory listing dla automatic discovery nowych plików. Idealny dla near real-time ingestion z małymi plikami.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **cloudFiles format**: Specjalny format Spark dla Auto Loader\n",
    "- **Schema inference**: Automatyczne wykrywanie i ewolucja schematu\n",
    "- **Checkpoint location**: Przechowuje progress i schema history\n",
    "- **File notification**: Event-driven approach dla cloud storage\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Near real-time ingestion (latency: sekundy-minuty)\n",
    "- Małe pliki arriving continuously\n",
    "- Schema evolution bez manual intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "04db630b-9516-4d0d-836e-230cab3502aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.1: Auto Loader - Basic Setup\n",
    "\n",
    "**Cel:** Konfiguracja Auto Loader z schema inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10e16d3f-170b-4b6c-b830-fc6e61258301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 2.1 - Auto Loader basic\n",
    "\n",
    "autoloader_table = f\"{BRONZE_SCHEMA}.orders_autoloader\"\n",
    "autoloader_checkpoint = f\"{CHECKPOINT_PATH}/autoloader_orders\"\n",
    "\n",
    "# Auto Loader z readStream - używa folderu (automatycznie znajdzie wszystkie pliki JSON)\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")  # Auto Loader format\n",
    "    .option(\"cloudFiles.format\", \"json\")  # Source format\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{autoloader_checkpoint}/schema\")  # Schema tracking\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")  # Infer types (not just strings)\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_STREAMING_DIR)  # Folder - Auto Loader znajdzie wszystkie pliki JSON\n",
    ")\n",
    "\n",
    "print(\"=== Auto Loader Stream Schema ===\")\n",
    "orders_stream.printSchema()\n",
    "\n",
    "# Zapis z trigger(once) dla demo (batch mode)\n",
    "query = (\n",
    "    orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", autoloader_checkpoint)\n",
    "    .trigger(once=True)  # Process all available data, then stop\n",
    "    .table(autoloader_table)\n",
    ")\n",
    "\n",
    "# Czekaj na zakończenie\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Auto Loader completed\")\n",
    "print(f\"Załadowano {spark.table(autoloader_table).count()} rekordów\")\n",
    "\n",
    "# Wyświetl dane\n",
    "print(\"\\n=== Załadowane dane ===\")\n",
    "display(spark.table(autoloader_table).limit(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad4ae641-55e9-41d0-b434-e8c7aa0b9e91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zapis z trigger(once) dla demo (batch mode)\n",
    "query = (\n",
    "    orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", autoloader_checkpoint)\n",
    "    .trigger(once=True)  # Process all available data, then stop\n",
    "    .table(autoloader_table)\n",
    ")\n",
    "\n",
    "# Czekaj na zakończenie\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Auto Loader completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90d28018-6733-4199-b10c-226ee26a69a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdź załadowane rekordy\n",
    "print(f\"Załadowano {spark.table(autoloader_table).count()} rekordów\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a960cb8-362f-419e-baf2-8392076576a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wyświetl dane\n",
    "print(\"=== Załadowane dane ===\")\n",
    "display(spark.table(autoloader_table).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "345d48aa-be3c-4f2e-9fdc-95122b32f70d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Auto Loader:\n",
    "- **cloudFiles format**: Specjalny format dla Structured Streaming\n",
    "- **trigger(once=True)**: Batch mode - process all files, then stop (użyteczne dla testing)\n",
    "- **checkpointLocation**: Obowiązkowe - przechowuje progress i schema\n",
    "- **Schema inference**: Automatyczne wykrywanie typów (inferColumnTypes=true)\n",
    "\n",
    "W produkcji: używamy `trigger(processingTime='5 minutes')` dla continuous processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fa06510-d42f-4994-a118-45b841678c49",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.2: Auto Loader - Schema Evolution\n",
    "\n",
    "**Cel:** Demonstracja automatycznej ewolucji schematu przy nowych plikach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77f3d182-de9d-4b4f-b981-a805190b2e89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 2.2 - Schema Evolution\n",
    "\n",
    "# Sprawdź schema location (gdzie Auto Loader przechowuje schema history)\n",
    "schema_location = f\"{autoloader_checkpoint}/schema\"\n",
    "print(f\"Schema location: {schema_location}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a87c8d52-4dcc-4292-8fd9-5b0e1d185581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lista plików w schema location\n",
    "print(\"=== Schema history files ===\")\n",
    "schema_files = dbutils.fs.ls(schema_location)\n",
    "for file in schema_files:\n",
    "    print(f\"  {file.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11dcdb8d-f809-4dd2-9401-fae87016473e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Odczytaj schema history\n",
    "print(\"=== Current Schema ===\")\n",
    "current_schema = spark.table(autoloader_table).schema\n",
    "for field in current_schema.fields:\n",
    "    print(f\"  {field.name}: {field.dataType}\")\n",
    "\n",
    "# W przypadku nowych plików z dodatkowymi kolumnami,\n",
    "# Auto Loader automatycznie zaktualizuje schemat\n",
    "print(\"\\n⚠️ Uwaga: Schema evolution działa automatycznie przy nowych plikach z dodatkowymi kolumnami\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "34ac9475-79ae-41c6-b576-c83ddc2eb04b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 3: Structured Streaming - Continuous Processing\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Structured Streaming to Spark API dla continuous data processing. Traktuje stream jako unbounded table z micro-batch execution. Zapewnia exactly-once semantics i fault tolerance.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **readStream / writeStream**: API dla streaming operations\n",
    "- **Trigger**: Processing interval (once, processingTime, availableNow)\n",
    "- **Output mode**: append, complete, update\n",
    "- **Watermark**: Time-based windowing dla late data handling\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Real-time ETL z Kafka, Event Hub, Kinesis\n",
    "- Continuous aggregations i windowing\n",
    "- Exactly-once processing semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6424b29d-a468-4db5-aa3b-10bbcc938550",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.1: Structured Streaming - Basic Stream\n",
    "\n",
    "**Cel:** Utworzenie basic streaming pipeline z transformacjami"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26295d30-338d-473d-9404-a43f9f30051a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 3.1 - Structured Streaming basic\n",
    "\n",
    "streaming_table = f\"{BRONZE_SCHEMA}.orders_streaming\"\n",
    "streaming_checkpoint = f\"{CHECKPOINT_PATH}/streaming_orders\"\n",
    "\n",
    "# ReadStream z transformacjami - używa folderu\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{streaming_checkpoint}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_STREAMING_DIR)  # Folder - Auto Loader znajdzie wszystkie pliki JSON\n",
    ")\n",
    "\n",
    "# Transformacje na streamie (jak na batch DataFrame)\n",
    "orders_transformed = (\n",
    "    orders_stream\n",
    "    .withColumn(\"order_date\", F.to_date(F.col(\"order_datetime\")))  # Ekstrakcja daty z timestamp\n",
    "    .withColumn(\"payment_method_upper\", F.upper(F.col(\"payment_method\")))  # Normalizacja payment_method\n",
    "    .withColumn(\"stream_processed_ts\", F.current_timestamp())\n",
    "    .filter(F.col(\"total_amount\") > 0)  # Quality check - pozytywna kwota\n",
    ")\n",
    "\n",
    "print(\"=== Transformed Stream Schema ===\")\n",
    "orders_transformed.printSchema()\n",
    "\n",
    "# WriteStream z trigger(once) dla demo\n",
    "query = (\n",
    "    orders_transformed\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", streaming_checkpoint)\n",
    "    .trigger(once=True)\n",
    "    .table(streaming_table)\n",
    ")\n",
    "\n",
    "# Czekaj na zakończenie\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Streaming pipeline completed\")\n",
    "print(f\"Przetworzono {spark.table(streaming_table).count()} rekordów\")\n",
    "\n",
    "# Wyświetl dane z transformation\n",
    "print(\"\\n=== Przetworzone dane ===\")\n",
    "display(\n",
    "    spark.table(streaming_table)\n",
    "    .select(\"order_id\", \"order_date\", \"payment_method\", \"payment_method_upper\", \"total_amount\", \"stream_processed_ts\")\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a13081d-fe25-4b01-8e24-c051596d8ca8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WriteStream z trigger(once) dla demo\n",
    "query = (\n",
    "    orders_transformed\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", streaming_checkpoint)\n",
    "    .trigger(once=True)\n",
    "    .table(streaming_table)\n",
    ")\n",
    "\n",
    "# Czekaj na zakończenie\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Streaming pipeline completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b545fd11-6570-4b87-9539-c4cf8269f0a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdź przetworzone rekordy\n",
    "print(f\"Przetworzono {spark.table(streaming_table).count()} rekordów\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40154bd4-3c87-4143-bff2-5d82d1a46dee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wyświetl dane z transformation\n",
    "print(\"=== Przetworzone dane ===\")\n",
    "display(\n",
    "    spark.table(streaming_table)\n",
    "    .select(\"order_id\", \"order_date\", \"payment_method\", \"payment_method_upper\", \"total_amount\", \"stream_processed_ts\")\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dbb3573-2d9f-4b7a-b614-3deb16a96aaf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Structured Streaming:\n",
    "- **Transformacje**: Możemy używać standardowych DataFrame API (withColumn, filter, join)\n",
    "- **trigger(once=True)**: Batch mode - użyteczne dla testing i backfill\n",
    "- **outputMode=\"append\"**: Tylko nowe rekordy zapisywane (domyślne dla streaming)\n",
    "- **checkpointLocation**: Fault tolerance - możliwość recovery po failure\n",
    "- **Przykładowe transformacje**: ekstrakcja daty z timestamp, normalizacja payment_method, quality checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6dd768f-c89a-495e-80b1-4d9456aef61c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.2: Streaming Aggregations\n",
    "\n",
    "**Cel:** Continuous aggregations na streaming data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b03d1e5c-a199-4960-a8ee-5211baadde4e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 3.2 - Streaming Aggregations\n",
    "\n",
    "streaming_agg_table = f\"{BRONZE_SCHEMA}.orders_streaming_agg\"\n",
    "agg_checkpoint = f\"{CHECKPOINT_PATH}/streaming_agg\"\n",
    "\n",
    "# ReadStream - używa folderu\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{agg_checkpoint}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_STREAMING_DIR)  # Folder - Auto Loader znajdzie wszystkie pliki JSON\n",
    ")\n",
    "\n",
    "# Agregacje: count i sum per payment_method\n",
    "orders_agg = (\n",
    "    orders_stream\n",
    "    .withColumn(\"payment_method_upper\", F.upper(F.col(\"payment_method\")))\n",
    "    .groupBy(\"payment_method_upper\")\n",
    "    .agg(\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_order_value\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# WriteStream z outputMode=\"complete\" dla agregacji\n",
    "query = (\n",
    "    orders_agg\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"complete\")  # Complete mode dla groupBy bez watermark\n",
    "    .option(\"checkpointLocation\", agg_checkpoint)\n",
    "    .trigger(once=True)\n",
    "    .table(streaming_agg_table)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Streaming aggregation completed\")\n",
    "print(\"\\n=== Wyniki agregacji per payment method ===\")\n",
    "display(spark.table(streaming_agg_table).orderBy(\"payment_method_upper\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "535d8bde-ea9e-4390-9b00-9ee2fee76c51",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WriteStream z outputMode=\"complete\" dla agregacji\n",
    "query = (\n",
    "    orders_agg\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"complete\")  # Complete mode dla groupBy bez watermark\n",
    "    .option(\"checkpointLocation\", agg_checkpoint)\n",
    "    .trigger(once=True)\n",
    "    .table(streaming_agg_table)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Streaming aggregation completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d6ab30b0-4b14-434c-8a39-88dce5e937f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wyświetl wyniki agregacji per payment method\n",
    "print(\"=== Wyniki agregacji per payment method ===\")\n",
    "display(spark.table(streaming_agg_table).orderBy(\"payment_method_upper\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aee95df1-b4ad-408c-a081-38f7dba2c08a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Streaming Aggregations:\n",
    "- **outputMode=\"complete\"**: Cała tabela wynikowa zapisywana przy każdym micro-batch (wymagane dla groupBy bez watermark)\n",
    "- **outputMode=\"update\"**: Tylko zaktualizowane wiersze (użyteczne z watermark)\n",
    "- **Stateful operations**: GroupBy/Aggregations wymagają state management (przechowywany w checkpoint)\n",
    "- **Use case**: Agregacje per payment method (Cash, Credit Card, Debit Card, PayPal) w czasie rzeczywistym\n",
    "\n",
    "W produkcji: używamy watermark dla windowed aggregations i outputMode=\"update\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95ec0f29-1735-435b-9865-13daaebffde0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.3: Różne tryby Triggering\n",
    "\n",
    "**Cel:** Demonstracja różnych trybów triggering: once, availableNow, processingTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16f6e790-24a5-4fc6-ae8a-c0ed58a8dfd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 3.3a - trigger(once=True)\n",
    "# Process all available data in single batch, then stop\n",
    "\n",
    "trigger_once_table = f\"{BRONZE_SCHEMA}.orders_trigger_once\"\n",
    "trigger_once_checkpoint = f\"{CHECKPOINT_PATH}/trigger_once\"\n",
    "\n",
    "print(\"=== Trigger: once=True ===\")\n",
    "print(\"Use case: Testing, backfill, one-time ingestion\")\n",
    "\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{trigger_once_checkpoint}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_STREAMING_DIR)\n",
    ")\n",
    "\n",
    "query = (\n",
    "    orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", trigger_once_checkpoint)\n",
    "    .trigger(once=True)  # Process all data once, then stop\n",
    "    .table(trigger_once_table)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(f\"✓ Trigger(once) completed: {spark.table(trigger_once_table).count()} rekordów\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "030d3777-1350-4593-974a-48a29217ec3f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 3.3b - trigger(availableNow=True)\n",
    "# Process all available data in multiple micro-batches, then stop\n",
    "\n",
    "trigger_available_table = f\"{BRONZE_SCHEMA}.orders_trigger_available\"\n",
    "trigger_available_checkpoint = f\"{CHECKPOINT_PATH}/trigger_available\"\n",
    "\n",
    "print(\"\\n=== Trigger: availableNow=True ===\")\n",
    "print(\"Use case: Backfill z zachowaniem micro-batch semantics\")\n",
    "\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{trigger_available_checkpoint}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_STREAMING_DIR)\n",
    ")\n",
    "\n",
    "query = (\n",
    "    orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", trigger_available_checkpoint)\n",
    "    .trigger(availableNow=True)  # Process all available data in micro-batches, then stop\n",
    "    .table(trigger_available_table)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "print(f\"✓ Trigger(availableNow) completed: {spark.table(trigger_available_table).count()} rekordów\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9fcdafcb-bcb5-40fc-8501-f2d61aaea1b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 3.3c - trigger(processingTime='10 seconds')\n",
    "# Continuous processing z micro-batch co 10 sekund\n",
    "\n",
    "trigger_continuous_table = f\"{BRONZE_SCHEMA}.orders_trigger_continuous\"\n",
    "trigger_continuous_checkpoint = f\"{CHECKPOINT_PATH}/trigger_continuous\"\n",
    "\n",
    "print(\"\\n=== Trigger: processingTime='10 seconds' ===\")\n",
    "print(\"Use case: Near real-time continuous processing\")\n",
    "print(\"⚠️ Ten przykład uruchomi streaming query na 30 sekund, potem zatrzyma\")\n",
    "\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{trigger_continuous_checkpoint}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_STREAMING_DIR)\n",
    ")\n",
    "\n",
    "query = (\n",
    "    orders_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", trigger_continuous_checkpoint)\n",
    "    .trigger(processingTime='10 seconds')  # Continuous: micro-batch co 10 sekund\n",
    "    .table(trigger_continuous_table)\n",
    ")\n",
    "\n",
    "# Start query (non-blocking)\n",
    "query_handle = query.start()\n",
    "\n",
    "print(\"Query running...\")\n",
    "print(\"Czekam 30 sekund (pozwoli na ~3 micro-batches)...\")\n",
    "time.sleep(30)\n",
    "\n",
    "# Stop query\n",
    "query_handle.stop()\n",
    "print(f\"\\n✓ Trigger(processingTime) stopped after 30s: {spark.table(trigger_continuous_table).count()} rekordów\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6fa243-c5b2-450c-89b9-fab31c12b4af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Porównanie triggerów:**\n",
    "\n",
    "| Trigger | Use Case | Zatrzyma się? | Micro-batches |\n",
    "|---------|----------|---------------|---------------|\n",
    "| `once=True` | Testing, backfill | ✅ Tak | 1 batch |\n",
    "| `availableNow=True` | Backfill z micro-batch | ✅ Tak | Multiple |\n",
    "| `processingTime='X'` | Continuous production | ❌ Nie | Infinite |\n",
    "\n",
    "**Rekomendacje:**\n",
    "- **once**: Testing w notebookach, one-time data load\n",
    "- **availableNow**: Backfill historycznych danych z zachowaniem micro-batch semantics\n",
    "- **processingTime**: Production continuous streaming (Kafka, Event Hub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db5a1e3-eda7-4aa5-9a60-828c5e2e3dbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 4: MERGE na Streamingu (Upsert)\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Structured Streaming może pisać do Delta z MERGE logic (upsert). Używamy foreachBatch dla custom write logic w każdym micro-batch.\n",
    "\n",
    "**Zastosowanie:**\n",
    "- CDC (Change Data Capture) streaming\n",
    "- Upsert streaming events do dimension tables\n",
    "- Deduplication w real-time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "657a6b98-8f0b-4165-81f9-95c6509375e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.1: Streaming MERGE (Upsert)\n",
    "\n",
    "**Cel:** Implementacja streaming upsert z MERGE INTO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1488030f-2350-4851-8ddb-0dcc39f518cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 4.1 - Streaming MERGE\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Target table dla upsert\n",
    "upsert_table = f\"{BRONZE_SCHEMA}.orders_upsert\"\n",
    "upsert_checkpoint = f\"{CHECKPOINT_PATH}/streaming_upsert\"\n",
    "\n",
    "# Utwórz target table jeśli nie istnieje - poprawne typy danych\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {upsert_table} (\n",
    "        order_id STRING,\n",
    "        customer_id STRING,\n",
    "        product_id STRING,\n",
    "        store_id STRING,\n",
    "        order_datetime TIMESTAMP,\n",
    "        quantity INT,\n",
    "        unit_price DOUBLE,\n",
    "        discount_percent INT,\n",
    "        total_amount DOUBLE,\n",
    "        payment_method STRING,\n",
    "        last_updated TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"✓ Utworzono upsert table: {upsert_table}\")\n",
    "\n",
    "# ForeachBatch function dla MERGE\n",
    "def upsert_to_delta(microBatchDF, batchId):\n",
    "    # Dodaj last_updated timestamp\n",
    "    microBatchDF = microBatchDF.withColumn(\"last_updated\", F.current_timestamp())\n",
    "    \n",
    "    # DeltaTable dla MERGE\n",
    "    deltaTable = DeltaTable.forName(spark, upsert_table)\n",
    "    \n",
    "    # MERGE INTO logic\n",
    "    (\n",
    "        deltaTable.alias(\"target\")\n",
    "        .merge(\n",
    "            microBatchDF.alias(\"source\"),\n",
    "            \"target.order_id = source.order_id\"\n",
    "        )\n",
    "        .whenMatchedUpdate(set = {\n",
    "            \"payment_method\": \"source.payment_method\",\n",
    "            \"total_amount\": \"source.total_amount\",\n",
    "            \"order_datetime\": \"source.order_datetime\",\n",
    "            \"quantity\": \"source.quantity\",\n",
    "            \"last_updated\": \"source.last_updated\"\n",
    "        })\n",
    "        .whenNotMatchedInsert(values = {\n",
    "            \"order_id\": \"source.order_id\",\n",
    "            \"customer_id\": \"source.customer_id\",\n",
    "            \"product_id\": \"source.product_id\",\n",
    "            \"store_id\": \"source.store_id\",\n",
    "            \"order_datetime\": \"source.order_datetime\",\n",
    "            \"quantity\": \"source.quantity\",\n",
    "            \"unit_price\": \"source.unit_price\",\n",
    "            \"discount_percent\": \"source.discount_percent\",\n",
    "            \"total_amount\": \"source.total_amount\",\n",
    "            \"payment_method\": \"source.payment_method\",\n",
    "            \"last_updated\": \"source.last_updated\"\n",
    "        })\n",
    "        .execute()\n",
    "    )\n",
    "    \n",
    "    print(f\"Batch {batchId}: Merged {microBatchDF.count()} records\")\n",
    "\n",
    "# ReadStream - używa wildcard dla plików streaming\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{upsert_checkpoint}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_STREAMING_JSON)  # Wildcard dla orders_stream_*.json\n",
    ")\n",
    "\n",
    "# WriteStream z foreachBatch\n",
    "query = (\n",
    "    orders_stream\n",
    "    .writeStream\n",
    "    .foreachBatch(upsert_to_delta)  # Custom MERGE logic\n",
    "    .option(\"checkpointLocation\", upsert_checkpoint)\n",
    "    .trigger(once=True)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Streaming MERGE completed\")\n",
    "print(f\"Final count: {spark.table(upsert_table).count()}\")\n",
    "\n",
    "print(\"\\n=== Upserted data ===\")\n",
    "display(spark.table(upsert_table).orderBy(\"order_id\").limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "66914160-9f7a-4880-a196-9012c25529b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ForeachBatch function dla MERGE\n",
    "def upsert_to_delta(microBatchDF, batchId):\n",
    "    \"\"\"\n",
    "    Custom MERGE logic dla każdego micro-batch.\n",
    "    Wykonuje UPSERT (UPDATE jeśli istnieje, INSERT jeśli nie).\n",
    "    \"\"\"\n",
    "    # Dodaj last_updated timestamp\n",
    "    microBatchDF = microBatchDF.withColumn(\"last_updated\", F.current_timestamp())\n",
    "    \n",
    "    # DeltaTable dla MERGE\n",
    "    deltaTable = DeltaTable.forName(spark, upsert_table)\n",
    "    \n",
    "    # MERGE INTO logic\n",
    "    (\n",
    "        deltaTable.alias(\"target\")\n",
    "        .merge(\n",
    "            microBatchDF.alias(\"source\"),\n",
    "            \"target.order_id = source.order_id\"\n",
    "        )\n",
    "        .whenMatchedUpdate(set = {\n",
    "            \"payment_method\": \"source.payment_method\",\n",
    "            \"total_amount\": \"source.total_amount\",\n",
    "            \"order_datetime\": \"source.order_datetime\",\n",
    "            \"quantity\": \"source.quantity\",\n",
    "            \"last_updated\": \"source.last_updated\"\n",
    "        })\n",
    "        .whenNotMatchedInsert(values = {\n",
    "            \"order_id\": \"source.order_id\",\n",
    "            \"customer_id\": \"source.customer_id\",\n",
    "            \"product_id\": \"source.product_id\",\n",
    "            \"store_id\": \"source.store_id\",\n",
    "            \"order_datetime\": \"source.order_datetime\",\n",
    "            \"quantity\": \"source.quantity\",\n",
    "            \"unit_price\": \"source.unit_price\",\n",
    "            \"discount_percent\": \"source.discount_percent\",\n",
    "            \"total_amount\": \"source.total_amount\",\n",
    "            \"payment_method\": \"source.payment_method\",\n",
    "            \"last_updated\": \"source.last_updated\"\n",
    "        })\n",
    "        .execute()\n",
    "    )\n",
    "    \n",
    "    print(f\"Batch {batchId}: Merged {microBatchDF.count()} records\")\n",
    "\n",
    "print(\"✓ Funkcja upsert_to_delta zdefiniowana\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a3d1876-1125-470f-a421-e6f7f2b384e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ReadStream - używa folderu\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{upsert_checkpoint}/schema\")\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_STREAMING_DIR)  # Folder - Auto Loader znajdzie wszystkie pliki JSON\n",
    ")\n",
    "\n",
    "print(\"✓ Stream configured dla MERGE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68f3a83f-f553-46bd-abae-ec0b17a2c15a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# WriteStream z foreachBatch\n",
    "query = (\n",
    "    orders_stream\n",
    "    .writeStream\n",
    "    .foreachBatch(upsert_to_delta)  # Custom MERGE logic\n",
    "    .option(\"checkpointLocation\", upsert_checkpoint)\n",
    "    .trigger(once=True)\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "print(f\"\\n✓ Streaming MERGE completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f442151-d08e-43f4-b10a-646eb00292ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Sprawdź final count\n",
    "print(f\"Final count: {spark.table(upsert_table).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95084d0e-383b-496e-9ba7-8f0f65a87f59",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wyświetl upserted data\n",
    "print(\"=== Upserted data ===\")\n",
    "display(spark.table(upsert_table).orderBy(\"order_id\").limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c974318d-f6e4-42f2-9cdc-e0d6a383300d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Streaming MERGE:\n",
    "- **foreachBatch**: Custom function wykonywana na każdym micro-batch\n",
    "- **MERGE INTO**: Upsert logic - UPDATE jeśli istnieje, INSERT jeśli nie\n",
    "- **Idempotency**: Ponowne procesowanie tego samego batch ID daje ten sam rezultat\n",
    "- **Use case**: CDC streaming, real-time dimension updates, deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d2f1197-c9e5-4334-9064-0e58f6efb673",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.2: Zarządzanie Checkpointami\n",
    "\n",
    "**Cel:** Demonstracja jak działa checkpoint location i jak go zarządzać"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d35d7ab-2e55-491f-8253-a5a97dac28b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 4.2 - Zarządzanie checkpointami\n",
    "\n",
    "# Checkpoint location przechowuje state streaming query\n",
    "# Jest krytyczny dla fault tolerance i exactly-once semantics\n",
    "\n",
    "print(\"=== Checkpoint Locations ===\")\n",
    "print(f\"Auto Loader: {autoloader_checkpoint}\")\n",
    "print(f\"Streaming: {streaming_checkpoint}\")\n",
    "print(f\"Upsert: {upsert_checkpoint}\")\n",
    "\n",
    "# Sprawdź strukturę checkpoint location\n",
    "print(f\"\\n=== Struktura checkpoint (autoloader) ===\")\n",
    "try:\n",
    "    checkpoint_files = dbutils.fs.ls(autoloader_checkpoint)\n",
    "    for file in checkpoint_files:\n",
    "        print(f\"  {file.name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Checkpoint nie istnieje lub jest pusty: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "46c2fec6-140d-45e9-a286-9e10b51842e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checkpoint zawiera:\n",
    "# - offsets/: Offset każdego micro-batch (dla recovery)\n",
    "# - commits/: Commited batches (dla exactly-once semantics)\n",
    "# - metadata: Stream metadata (id, configuration)\n",
    "# - sources/: Source-specific tracking (np. file tracking dla Auto Loader)\n",
    "\n",
    "print(\"\\n=== Checkpoint Best Practices ===\")\n",
    "print(\"✅ Zawsze używaj checkpoint location dla production streams\")\n",
    "print(\"✅ Przechowuj w external location (S3, ADLS, DBFS)\")\n",
    "print(\"✅ Backup przed schema changes\")\n",
    "print(\"⚠️ Nie usuwaj checkpoint - loss of progress!\")\n",
    "print(\"⚠️ Nie współdziel checkpoint między różnymi queries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09158a7b-9949-464f-a5e5-3d43202d3f0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Porównanie metod ingestion\n",
    "\n",
    "| Feature | COPY INTO | Auto Loader | Structured Streaming |\n",
    "|---------|-----------|-------------|---------------------|\n",
    "| **Latency** | Minuty-godziny | Sekundy-minuty | Sub-sekundy |\n",
    "| **Use case** | Scheduled batch | Near real-time files | Pure streaming (Kafka) |\n",
    "| **Idempotency** | ✅ Built-in | ✅ Built-in | ⚠️ Requires checkpoint |\n",
    "| **Schema evolution** | ⚠️ Manual | ✅ Automatic | ⚠️ Manual |\n",
    "| **Complexity** | Low | Medium | High |\n",
    "| **Cost** | Lowest | Medium | Highest |\n",
    "| **File tracking** | Delta Log | Checkpoint | Checkpoint |\n",
    "\n",
    "**Rekomendacje:**\n",
    "- **COPY INTO**: Batch loads (hourly, daily), duże pliki, niskie koszty\n",
    "- **Auto Loader**: Near real-time, małe pliki, schema evolution\n",
    "- **Structured Streaming**: Pure streaming sources (Kafka), sub-second latency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36312af7-eea5-4ce5-b8ac-bff4000186d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "**COPY INTO:**\n",
    "- Używaj dla scheduled batch jobs (daily, hourly)\n",
    "- Zawsze dodawaj _metadata column dla audytu\n",
    "- Używaj PATTERN dla filtrowania plików\n",
    "- Monitor operationMetrics w DESCRIBE HISTORY\n",
    "\n",
    "**Auto Loader:**\n",
    "- Włącz schema inference (inferColumnTypes=true)\n",
    "- Używaj trigger(availableNow=True) dla backfill\n",
    "- Monitor schema evolution w schemaLocation\n",
    "- Rozważ file notification dla dużej liczby plików (>10k)\n",
    "\n",
    "**Structured Streaming:**\n",
    "- Zawsze ustawiaj checkpointLocation\n",
    "- Używaj trigger(processingTime) dla continuous streams\n",
    "- Implementuj watermark dla windowed aggregations\n",
    "- Monitor stream metrics (numInputRows, inputRowsPerSecond)\n",
    "\n",
    "**Checkpoints:**\n",
    "- Przechowuj w niezależnej lokalizacji (nie w table location)\n",
    "- Backup przed schema changes\n",
    "- Nie usuwaj checkpoint - loss of progress!\n",
    "- Używaj external location (S3, ADLS) dla durability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "663558e9-e563-4440-80b2-2a9eb08c6b5d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Problem 1: \"Stream stopped unexpectedly\"**\n",
    "**Rozwiązanie:**\n",
    "- Sprawdź checkpoint location - czy istnieje i jest writable\n",
    "- Sprawdź logi streaming query: `query.lastProgress`\n",
    "- Monitor exceptions: `query.exception()`\n",
    "\n",
    "**Problem 2: \"Schema mismatch in Auto Loader\"**\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Włącz schema evolution\n",
    ".option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    ".option(\"mergeSchema\", \"true\")\n",
    "```\n",
    "\n",
    "**Problem 3: COPY INTO nie wykrywa nowych plików**\n",
    "**Rozwiązanie:**\n",
    "- COPY INTO śledzi tylko file path - zmiana zawartości nie jest wykrywana\n",
    "- Użyj COPY_OPTIONS ('force' = 'true') dla re-ingestion\n",
    "\n",
    "**Problem 4: Streaming aggregation state grows indefinitely**\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Dodaj watermark dla time-based cleanup\n",
    ".withWatermark(\"event_time\", \"1 hour\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16ef6340-7da5-4eb0-bc3f-ca8be642ee9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "**W tym notebooku nauczyliśmy się:**\n",
    "\n",
    "✅ **COPY INTO:**\n",
    "- Idempotent batch loads z automatic file tracking\n",
    "- Pattern matching dla selective ingestion\n",
    "- _metadata column dla audytu\n",
    "\n",
    "✅ **Auto Loader:**\n",
    "- Near real-time file ingestion z cloudFiles format\n",
    "- Automatic schema inference i evolution\n",
    "- Checkpoint-based progress tracking\n",
    "\n",
    "✅ **Structured Streaming:**\n",
    "- Continuous processing z micro-batch architecture\n",
    "- Transformacje i agregacje na streaming data\n",
    "- foreachBatch dla custom write logic (MERGE)\n",
    "\n",
    "✅ **Triggering modes:**\n",
    "- trigger(once=True) - batch mode dla testing\n",
    "- trigger(processingTime) - continuous processing\n",
    "- trigger(availableNow=True) - backfill mode\n",
    "\n",
    "**Kluczowe wnioski:**\n",
    "1. Wybór metody ingestion zależy od latency requirements i source type\n",
    "2. COPY INTO dla scheduled batch, Auto Loader dla near real-time files\n",
    "3. Structured Streaming dla pure streaming sources (Kafka)\n",
    "4. Checkpoint location jest krytyczny dla fault tolerance\n",
    "\n",
    "**Następne kroki:**\n",
    "- **Kolejny notebook**: 04_bronze_silver_gold_pipeline.ipynb\n",
    "- **Warsztat praktyczny**: 02_ingestion_pipeline_workshop.ipynb\n",
    "- **Delta Live Tables**: Declarative pipelines z automatic orchestration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea903a0e-7a51-46d8-9d72-202e273640b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Posprzątaj zasoby utworzone podczas notebooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef7c4e3b-1d09-48bc-a5fa-d2c914f4c2b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Opcjonalne czyszczenie zasobów testowych\n",
    "# UWAGA: Uruchom tylko jeśli chcesz usunąć wszystkie utworzone dane\n",
    "\n",
    "# Usuń tabele\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {copy_into_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {validation_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {pattern_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {autoloader_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {streaming_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {streaming_agg_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {trigger_once_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {trigger_available_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {trigger_continuous_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {upsert_table}\")\n",
    "\n",
    "# Usuń checkpointy\n",
    "# dbutils.fs.rm(CHECKPOINT_PATH, recurse=True)\n",
    "\n",
    "# spark.catalog.clearCache()\n",
    "# print(\"Zasoby zostały wyczyszczone\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "03_batch_streaming_load",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
