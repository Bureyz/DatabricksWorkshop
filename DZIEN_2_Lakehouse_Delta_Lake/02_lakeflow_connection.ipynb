{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "888da69b",
   "metadata": {},
   "source": [
    "# Medallion Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e39112",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **DzieÅ„ szkolenia**: DzieÅ„ 2 - Delta Lake & Lakehouse\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 16.4 LTS lub nowszy (zalecane: 17.3 LTS)\n",
    "  - Unity Catalog wÅ‚Ä…czony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard lub **Serverless Compute** (zalecane)\n",
    "- **ZaleÅ¼noÅ›ci**: Wykonany notebook `00_setup.ipynb`\n",
    "- **Czas realizacji**: ~60 minut\n",
    "\n",
    "> **Uwaga (2025):** Serverless Compute jest teraz domyÅ›lnym trybem dla nowych workloadÃ³w."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf1c0c",
   "metadata": {},
   "source": [
    "## WstÄ™p teoretyczny\n",
    "\n",
    "### Lakeflow - Platforma Data Ingestion\n",
    "\n",
    "**Lakeflow** to zintegrowana platforma Databricks do zarzÄ…dzania przepÅ‚ywem danych:\n",
    "\n",
    "| Komponent | Opis | Use Case |\n",
    "|-----------|------|----------|\n",
    "| **COPY INTO** | Batch loading z cloud storage | Scheduled ETL, duÅ¼e pliki |\n",
    "| **Auto Loader** | Streaming z cloud storage | Continuous ingestion, maÅ‚e pliki |\n",
    "| **Lakeflow Connect** | Managed connectors do SaaS | Salesforce, SAP, Workday |\n",
    "| **Lakeflow Pipelines** | Declarative ETL (DLT) | Bronze â†’ Silver â†’ Gold |\n",
    "\n",
    "---\n",
    "\n",
    "### Batch vs Streaming - Decision Matrix\n",
    "\n",
    "| Cecha | Batch (COPY INTO) | Streaming (Auto Loader) |\n",
    "|-------|-------------------|-------------------------|\n",
    "| **Latency** | Minuty-Godziny | Sekundy-Minuty |\n",
    "| **File size** | DuÅ¼e (>1GB) | MaÅ‚e (<100MB) |\n",
    "| **Frequency** | Scheduled (hourly/daily) | Continuous |\n",
    "| **Cost** | NiÅ¼szy (on-demand) | WyÅ¼szy (always-on) |\n",
    "| **Complexity** | Niski | Åšredni |\n",
    "| **Idempotency** | âœ… Built-in | âœ… Checkpoint-based |\n",
    "| **Schema evolution** | Manual | Automatic |\n",
    "\n",
    "**Kiedy Batch (COPY INTO):**\n",
    "- âœ… Dane przychodzÄ… w scheduled intervals\n",
    "- âœ… DuÅ¼e pliki (> 1GB)\n",
    "- âœ… Latency nie jest krytyczna\n",
    "- âœ… Lower cost requirement\n",
    "\n",
    "**Kiedy Streaming (Auto Loader):**\n",
    "- âœ… Dane przychodzÄ… ciÄ…gle (< 1h intervals)\n",
    "- âœ… Potrzebujesz low latency (< 5 min)\n",
    "- âœ… MaÅ‚e pliki (< 100MB each)\n",
    "- âœ… Real-time dashboards/analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02bc795",
   "metadata": {},
   "source": [
    "## Izolacja per uÅ¼ytkownik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261549af",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7092811",
   "metadata": {},
   "source": [
    "## Konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a12a344",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d2f674",
   "metadata": {},
   "source": [
    "**Konfiguracja Å›cieÅ¼ek i zmiennych:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18908bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ustawienie domyÅ›lnego katalogu i schematu\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# === ÅšCIEÅ»KI DO DANYCH BATCH ===\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "# === ÅšCIEÅ»KI DO DANYCH STREAMING ===\n",
    "STREAMING_SOURCE_PATH = f\"{DATASET_BASE_PATH}/orders/stream\"\n",
    "\n",
    "# === ÅšCIEÅ»KI TECHNICZNE ===\n",
    "CHECKPOINT_BASE_PATH = f\"/tmp/{raw_user}/lakeflow_checkpoints\"\n",
    "SCHEMA_BASE_PATH = f\"/tmp/{raw_user}/lakeflow_schemas\"\n",
    "BAD_RECORDS_PATH = f\"/tmp/{raw_user}/bad_records\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d32cfc",
   "metadata": {},
   "source": [
    "**Czyszczenie poprzednich uruchomieÅ„ (dla demo):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abf11ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dbutils.fs.rm(CHECKPOINT_BASE_PATH, True)\n",
    "    dbutils.fs.rm(SCHEMA_BASE_PATH, True)\n",
    "    dbutils.fs.rm(BAD_RECORDS_PATH, True)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd99bd3",
   "metadata": {},
   "source": [
    "**Weryfikacja konfiguracji:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceae964",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"CATALOG\", CATALOG),\n",
    "        (\"BRONZE_SCHEMA\", BRONZE_SCHEMA),\n",
    "        (\"SILVER_SCHEMA\", SILVER_SCHEMA),\n",
    "        (\"USER\", raw_user),\n",
    "        (\"CUSTOMERS_CSV\", CUSTOMERS_CSV),\n",
    "        (\"STREAMING_SOURCE_PATH\", STREAMING_SOURCE_PATH)\n",
    "    ], [\"Variable\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea5493",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 1: COPY INTO - Batch Loading\n",
    "\n",
    "**COPY INTO** to rekomendowana metoda batch ingestion:\n",
    "- **Idempotency**: Automatyczne Å›ledzenie przetworzonych plikÃ³w\n",
    "- **File tracking**: Tylko nowe pliki sÄ… Å‚adowane przy ponownym uruchomieniu\n",
    "- **Format support**: CSV, JSON, Parquet, Avro, ORC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e298971",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 1.1: COPY INTO z CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2a1d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_CUSTOMERS = f\"{BRONZE_SCHEMA}.customers_batch\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f930f3",
   "metadata": {},
   "source": [
    "**Tworzenie target table:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cbb355",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE_CUSTOMERS} (\n",
    "  customer_id STRING,\n",
    "  first_name STRING,\n",
    "  last_name STRING,\n",
    "  email STRING,\n",
    "  phone STRING,\n",
    "  city STRING,\n",
    "  state STRING,\n",
    "  country STRING,\n",
    "  registration_date DATE,\n",
    "  customer_segment STRING,\n",
    "  _ingestion_timestamp TIMESTAMP\n",
    ") USING DELTA\n",
    "COMMENT 'Customers data - Bronze layer'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9a684",
   "metadata": {},
   "source": [
    "**Wykonanie COPY INTO:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf99c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = spark.sql(f\"\"\"\n",
    "COPY INTO {TABLE_CUSTOMERS}\n",
    "FROM (\n",
    "  SELECT \n",
    "    customer_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    email,\n",
    "    phone,\n",
    "    city,\n",
    "    state,\n",
    "    country,\n",
    "    TO_DATE(registration_date, 'yyyy-MM-dd') as registration_date,\n",
    "    customer_segment,\n",
    "    current_timestamp() as _ingestion_timestamp\n",
    "  FROM '{CUSTOMERS_CSV}'\n",
    ")\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'false')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210541c8",
   "metadata": {},
   "source": [
    "**Weryfikacja wynikÃ³w:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce6633",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Table\", TABLE_CUSTOMERS),\n",
    "        (\"Records\", str(spark.table(TABLE_CUSTOMERS).count()))\n",
    "    ], [\"Metric\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7919fca",
   "metadata": {},
   "source": [
    "**PrzykÅ‚adowe dane:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5ce3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.table(TABLE_CUSTOMERS).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6b03bc",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 1.2: Test idempotency\n",
    "\n",
    "Uruchomienie COPY INTO ponownie **nie** doda duplikatÃ³w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df697df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_before = spark.table(TABLE_CUSTOMERS).count()\n",
    "\n",
    "# Ponowne uruchomienie COPY INTO\n",
    "spark.sql(f\"\"\"\n",
    "COPY INTO {TABLE_CUSTOMERS}\n",
    "FROM (\n",
    "  SELECT \n",
    "    customer_id, first_name, last_name, email, phone,\n",
    "    city, state, country,\n",
    "    TO_DATE(registration_date, 'yyyy-MM-dd') as registration_date,\n",
    "    customer_segment,\n",
    "    current_timestamp() as _ingestion_timestamp\n",
    "  FROM '{CUSTOMERS_CSV}'\n",
    ")\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'false')\n",
    "\"\"\")\n",
    "\n",
    "count_after = spark.table(TABLE_CUSTOMERS).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df9ecaf",
   "metadata": {},
   "source": [
    "**PorÃ³wnanie:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbc3569",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Before\", count_before),\n",
    "        (\"After\", count_after),\n",
    "        (\"Difference\", count_after - count_before)\n",
    "    ], [\"State\", \"Count\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8be4138",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 2: Auto Loader - Streaming Ingestion\n",
    "\n",
    "**Auto Loader (cloudFiles)** to Databricks-managed streaming source:\n",
    "- Automatyczne file discovery (file notifications)\n",
    "- Incremental processing (tylko nowe pliki)\n",
    "- Schema inference & evolution\n",
    "- Checkpoint management\n",
    "- Skaluje do milionÃ³w plikÃ³w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886c364e",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 2.1: Auto Loader z trigger `availableNow`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b2b1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_TABLE_AL = f\"{BRONZE_SCHEMA}.orders_autoloader\"\n",
    "CHECKPOINT_AL = f\"{CHECKPOINT_BASE_PATH}/autoloader\"\n",
    "SCHEMA_AL = f\"{SCHEMA_BASE_PATH}/autoloader\"\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE_AL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca275c",
   "metadata": {},
   "source": [
    "**Konfiguracja Auto Loader readStream:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cda768",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_autoloader = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", SCHEMA_AL)\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "    .load(STREAMING_SOURCE_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335c1442",
   "metadata": {},
   "source": [
    "**Dodanie metadata columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4bb0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_enriched = (df_autoloader\n",
    "    .withColumn(\"_processing_time\", F.current_timestamp())\n",
    "    .withColumn(\"_source_file\", F.input_file_name())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27dc5e5c",
   "metadata": {},
   "source": [
    "**Uruchomienie streaming z trigger `availableNow`:**\n",
    "\n",
    "> `availableNow` - przetwarza wszystkie dostÄ™pne dane i zatrzymuje siÄ™ (batch-like streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8573ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (df_enriched.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_AL)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(TARGET_TABLE_AL)\n",
    ")\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c093780",
   "metadata": {},
   "source": [
    "**Wyniki Auto Loader:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed92c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Records Loaded\", str(spark.table(TARGET_TABLE_AL).count())),\n",
    "        (\"Source Files\", str(spark.table(TARGET_TABLE_AL).select(\"_source_file\").distinct().count()))\n",
    "    ], [\"Metric\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb55b8a",
   "metadata": {},
   "source": [
    "**Inferred schema:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1d143c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(TARGET_TABLE_AL).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051a9790",
   "metadata": {},
   "source": [
    "**PrzykÅ‚adowe dane:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca79b4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.table(TARGET_TABLE_AL).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f08f42",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 3: Rescued Data Column\n",
    "\n",
    "**Rescued Data Column** to mechanizm Auto Loader do obsÅ‚ugi nieoczekiwanych danych:\n",
    "\n",
    "| Scenariusz | Behavior |\n",
    "|------------|----------|\n",
    "| Nowe kolumny | Zapisane w `_rescued_data` |\n",
    "| NiepasujÄ…ce typy | Zapisane w `_rescued_data` |\n",
    "| Malformed records | Zapisane w `_rescued_data` |\n",
    "\n",
    "**Konfiguracja:**\n",
    "```python\n",
    ".option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    "```\n",
    "\n",
    "**Tryby schemaEvolutionMode:**\n",
    "- `addNewColumns`: Automatycznie dodaje nowe kolumny\n",
    "- `rescue`: Nowe kolumny â†’ `_rescued_data` JSON\n",
    "- `failOnNewColumns`: Fail jeÅ›li schema siÄ™ zmieni\n",
    "- `none`: Ignoruje nowe kolumny (ryzykowne!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c869604d",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 3.1: Auto Loader z rescue mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb42ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_TABLE_RESCUE = f\"{BRONZE_SCHEMA}.orders_rescued\"\n",
    "CHECKPOINT_RESCUE = f\"{CHECKPOINT_BASE_PATH}/rescue\"\n",
    "SCHEMA_RESCUE = f\"{SCHEMA_BASE_PATH}/rescue\"\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE_RESCUE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bf0694",
   "metadata": {},
   "source": [
    "**Definiujemy explicit schema (czÄ™Å›ciowy):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474f4b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celowo definiujemy tylko czÄ™Å›Ä‡ kolumn - reszta trafi do _rescued_data\n",
    "partial_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed17508",
   "metadata": {},
   "source": [
    "**Auto Loader z rescue mode:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1365888",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rescue = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", SCHEMA_RESCUE)\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")  # Rescue mode!\n",
    "    .schema(partial_schema)  # Partial schema\n",
    "    .load(STREAMING_SOURCE_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db90b03",
   "metadata": {},
   "source": [
    "**Uruchomienie stream:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4f8e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_rescue = (df_rescue.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_RESCUE)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(TARGET_TABLE_RESCUE)\n",
    ")\n",
    "\n",
    "query_rescue.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af73ccb",
   "metadata": {},
   "source": [
    "**Schema z kolumnÄ… `_rescued_data`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06aa2187",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.table(TARGET_TABLE_RESCUE).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f58444f",
   "metadata": {},
   "source": [
    "**Dane z rescued columns:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae667d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.table(TARGET_TABLE_RESCUE)\n",
    "    .select(\"order_id\", \"customer_id\", \"total_amount\", \"_rescued_data\")\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10296b6",
   "metadata": {},
   "source": [
    "**Analiza rescued data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d4c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "rescued_count = spark.table(TARGET_TABLE_RESCUE).filter(F.col(\"_rescued_data\").isNotNull()).count()\n",
    "total_count = spark.table(TARGET_TABLE_RESCUE).count()\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Total Records\", total_count),\n",
    "        (\"With Rescued Data\", rescued_count),\n",
    "        (\"Rescued %\", round(rescued_count/total_count*100, 2) if total_count > 0 else 0)\n",
    "    ], [\"Metric\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a1f3aa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 4: Error Handling\n",
    "\n",
    "**Strategie obsÅ‚ugi bÅ‚Ä™dÃ³w w COPY INTO:**\n",
    "\n",
    "| Mode | Zachowanie |\n",
    "|------|------------|\n",
    "| `PERMISSIVE` | Parsuje co siÄ™ da, bÅ‚Ä™dy â†’ `_corrupt_record` |\n",
    "| `DROPMALFORMED` | Usuwa bÅ‚Ä™dne rekordy |\n",
    "| `FAILFAST` | Zatrzymuje na pierwszym bÅ‚Ä™dzie |\n",
    "\n",
    "**`badRecordsPath`** - zapisuje bÅ‚Ä™dne rekordy do folderu dla pÃ³Åºniejszej analizy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121c8b32",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 4.1: Error handling z badRecordsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cad6a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLE_ERRORS = f\"{BRONZE_SCHEMA}.customers_with_validation\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd04318",
   "metadata": {},
   "source": [
    "**Tworzenie tabeli z `_corrupt_record`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfad1381",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"DROP TABLE IF EXISTS {TABLE_ERRORS}\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {TABLE_ERRORS} (\n",
    "  customer_id STRING,\n",
    "  first_name STRING,\n",
    "  last_name STRING,\n",
    "  email STRING,\n",
    "  phone STRING,\n",
    "  city STRING,\n",
    "  state STRING,\n",
    "  country STRING,\n",
    "  registration_date STRING,\n",
    "  customer_segment STRING,\n",
    "  _corrupt_record STRING,\n",
    "  _ingestion_timestamp TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f7bc2a",
   "metadata": {},
   "source": [
    "**Wczytywanie z error handling:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c13013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_errors = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "    .option(\"badRecordsPath\", BAD_RECORDS_PATH)\n",
    "    .load(CUSTOMERS_CSV)\n",
    "    .withColumn(\"_ingestion_timestamp\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "df_with_errors.write.mode(\"overwrite\").saveAsTable(TABLE_ERRORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac596098",
   "metadata": {},
   "source": [
    "**Statystyki bÅ‚Ä™dnych rekordÃ³w:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eb2fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "total = spark.table(TABLE_ERRORS).count()\n",
    "corrupt = spark.table(TABLE_ERRORS).filter(F.col(\"_corrupt_record\").isNotNull()).count()\n",
    "valid = total - corrupt\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Total Records\", total),\n",
    "        (\"Valid Records\", valid),\n",
    "        (\"Corrupt Records\", corrupt),\n",
    "        (\"Quality Rate %\", round(valid/total*100, 2) if total > 0 else 0)\n",
    "    ], [\"Metric\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc331e3b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 5: Lakeflow Connect (Informacyjna)\n",
    "\n",
    "**Lakeflow Connect** to managed SaaS integration bez pisania kodu:\n",
    "\n",
    "### Wspierane ÅºrÃ³dÅ‚a:\n",
    "- Salesforce\n",
    "- Workday\n",
    "- Google Analytics\n",
    "- HubSpot\n",
    "- Stripe\n",
    "- SAP\n",
    "- Netsuite\n",
    "- ServiceNow\n",
    "\n",
    "### Kluczowe cechy:\n",
    "\n",
    "| Cecha | Opis |\n",
    "|-------|------|\n",
    "| **Zero-code** | Konfiguracja przez UI |\n",
    "| **Managed** | Automatyczne skalowanie |\n",
    "| **CDC Support** | Change Data Capture |\n",
    "| **Schema Evolution** | Automatyczne aktualizacje |\n",
    "| **Unity Catalog** | PeÅ‚na integracja |\n",
    "\n",
    "### Jak uruchomiÄ‡:\n",
    "1. Workspace â†’ **Data** â†’ **Lakeflow** â†’ **Connect**\n",
    "2. Wybierz connector (np. Salesforce)\n",
    "3. Podaj credentials\n",
    "4. Wybierz obiekty do synchronizacji\n",
    "5. Ustaw schedule\n",
    "\n",
    "### PorÃ³wnanie metod ingestion:\n",
    "\n",
    "| Metoda | Use Case |\n",
    "|--------|----------|\n",
    "| **COPY INTO** | Pliki w cloud storage (batch) |\n",
    "| **Auto Loader** | Pliki w cloud storage (streaming) |\n",
    "| **Lakeflow Connect** | Dane z systemÃ³w SaaS |\n",
    "| **Lakeflow Pipelines** | Transformacje Bronze â†’ Silver â†’ Gold |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409dba02",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 6: Trigger Modes\n",
    "\n",
    "**Trigger** okreÅ›la jak czÄ™sto streaming query wykonuje micro-batches:\n",
    "\n",
    "| Mode | Zachowanie | Use Case |\n",
    "|------|------------|----------|\n",
    "| `availableNow=True` | Przetworz wszystko â†’ stop | Scheduled jobs â­ |\n",
    "| `once=True` | Legacy (deprecated) | - |\n",
    "| `processingTime=\"10s\"` | Co 10 sekund | Real-time |\n",
    "| `continuous=\"1s\"` | Ultra-low latency | Experimental |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7731957",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 6.1: Test trigger modes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c36d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_TABLE_TRIGGER = f\"{BRONZE_SCHEMA}.orders_trigger_test\"\n",
    "CHECKPOINT_TRIGGER = f\"{CHECKPOINT_BASE_PATH}/trigger\"\n",
    "SCHEMA_TRIGGER = f\"{SCHEMA_BASE_PATH}/trigger\"\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE_TRIGGER}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a405fa",
   "metadata": {},
   "source": [
    "**Test `availableNow` (zalecane dla scheduled jobs):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9a263a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "df_trigger = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", SCHEMA_TRIGGER)\n",
    "    .load(STREAMING_SOURCE_PATH)\n",
    ")\n",
    "\n",
    "query_trigger = (df_trigger.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_TRIGGER)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(TARGET_TABLE_TRIGGER)\n",
    ")\n",
    "\n",
    "query_trigger.awaitTermination()\n",
    "elapsed = time.time() - start_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235c16be",
   "metadata": {},
   "source": [
    "**Wyniki:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8ea82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Trigger Mode\", \"availableNow\"),\n",
    "        (\"Execution Time\", f\"{elapsed:.2f}s\"),\n",
    "        (\"Records Loaded\", str(spark.table(TARGET_TABLE_TRIGGER).count()))\n",
    "    ], [\"Metric\", \"Value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27651a9c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "### Co osiÄ…gnÄ™liÅ›my:\n",
    "\n",
    "âœ… **COPY INTO** - Idempotent batch loading z CSV/JSON/Parquet\n",
    "\n",
    "âœ… **Auto Loader** - Streaming ingestion z file notifications\n",
    "\n",
    "âœ… **Rescued Data Column** - ObsÅ‚uga nieoczekiwanych danych\n",
    "\n",
    "âœ… **Error Handling** - badRecordsPath i corrupt record handling\n",
    "\n",
    "âœ… **Lakeflow Connect** - Integracja z systemami SaaS\n",
    "\n",
    "âœ… **Trigger Modes** - availableNow vs processingTime\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "\n",
    "| # | Zasada |\n",
    "|---|--------|\n",
    "| 1 | **COPY INTO** dla batch, **Auto Loader** dla streaming |\n",
    "| 2 | Zawsze uÅ¼ywaj **checkpointLocation** dla streaming |\n",
    "| 3 | **Rescue mode** dla bezpiecznej obsÅ‚ugi zmian schematu |\n",
    "| 4 | **availableNow** dla scheduled jobs (cost-effective) |\n",
    "| 5 | **Lakeflow Connect** dla integracji SaaS (zero-code) |\n",
    "\n",
    "### NastÄ™pne kroki:\n",
    "\n",
    "ðŸ“š **Kolejny Notebook:** `03_medallion_architecture.ipynb`\n",
    "- Bronze â†’ Silver â†’ Gold pattern\n",
    "- Data quality layers\n",
    "- Transformacje i agregacje"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae99581e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup (Opcjonalnie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d5a7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista utworzonych tabel\n",
    "created_tables = [\n",
    "    \"customers_batch\",\n",
    "    \"orders_autoloader\",\n",
    "    \"orders_rescued\",\n",
    "    \"customers_with_validation\",\n",
    "    \"orders_trigger_test\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40a6611",
   "metadata": {},
   "source": [
    "**Weryfikacja utworzonych zasobÃ³w:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b7f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for table in created_tables:\n",
    "    full_table = f\"{CATALOG}.{BRONZE_SCHEMA}.{table}\"\n",
    "    try:\n",
    "        if spark.catalog.tableExists(full_table):\n",
    "            count = spark.table(full_table).count()\n",
    "            results.append((table, \"EXISTS\", str(count)))\n",
    "        else:\n",
    "            results.append((table, \"NOT FOUND\", \"-\"))\n",
    "    except Exception as e:\n",
    "        results.append((table, \"ERROR\", str(e)[:30]))\n",
    "\n",
    "display(spark.createDataFrame(results, [\"Table\", \"Status\", \"Records\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723368d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flaga cleanup\n",
    "CLEANUP_ENABLED = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c886011f",
   "metadata": {},
   "source": [
    "**Wykonanie cleanup (jeÅ›li wÅ‚Ä…czone):**\n",
    "> ðŸ’¡ *Zalecane: Zostaw dane dla kolejnych notebookÃ³w!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f9dd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLEANUP_ENABLED:\n",
    "    results = []\n",
    "    for table in created_tables:\n",
    "        full_table = f\"{CATALOG}.{BRONZE_SCHEMA}.{table}\"\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {full_table}\")\n",
    "            results.append((table, \"DROPPED\"))\n",
    "        except Exception as e:\n",
    "            results.append((table, f\"ERROR: {str(e)[:30]}\"))\n",
    "    \n",
    "    # Cleanup checkpoints\n",
    "    try:\n",
    "        dbutils.fs.rm(CHECKPOINT_BASE_PATH, True)\n",
    "        results.append((\"checkpoints\", \"REMOVED\"))\n",
    "    except:\n",
    "        results.append((\"checkpoints\", \"NOT FOUND\"))\n",
    "    \n",
    "    display(spark.createDataFrame(results, [\"Resource\", \"Status\"]))\n",
    "else:\n",
    "    display(spark.createDataFrame([\n",
    "        (\"CLEANUP_ENABLED\", \"False\"),\n",
    "        (\"Akcja\", \"ZmieÅ„ na True aby usunÄ…Ä‡ zasoby\")\n",
    "    ], [\"Setting\", \"Value\"]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
