{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "577fb86f-84ed-49a2-b1a1-aa785feceefc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Medallion Architecture - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Zrozumienie koncepcji architektury medalionowej (Bronze/Silver/Gold) i zasad projektowania data lakehouse.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Bronze / Silver / Gold - logika warstw\n",
    "- ETL vs ELT approach\n",
    "- Zasady projektowania pipeline'√≥w\n",
    "- Partitioning strategy\n",
    "- Audyt i lineage - metadane w ka≈ºdym kroku\n",
    "- Data quality w kontek≈õcie warstw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25cba1ec-bc2f-4a95-8b19-097852590dfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzie≈Ñ szkolenia**: Dzie≈Ñ 2 - Lakehouse & Delta Lake\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog w≈ÇƒÖczony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1aefc3a3-9caf-4ec6-a347-da3087d56868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wstƒôp teoretyczny - Medallion Architecture\n",
    "\n",
    "**Cel sekcji:** Dog≈Çƒôbne zrozumienie architektury medalionowej jako fundamentalnego design pattern dla data lakehouse.\n",
    "\n",
    "---\n",
    "\n",
    "### Czym jest Medallion Architecture?\n",
    "\n",
    "**Medallion Architecture** to wielowarstwowy wzorzec organizacji danych w data lakehouse, kt√≥ry dzieli dane na trzy warstwy o rosnƒÖcej jako≈õci i warto≈õci biznesowej:\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    DATA SOURCES                              ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Transactional DBs  ‚Ä¢ APIs  ‚Ä¢ Files  ‚Ä¢ IoT  ‚Ä¢ Streams     ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                        ‚îÇ Extract & Load\n",
    "                        ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ           ü•â BRONZE LAYER (Raw / Landing Zone)              ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ  Charakterystyka:                                            ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Dane \"as-is\" bez transformacji warto≈õci                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Append-only, immutable                                   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Audit metadata: timestamp, source, user                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Multi-format: JSON, CSV, Parquet, Avro                   ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ  Retention: 3-7 lat (d≈Çugoterminowa historia)              ‚îÇ\n",
    "‚îÇ  SLA: Real-time lub near-real-time                          ‚îÇ\n",
    "‚îÇ  Use case: Data recovery, reprocessing, compliance          ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                        ‚îÇ Transform (Data Quality)\n",
    "                        ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ       ü•à SILVER LAYER (Cleansed / Validated)                ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ  Charakterystyka:                                            ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Deduplikacja po kluczu biznesowym                        ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Walidacja: NOT NULL, data types, ranges                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Standaryzacja: dates, text, formats                      ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Business rules enforcement                                ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Schema evolution support                                  ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ  Retention: 1-2 lata (medium-term history)                  ‚îÇ\n",
    "‚îÇ  SLA: Hourly lub daily batch                                ‚îÇ\n",
    "‚îÇ  Use case: Analytics prep, joins, enrichment                ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                        ‚îÇ Transform (Business Logic)\n",
    "                        ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ         ü•á GOLD LAYER (Business / Aggregates)               ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ  Charakterystyka:                                            ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Pre-aggregated summaries (daily, monthly)                ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Denormalized tables (joins pre-computed)                 ‚îÇ\n",
    "‚îÇ  ‚Ä¢ KPI calculations & business metrics                      ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Star schema / dimensional models                          ‚îÇ\n",
    "‚îÇ  ‚Ä¢ ML feature stores                                         ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ  Retention: 6-12 miesiƒôcy (short-term, refreshable)        ‚îÇ\n",
    "‚îÇ  SLA: Daily lub on-demand                                    ‚îÇ\n",
    "‚îÇ  Use case: BI dashboards, reports, ML models                ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                        ‚îÇ\n",
    "                        ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    CONSUMPTION LAYER                         ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Power BI  ‚Ä¢ Tableau  ‚Ä¢ Looker  ‚Ä¢ ML Models               ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Kluczowe zasady Medallion Architecture\n",
    "\n",
    "**1. Separation of Concerns**\n",
    "- Bronze: Data engineers (ingestion, audit)\n",
    "- Silver: Data engineers + Analytics engineers (quality, standardization)\n",
    "- Gold: Analytics engineers + Data analysts (business logic, KPIs)\n",
    "\n",
    "**2. Incremental Processing**\n",
    "- Ka≈ºda warstwa procesuje tylko nowe/zmienione dane (watermarking)\n",
    "- Minimalizacja reprocessingu ca≈Çych tabel\n",
    "- MERGE operations dla slowly changing dimensions\n",
    "\n",
    "**3. Data Quality Gates**\n",
    "- Bronze ‚Üí Silver: Schema validation, NOT NULL checks\n",
    "- Silver ‚Üí Gold: Business rule validation, referential integrity\n",
    "- Reject/Quarantine invalid records\n",
    "\n",
    "**4. Immutability & Auditability**\n",
    "- Bronze: Immutable landing zone (append-only)\n",
    "- Audit metadata na ka≈ºdym etapie (timestamp, source, user)\n",
    "- Mo≈ºliwo≈õƒá reprocessingu Silver/Gold z Bronze\n",
    "\n",
    "**5. Different SLAs per Layer**\n",
    "- Bronze: Real-time (streaming) lub near-real-time (micro-batch)\n",
    "- Silver: Hourly lub daily batch\n",
    "- Gold: Daily lub on-demand refresh\n",
    "\n",
    "---\n",
    "\n",
    "### Dlaczego Medallion Architecture?\n",
    "\n",
    "**‚úÖ Zalety:**\n",
    "\n",
    "1. **Reprocessing bez re-ingestion**\n",
    "   - Bronze jako immutable source of truth\n",
    "   - Silver/Gold mo≈ºna przeliczyƒá bez ponownego ≈Çadowania z source systems\n",
    "\n",
    "2. **Data Quality Governance**\n",
    "   - Jasne quality gates miƒôdzy warstwami\n",
    "   - Tracking rejection rates i invalid records\n",
    "\n",
    "3. **Performance Optimization**\n",
    "   - Bronze: Append-only (fast writes)\n",
    "   - Silver: Partitioned, optimized for joins\n",
    "   - Gold: Denormalized, optimized for reads (BI queries)\n",
    "\n",
    "4. **Compliance & Audit**\n",
    "   - Pe≈Çna historia danych w Bronze (GDPR, SOX)\n",
    "   - Lineage tracking: source ‚Üí Bronze ‚Üí Silver ‚Üí Gold ‚Üí BI\n",
    "\n",
    "5. **Cost Optimization**\n",
    "   - Different retention policies per warstwa\n",
    "   - Archive/delete older Bronze data po reprocessingu Silver\n",
    "\n",
    "**‚ö†Ô∏è Trade-offs:**\n",
    "\n",
    "1. **Storage Cost**: Dane powielone w 3 warstwach\n",
    "2. **Complexity**: Wiƒôcej pipeline'√≥w do zarzƒÖdzania\n",
    "3. **Latency**: Multi-hop processing mo≈ºe zwiƒôkszaƒá end-to-end latency\n",
    "\n",
    "---\n",
    "\n",
    "### Medallion vs Traditional DWH\n",
    "\n",
    "| Aspekt | Traditional DWH (ETL) | Medallion (ELT) |\n",
    "|--------|----------------------|-----------------|\n",
    "| **Transform Location** | Poza DWH (ETL tools) | W lakehouse (Spark) |\n",
    "| **Raw Data** | Brak surowych danych | Bronze layer |\n",
    "| **Schema** | Schema-on-write | Schema-on-read + evolution |\n",
    "| **Reprocessing** | Re-extract z source | Bronze ‚Üí Silver/Gold |\n",
    "| **Cost** | Wysoki (DWH storage) | Ni≈ºszy (object storage) |\n",
    "| **Scalability** | Vertical (scale-up) | Horizontal (scale-out) |\n",
    "\n",
    "---\n",
    "\n",
    "### Podstawowe pojƒôcia\n",
    "\n",
    "- **Medallion Architecture**: Wielowarstwowa architektura danych (Bronze ‚Üí Silver ‚Üí Gold)\n",
    "- **Bronze Layer**: Raw data landing zone - dane bez transformacji, tylko audit metadata\n",
    "- **Silver Layer**: Cleansed and conformed data - deduplikacja, walidacja, standardization\n",
    "- **Gold Layer**: Business-level aggregates - modele KPI, reporty, ML features\n",
    "- **ELT (Extract-Load-Transform)**: ≈Åaduj surowe dane, transformuj w lakehouse\n",
    "- **Data Lineage**: ≈öledzenie przep≈Çywu danych od ≈∫r√≥d≈Ça do consumption\n",
    "- **Quality Gates**: Walidacje miƒôdzy warstwami (reject invalid records)\n",
    "\n",
    "**Dlaczego to wa≈ºne?**\n",
    "\n",
    "Medallion architecture zapewnia:\n",
    "- **Separation of concerns** - jasne role per warstwa\n",
    "- **Jasne SLA per warstwa** - r√≥≈ºne tempo procesowania\n",
    "- **Incremental processing** - tylko nowe/zmienione dane\n",
    "- **Data quality gates** - walidacje na ka≈ºdym etapie\n",
    "- **R√≥≈ºne retention policies** - optymalizacja koszt√≥w storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a3390ab-18af-48bd-a851-c545b46dff6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Izolacja per u≈ºytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalog√≥w i schemat√≥w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ac68da3-6d62-499e-8d3c-ae1c283c823e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb179fb0-e5d9-4e48-9982-2a9c96ded80c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych ≈õrodowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30106da3-ad9d-4d3e-9a61-34dbf776e11b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import uuid\n",
    "\n",
    "# Wy≈õwietl kontekst u≈ºytkownika\n",
    "print(\"=== Kontekst u≈ºytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"U≈ºytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog jako domy≈õlny\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "\n",
    "# ≈öcie≈ºki do danych ≈∫r√≥d≈Çowych\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "\n",
    "print(f\"\\n=== ≈öcie≈ºki do danych ===\")\n",
    "print(f\"Orders: {ORDERS_JSON}\")\n",
    "print(f\"Customers: {CUSTOMERS_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abfc7abf-3ec7-454a-92f9-24890b34f7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 1: Koncepcja Medallion Architecture\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Medallion Architecture dzieli data lakehouse na trzy warstwy o rosnƒÖcej jako≈õci danych. Ka≈ºda warstwa ma okre≈õlone SLA, retention policy i data quality requirements.\n",
    "\n",
    "**Kluczowe pojƒôcia:**\n",
    "- **Bronze (Raw)**: Append-only, immutable landing zone. Dane \"as-is\" z systemu ≈∫r√≥d≈Çowego + audit metadata (ingestion timestamp, source file, version)\n",
    "- **Silver (Cleansed)**: Validated, deduplicated, standardized. Business rules enforcement, schema evolution, data quality checks\n",
    "- **Gold (Curated)**: Aggregated, denormalized, business-level. KPI models, reporting tables, ML features, star schema\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Separacja odpowiedzialno≈õci: data engineers (Bronze/Silver), analytics engineers (Gold)\n",
    "- Incremental processing: tylko nowe/zmienione dane propagowane przez warstwy\n",
    "- Debug-friendly: mo≈ºliwo≈õƒá reprocessingu Silver/Gold z Bronze bez re-ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3d610af-9496-4c55-ad10-005930465309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 1.1: Bronze Layer - Raw Data Landing\n",
    "\n",
    "**Cel:** Utworzenie Bronze layer z audit metadata\n",
    "\n",
    "**Podej≈õcie:**\n",
    "1. Wczytanie surowych danych z JSON\n",
    "2. Dodanie audit columns: ingest_timestamp, source_file, ingested_by\n",
    "3. Zapis do Bronze schema bez transformacji biznesowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9db9ee0f-d63e-4cc5-bd1b-5ca221129d09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.1 - Bronze Layer (czƒô≈õƒá 1: wczytanie surowych danych)\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "bronze_table = f\"{BRONZE_SCHEMA}.orders_bronze\"\n",
    "\n",
    "# Wczytaj surowe dane orders z JSON\n",
    "orders_raw = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "print(\"=== Surowe dane (schema) ===\")\n",
    "orders_raw.printSchema()\n",
    "print(f\"\\n‚úì Wczytano {orders_raw.count()} rekord√≥w z JSON\")\n",
    "\n",
    "# PodglƒÖd surowych danych\n",
    "display(orders_raw.limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.1 - Bronze Layer (czƒô≈õƒá 2: dodanie audit metadata)\n",
    "\n",
    "# Dodaj audit metadata (Bronze best practice)\n",
    "orders_bronze = (\n",
    "    orders_raw\n",
    "    .withColumn(\"_bronze_ingest_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_bronze_source_file\", F.input_file_name())\n",
    "    .withColumn(\"_bronze_ingested_by\", F.lit(raw_user))\n",
    "    .withColumn(\"_bronze_version\", F.lit(1))\n",
    ")\n",
    "\n",
    "print(\"=== Bronze layer z audit metadata ===\")\n",
    "print(\"\\nAudit columns:\")\n",
    "display(orders_bronze.select(\n",
    "    \"_bronze_ingest_timestamp\",\n",
    "    \"_bronze_source_file\", \n",
    "    \"_bronze_ingested_by\",\n",
    "    \"_bronze_version\"\n",
    ").limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.1 - Bronze Layer (czƒô≈õƒá 3: zapis do Delta table)\n",
    "\n",
    "# Zapisz do Bronze schema\n",
    "(\n",
    "    orders_bronze\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(bronze_table)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Utworzono Bronze table: {bronze_table}\")\n",
    "print(f\"Liczba rekord√≥w: {spark.table(bronze_table).count()}\")\n",
    "\n",
    "# Weryfikacja danych\n",
    "print(\"\\n=== Pe≈Çny rekord z Bronze ===\")\n",
    "display(spark.table(bronze_table).limit(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1489eec-edc1-4cd0-88a2-7d1baff81de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Bronze layer:\n",
    "- **Immutable**: Dane zapisane \"as-is\" bez modyfikacji warto≈õci biznesowych\n",
    "- **Audit trail**: Ka≈ºdy rekord ma metadata: kiedy, skƒÖd, przez kogo zosta≈Ç za≈Çadowany\n",
    "- **Append-only**: Idealnie nadaje siƒô do incremental loads z COPY INTO lub Auto Loader\n",
    "- **Retention**: Czƒôsto d≈Çuga (lata) jako ≈∫r√≥d≈Ço prawdy do reprocessingu\n",
    "\n",
    "**Struktura danych ≈∫r√≥d≈Çowych (orders_batch.json)**:\n",
    "- `order_id`: String (np. \"ORD00000001\")\n",
    "- `customer_id`: String (np. \"CUST005909\")\n",
    "- `order_datetime`: Timestamp (np. \"2024-12-31T23:56:00\")\n",
    "- `total_amount`: Double - ca≈Çkowita warto≈õƒá zam√≥wienia\n",
    "- `payment_method`: String - metoda p≈Çatno≈õci"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2baf140-5390-469b-9604-301840271fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 1.2: Silver Layer - Cleansed & Validated\n",
    "\n",
    "**Cel:** Transformacja Bronze ‚Üí Silver z data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35a4cccb-7941-45d4-8ed7-fd78138541cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.2 - Silver Layer (czƒô≈õƒá 1: deduplikacja i walidacja NOT NULL)\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {SILVER_SCHEMA}\")\n",
    "\n",
    "silver_table = f\"{SILVER_SCHEMA}.orders_silver\"\n",
    "\n",
    "# Wczytaj dane z Bronze\n",
    "orders_bronze_df = spark.table(bronze_table)\n",
    "\n",
    "print(f\"=== Bronze input ===\")\n",
    "print(f\"Liczba rekord√≥w: {orders_bronze_df.count()}\")\n",
    "\n",
    "# Deduplikacja po kluczu biznesowym\n",
    "orders_deduped = orders_bronze_df.dropDuplicates([\"order_id\"])\n",
    "print(f\"\\n‚úì Po deduplikacji: {orders_deduped.count()} rekord√≥w\")\n",
    "\n",
    "# Walidacja: usu≈Ñ rekordy z NULL w kluczowych kolumnach\n",
    "orders_validated = (\n",
    "    orders_deduped\n",
    "    .filter(F.col(\"order_id\").isNotNull())\n",
    "    .filter(F.col(\"customer_id\").isNotNull())\n",
    "    .filter(F.col(\"product_id\").isNotNull())\n",
    ")\n",
    "\n",
    "print(f\"‚úì Po walidacji NOT NULL: {orders_validated.count()} rekord√≥w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.2 - Silver Layer (czƒô≈õƒá 2: walidacja biznesowa i standaryzacja)\n",
    "\n",
    "# Walidacja biznesowa: total_amount > 0\n",
    "orders_business_validated = (\n",
    "    orders_validated\n",
    "    .filter(F.col(\"total_amount\") > 0)\n",
    ")\n",
    "\n",
    "print(f\"=== Walidacja biznesowa ===\")\n",
    "print(f\"‚úì Po walidacji total_amount > 0: {orders_business_validated.count()} rekord√≥w\")\n",
    "\n",
    "# Standaryzacja dat i tekst√≥w\n",
    "orders_standardized = (\n",
    "    orders_business_validated\n",
    "    \n",
    "    # Standaryzacja dat - u≈ºyj order_datetime zamiast order_date\n",
    "    .withColumn(\"order_date\", F.to_date(F.col(\"order_datetime\")))\n",
    "    .withColumn(\"order_timestamp\", F.to_timestamp(F.col(\"order_datetime\")))\n",
    "    \n",
    "    # Standaryzacja tekst√≥w\n",
    "    .withColumn(\"payment_method\", F.upper(F.trim(F.col(\"payment_method\"))))\n",
    "    \n",
    "    # Type casting\n",
    "    .withColumn(\"total_amount\", F.col(\"total_amount\").cast(\"decimal(10,2)\"))\n",
    "    \n",
    "    # Derived column: order_status (business logic example)\n",
    "    # COMPLETED: positive amounts (normal transactions)\n",
    "    # (W praktyce, status m√≥g≈Çby byƒá wyciƒÖgniƒôty z systemu ≈∫r√≥d≈Çowego)\n",
    "    .withColumn(\"order_status\", \n",
    "                F.when(F.col(\"total_amount\") > 0, \"COMPLETED\")\n",
    "                .otherwise(\"UNKNOWN\"))\n",
    ")\n",
    "\n",
    "print(\"\\n=== Silver transformations complete (with derived columns) ===\")\n",
    "display(orders_standardized.select(\n",
    "    \"order_id\", \n",
    "    \"order_date\", \n",
    "    \"order_timestamp\",\n",
    "    \"total_amount\", \n",
    "    \"payment_method\",\n",
    "    \"order_status\"  # New derived column\n",
    ").limit(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.2 - Silver Layer (czƒô≈õƒá 3: Silver metadata i zapis)\n",
    "\n",
    "# Dodaj Silver metadata\n",
    "orders_silver = (\n",
    "    orders_standardized\n",
    "    .withColumn(\"_silver_processed_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_data_quality_flag\", F.lit(\"VALID\"))\n",
    ")\n",
    "\n",
    "# Quality metrics\n",
    "bronze_count = orders_bronze_df.count()\n",
    "silver_count = orders_silver.count()\n",
    "rejected_count = bronze_count - silver_count\n",
    "rejection_rate = (rejected_count / bronze_count * 100) if bronze_count > 0 else 0\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"SILVER LAYER - DATA QUALITY METRICS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Bronze input:  {bronze_count:,} records\")\n",
    "print(f\"Silver output: {silver_count:,} records\")\n",
    "print(f\"Rejected:      {rejected_count:,} records ({rejection_rate:.2f}%)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Zapisz do Silver schema\n",
    "(\n",
    "    orders_silver\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(silver_table)\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Utworzono Silver table: {silver_table}\")\n",
    "display(spark.table(silver_table).limit(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76fb36bc-5d0a-4db0-b028-b3cb02034c27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Silver layer:\n",
    "- **Data Quality**: Walidacja biznesowa (total_amount > 0), walidacja schemat√≥w (NOT NULL)\n",
    "- **Deduplikacja**: Usuniƒôcie duplikat√≥w po kluczu biznesowym\n",
    "- **Standardizacja**: Ujednolicenie format√≥w (daty, teksty, case sensitivity)\n",
    "- **Derived Columns**: Tworzenie nowych kolumn na podstawie logiki biznesowej (np. `order_status` z `total_amount`)\n",
    "- **Incremental friendly**: Mo≈ºna u≈ºywaƒá MERGE dla slowly changing dimensions\n",
    "\n",
    "**Nota**: W tym przyk≈Çadzie:\n",
    "1. Transformujemy `order_datetime` (timestamp) na `order_date` (date) dla ≈Çatwiejszego partycjonowania\n",
    "2. Dodajemy derived column `order_status = COMPLETED` (w rzeczywistych scenariuszach status m√≥g≈Çby byƒá wyciƒÖgany z systemu ≈∫r√≥d≈Çowego lub obliczany na podstawie bardziej z≈Ço≈ºonej logiki biznesowej)\n",
    "3. Silver Layer jest miejscem, gdzie dodajemy business context do surowych danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 1.4: Data Quality w kontek≈õcie warstw\n",
    "\n",
    "**Cel:** Zrozumienie jak data quality gates sƒÖ implementowane na ka≈ºdym etapie Medallion Architecture.\n",
    "\n",
    "---\n",
    "\n",
    "### Data Quality per Layer\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    BRONZE LAYER                               ‚îÇ\n",
    "‚îÇ  Quality Focus: Completeness & Audit                         ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  ‚úÖ Checks:                                                   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ File successfully loaded?                                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ All expected files arrived?                               ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Audit metadata present?                                    ‚îÇ\n",
    "‚îÇ                                                               ‚îÇ\n",
    "‚îÇ  ‚ö†Ô∏è  NO Business Validation (raw = raw)                      ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                        ‚îÇ Quality Gate 1\n",
    "                        ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    SILVER LAYER                               ‚îÇ\n",
    "‚îÇ  Quality Focus: Validity & Consistency                       ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  ‚úÖ Checks:                                                   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Schema validation (NOT NULL, data types)                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Business rules (amount > 0, valid dates)                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Deduplikacja (unique business keys)                       ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Referential integrity (FK checks)                         ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Data standarization (formats, case, trim)                 ‚îÇ\n",
    "‚îÇ                                                               ‚îÇ\n",
    "‚îÇ  üìä Metrics:                                                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Rejection rate (Bronze ‚Üí Silver)                          ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Invalid records count                                     ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Data quality score per column                             ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "                        ‚îÇ Quality Gate 2\n",
    "                        ‚ñº\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ                    GOLD LAYER                                 ‚îÇ\n",
    "‚îÇ  Quality Focus: Completeness & Business Logic                ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  ‚úÖ Checks:                                                   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Aggregate completeness (all partitions?)                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ KPI calculations correct?                                 ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Dimension joins successful (no orphans)?                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Business metric thresholds met?                           ‚îÇ\n",
    "‚îÇ                                                               ‚îÇ\n",
    "‚îÇ  üìä Metrics:                                                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Unmatched dimension records                               ‚îÇ\n",
    "‚îÇ  ‚Ä¢ KPI variance vs expected                                  ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Data freshness (last update time)                         ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Data Quality Framework\n",
    "\n",
    "**1. Bronze Layer Quality**\n",
    "\n",
    "**Focus:** Audit trail i completeness\n",
    "\n",
    "```python\n",
    "# Bronze quality checks\n",
    "bronze_quality_checks = {\n",
    "    \"file_loaded\": spark.table(bronze_table).count() > 0,\n",
    "    \"audit_columns_present\": all(col in spark.table(bronze_table).columns \n",
    "                                  for col in [\"_bronze_ingest_timestamp\", \"_bronze_source_file\"]),\n",
    "    \"no_duplicates_in_batch\": spark.table(bronze_table)\n",
    "                                .groupBy(\"_bronze_source_file\").count()\n",
    "                                .filter(\"count > 1\").count() == 0\n",
    "}\n",
    "```\n",
    "\n",
    "**Key Metrics:**\n",
    "- Files loaded successfully\n",
    "- Records per file\n",
    "- Load time per file\n",
    "- Schema drift detection\n",
    "\n",
    "---\n",
    "\n",
    "**2. Silver Layer Quality**\n",
    "\n",
    "**Focus:** Validity, consistency, standardization\n",
    "\n",
    "```python\n",
    "# Silver quality checks\n",
    "silver_quality_checks = {\n",
    "    \"not_null\": {\n",
    "        \"order_id\": orders_silver.filter(F.col(\"order_id\").isNull()).count() == 0,\n",
    "        \"customer_id\": orders_silver.filter(F.col(\"customer_id\").isNull()).count() == 0\n",
    "    },\n",
    "    \"business_rules\": {\n",
    "        \"positive_amount\": orders_silver.filter(F.col(\"total_amount\") <= 0).count() == 0,\n",
    "        \"valid_dates\": orders_silver.filter(F.col(\"order_date\") > F.current_date()).count() == 0\n",
    "    },\n",
    "    \"uniqueness\": {\n",
    "        \"unique_order_ids\": orders_silver.select(\"order_id\").distinct().count() == orders_silver.count()\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Key Metrics:**\n",
    "- **Rejection rate**: (Bronze_count - Silver_count) / Bronze_count\n",
    "- **Null percentage** per kolumna\n",
    "- **Invalid records** per validation rule\n",
    "- **Duplicate rate**\n",
    "- **Standardization coverage** (% records standardized)\n",
    "\n",
    "---\n",
    "\n",
    "**3. Gold Layer Quality**\n",
    "\n",
    "**Focus:** Business logic correctness, completeness\n",
    "\n",
    "```python\n",
    "# Gold quality checks\n",
    "gold_quality_checks = {\n",
    "    \"completeness\": {\n",
    "        \"all_dates_present\": check_date_continuity(gold_daily_summary),\n",
    "        \"no_missing_partitions\": check_partition_completeness(gold_table)\n",
    "    },\n",
    "    \"referential_integrity\": {\n",
    "        \"orphan_customers\": check_unmatched_fk(gold_fact, dim_customer, \"customer_id\"),\n",
    "        \"orphan_products\": check_unmatched_fk(gold_fact, dim_product, \"product_id\")\n",
    "    },\n",
    "    \"business_logic\": {\n",
    "        \"revenue_matches\": sum(gold_fact.total_amount) == sum(silver.total_amount),\n",
    "        \"kpi_thresholds\": check_kpi_thresholds(gold_kpis)\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Key Metrics:**\n",
    "- **Unmatched foreign keys** (orphans)\n",
    "- **Aggregate variance** (Gold vs Silver source)\n",
    "- **Data freshness** (time since last refresh)\n",
    "- **KPI anomalies** (unexpected spikes/drops)\n",
    "\n",
    "---\n",
    "\n",
    "### Quality Monitoring Dashboard\n",
    "\n",
    "**Przyk≈Çad: Real-time quality monitoring**\n",
    "\n",
    "```python\n",
    "# Quality dashboard per layer\n",
    "quality_dashboard = {\n",
    "    \"Bronze\": {\n",
    "        \"total_records\": bronze_count,\n",
    "        \"unique_files\": bronze_df.select(\"_bronze_source_file\").distinct().count(),\n",
    "        \"load_timestamp\": bronze_df.agg(F.max(\"_bronze_ingest_timestamp\")).collect()[0][0]\n",
    "    },\n",
    "    \"Silver\": {\n",
    "        \"total_records\": silver_count,\n",
    "        \"rejection_rate\": rejection_rate,\n",
    "        \"null_percentage\": calculate_null_percentage(silver_df),\n",
    "        \"quality_flag_distribution\": silver_df.groupBy(\"_data_quality_flag\").count().collect()\n",
    "    },\n",
    "    \"Gold\": {\n",
    "        \"total_records\": gold_count,\n",
    "        \"orphan_rate\": calculate_orphan_rate(gold_fact),\n",
    "        \"freshness_hours\": calculate_freshness_hours(gold_df)\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Alerting Thresholds\n",
    "\n",
    "**Ustaw alerty dla anomalii w data quality:**\n",
    "\n",
    "| Metric | Threshold | Action |\n",
    "|--------|-----------|--------|\n",
    "| Bronze rejection rate | > 5% | ‚ö†Ô∏è Warning |\n",
    "| Bronze rejection rate | > 10% | ‚ùå Critical - Stop pipeline |\n",
    "| Silver null % | > 2% | ‚ö†Ô∏è Warning |\n",
    "| Gold orphan rate | > 1% | ‚ö†Ô∏è Warning - Check dimension loads |\n",
    "| Data freshness | > 24h | ‚ö†Ô∏è Warning - Pipeline delayed |\n",
    "\n",
    "---\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**‚úÖ Bronze Quality:**\n",
    "- Monitor file arrival patterns (missing files?)\n",
    "- Track schema evolution (new columns?)\n",
    "- Audit trail completeness\n",
    "\n",
    "**‚úÖ Silver Quality:**\n",
    "- Define clear validation rules per column\n",
    "- Quarantine invalid records (don't drop silently)\n",
    "- Monitor rejection rate trends\n",
    "- Alert on sudden spikes in rejections\n",
    "\n",
    "**‚úÖ Gold Quality:**\n",
    "- Validate aggregate completeness (all partitions?)\n",
    "- Check referential integrity (no orphans)\n",
    "- Monitor KPI variance (expected ranges?)\n",
    "- Track data freshness (SLA compliance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c576e84-79c6-4c93-bf0d-88c90f91605c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 1.3: Gold Layer - Business Aggregates\n",
    "\n",
    "**Cel:** Utworzenie Gold layer z KPI dla analityki i raportowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.3 - Gold Layer (czƒô≈õƒá 1: Wczytanie danych z Silver + data quality check)\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {GOLD_SCHEMA}\")\n",
    "\n",
    "# Wczytaj dane z Silver\n",
    "orders_silver_df = spark.table(silver_table)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GOLD LAYER - DATA QUALITY CHECK (COMPLETENESS)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Data quality check: Completeness\n",
    "silver_count = orders_silver_df.count()\n",
    "silver_date_range = orders_silver_df.agg(\n",
    "    F.min(\"order_date\").alias(\"min_date\"),\n",
    "    F.max(\"order_date\").alias(\"max_date\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"Silver records: {silver_count:,}\")\n",
    "print(f\"Date range: {silver_date_range['min_date']} to {silver_date_range['max_date']}\")\n",
    "\n",
    "# Check date continuity (czy wszystkie dni sƒÖ obecne?)\n",
    "expected_days = (silver_date_range['max_date'] - silver_date_range['min_date']).days + 1\n",
    "actual_days = orders_silver_df.select(\"order_date\").distinct().count()\n",
    "\n",
    "print(f\"\\nExpected days: {expected_days}\")\n",
    "print(f\"Actual days: {actual_days}\")\n",
    "if expected_days == actual_days:\n",
    "    print(\"‚úÖ Date continuity: OK (no missing dates)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Date continuity: INCOMPLETE (missing {expected_days - actual_days} days)\")\n",
    "\n",
    "# Check for null critical columns\n",
    "null_checks = {\n",
    "    \"order_id\": orders_silver_df.filter(F.col(\"order_id\").isNull()).count(),\n",
    "    \"customer_id\": orders_silver_df.filter(F.col(\"customer_id\").isNull()).count(),\n",
    "    \"total_amount\": orders_silver_df.filter(F.col(\"total_amount\").isNull()).count(),\n",
    "    \"order_date\": orders_silver_df.filter(F.col(\"order_date\").isNull()).count()\n",
    "}\n",
    "\n",
    "print(\"\\n[NULL CHECKS]\")\n",
    "all_passed = True\n",
    "for col_name, null_count in null_checks.items():\n",
    "    status = \"‚úÖ\" if null_count == 0 else \"‚ùå\"\n",
    "    print(f\"{status} {col_name}: {null_count} nulls\")\n",
    "    if null_count > 0:\n",
    "        all_passed = False\n",
    "\n",
    "if all_passed:\n",
    "    print(\"\\n‚úÖ All data quality checks PASSED - proceeding to Gold aggregation\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Some quality checks FAILED - review Silver layer before Gold aggregation\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.3 - Gold Layer (czƒô≈õƒá 2: Agregacja biznesowa + KPI calculation)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GOLD LAYER - BUSINESS AGGREGATION & KPI CALCULATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Gold aggregation: Daily order summary z KPI\n",
    "daily_summary = (\n",
    "    orders_silver_df\n",
    "    .groupBy(\"order_date\", \"order_status\")\n",
    "    .agg(\n",
    "        # Volume metrics\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "        \n",
    "        # Revenue metrics\n",
    "        F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "        F.min(\"total_amount\").alias(\"min_order_value\"),\n",
    "        F.max(\"total_amount\").alias(\"max_order_value\"),\n",
    "        \n",
    "        # Payment method distribution\n",
    "        F.count(F.when(F.col(\"payment_method\") == \"credit_card\", 1)).alias(\"credit_card_orders\"),\n",
    "        F.count(F.when(F.col(\"payment_method\") == \"paypal\", 1)).alias(\"paypal_orders\"),\n",
    "        F.count(F.when(F.col(\"payment_method\") == \"bank_transfer\", 1)).alias(\"bank_transfer_orders\")\n",
    "    )\n",
    "    .orderBy(\"order_date\", \"order_status\")\n",
    ")\n",
    "\n",
    "# Calculate derived KPIs\n",
    "daily_summary_with_kpis = (\n",
    "    daily_summary\n",
    "    .withColumn(\"revenue_per_customer\", \n",
    "                F.round(F.col(\"total_revenue\") / F.col(\"unique_customers\"), 2))\n",
    "    .withColumn(\"credit_card_penetration_pct\",\n",
    "                F.round(F.col(\"credit_card_orders\") / F.col(\"total_orders\") * 100, 2))\n",
    ")\n",
    "\n",
    "print(\"\\n[GOLD AGGREGATION PREVIEW]\")\n",
    "display(daily_summary_with_kpis.limit(5))\n",
    "\n",
    "# Summary statistics\n",
    "total_days = daily_summary_with_kpis.select(\"order_date\").distinct().count()\n",
    "total_orders_gold = daily_summary_with_kpis.agg(F.sum(\"total_orders\")).collect()[0][0]\n",
    "total_revenue_gold = daily_summary_with_kpis.agg(F.sum(\"total_revenue\")).collect()[0][0]\n",
    "\n",
    "print(f\"\\n[GOLD SUMMARY STATISTICS]\")\n",
    "print(f\"Total days aggregated: {total_days}\")\n",
    "print(f\"Total orders (all days): {total_orders_gold:,}\")\n",
    "print(f\"Total revenue (all days): ${total_revenue_gold:,.2f}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.3 - Gold Layer (czƒô≈õƒá 3: Gold audit metadata + zapis do Delta)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GOLD LAYER - AUDIT METADATA & DELTA WRITE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Dodaj Gold audit metadata\n",
    "gold_table = f\"{GOLD_SCHEMA}.daily_order_summary\"\n",
    "\n",
    "daily_summary_final = (\n",
    "    daily_summary_with_kpis\n",
    "    .withColumn(\"_gold_created_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_gold_aggregation_level\", F.lit(\"DAILY\"))\n",
    "    .withColumn(\"_gold_source_silver_table\", F.lit(silver_table))\n",
    "    .withColumn(\"_gold_refresh_id\", F.lit(str(uuid.uuid4())))\n",
    ")\n",
    "\n",
    "# Zapisz do Gold schema\n",
    "(\n",
    "    daily_summary_final\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(gold_table)\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Utworzono Gold table: {gold_table}\")\n",
    "\n",
    "# Verify Gold table\n",
    "gold_verification = spark.table(gold_table)\n",
    "gold_count = gold_verification.count()\n",
    "\n",
    "print(f\"\\n[GOLD TABLE VERIFICATION]\")\n",
    "print(f\"Gold records: {gold_count}\")\n",
    "print(f\"Gold schema columns: {len(gold_verification.columns)}\")\n",
    "\n",
    "# Gold lineage tracking\n",
    "print(f\"\\n[GOLD LINEAGE]\")\n",
    "print(f\"Source: {silver_table}\")\n",
    "print(f\"Target: {gold_table}\")\n",
    "print(f\"Aggregation level: DAILY\")\n",
    "print(f\"Aggregation keys: order_date, order_status\")\n",
    "print(f\"Created at: {daily_summary_final.select('_gold_created_timestamp').first()[0]}\")\n",
    "\n",
    "# Display sample Gold records with audit metadata\n",
    "print(\"\\n[GOLD TABLE PREVIEW - with audit metadata]\")\n",
    "display(gold_verification.select(\n",
    "    \"order_date\",\n",
    "    \"order_status\",\n",
    "    \"total_orders\",\n",
    "    \"total_revenue\",\n",
    "    \"unique_customers\",\n",
    "    \"_gold_created_timestamp\",\n",
    "    \"_gold_aggregation_level\"\n",
    ").limit(5))\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d958525c-9c99-4f91-bf6d-ff0c23db890e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Gold layer:\n",
    "- **Business-level**: KPI i metryki zgodne z business definitions\n",
    "- **Denormalized**: Czƒôsto szeroka tabela z joinami ju≈º wykonanymi (performance dla BI)\n",
    "- **Aggregated**: Dane pre-aggregowane (daily, weekly, monthly) dla szybkich dashboard√≥w\n",
    "- **BI-ready**: Bezpo≈õrednie ≈∫r√≥d≈Ço dla Power BI, Tableau, Looker\n",
    "\n",
    "**Nota**: W tym przyk≈Çadzie agregujemy zam√≥wienia po dacie i metodzie p≈Çatno≈õci (payment_method). W rzeczywistych scenariuszach mo≈ºesz agregowaƒá po innych wymiarach biznesowych jak region, kategoria produktu, czy segment klienta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Gold Layer - Monitoring & Lineage\n",
    "\n",
    "**Dlaczego monitoring Gold layer jest krytyczny?**\n",
    "\n",
    "Gold layer to ko≈Ñcowy produkt dla biznesu - dashboardy, raporty, modele ML. Problemy w Gold = bezpo≈õredni wp≈Çyw na decyzje biznesowe.\n",
    "\n",
    "---\n",
    "\n",
    "**Kluczowe metryki Gold Layer:**\n",
    "\n",
    "**1. Data Freshness**\n",
    "```python\n",
    "# Sprawd≈∫ jak ≈õwie≈ºe sƒÖ dane w Gold\n",
    "gold_freshness = (\n",
    "    spark.table(gold_table)\n",
    "    .agg(F.max(\"_gold_created_timestamp\").alias(\"last_refresh\"))\n",
    "    .collect()[0][0]\n",
    ")\n",
    "\n",
    "hours_since_refresh = (datetime.now() - gold_freshness).total_seconds() / 3600\n",
    "\n",
    "if hours_since_refresh > 24:\n",
    "    print(f\"‚ö†Ô∏è Gold layer is {hours_since_refresh:.1f} hours old - SLA breach!\")\n",
    "else:\n",
    "    print(f\"‚úÖ Gold layer is fresh ({hours_since_refresh:.1f} hours old)\")\n",
    "```\n",
    "\n",
    "**2. Aggregate Completeness**\n",
    "```python\n",
    "# Czy wszystkie dni sƒÖ zagregowane?\n",
    "from datetime import timedelta\n",
    "\n",
    "max_silver_date = spark.table(silver_table).agg(F.max(\"order_date\")).collect()[0][0]\n",
    "max_gold_date = spark.table(gold_table).agg(F.max(\"order_date\")).collect()[0][0]\n",
    "\n",
    "if max_gold_date < max_silver_date:\n",
    "    missing_days = (max_silver_date - max_gold_date).days\n",
    "    print(f\"‚ö†Ô∏è Gold is behind Silver by {missing_days} days\")\n",
    "else:\n",
    "    print(f\"‚úÖ Gold is up-to-date with Silver\")\n",
    "```\n",
    "\n",
    "**3. Revenue Reconciliation (Gold vs Silver)**\n",
    "```python\n",
    "# Gold revenue musi siƒô zgadzaƒá z Silver revenue\n",
    "silver_revenue = spark.table(silver_table).agg(F.sum(\"total_amount\")).collect()[0][0]\n",
    "gold_revenue = spark.table(gold_table).agg(F.sum(\"total_revenue\")).collect()[0][0]\n",
    "\n",
    "variance_pct = abs((gold_revenue - silver_revenue) / silver_revenue * 100)\n",
    "\n",
    "if variance_pct > 0.01:  # 0.01% tolerance\n",
    "    print(f\"‚ùå Revenue mismatch: {variance_pct:.4f}% variance!\")\n",
    "else:\n",
    "    print(f\"‚úÖ Revenue reconciled: {variance_pct:.4f}% variance\")\n",
    "```\n",
    "\n",
    "**4. Unmatched Foreign Keys (Orphans)**\n",
    "```python\n",
    "# Przyk≈Çad: Sprawd≈∫ czy wszystkie customer_id z Gold majƒÖ odpowiednik w dim_customer\n",
    "# (zak≈ÇadajƒÖc, ≈ºe mamy dimension table)\n",
    "\n",
    "# orphan_customers = (\n",
    "#     gold_fact\n",
    "#     .join(dim_customer, \"customer_id\", \"left_anti\")\n",
    "#     .count()\n",
    "# )\n",
    "# \n",
    "# if orphan_customers > 0:\n",
    "#     print(f\"‚ö†Ô∏è {orphan_customers} orphan customers in Gold\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Gold Layer Lineage Tracking:**\n",
    "\n",
    "Gold layer czƒôsto ≈ÇƒÖczy wiele Silver tables (joins). Wa≈ºne jest ≈õledzenie, kt√≥re Silver tables by≈Çy u≈ºyte:\n",
    "\n",
    "```python\n",
    "# Przyk≈Çad: Multi-source Gold table\n",
    "gold_with_lineage = (\n",
    "    orders_silver\n",
    "    .join(customers_silver, \"customer_id\")\n",
    "    .join(products_silver, \"product_id\")\n",
    "    .groupBy(\"order_date\")\n",
    "    .agg(...)\n",
    "    .withColumn(\"_gold_source_tables\", F.lit([\n",
    "        \"silver.orders\",\n",
    "        \"silver.customers\", \n",
    "        \"silver.products\"\n",
    "    ]))\n",
    ")\n",
    "```\n",
    "\n",
    "Dziƒôki temu wiemy: je≈õli `silver.customers` ma problem, kt√≥re Gold tables sƒÖ affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 1.5: Audyt i Lineage - Metadane w ka≈ºdym kroku\n",
    "\n",
    "**Cel:** Zrozumienie jak implementowaƒá audit trail i data lineage w Medallion Architecture.\n",
    "\n",
    "---\n",
    "\n",
    "### Czym jest Data Lineage?\n",
    "\n",
    "**Data Lineage** to ≈õledzenie przep≈Çywu danych od ≈∫r√≥d≈Ça do destination, w tym wszystkie transformacje, filtry i agregacje po drodze.\n",
    "\n",
    "```\n",
    "Source System ‚Üí Bronze ‚Üí Silver ‚Üí Gold ‚Üí BI Dashboard\n",
    "     ‚îÇ            ‚îÇ        ‚îÇ        ‚îÇ          ‚îÇ\n",
    "     ‚îî‚îÄ When?     ‚îî‚îÄ Who?  ‚îî‚îÄ What? ‚îî‚îÄ How?  ‚îî‚îÄ Used by?\n",
    "```\n",
    "\n",
    "**Dlaczego lineage jest kluczowy?**\n",
    "- **Compliance** (GDPR, SOX): \"Where did this data come from?\"\n",
    "- **Troubleshooting**: \"Why is this metric wrong?\"\n",
    "- **Impact Analysis**: \"What breaks if I change this table?\"\n",
    "- **Data Quality**: \"Which source file had bad data?\"\n",
    "\n",
    "---\n",
    "\n",
    "### Audit Metadata per Layer\n",
    "\n",
    "**Bronze Layer Audit Columns:**\n",
    "\n",
    "```python\n",
    "bronze_audit_columns = [\n",
    "    \"_bronze_ingest_timestamp\",    # Kiedy dane trafi≈Çy do Bronze\n",
    "    \"_bronze_source_file\",          # SkƒÖd pochodzƒÖ (file path)\n",
    "    \"_bronze_ingested_by\",          # Kto/co za≈Çadowa≈Ço (user/job ID)\n",
    "    \"_bronze_version\",              # Wersja schematu/procesu\n",
    "    \"_bronze_batch_id\"              # UUID dla batch tracking (optional)\n",
    "]\n",
    "```\n",
    "\n",
    "**Silver Layer Audit Columns:**\n",
    "\n",
    "```python\n",
    "silver_audit_columns = [\n",
    "    \"_silver_processed_timestamp\",  # Kiedy Bronze ‚Üí Silver\n",
    "    \"_data_quality_flag\",           # VALID / INVALID / QUARANTINE\n",
    "    \"_silver_transformation_id\",    # ID pipeline'u kt√≥ry przetworzy≈Ç\n",
    "    \"_silver_source_bronze_version\" # Z kt√≥rej wersji Bronze\n",
    "]\n",
    "```\n",
    "\n",
    "**Gold Layer Audit Columns:**\n",
    "\n",
    "```python\n",
    "gold_audit_columns = [\n",
    "    \"_gold_created_timestamp\",      # Kiedy Silver ‚Üí Gold\n",
    "    \"_gold_refresh_timestamp\",      # Ostatni refresh (dla aggregates)\n",
    "    \"_gold_aggregation_level\",      # DAILY / MONTHLY / YEARLY\n",
    "    \"_gold_source_silver_tables\"    # Lista Silver tables u≈ºytych w aggregation\n",
    "]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Implementacja Audit Trail\n",
    "\n",
    "**Przyk≈Çad: Tracking kompletnego pipeline'u**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.5 - Audit Trail Tracking\n",
    "\n",
    "import uuid\n",
    "\n",
    "# Generate unique batch ID\n",
    "batch_id = str(uuid.uuid4())\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"AUDIT TRAIL TRACKING - END-TO-END LINEAGE\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nBatch ID: {batch_id}\")\n",
    "\n",
    "# Bronze: Track source\n",
    "bronze_lineage = spark.table(bronze_table).select(\n",
    "    \"_bronze_ingest_timestamp\",\n",
    "    \"_bronze_source_file\",\n",
    "    \"_bronze_ingested_by\",\n",
    "    \"_bronze_version\"\n",
    ").distinct()\n",
    "\n",
    "print(\"\\n[BRONZE LAYER - Source Tracking]\")\n",
    "display(bronze_lineage)\n",
    "\n",
    "# Silver: Track transformation\n",
    "silver_lineage = spark.table(silver_table).select(\n",
    "    \"_silver_processed_timestamp\",\n",
    "    \"_data_quality_flag\"\n",
    ").distinct()\n",
    "\n",
    "print(\"\\n[SILVER LAYER - Transformation Tracking]\")\n",
    "display(silver_lineage)\n",
    "\n",
    "# Lineage query: Kt√≥re Bronze files stworzy≈Çy kt√≥re Silver records?\n",
    "bronze_to_silver_lineage = (\n",
    "    spark.table(silver_table)\n",
    "    .join(\n",
    "        spark.table(bronze_table),\n",
    "        [\"order_id\"],\n",
    "        \"inner\"\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"order_id\"),\n",
    "        F.col(\"_bronze_source_file\").alias(\"source_file\"),\n",
    "        F.col(\"_bronze_ingest_timestamp\").alias(\"bronze_loaded_at\"),\n",
    "        F.col(\"_silver_processed_timestamp\").alias(\"silver_processed_at\")\n",
    "    )\n",
    "    .distinct()\n",
    ")\n",
    "\n",
    "print(\"\\n[LINEAGE - Bronze ‚Üí Silver Tracking]\")\n",
    "display(bronze_to_silver_lineage.limit(10))\n",
    "\n",
    "# Calculate processing lag\n",
    "processing_lag = (\n",
    "    bronze_to_silver_lineage\n",
    "    .withColumn(\n",
    "        \"processing_lag_seconds\",\n",
    "        F.unix_timestamp(\"silver_processed_at\") - F.unix_timestamp(\"bronze_loaded_at\")\n",
    "    )\n",
    "    .agg(\n",
    "        F.avg(\"processing_lag_seconds\").alias(\"avg_lag_seconds\"),\n",
    "        F.max(\"processing_lag_seconds\").alias(\"max_lag_seconds\")\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"\\n[PROCESSING METRICS]\")\n",
    "display(processing_lag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cdff76a-5207-406e-8143-390d787b3c20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 2: ETL vs ELT Approach\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Medallion architecture wspiera ELT (Extract-Load-Transform) approach, w przeciwie≈Ñstwie do tradycyjnego ETL. Dane sƒÖ najpierw ≈Çadowane do Bronze (Load), a potem transformowane w Silver/Gold (Transform).\n",
    "\n",
    "**Kluczowe r√≥≈ºnice:**\n",
    "- **ETL**: Transform before load - dane sƒÖ czyszczone poza data warehouse\n",
    "- **ELT**: Load then transform - surowe dane w Bronze, transformacje w lakehouse\n",
    "- **Zalety ELT**: Mo≈ºliwo≈õƒá reprocessingu, data lineage, audit trail, schema evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 2.5: Zasady projektowania Pipeline'√≥w\n",
    "\n",
    "**Cel:** Zrozumienie best practices przy projektowaniu production-ready Medallion pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "### 10 Zasad Projektowania Pipeline'√≥w\n",
    "\n",
    "**1. Idempotency (Powtarzalno≈õƒá)**\n",
    "\n",
    "Pipeline musi daƒá ten sam wynik przy wielokrotnym uruchomieniu na tych samych danych.\n",
    "\n",
    "```python\n",
    "# ‚úÖ DOBRZE: Idempotent (overwrite lub MERGE z kluczem)\n",
    "df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "# ‚ùå ≈πLE: Non-idempotent (append bez deduplikacji)\n",
    "df.write.mode(\"append\").saveAsTable(table_name)  # Duplikaty przy retry!\n",
    "```\n",
    "\n",
    "**Best Practice:**\n",
    "- Bronze: COPY INTO (automatic idempotency)\n",
    "- Silver: MERGE na business key\n",
    "- Gold: Overwrite partition lub MERGE\n",
    "\n",
    "---\n",
    "\n",
    "**2. Incrementality (Przetwarzanie przyrostowe)**\n",
    "\n",
    "Procesuj tylko nowe/zmienione dane, nie ca≈ÇƒÖ tabelƒô.\n",
    "\n",
    "```python\n",
    "# ‚úÖ DOBRZE: Incremental processing\n",
    "max_processed_ts = spark.table(silver_table).agg(F.max(\"_bronze_ingest_timestamp\")).collect()[0][0]\n",
    "new_bronze = spark.table(bronze_table).filter(F.col(\"_bronze_ingest_timestamp\") > max_processed_ts)\n",
    "\n",
    "# ‚ùå ≈πLE: Full table scan ka≈ºdorazowo\n",
    "all_bronze = spark.table(bronze_table)  # Procesuje wszystko od nowa\n",
    "```\n",
    "\n",
    "**Best Practice:**\n",
    "- Watermarking: Track last processed timestamp\n",
    "- Partitioning: Proces tylko affected partitions\n",
    "- Change Data Capture (CDC): Track tylko changed records\n",
    "\n",
    "---\n",
    "\n",
    "**3. Separation of Concerns (Izolacja warstw)**\n",
    "\n",
    "Ka≈ºda warstwa ma okre≈õlone responsibility.\n",
    "\n",
    "```\n",
    "Bronze:  Data Ingestion + Audit\n",
    "         ‚Üì\n",
    "Silver:  Data Quality + Standardization\n",
    "         ‚Üì\n",
    "Gold:    Business Logic + Aggregation\n",
    "```\n",
    "\n",
    "**Best Practice:**\n",
    "- Nie mieszaj business logic w Bronze (raw = raw)\n",
    "- Nie r√≥b agregacji w Silver (tylko cleaning)\n",
    "- Nie r√≥b walidacji w Gold (ju≈º zwalidowane w Silver)\n",
    "\n",
    "---\n",
    "\n",
    "**4. Schema Evolution (Obs≈Çuga zmian schematu)**\n",
    "\n",
    "Pipeline musi obs≈Çugiwaƒá nowe kolumny bez breaking.\n",
    "\n",
    "```python\n",
    "# ‚úÖ DOBRZE: Merge schema\n",
    "df.write.option(\"mergeSchema\", \"true\").mode(\"append\").saveAsTable(table_name)\n",
    "\n",
    "# ‚úÖ DOBRZE: Schema validation\n",
    "if new_col in df.columns:\n",
    "    # Process new column\n",
    "    pass\n",
    "```\n",
    "\n",
    "**Best Practice:**\n",
    "- Bronze: Allow all schema changes (permissive mode)\n",
    "- Silver: Validate schema against expected (fail on breaking changes)\n",
    "- Gold: Fixed schema (breaking changes require re-architecture)\n",
    "\n",
    "---\n",
    "\n",
    "**5. Error Handling & Quarantine**\n",
    "\n",
    "Nie fail ca≈Çego pipeline'u przez kilka bad records.\n",
    "\n",
    "```python\n",
    "# ‚úÖ DOBRZE: Quarantine invalid records\n",
    "valid_records = df.filter(validation_condition)\n",
    "invalid_records = df.filter(~validation_condition)\n",
    "\n",
    "invalid_records.write.mode(\"append\").saveAsTable(\"quarantine_table\")\n",
    "valid_records.write.mode(\"append\").saveAsTable(\"silver_table\")\n",
    "```\n",
    "\n",
    "**Best Practice:**\n",
    "- Log invalid records do quarantine table\n",
    "- Alert on spike w rejection rate\n",
    "- Manual review quarantine periodically\n",
    "\n",
    "---\n",
    "\n",
    "**6. Monitoring & Observability**\n",
    "\n",
    "Co nie jest monitorowane, nie istnieje.\n",
    "\n",
    "```python\n",
    "# Track key metrics\n",
    "metrics = {\n",
    "    \"records_processed\": df.count(),\n",
    "    \"processing_time_seconds\": end_time - start_time,\n",
    "    \"rejection_rate\": rejected_count / total_count,\n",
    "    \"pipeline_status\": \"SUCCESS\" if no_errors else \"FAILED\"\n",
    "}\n",
    "\n",
    "# Log do monitoring system\n",
    "log_metrics(metrics, table_name, timestamp)\n",
    "```\n",
    "\n",
    "**Best Practice:**\n",
    "- Monitor per layer: Bronze count, Silver rejection, Gold freshness\n",
    "- Alert thresholds: rejection > 5%, lag > 1h\n",
    "- Dashboard per pipeline: throughput, latency, errors\n",
    "\n",
    "---\n",
    "\n",
    "**7. Partitioning Strategy**\n",
    "\n",
    "Partycjonuj tylko je≈õli tabela > 1TB i partition > 1GB.\n",
    "\n",
    "```python\n",
    "# ‚úÖ DOBRZE: Partitioning po czƒôsto filtrowanej kolumnie\n",
    "df.write.partitionBy(\"order_date\").saveAsTable(table_name)\n",
    "\n",
    "# ‚ùå ≈πLE: Over-partitioning (< 1GB partitions)\n",
    "df.write.partitionBy(\"order_date\", \"order_hour\", \"customer_id\").saveAsTable(table_name)\n",
    "```\n",
    "\n",
    "**Best Practice:**\n",
    "- Bronze: Rzadko partycjonuj (bulk operations)\n",
    "- Silver: Partition po date lub region\n",
    "- Gold: Partition wg query patterns (date, region, category)\n",
    "\n",
    "---\n",
    "\n",
    "**8. Retry & Resilience**\n",
    "\n",
    "Pipeline musi recovery z failures.\n",
    "\n",
    "```python\n",
    "# ‚úÖ DOBRZE: Checkpoint dla streaming\n",
    "(\n",
    "    spark.readStream\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .table(source_table)\n",
    "    .writeStream\n",
    "    .table(target_table)\n",
    ")\n",
    "```\n",
    "\n",
    "**Best Practice:**\n",
    "- Checkpoint locations dla streaming\n",
    "- Exponential backoff dla retries\n",
    "- Transaction logs (Delta) dla consistency\n",
    "\n",
    "---\n",
    "\n",
    "**9. Data Retention & Lifecycle**\n",
    "\n",
    "R√≥≈ºne retention policies per warstwa.\n",
    "\n",
    "```\n",
    "Bronze:  3-7 lat   (compliance, reprocessing)\n",
    "Silver:  1-2 lata  (operational analytics)\n",
    "Gold:    6-12 mies (refreshable from Silver)\n",
    "```\n",
    "\n",
    "**Best Practice:**\n",
    "- VACUUM starsze partycje regularnie\n",
    "- Archive do cold storage (S3 Glacier, Azure Archive)\n",
    "- Automated lifecycle policies (TTL)\n",
    "\n",
    "---\n",
    "\n",
    "**10. Testing & Validation**\n",
    "\n",
    "Test pipeline przed production.\n",
    "\n",
    "```python\n",
    "# Unit tests\n",
    "def test_silver_transformation():\n",
    "    input_df = create_test_bronze_data()\n",
    "    output_df = silver_transformation(input_df)\n",
    "    assert output_df.filter(F.col(\"total_amount\") <= 0).count() == 0\n",
    "\n",
    "# Integration tests\n",
    "def test_end_to_end_pipeline():\n",
    "    run_pipeline(test_data_path)\n",
    "    assert_gold_count_matches_expected()\n",
    "```\n",
    "\n",
    "**Best Practice:**\n",
    "- Unit tests per transformation\n",
    "- Integration tests end-to-end\n",
    "- Data quality tests (Great Expectations, Deequ)\n",
    "- Smoke tests w production (sanity checks)\n",
    "\n",
    "---\n",
    "\n",
    "### Pipeline Design Checklist\n",
    "\n",
    "Przed deploymentem production pipeline, sprawd≈∫:\n",
    "\n",
    "- [ ] ‚úÖ Idempotent? (retry-safe)\n",
    "- [ ] ‚úÖ Incremental? (tylko nowe dane)\n",
    "- [ ] ‚úÖ Schema evolution? (handle new columns)\n",
    "- [ ] ‚úÖ Error handling? (quarantine invalid)\n",
    "- [ ] ‚úÖ Monitoring? (metrics + alerts)\n",
    "- [ ] ‚úÖ Partitioning? (je≈õli > 1TB)\n",
    "- [ ] ‚úÖ Checkpoint? (dla streaming)\n",
    "- [ ] ‚úÖ Retention policy? (lifecycle management)\n",
    "- [ ] ‚úÖ Tests? (unit + integration)\n",
    "- [ ] ‚úÖ Documentation? (README, lineage diagram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5467410-81ef-454e-85dd-7a4a0124f6c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 2.1: ELT Pipeline - Incremental Processing\n",
    "\n",
    "**Cel:** Demonstracja incremental ELT: nowe dane w Bronze ‚Üí automatyczna propagacja do Silver/Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd361f35-381a-4bc1-a9f2-afbbe5145050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 2.1 - Incremental ELT\n",
    "\n",
    "# Symulacja: nowe dane przychodzƒÖ do Bronze\n",
    "new_orders_data = [\n",
    "    (\"ORD99990001\", \"CUST009901\", \"PROD000001\", \"STORE001\", \"2025-01-20T10:00:00\", 2, 175.00, 0, 350.00, \"Credit Card\"),\n",
    "    (\"ORD99990002\", \"CUST009902\", \"PROD000002\", \"STORE002\", \"2025-01-20T11:00:00\", 1, 120.50, 0, 120.50, \"Debit Card\"),\n",
    "    (\"ORD99990003\", \"CUST009903\", \"PROD000003\", \"STORE003\", \"2025-01-21T09:00:00\", 1, 499.99, 0, 499.99, \"PayPal\")\n",
    "]\n",
    "\n",
    "new_orders_df = spark.createDataFrame(\n",
    "    new_orders_data,\n",
    "    [\"order_id\", \"customer_id\", \"product_id\", \"store_id\", \"order_datetime\", \"quantity\", \"unit_price\", \"discount_percent\", \"total_amount\", \"payment_method\"]\n",
    ")\n",
    "\n",
    "# Dodaj audit metadata (Bronze standard)\n",
    "new_orders_bronze = (\n",
    "    new_orders_df\n",
    "    .withColumn(\"_bronze_ingest_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_bronze_source_file\", F.lit(\"incremental_batch_2\"))\n",
    "    .withColumn(\"_bronze_ingested_by\", F.lit(raw_user))\n",
    "    .withColumn(\"_bronze_version\", F.lit(2))\n",
    ")\n",
    "\n",
    "# Append do Bronze (ELT: Load first)\n",
    "(\n",
    "    new_orders_bronze\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(bronze_table)\n",
    ")\n",
    "\n",
    "print(f\"‚úì Dodano {new_orders_df.count()} nowych rekord√≥w do Bronze\")\n",
    "print(f\"Bronze total: {spark.table(bronze_table).count()} records\")\n",
    "\n",
    "# Incremental Silver processing: tylko nowe Bronze records (version 2)\n",
    "new_bronze_records = (\n",
    "    spark.table(bronze_table)\n",
    "    .filter(F.col(\"_bronze_version\") == 2)\n",
    ")\n",
    "\n",
    "# Apply Silver transformations (zgodnie z kom√≥rkƒÖ Silver Layer)\n",
    "new_silver_records = (\n",
    "    new_bronze_records\n",
    "    .dropDuplicates([\"order_id\"])\n",
    "    .filter(F.col(\"order_id\").isNotNull())\n",
    "    .filter(F.col(\"customer_id\").isNotNull())\n",
    "    .filter(F.col(\"total_amount\") > 0)\n",
    "    .withColumn(\"order_date\", F.to_date(F.col(\"order_datetime\")))\n",
    "    .withColumn(\"order_timestamp\", F.to_timestamp(F.col(\"order_datetime\")))\n",
    "    .withColumn(\"payment_method\", F.upper(F.trim(F.col(\"payment_method\"))))\n",
    "    .withColumn(\"total_amount\", F.col(\"total_amount\").cast(\"decimal(10,2)\"))\n",
    "    .withColumn(\"order_status\", F.when(F.col(\"total_amount\") > 0, \"COMPLETED\").otherwise(\"UNKNOWN\"))\n",
    "    .withColumn(\"_silver_processed_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_data_quality_flag\", F.lit(\"VALID\"))\n",
    ")\n",
    "\n",
    "# Append do Silver\n",
    "(\n",
    "    new_silver_records\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(silver_table)\n",
    ")\n",
    "\n",
    "print(f\"‚úì Propagowano {new_silver_records.count()} rekord√≥w do Silver\")\n",
    "print(f\"Silver total: {spark.table(silver_table).count()} records\")\n",
    "\n",
    "# Gold: re-aggregate (lub incremental z MERGE)\n",
    "# Dla uproszczenia: pe≈Çna re-agregacja po order_date + order_status\n",
    "updated_daily_summary = (\n",
    "    spark.table(silver_table)\n",
    "    .groupBy(\"order_date\", \"order_status\")\n",
    "    .agg(\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "        F.min(\"total_amount\").alias(\"min_order_value\"),\n",
    "        F.max(\"total_amount\").alias(\"max_order_value\"),\n",
    "        F.countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    "    )\n",
    "    .withColumn(\"_gold_created_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_gold_aggregation_level\", F.lit(\"DAILY\"))\n",
    ")\n",
    "\n",
    "(\n",
    "    updated_daily_summary\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(gold_table)\n",
    ")\n",
    "\n",
    "print(f\"‚úì Zaktualizowano Gold layer\")\n",
    "print(\"\\n=== Updated Gold Summary ===\")\n",
    "display(spark.table(gold_table).orderBy(\"order_date\", \"order_status\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24775669-45c9-488f-b469-a191bc8dc0ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Incremental ELT pattern:\n",
    "1. **Bronze**: Append nowych danych z wersjonowaniem (bronze_version)\n",
    "2. **Silver**: Proces tylko nowe Bronze records (watermark lub version)\n",
    "3. **Gold**: Re-aggregate lub MERGE dla affected partitions\n",
    "\n",
    "W produkcji: u≈ºywamy Delta Live Tables lub Structured Streaming dla automatic incrementality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Przyk≈Çad 2.2: Gold Incremental Refresh\n",
    "\n",
    "**Cel:** Demonstracja incremental refresh dla Gold layer (zamiast full overwrite)\n",
    "\n",
    "**Scenariusz:** Nowe dane w Silver tylko dla okre≈õlonych dat - aktualizujemy tylko affected partitions w Gold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 2.2 - Gold Incremental Refresh (MERGE INTO)\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GOLD INCREMENTAL REFRESH - MERGE INTO PATTERN\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Symulacja: nowe dane w Silver tylko dla 2025-01-20 i 2025-01-21\n",
    "# (te daty zosta≈Çy dodane w poprzednim przyk≈Çadzie incremental ELT)\n",
    "\n",
    "# Identyfikuj affected dates (dni, kt√≥re majƒÖ nowe/zmienione dane w Silver)\n",
    "affected_dates = [\"2025-01-20\", \"2025-01-21\"]\n",
    "\n",
    "print(f\"\\n[AFFECTED DATES]\")\n",
    "print(f\"Updating Gold for dates: {affected_dates}\")\n",
    "\n",
    "# Re-aggregate tylko dla affected dates\n",
    "incremental_gold_updates = (\n",
    "    spark.table(silver_table)\n",
    "    .filter(F.col(\"order_date\").isin(affected_dates))\n",
    "    .groupBy(\"order_date\", \"order_status\")\n",
    "    .agg(\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "        F.min(\"total_amount\").alias(\"min_order_value\"),\n",
    "        F.max(\"total_amount\").alias(\"max_order_value\"),\n",
    "        F.countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    "    )\n",
    "    .withColumn(\"_gold_created_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_gold_aggregation_level\", F.lit(\"DAILY\"))\n",
    "    .withColumn(\"_gold_source_silver_table\", F.lit(silver_table))\n",
    ")\n",
    "\n",
    "print(f\"\\n[INCREMENTAL UPDATES]\")\n",
    "print(f\"Rows to update/insert: {incremental_gold_updates.count()}\")\n",
    "display(incremental_gold_updates)\n",
    "\n",
    "# MERGE INTO Gold (update je≈õli istnieje, insert je≈õli nowe)\n",
    "incremental_gold_updates.createOrReplaceTempView(\"gold_updates\")\n",
    "\n",
    "merge_sql = f\"\"\"\n",
    "MERGE INTO {gold_table} AS target\n",
    "USING gold_updates AS source\n",
    "ON target.order_date = source.order_date \n",
    "   AND target.order_status = source.order_status\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET\n",
    "    target.total_orders = source.total_orders,\n",
    "    target.total_revenue = source.total_revenue,\n",
    "    target.avg_order_value = source.avg_order_value,\n",
    "    target.min_order_value = source.min_order_value,\n",
    "    target.max_order_value = source.max_order_value,\n",
    "    target.unique_customers = source.unique_customers,\n",
    "    target._gold_created_timestamp = source._gold_created_timestamp\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT *\n",
    "\"\"\"\n",
    "\n",
    "result = spark.sql(merge_sql)\n",
    "\n",
    "print(f\"\\n[MERGE RESULTS]\")\n",
    "print(f\"‚úÖ Gold table incrementally refreshed for {len(affected_dates)} dates\")\n",
    "print(f\"Total Gold records now: {spark.table(gold_table).count()}\")\n",
    "\n",
    "# Verification: Check updated timestamps dla affected dates\n",
    "updated_records = (\n",
    "    spark.table(gold_table)\n",
    "    .filter(F.col(\"order_date\").isin(affected_dates))\n",
    "    .select(\"order_date\", \"order_status\", \"total_orders\", \"_gold_created_timestamp\")\n",
    ")\n",
    "\n",
    "print(f\"\\n[VERIFICATION - Updated Records]\")\n",
    "display(updated_records)\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Gold Incremental Refresh pattern:\n",
    "- **MERGE INTO**: Update dla existing dates, INSERT dla nowych\n",
    "- **Performance**: Tylko affected partitions sƒÖ przepisywane (partition pruning)\n",
    "- **Idempotency**: Wielokrotne uruchomienie daje ten sam wynik\n",
    "- **Production**: U≈ºywaj MERGE dla Gold daily refreshes zamiast full overwrite\n",
    "\n",
    "**Alternatywa:** Partition overwrite\n",
    "```python\n",
    "# Zamiast MERGE, mo≈ºesz u≈ºyƒá partition overwrite\n",
    "(\n",
    "    incremental_gold_updates\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"replaceWhere\", f\"order_date IN ({affected_dates_str})\")\n",
    "    .saveAsTable(gold_table)\n",
    ")\n",
    "```\n",
    "\n",
    "**Trade-off:**\n",
    "- MERGE: Precyzyjny (row-level), wolniejszy dla du≈ºych partitions\n",
    "- Partition Overwrite: Szybszy, ale przepisuje ca≈ÇƒÖ partycjƒô"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "412caac0-e7bf-4337-8060-dbaad7a6810f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 3: Partitioning Strategy\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Partycjonowanie to kluczowa decyzja architektoniczna w Medallion. Z≈Çe partycjonowanie prowadzi do small files problem lub nieefektywnych queries.\n",
    "\n",
    "---\n",
    "\n",
    "### Kiedy partycjonowaƒá?\n",
    "\n",
    "**Zasada g≈Ç√≥wna:** Partycjonuj **TYLKO** je≈õli:\n",
    "1. Tabela > 1 TB\n",
    "2. Partition size > 1 GB\n",
    "3. 80% queries filtruje po partitioning column\n",
    "\n",
    "**Przyk≈Çad:**\n",
    "\n",
    "```python\n",
    "# ‚úÖ DOBRZE: Du≈ºa tabela (10 TB), 365 partycji = ~27 GB/partition\n",
    "df.write.partitionBy(\"order_date\").saveAsTable(large_table)\n",
    "\n",
    "# ‚ùå ≈πLE: Ma≈Ça tabela (10 GB), 365 partycji = ~27 MB/partition (small files!)\n",
    "df.write.partitionBy(\"order_date\").saveAsTable(small_table)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Partitioning per Layer\n",
    "\n",
    "```\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  BRONZE LAYER                                                ‚îÇ\n",
    "‚îÇ  Recommendation: NO PARTITIONING (lub minimal)               ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Dlaczego?                                                   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Append-only bulk operations (nie ma filtr√≥w)             ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Small files problem (ka≈ºdy ingest = nowa partycja)       ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Bulk reads dla Silver processing                          ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ  WyjƒÖtek: Je≈õli > 10 TB, partition po ingest_date           ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  SILVER LAYER                                                ‚îÇ\n",
    "‚îÇ  Recommendation: Partition po DATE lub REGION               ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Dlaczego?                                                   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Incremental MERGE (tylko affected partitions)            ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Time-based queries (last 7 days, last month)             ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Partition pruning dla Gold aggregations                   ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ  Best Practice: Partition po order_date (daily granularity) ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "‚îÇ  GOLD LAYER                                                  ‚îÇ\n",
    "‚îÇ  Recommendation: Partition po Business Dimensions           ‚îÇ\n",
    "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
    "‚îÇ  Dlaczego?                                                   ‚îÇ\n",
    "‚îÇ  ‚Ä¢ BI queries filtrujƒÖ po date, region, category            ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Partition pruning = fast dashboard loads                 ‚îÇ\n",
    "‚îÇ  ‚Ä¢ Incremental refresh (tylko affected partitions)          ‚îÇ\n",
    "‚îÇ                                                              ‚îÇ\n",
    "‚îÇ  Best Practice: Partition po year + month (lub date)        ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Partition Size Guidelines\n",
    "\n",
    "**Cel:** Partition size 1-10 GB (sweet spot: 2-4 GB)\n",
    "\n",
    "| Partition Size | Status | Action |\n",
    "|----------------|--------|--------|\n",
    "| < 100 MB | ‚ùå Too small | Increase partitioning granularity |\n",
    "| 100 MB - 1 GB | ‚ö†Ô∏è Small | Consider coarser partitioning |\n",
    "| 1 GB - 10 GB | ‚úÖ Optimal | Good |\n",
    "| > 10 GB | ‚ö†Ô∏è Large | Consider finer partitioning |\n",
    "\n",
    "**Przyk≈Çad kalkulacji:**\n",
    "\n",
    "```\n",
    "Tabela: 10 TB\n",
    "Partitioning: order_date (daily)\n",
    "Dane za: 2 lata = 730 dni\n",
    "\n",
    "Partition size = 10 TB / 730 = ~13.7 GB/partition ‚ö†Ô∏è (trochƒô za du≈ºy)\n",
    "\n",
    "Solution: Partition po order_date + region (5 regions)\n",
    "Partition size = 10 TB / (730 * 5) = ~2.7 GB/partition ‚úÖ\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Partitioning Patterns\n",
    "\n",
    "**Pattern 1: Date-based (najczƒôstszy)**\n",
    "\n",
    "```python\n",
    "# Daily partitions\n",
    "df.write.partitionBy(\"order_date\").saveAsTable(table_name)\n",
    "\n",
    "# Monthly partitions (dla d≈Çugich historii)\n",
    "df = df.withColumn(\"year_month\", F.date_format(\"order_date\", \"yyyy-MM\"))\n",
    "df.write.partitionBy(\"year_month\").saveAsTable(table_name)\n",
    "\n",
    "# Year + Month (hierarchical)\n",
    "df = df.withColumn(\"year\", F.year(\"order_date\"))\n",
    "df = df.withColumn(\"month\", F.month(\"order_date\"))\n",
    "df.write.partitionBy(\"year\", \"month\").saveAsTable(table_name)\n",
    "```\n",
    "\n",
    "**Pattern 2: Region-based**\n",
    "\n",
    "```python\n",
    "# Geographic partitioning\n",
    "df.write.partitionBy(\"country\").saveAsTable(table_name)\n",
    "\n",
    "# Hierarchical: region ‚Üí country\n",
    "df.write.partitionBy(\"region\", \"country\").saveAsTable(table_name)\n",
    "```\n",
    "\n",
    "**Pattern 3: Category-based**\n",
    "\n",
    "```python\n",
    "# Product category\n",
    "df.write.partitionBy(\"product_category\").saveAsTable(table_name)\n",
    "\n",
    "# Multi-dimensional: date + category\n",
    "df.write.partitionBy(\"order_date\", \"product_category\").saveAsTable(table_name)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Partition Pruning\n",
    "\n",
    "**Partition pruning** = Spark czyta tylko partycje spe≈ÇniajƒÖce WHERE predicate.\n",
    "\n",
    "```python\n",
    "# ‚úÖ DOBRZE: Partition pruning enabled\n",
    "spark.table(partitioned_table).filter(\"order_date = '2025-01-20'\")\n",
    "# Spark czyta TYLKO partition order_date=2025-01-20 (fast!)\n",
    "\n",
    "# ‚ùå ≈πLE: No partition pruning (full table scan)\n",
    "spark.table(partitioned_table).filter(F.dayofweek(\"order_date\") == 1)\n",
    "# Spark musi czytaƒá WSZYSTKIE partycje (slow!)\n",
    "```\n",
    "\n",
    "**Best Practice:** Query patterns powinny filtrowaƒá po partitioning column.\n",
    "\n",
    "---\n",
    "\n",
    "### Small Files Problem\n",
    "\n",
    "**Problem:** Setki ma≈Çych plik√≥w w partycji (< 100 MB ka≈ºdy)\n",
    "\n",
    "**Przyczyny:**\n",
    "- Czƒôste ma≈Çe zapisy (micro-batches)\n",
    "- Over-partitioning (partition size < 1 GB)\n",
    "- Brak compaction\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "\n",
    "```python\n",
    "# 1. Auto Optimize (Databricks)\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {table_name} \n",
    "    SET TBLPROPERTIES (\n",
    "        'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "        'delta.autoOptimize.autoCompact' = 'true'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# 2. Manual OPTIMIZE\n",
    "spark.sql(f\"OPTIMIZE {table_name}\")\n",
    "\n",
    "# 3. ZORDER BY (multi-dimensional queries)\n",
    "spark.sql(f\"OPTIMIZE {table_name} ZORDER BY (country, product_category)\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Partitioning Anti-Patterns\n",
    "\n",
    "**‚ùå Over-partitioning**\n",
    "\n",
    "```python\n",
    "# BAD: Partition size < 100 MB\n",
    "df.write.partitionBy(\"order_date\", \"order_hour\", \"customer_id\").saveAsTable(table_name)\n",
    "# Result: TysiƒÖce ma≈Çych partycji\n",
    "```\n",
    "\n",
    "**‚ùå Partitioning po high-cardinality column**\n",
    "\n",
    "```python\n",
    "# BAD: 1M unique customer_ids = 1M partitions!\n",
    "df.write.partitionBy(\"customer_id\").saveAsTable(table_name)\n",
    "```\n",
    "\n",
    "**‚ùå Partitioning bez query pattern analysis**\n",
    "\n",
    "```python\n",
    "# BAD: Partitioning po kolumnie nigdy nie u≈ºywanej w WHERE\n",
    "df.write.partitionBy(\"internal_id\").saveAsTable(table_name)\n",
    "# Queries: SELECT * FROM table WHERE order_date = '...'  (no pruning!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Zasady partycjonowania\n",
    "\n",
    "- **Bronze**: Rzadko partycjonujemy (append-only, bulk operations)\n",
    "- **Silver**: Partycjonowanie po dacie lub region dla incremental MERGE\n",
    "- **Gold**: Partycjonowanie wg wymiar√≥w zapyta≈Ñ (date, region, product_category)\n",
    "- **Regu≈Ça**: Partycjonuj tylko je≈õli tabela > 1 TB i partition size > 1 GB\n",
    "- **Query patterns**: 80% queries powinno filtrowaƒá po partitioning column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed299e03-b38d-4e07-bfea-9693a406dd6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 3.1: Partycjonowanie Silver layer po dacie\n",
    "\n",
    "**Cel:** Demonstracja partitioned table dla efektywnych incremental updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af3de11d-a1d2-4f0b-b425-f1a50ec9c063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 3.1 - Partitioned Silver table\n",
    "\n",
    "# Utw√≥rz Silver table z partycjonowaniem po order_date\n",
    "silver_partitioned_table = f\"{SILVER_SCHEMA}.orders_silver_partitioned\"\n",
    "\n",
    "(\n",
    "    spark.table(silver_table)\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"order_date\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(silver_partitioned_table)\n",
    ")\n",
    "\n",
    "print(f\"‚úì Utworzono partycjonowanƒÖ tabelƒô: {silver_partitioned_table}\")\n",
    "\n",
    "# Sprawd≈∫ partycje\n",
    "partitions = spark.sql(f\"SHOW PARTITIONS {silver_partitioned_table}\")\n",
    "print(\"\\n=== Partycje ===\")\n",
    "display(partitions)\n",
    "\n",
    "# DESCRIBE DETAIL - sprawd≈∫ partitioning columns\n",
    "detail = spark.sql(f\"DESCRIBE DETAIL {silver_partitioned_table}\")\n",
    "print(\"\\n=== Detail (partitionColumns) ===\")\n",
    "display(detail.select(\"name\", \"partitionColumns\", \"numFiles\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08f316d1-6f5f-44d5-b872-b68d6c12dde9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "Partycjonowanie:\n",
    "- **Partition pruning**: Spark czyta tylko partycje spe≈ÇniajƒÖce predicate (WHERE order_date = '2025-01-20')\n",
    "- **Incremental MERGE**: UPDATE/DELETE tylko affected partitions\n",
    "- **Trade-off**: Zbyt du≈ºo partycji (< 1 GB) powoduje small files problem\n",
    "\n",
    "Best practice: Partycjonuj po kolumnie u≈ºywanej w 80% zapyta≈Ñ (czƒôsto: date, region)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8bcafb4-68ad-4411-96ac-9bd78d08760d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "**Projektowanie warstw:**\n",
    "- **Bronze**: Immutable, append-only. D≈Çuga retention (lata). Audit metadata obowiƒÖzkowe.\n",
    "- **Silver**: Idempotentne transformacje. Mo≈ºliwo≈õƒá reprocessingu z Bronze. MERGE dla SCD.\n",
    "- **Gold**: Denormalized, aggregated. Partition wg business dimensions. Kr√≥tka retention (miesiƒôcy).\n",
    "\n",
    "**Data Quality:**\n",
    "- **Bronze ‚Üí Silver**: Walidacja schemat√≥w, business rules, deduplikacja\n",
    "- **Silver ‚Üí Gold**: Sprawdzenie completeness (czy wszystkie Bronze records dotar≈Çy?)\n",
    "- **Gold**: Revenue reconciliation, orphan checks, data freshness monitoring\n",
    "- **Expectations**: U≈ºywaj Delta Live Tables expectations (warn/drop/fail)\n",
    "\n",
    "**Performance:**\n",
    "- **Partycjonowanie**: Tylko dla du≈ºych tabel (>1TB), partition size > 1GB\n",
    "- **ZORDER**: Silver/Gold - po kluczu biznesowym lub czƒôsto filtrowanych kolumnach\n",
    "- **Auto Optimize**: W≈ÇƒÖcz dla Silver/Gold (czƒôste ma≈Çe zapisy)\n",
    "- **Gold Incremental**: U≈ºywaj MERGE lub partition overwrite zamiast full table overwrite\n",
    "\n",
    "**Governance:**\n",
    "- **Unity Catalog**: Bronze/Silver/Gold jako osobne schemas z r√≥≈ºnymi permissions\n",
    "- **Lineage**: U≈ºywaj Delta Lake lineage do ≈õledzenia Bronze ‚Üí Silver ‚Üí Gold\n",
    "- **Retention**: Bronze (3-7 lat), Silver (1-2 lata), Gold (6-12 miesiƒôcy)\n",
    "- **Access Control**: Bronze (data engineers only), Silver (analytics engineers), Gold (BI/analysts read-only)\n",
    "\n",
    "---\n",
    "\n",
    "## Gold Layer Best Practices (Szczeg√≥≈Çowo)\n",
    "\n",
    "**1. Incremental Refresh Strategy**\n",
    "\n",
    "```python\n",
    "# ‚úÖ DOBRZE: Incremental refresh tylko dla affected dates\n",
    "affected_dates = get_new_dates_from_silver()\n",
    "incremental_gold = aggregate_silver(dates=affected_dates)\n",
    "merge_into_gold(incremental_gold)\n",
    "\n",
    "# ‚ùå ≈πLE: Full table overwrite codziennie (wolne!)\n",
    "full_gold = aggregate_all_silver()  # Skanuje ca≈ÇƒÖ Silver\n",
    "overwrite_gold(full_gold)\n",
    "```\n",
    "\n",
    "**2. Pre-join Dimensions (Denormalization)**\n",
    "\n",
    "```python\n",
    "# ‚úÖ DOBRZE: Join w Gold layer (denormalizacja)\n",
    "gold_sales = (\n",
    "    silver_orders\n",
    "    .join(silver_customers, \"customer_id\")\n",
    "    .join(silver_products, \"product_id\")\n",
    "    .select(\n",
    "        \"order_id\",\n",
    "        \"customer_name\",  # From dim_customer\n",
    "        \"product_name\",   # From dim_product\n",
    "        \"total_amount\"\n",
    "    )\n",
    ")\n",
    "# BI queries: SELECT * FROM gold_sales (szybkie, bez join√≥w)\n",
    "\n",
    "# ‚ùå ≈πLE: Pozostaw joins dla BI (wolniejsze dashboardy)\n",
    "# BI musi joinowaƒá Gold fact ‚Üí dim_customer ‚Üí dim_product ka≈ºdorazowo\n",
    "```\n",
    "\n",
    "**3. Aggregate Pre-computation**\n",
    "\n",
    "```python\n",
    "# ‚úÖ DOBRZE: Pre-compute common aggregations\n",
    "gold_daily_kpi = aggregate_by_day()\n",
    "gold_monthly_kpi = aggregate_by_month()\n",
    "gold_by_region = aggregate_by_region()\n",
    "\n",
    "# BI queries: SELECT * FROM gold_daily_kpi WHERE date = '2025-01-20' (instant!)\n",
    "\n",
    "# ‚ùå ≈πLE: Force BI to aggregate on-the-fly\n",
    "# SELECT SUM(amount) FROM silver_orders WHERE ... (slow dla large data)\n",
    "```\n",
    "\n",
    "**4. Gold Monitoring & Alerting**\n",
    "\n",
    "```python\n",
    "# ‚úÖ DOBRZE: Continuous monitoring\n",
    "def gold_quality_checks():\n",
    "    checks = {\n",
    "        \"freshness\": check_data_freshness(gold_table, sla_hours=24),\n",
    "        \"completeness\": check_date_continuity(gold_table),\n",
    "        \"reconciliation\": check_revenue_match(silver, gold),\n",
    "        \"orphans\": check_foreign_keys(gold_fact, dimensions)\n",
    "    }\n",
    "    \n",
    "    if any(not check for check in checks.values()):\n",
    "        send_alert(\"Gold Quality Failed\", checks)\n",
    "```\n",
    "\n",
    "**5. Partitioning Strategy dla Gold**\n",
    "\n",
    "```python\n",
    "# ‚úÖ DOBRZE: Partition po BI query patterns\n",
    "# Je≈õli 90% queries: WHERE order_date = '...' AND region = '...'\n",
    "gold_df.write.partitionBy(\"order_date\", \"region\").saveAsTable(gold_table)\n",
    "\n",
    "# ‚ö†Ô∏è Trade-off: Balance partition count vs partition size\n",
    "# 2 years * 365 days * 5 regions = 3,650 partitions\n",
    "# Ensure ka≈ºda partycja > 1 GB\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb006e4f-619f-46ac-b83f-b8b7530398d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Problem 1: Small files w Bronze**\n",
    "**Objawy:** Setki ma≈Çych plik√≥w Parquet po ka≈ºdym inge≈õcie\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "```python\n",
    "# W≈ÇƒÖcz Auto Optimize dla Bronze\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {bronze_table} \n",
    "    SET TBLPROPERTIES (\n",
    "        'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "        'delta.autoOptimize.autoCompact' = 'true'\n",
    "    )\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 2: Silver processing zbyt wolny**\n",
    "**Objawy:** Transformacja Bronze ‚Üí Silver trwa godziny zamiast minut\n",
    "\n",
    "**RozwiƒÖzanie:** U≈ºyj incremental processing z watermark zamiast full table scan:\n",
    "```python\n",
    "# Proces tylko rekordy nowsze ni≈º ostatni Silver timestamp\n",
    "max_silver_ts = spark.table(silver_table).agg(F.max(\"_bronze_ingest_timestamp\")).collect()[0][0]\n",
    "new_bronze = spark.table(bronze_table).filter(F.col(\"_bronze_ingest_timestamp\") > max_silver_ts)\n",
    "\n",
    "# Process only new Bronze records\n",
    "process_to_silver(new_bronze)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 3: Gold re-aggregation trwa zbyt d≈Çugo**\n",
    "**Objawy:** Pe≈Çna re-agregacja Gold trwa kilka godzin, BI dashboardy czekajƒÖ\n",
    "\n",
    "**RozwiƒÖzanie:** U≈ºyj MERGE zamiast overwrite dla incremental Gold:\n",
    "```python\n",
    "# Identyfikuj tylko affected dates\n",
    "affected_dates = new_silver.select(\"order_date\").distinct().collect()\n",
    "\n",
    "# Re-aggregate tylko affected dates\n",
    "incremental_gold = aggregate_silver(dates=affected_dates)\n",
    "\n",
    "# MERGE tylko affected partitions\n",
    "merge_into_gold(incremental_gold)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 4: Gold dashboards pokazujƒÖ stare dane**\n",
    "**Objawy:** BI reports sƒÖ op√≥≈∫nione o 2+ dni, u≈ºytkownicy narzekajƒÖ\n",
    "\n",
    "**Diagnoza:**\n",
    "```python\n",
    "# Check Gold freshness\n",
    "max_gold_date = spark.table(gold_table).agg(F.max(\"order_date\")).collect()[0][0]\n",
    "max_silver_date = spark.table(silver_table).agg(F.max(\"order_date\")).collect()[0][0]\n",
    "\n",
    "lag_days = (max_silver_date - max_gold_date).days\n",
    "print(f\"Gold is {lag_days} days behind Silver\")\n",
    "```\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "1. Check Databricks Job logs (czy Gold refresh job failed?)\n",
    "2. Implement alerting dla Gold freshness SLA\n",
    "3. Automate Gold refresh (daily cron lub event-driven)\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 5: Revenue mismatch miƒôdzy Silver i Gold**\n",
    "**Objawy:** `SUM(silver.total_amount) ‚â† SUM(gold.total_revenue)`\n",
    "\n",
    "**Diagnoza:**\n",
    "```python\n",
    "# Reconciliation check\n",
    "silver_revenue = spark.table(silver_table).agg(F.sum(\"total_amount\")).collect()[0][0]\n",
    "gold_revenue = spark.table(gold_table).agg(F.sum(\"total_revenue\")).collect()[0][0]\n",
    "variance = abs(silver_revenue - gold_revenue)\n",
    "\n",
    "print(f\"Variance: ${variance:,.2f}\")\n",
    "\n",
    "# Find problematic dates\n",
    "by_date_check = (\n",
    "    spark.table(silver_table)\n",
    "    .groupBy(\"order_date\")\n",
    "    .agg(F.sum(\"total_amount\").alias(\"silver_revenue\"))\n",
    "    .join(\n",
    "        spark.table(gold_table).groupBy(\"order_date\").agg(F.sum(\"total_revenue\").alias(\"gold_revenue\")),\n",
    "        \"order_date\",\n",
    "        \"full_outer\"\n",
    "    )\n",
    "    .withColumn(\"variance\", F.col(\"silver_revenue\") - F.col(\"gold_revenue\"))\n",
    "    .filter(F.abs(F.col(\"variance\")) > 0.01)\n",
    ")\n",
    "\n",
    "display(by_date_check)\n",
    "```\n",
    "\n",
    "**Mo≈ºliwe przyczyny:**\n",
    "- Gold aggregation bug (incorrect groupBy key)\n",
    "- Missing partitions w Gold (niekt√≥re daty nie zagregowane)\n",
    "- Double counting (duplikaty w Gold)\n",
    "- Filtry w Gold (np. `WHERE status = 'COMPLETED'` ale Silver ma wszystkie statusy)\n",
    "\n",
    "---\n",
    "\n",
    "**Problem 6: Orphan records w Gold (unmatched foreign keys)**\n",
    "**Objawy:** Gold fact table ma `customer_id` kt√≥rego nie ma w `dim_customer`\n",
    "\n",
    "**Diagnoza:**\n",
    "```python\n",
    "# Find orphan customers\n",
    "orphan_customers = (\n",
    "    gold_fact\n",
    "    .select(\"customer_id\")\n",
    "    .distinct()\n",
    "    .join(dim_customer.select(\"customer_id\"), \"customer_id\", \"left_anti\")\n",
    ")\n",
    "\n",
    "orphan_count = orphan_customers.count()\n",
    "print(f\"Orphan customers: {orphan_count}\")\n",
    "\n",
    "if orphan_count > 0:\n",
    "    display(orphan_customers.limit(10))\n",
    "```\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "1. **Left join z NULL handling**:\n",
    "```python\n",
    "gold_with_safeguard = (\n",
    "    silver_orders\n",
    "    .join(dim_customer, \"customer_id\", \"left\")  # Left join zamiast inner\n",
    "    .withColumn(\"customer_name\", F.coalesce(F.col(\"customer_name\"), F.lit(\"UNKNOWN\")))\n",
    ")\n",
    "```\n",
    "\n",
    "2. **Data quality check w Silver ‚Üí Gold**:\n",
    "```python\n",
    "# Reject orders z invalid customer_id\n",
    "valid_customer_ids = dim_customer.select(\"customer_id\").distinct()\n",
    "validated_orders = silver_orders.join(valid_customer_ids, \"customer_id\", \"inner\")\n",
    "```\n",
    "\n",
    "3. **Alert dla nowych orphans**:\n",
    "```python\n",
    "if orphan_count > threshold:\n",
    "    send_alert(f\"Orphan customers detected: {orphan_count}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11249d20-d238-4d58-a93f-8811a5089d1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "**W tym notebooku nauczyli≈õmy siƒô:**\n",
    "\n",
    "‚úÖ **Medallion Architecture:**\n",
    "- Bronze: Raw data landing zone z audit metadata (immutable, append-only)\n",
    "- Silver: Cleansed, validated, deduplicated data (business rules enforcement)\n",
    "- Gold: Business-level aggregates i KPI (BI-ready, denormalized)\n",
    "\n",
    "‚úÖ **ETL vs ELT:**\n",
    "- ELT approach: Load first (Bronze), then Transform (Silver/Gold)\n",
    "- Mo≈ºliwo≈õƒá reprocessingu bez re-ingestion\n",
    "- Incremental processing dla ka≈ºdej warstwy\n",
    "\n",
    "‚úÖ **Partitioning Strategy:**\n",
    "- Partycjonuj tylko du≈ºe tabele (>1TB)\n",
    "- Partition size > 1GB dla unikniƒôcia small files\n",
    "- Silver/Gold: partycjonowanie po dacie lub business dimensions\n",
    "\n",
    "**Kluczowe wnioski:**\n",
    "1. Medallion Architecture zapewnia separation of concerns i data quality gates\n",
    "2. Ka≈ºda warstwa ma okre≈õlone SLA, retention policy i access patterns\n",
    "3. Bronze jako immutable source of truth umo≈ºliwia reprocessing\n",
    "4. Incremental processing jest kluczowy dla performance w du≈ºej skali\n",
    "\n",
    "**Nastƒôpne kroki:**\n",
    "- **Kolejny notebook**: 03_batch_streaming_load.ipynb - COPY INTO, Auto Loader, Structured Streaming\n",
    "- **Warsztat praktyczny**: 01_delta_medallion_workshop.ipynb\n",
    "- **Delta Live Tables**: Automatyczna implementacja Medallion z deklaratywnym API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fbc8fff-4a63-438d-91db-6270c0dbe262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Opcjonalnie: usu≈Ñ utworzone tabele Demo po zako≈Ñczeniu ƒáwicze≈Ñ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e282e534-095c-48b6-ba80-eefa389da3f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Opcjonalne czyszczenie zasob√≥w testowych\n",
    "# UWAGA: Uruchom tylko je≈õli chcesz usunƒÖƒá wszystkie utworzone dane\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {bronze_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {silver_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {silver_partitioned_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {gold_table}\")\n",
    "\n",
    "# spark.catalog.clearCache()\n",
    "# print(\"Zasoby zosta≈Çy wyczyszczone\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_medallion_architecture",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
