{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "577fb86f-84ed-49a2-b1a1-aa785feceefc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Medallion Architecture - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Zrozumienie koncepcji architektury medalionowej (Bronze/Silver/Gold) i zasad projektowania data lakehouse.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Bronze / Silver / Gold - logika warstw\n",
    "- ETL vs ELT approach\n",
    "- Zasady projektowania pipeline'ów\n",
    "- Partitioning strategy\n",
    "- Audyt i lineage - metadane w każdym kroku\n",
    "- Data quality w kontekście warstw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25cba1ec-bc2f-4a95-8b19-097852590dfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 2 - Lakehouse & Delta Lake\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1aefc3a3-9caf-4ec6-a347-da3087d56868",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie architektury medalionowej jako design pattern dla data lakehouse.\n",
    "\n",
    "**Podstawowe pojęcia:**\n",
    "- **Medallion Architecture**: Wielowarstwowa architektura danych (Bronze → Silver → Gold)\n",
    "- **Bronze Layer**: Raw data landing zone - dane bez transformacji, tylko audit metadata\n",
    "- **Silver Layer**: Cleansed and conformed data - deduplikacja, walidacja, standardizacja\n",
    "- **Gold Layer**: Business-level aggregates - modele KPI, reporty, ML features\n",
    "\n",
    "**Dlaczego to ważne?**\n",
    "Medallion architecture zapewnia separation of concerns, jasne SLA per warstwa, incremental processing, oraz data quality gates. Umożliwia różne tempo procesowania (Bronze: real-time, Silver: hourly, Gold: daily) i różne retention policies per warstwa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a3390ab-18af-48bd-a851-c545b46dff6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Izolacja per użytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ac68da3-6d62-499e-8d3c-ae1c283c823e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb179fb0-e5d9-4e48-9982-2a9c96ded80c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30106da3-ad9d-4d3e-9a61-34dbf776e11b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Wyświetl kontekst użytkownika\n",
    "print(\"=== Kontekst użytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"Użytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog jako domyślny\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "\n",
    "# Ścieżki do danych źródłowych\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "\n",
    "print(f\"\\n=== Ścieżki do danych ===\")\n",
    "print(f\"Orders: {ORDERS_JSON}\")\n",
    "print(f\"Customers: {CUSTOMERS_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abfc7abf-3ec7-454a-92f9-24890b34f7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 1: Koncepcja Medallion Architecture\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Medallion Architecture dzieli data lakehouse na trzy warstwy o rosnącej jakości danych. Każda warstwa ma określone SLA, retention policy i data quality requirements.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **Bronze (Raw)**: Append-only, immutable landing zone. Dane \"as-is\" z systemu źródłowego + audit metadata (ingestion timestamp, source file, version)\n",
    "- **Silver (Cleansed)**: Validated, deduplicated, standardized. Business rules enforcement, schema evolution, data quality checks\n",
    "- **Gold (Curated)**: Aggregated, denormalized, business-level. KPI models, reporting tables, ML features, star schema\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Separacja odpowiedzialności: data engineers (Bronze/Silver), analytics engineers (Gold)\n",
    "- Incremental processing: tylko nowe/zmienione dane propagowane przez warstwy\n",
    "- Debug-friendly: możliwość reprocessingu Silver/Gold z Bronze bez re-ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d3d610af-9496-4c55-ad10-005930465309",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.1: Bronze Layer - Raw Data Landing\n",
    "\n",
    "**Cel:** Utworzenie Bronze layer z audit metadata\n",
    "\n",
    "**Podejście:**\n",
    "1. Wczytanie surowych danych z JSON\n",
    "2. Dodanie audit columns: ingest_timestamp, source_file, ingested_by\n",
    "3. Zapis do Bronze schema bez transformacji biznesowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9db9ee0f-d63e-4cc5-bd1b-5ca221129d09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 1.1 - Bronze Layer\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Wczytaj surowe dane orders\n",
    "orders_raw = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "print(\"=== Surowe dane (schema) ===\")\n",
    "orders_raw.printSchema()\n",
    "\n",
    "# Dodaj audit metadata (Bronze best practice)\n",
    "orders_bronze = (\n",
    "    orders_raw\n",
    "    .withColumn(\"ingest_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"source_file\", F.input_file_name())\n",
    "    .withColumn(\"ingested_by\", F.lit(raw_user))\n",
    "    .withColumn(\"bronze_version\", F.lit(1))\n",
    ")\n",
    "\n",
    "print(\"\\n=== Bronze layer z audit metadata ===\")\n",
    "display(orders_bronze.limit(3))\n",
    "\n",
    "# Zapisz do Bronze schema\n",
    "bronze_table = f\"{BRONZE_SCHEMA}.orders_bronze\"\n",
    "\n",
    "(\n",
    "    orders_bronze\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(bronze_table)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Utworzono Bronze table: {bronze_table}\")\n",
    "print(f\"Liczba rekordów: {spark.table(bronze_table).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1489eec-edc1-4cd0-88a2-7d1baff81de4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Bronze layer:\n",
    "- **Immutable**: Dane zapisane \"as-is\" bez modyfikacji wartości biznesowych\n",
    "- **Audit trail**: Każdy rekord ma metadata: kiedy, skąd, przez kogo został załadowany\n",
    "- **Append-only**: Idealnie nadaje się do incremental loads z COPY INTO lub Auto Loader\n",
    "- **Retention**: Często długa (lata) jako źródło prawdy do reprocessingu\n",
    "\n",
    "**Struktura danych źródłowych (orders_batch.json)**:\n",
    "- `order_id`: String (np. \"ORD00000001\")\n",
    "- `customer_id`: String (np. \"CUST005909\")\n",
    "- `order_datetime`: Timestamp (np. \"2024-12-31T23:56:00\")\n",
    "- `total_amount`: Double - całkowita wartość zamówienia\n",
    "- `payment_method`: String - metoda płatności"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2baf140-5390-469b-9604-301840271fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.2: Silver Layer - Cleansed & Validated\n",
    "\n",
    "**Cel:** Transformacja Bronze → Silver z data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "35a4cccb-7941-45d4-8ed7-fd78138541cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 1.2 - Silver Layer\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {SILVER_SCHEMA}\")\n",
    "\n",
    "# Wczytaj dane z Bronze\n",
    "orders_bronze_df = spark.table(bronze_table)\n",
    "\n",
    "# Silver transformations: cleaning, validation, standardization\n",
    "orders_silver = (\n",
    "    orders_bronze_df\n",
    "    # Deduplikacja po kluczu biznesowym\n",
    "    .dropDuplicates([\"order_id\"])\n",
    "    \n",
    "    # Walidacja: usuń rekordy z NULL w kluczowych kolumnach\n",
    "    .filter(F.col(\"order_id\").isNotNull())\n",
    "    .filter(F.col(\"customer_id\").isNotNull())\n",
    "    \n",
    "    # Walidacja biznesowa: total_amount > 0\n",
    "    .filter(F.col(\"total_amount\") > 0)\n",
    "    \n",
    "    # Standaryzacja dat - użyj order_datetime zamiast order_date\n",
    "    .withColumn(\"order_date\", F.to_date(F.col(\"order_datetime\")))\n",
    "    \n",
    "    # Standaryzacja tekstów\n",
    "    .withColumn(\"payment_method\", F.upper(F.trim(F.col(\"payment_method\"))))\n",
    "    \n",
    "    # Dodaj Silver metadata\n",
    "    .withColumn(\"silver_processed_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"data_quality_flag\", F.lit(\"VALID\"))\n",
    ")\n",
    "\n",
    "print(\"=== Silver layer - cleansed data ===\")\n",
    "display(orders_silver.limit(5))\n",
    "\n",
    "# Zapisz do Silver schema\n",
    "silver_table = f\"{SILVER_SCHEMA}.orders_silver\"\n",
    "\n",
    "(\n",
    "    orders_silver\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(silver_table)\n",
    ")\n",
    "\n",
    "bronze_count = orders_bronze_df.count()\n",
    "silver_count = spark.table(silver_table).count()\n",
    "\n",
    "print(f\"\\n✓ Utworzono Silver table: {silver_table}\")\n",
    "print(f\"Bronze records: {bronze_count}\")\n",
    "print(f\"Silver records: {silver_count}\")\n",
    "print(f\"Filtered out: {bronze_count - silver_count} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76fb36bc-5d0a-4db0-b028-b3cb02034c27",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Silver layer:\n",
    "- **Data Quality**: Walidacja biznesowa (total_amount > 0), walidacja schematów (NOT NULL)\n",
    "- **Deduplikacja**: Usunięcie duplikatów po kluczu biznesowym\n",
    "- **Standardizacja**: Ujednolicenie formatów (daty, teksty, case sensitivity)\n",
    "- **Incremental friendly**: Można używać MERGE dla slowly changing dimensions\n",
    "\n",
    "**Nota**: W tym przykładzie transformujemy `order_datetime` (timestamp) na `order_date` (date) dla łatwiejszego partycjonowania i agregacji w Gold layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c576e84-79c6-4c93-bf0d-88c90f91605c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.3: Gold Layer - Business Aggregates\n",
    "\n",
    "**Cel:** Utworzenie Gold layer z KPI dla analityki i raportowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "edabca41-15fd-4541-a942-400411614814",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 1.3 - Gold Layer: Daily Order Summary\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {GOLD_SCHEMA}\")\n",
    "\n",
    "# Wczytaj dane z Silver\n",
    "orders_silver_df = spark.table(silver_table)\n",
    "\n",
    "# Gold aggregation: Daily order summary\n",
    "daily_summary = (\n",
    "    orders_silver_df\n",
    "    .groupBy(\"order_date\", \"order_status\")\n",
    "    .agg(\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "        F.min(\"total_amount\").alias(\"min_order_value\"),\n",
    "        F.max(\"total_amount\").alias(\"max_order_value\"),\n",
    "        F.countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    "    )\n",
    "    .withColumn(\"gold_created_timestamp\", F.current_timestamp())\n",
    "    .orderBy(\"order_date\", \"order_status\")\n",
    ")\n",
    "\n",
    "print(\"=== Gold layer - Daily Order Summary ===\")\n",
    "display(daily_summary)\n",
    "\n",
    "# Zapisz do Gold schema\n",
    "gold_table = f\"{GOLD_SCHEMA}.daily_order_summary\"\n",
    "\n",
    "(\n",
    "    daily_summary\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(gold_table)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Utworzono Gold table: {gold_table}\")\n",
    "print(f\"Liczba agregowanych dni: {spark.table(gold_table).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d958525c-9c99-4f91-bf6d-ff0c23db890e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Gold layer:\n",
    "- **Business-level**: KPI i metryki zgodne z business definitions\n",
    "- **Denormalized**: Często szeroka tabela z joinami już wykonanymi (performance dla BI)\n",
    "- **Aggregated**: Dane pre-aggregowane (daily, weekly, monthly) dla szybkich dashboardów\n",
    "- **BI-ready**: Bezpośrednie źródło dla Power BI, Tableau, Looker\n",
    "\n",
    "**Nota**: W tym przykładzie agregujemy zamówienia po dacie i metodzie płatności (payment_method). W rzeczywistych scenariuszach możesz agregować po innych wymiarach biznesowych jak region, kategoria produktu, czy segment klienta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5cdff76a-5207-406e-8143-390d787b3c20",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 2: ETL vs ELT Approach\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Medallion architecture wspiera ELT (Extract-Load-Transform) approach, w przeciwieństwie do tradycyjnego ETL. Dane są najpierw ładowane do Bronze (Load), a potem transformowane w Silver/Gold (Transform).\n",
    "\n",
    "**Kluczowe różnice:**\n",
    "- **ETL**: Transform before load - dane są czyszczone poza data warehouse\n",
    "- **ELT**: Load then transform - surowe dane w Bronze, transformacje w lakehouse\n",
    "- **Zalety ELT**: Możliwość reprocessingu, data lineage, audit trail, schema evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5467410-81ef-454e-85dd-7a4a0124f6c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.1: ELT Pipeline - Incremental Processing\n",
    "\n",
    "**Cel:** Demonstracja incremental ELT: nowe dane w Bronze → automatyczna propagacja do Silver/Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd361f35-381a-4bc1-a9f2-afbbe5145050",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 2.1 - Incremental ELT\n",
    "\n",
    "# Symulacja: nowe dane przychodzą do Bronze\n",
    "new_orders_data = [\n",
    "    (\"ORD99990001\", \"CUST009901\", \"PROD000001\", \"STORE001\", \"2025-01-20T10:00:00\", 2, 175.00, 0, 350.00, \"Credit Card\"),\n",
    "    (\"ORD99990002\", \"CUST009902\", \"PROD000002\", \"STORE002\", \"2025-01-20T11:00:00\", 1, 120.50, 0, 120.50, \"Debit Card\"),\n",
    "    (\"ORD99990003\", \"CUST009903\", \"PROD000003\", \"STORE003\", \"2025-01-21T09:00:00\", 1, 499.99, 0, 499.99, \"PayPal\")\n",
    "]\n",
    "\n",
    "new_orders_df = spark.createDataFrame(\n",
    "    new_orders_data,\n",
    "    [\"order_id\", \"customer_id\", \"product_id\", \"store_id\", \"order_datetime\", \"quantity\", \"unit_price\", \"discount_percent\", \"total_amount\", \"payment_method\"]\n",
    ")\n",
    "\n",
    "# Dodaj audit metadata (Bronze standard)\n",
    "new_orders_bronze = (\n",
    "    new_orders_df\n",
    "    .withColumn(\"ingest_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"source_file\", F.lit(\"incremental_batch_2\"))\n",
    "    .withColumn(\"ingested_by\", F.lit(raw_user))\n",
    "    .withColumn(\"bronze_version\", F.lit(2))\n",
    ")\n",
    "\n",
    "# Append do Bronze (ELT: Load first)\n",
    "(\n",
    "    new_orders_bronze\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(bronze_table)\n",
    ")\n",
    "\n",
    "print(f\"✓ Dodano {new_orders_df.count()} nowych rekordów do Bronze\")\n",
    "print(f\"Bronze total: {spark.table(bronze_table).count()} records\")\n",
    "\n",
    "# Incremental Silver processing: tylko nowe Bronze records (version 2)\n",
    "new_bronze_records = (\n",
    "    spark.table(bronze_table)\n",
    "    .filter(F.col(\"bronze_version\") == 2)\n",
    ")\n",
    "\n",
    "# Apply Silver transformations\n",
    "new_silver_records = (\n",
    "    new_bronze_records\n",
    "    .dropDuplicates([\"order_id\"])\n",
    "    .filter(F.col(\"order_id\").isNotNull())\n",
    "    .filter(F.col(\"total_amount\") > 0)\n",
    "    .withColumn(\"order_date\", F.to_date(F.col(\"order_datetime\")))\n",
    "    .withColumn(\"payment_method\", F.upper(F.trim(F.col(\"payment_method\"))))\n",
    "    .withColumn(\"silver_processed_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"data_quality_flag\", F.lit(\"VALID\"))\n",
    ")\n",
    "\n",
    "# Append do Silver\n",
    "(\n",
    "    new_silver_records\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(silver_table)\n",
    ")\n",
    "\n",
    "print(f\"✓ Propagowano {new_silver_records.count()} rekordów do Silver\")\n",
    "print(f\"Silver total: {spark.table(silver_table).count()} records\")\n",
    "\n",
    "# Gold: re-aggregate (lub incremental z MERGE)\n",
    "# Dla uproszczenia: pełna re-agregacja\n",
    "updated_daily_summary = (\n",
    "    spark.table(silver_table)\n",
    "    .groupBy(\"order_date\", \"payment_method\")\n",
    "    .agg(\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_order_value\"),\n",
    "        F.min(\"total_amount\").alias(\"min_order_value\"),\n",
    "        F.max(\"total_amount\").alias(\"max_order_value\"),\n",
    "        F.countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    "    )\n",
    "    .withColumn(\"gold_created_timestamp\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "(\n",
    "    updated_daily_summary\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(gold_table)\n",
    ")\n",
    "\n",
    "print(f\"✓ Zaktualizowano Gold layer\")\n",
    "print(\"\\n=== Updated Gold Summary ===\")\n",
    "display(spark.table(gold_table).orderBy(\"order_date\", \"payment_method\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24775669-45c9-488f-b469-a191bc8dc0ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Incremental ELT pattern:\n",
    "1. **Bronze**: Append nowych danych z wersjonowaniem (bronze_version)\n",
    "2. **Silver**: Proces tylko nowe Bronze records (watermark lub version)\n",
    "3. **Gold**: Re-aggregate lub MERGE dla affected partitions\n",
    "\n",
    "W produkcji: używamy Delta Live Tables lub Structured Streaming dla automatic incrementality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "412caac0-e7bf-4337-8060-dbaad7a6810f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 3: Partitioning Strategy\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Partycjonowanie to kluczowa decyzja architektoniczna w Medallion. Złe partycjonowanie powoduje small files problem lub inefficient queries.\n",
    "\n",
    "**Zasady partycjonowania:**\n",
    "- **Bronze**: Rzadko partycjonujemy (append-only, bulk operations)\n",
    "- **Silver**: Partycjonowanie po dacie lub region dla incremental MERGE\n",
    "- **Gold**: Partycjonowanie wg wymiarów zapytań (date, region, product_category)\n",
    "- **Reguła**: Partycjonuj tylko jeśli tabela > 1 TB i partition size > 1 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed299e03-b38d-4e07-bfea-9693a406dd6b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.1: Partycjonowanie Silver layer po dacie\n",
    "\n",
    "**Cel:** Demonstracja partitioned table dla efektywnych incremental updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af3de11d-a1d2-4f0b-b425-f1a50ec9c063",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 3.1 - Partitioned Silver table\n",
    "\n",
    "# Utwórz Silver table z partycjonowaniem po order_date\n",
    "silver_partitioned_table = f\"{SILVER_SCHEMA}.orders_silver_partitioned\"\n",
    "\n",
    "(\n",
    "    spark.table(silver_table)\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"order_date\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(silver_partitioned_table)\n",
    ")\n",
    "\n",
    "print(f\"✓ Utworzono partycjonowaną tabelę: {silver_partitioned_table}\")\n",
    "\n",
    "# Sprawdź partycje\n",
    "partitions = spark.sql(f\"SHOW PARTITIONS {silver_partitioned_table}\")\n",
    "print(\"\\n=== Partycje ===\")\n",
    "display(partitions)\n",
    "\n",
    "# DESCRIBE DETAIL - sprawdź partitioning columns\n",
    "detail = spark.sql(f\"DESCRIBE DETAIL {silver_partitioned_table}\")\n",
    "print(\"\\n=== Detail (partitionColumns) ===\")\n",
    "display(detail.select(\"name\", \"partitionColumns\", \"numFiles\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08f316d1-6f5f-44d5-b872-b68d6c12dde9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Partycjonowanie:\n",
    "- **Partition pruning**: Spark czyta tylko partycje spełniające predicate (WHERE order_date = '2025-01-20')\n",
    "- **Incremental MERGE**: UPDATE/DELETE tylko affected partitions\n",
    "- **Trade-off**: Zbyt dużo partycji (< 1 GB) powoduje small files problem\n",
    "\n",
    "Best practice: Partycjonuj po kolumnie używanej w 80% zapytań (często: date, region)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8bcafb4-68ad-4411-96ac-9bd78d08760d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "**Projektowanie warstw:**\n",
    "- **Bronze**: Immutable, append-only. Długa retention (lata). Audit metadata obowiązkowe.\n",
    "- **Silver**: Idempotentne transformacje. Możliwość reprocessingu z Bronze. MERGE dla SCD.\n",
    "- **Gold**: Denormalized, aggregated. Partition wg business dimensions. Krótka retention (miesięcy).\n",
    "\n",
    "**Data Quality:**\n",
    "- **Bronze → Silver**: Walidacja schematów, business rules, deduplikacja\n",
    "- **Silver → Gold**: Sprawdzenie completeness (czy wszystkie Bronze records dotarły?)\n",
    "- **Expectations**: Używaj Delta Live Tables expectations (warn/drop/fail)\n",
    "\n",
    "**Performance:**\n",
    "- **Partycjonowanie**: Tylko dla dużych tabel (>1TB), partition size > 1GB\n",
    "- **ZORDER**: Silver/Gold - po kluczu biznesowym lub często filtrowanych kolumnach\n",
    "- **Auto Optimize**: Włącz dla Silver/Gold (częste małe zapisy)\n",
    "\n",
    "**Governance:**\n",
    "- **Unity Catalog**: Bronze/Silver/Gold jako osobne schemas z różnymi permissions\n",
    "- **Lineage**: Używaj Delta Lake lineage do śledzenia Bronze → Silver → Gold\n",
    "- **Retention**: Bronze (3-7 lat), Silver (1-2 lata), Gold (6-12 miesięcy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eb006e4f-619f-46ac-b83f-b8b7530398d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Problem 1: Small files w Bronze**\n",
    "**Objawy:** Setki małych plików Parquet po każdym ingeście\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Włącz Auto Optimize dla Bronze\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {bronze_table} \n",
    "    SET TBLPROPERTIES (\n",
    "        'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "        'delta.autoOptimize.autoCompact' = 'true'\n",
    "    )\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**Problem 2: Silver processing zbyt wolny**\n",
    "**Rozwiązanie:** Użyj incremental processing z watermark zamiast full table scan:\n",
    "```python\n",
    "# Proces tylko rekordy nowsze niż ostatni Silver timestamp\n",
    "max_silver_ts = spark.table(silver_table).agg(F.max(\"ingest_timestamp\")).collect()[0][0]\n",
    "new_bronze = spark.table(bronze_table).filter(F.col(\"ingest_timestamp\") > max_silver_ts)\n",
    "```\n",
    "\n",
    "**Problem 3: Gold re-aggregation trwa zbyt długo**\n",
    "**Rozwiązanie:** Użyj MERGE zamiast overwrite dla incremental Gold:\n",
    "```python\n",
    "# Tylko affected dates\n",
    "affected_dates = new_silver.select(\"order_date\").distinct()\n",
    "# DELETE affected partitions, INSERT new aggregates\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11249d20-d238-4d58-a93f-8811a5089d1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "**W tym notebooku nauczyliśmy się:**\n",
    "\n",
    "✅ **Medallion Architecture:**\n",
    "- Bronze: Raw data landing zone z audit metadata (immutable, append-only)\n",
    "- Silver: Cleansed, validated, deduplicated data (business rules enforcement)\n",
    "- Gold: Business-level aggregates i KPI (BI-ready, denormalized)\n",
    "\n",
    "✅ **ETL vs ELT:**\n",
    "- ELT approach: Load first (Bronze), then Transform (Silver/Gold)\n",
    "- Możliwość reprocessingu bez re-ingestion\n",
    "- Incremental processing dla każdej warstwy\n",
    "\n",
    "✅ **Partitioning Strategy:**\n",
    "- Partycjonuj tylko duże tabele (>1TB)\n",
    "- Partition size > 1GB dla uniknięcia small files\n",
    "- Silver/Gold: partycjonowanie po dacie lub business dimensions\n",
    "\n",
    "**Kluczowe wnioski:**\n",
    "1. Medallion Architecture zapewnia separation of concerns i data quality gates\n",
    "2. Każda warstwa ma określone SLA, retention policy i access patterns\n",
    "3. Bronze jako immutable source of truth umożliwia reprocessing\n",
    "4. Incremental processing jest kluczowy dla performance w dużej skali\n",
    "\n",
    "**Następne kroki:**\n",
    "- **Kolejny notebook**: 03_batch_streaming_load.ipynb - COPY INTO, Auto Loader, Structured Streaming\n",
    "- **Warsztat praktyczny**: 01_delta_medallion_workshop.ipynb\n",
    "- **Delta Live Tables**: Automatyczna implementacja Medallion z deklaratywnym API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2fbc8fff-4a63-438d-91db-6270c0dbe262",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Opcjonalnie: usuń utworzone tabele Demo po zakończeniu ćwiczeń:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e282e534-095c-48b6-ba80-eefa389da3f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Opcjonalne czyszczenie zasobów testowych\n",
    "# UWAGA: Uruchom tylko jeśli chcesz usunąć wszystkie utworzone dane\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {bronze_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {silver_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {silver_partitioned_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {gold_table}\")\n",
    "\n",
    "# spark.catalog.clearCache()\n",
    "# print(\"Zasoby zostały wyczyszczone\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_medallion_architecture",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
