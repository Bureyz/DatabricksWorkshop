{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f981b44",
   "metadata": {},
   "source": [
    "# Medallion Architecture - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Zrozumienie koncepcji architektury medalionowej (Bronze/Silver/Gold) i zasad projektowania data lakehouse.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Bronze / Silver / Gold - logika warstw\n",
    "- ETL vs ELT approach\n",
    "- Zasady projektowania pipeline'ów\n",
    "- Partitioning strategy\n",
    "- Audyt i lineage - metadane w każdym kroku\n",
    "- Data quality w kontekście warstw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b2683f",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 2 - Lakehouse & Delta Lake\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a337419",
   "metadata": {},
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie architektury medalionowej jako design pattern dla data lakehouse.\n",
    "\n",
    "**Podstawowe pojęcia:**\n",
    "- **Medallion Architecture**: Wielowarstwowa architektura danych (Bronze → Silver → Gold)\n",
    "- **Bronze Layer**: Raw data landing zone - dane bez transformacji, tylko audit metadata\n",
    "- **Silver Layer**: Cleansed and conformed data - deduplikacja, walidacja, standardizacja\n",
    "- **Gold Layer**: Business-level aggregates - modele KPI, reporty, ML features\n",
    "\n",
    "**Dlaczego to ważne?**\n",
    "Medallion architecture zapewnia separation of concerns, jasne SLA per warstwa, incremental processing, oraz data quality gates. Umożliwia różne tempo procesowania (Bronze: real-time, Silver: hourly, Gold: daily) i różne retention policies per warstwa."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d2f04e",
   "metadata": {},
   "source": [
    "## Izolacja per użytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccada9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be9e99f",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b710fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Wyświetl kontekst użytkownika\n",
    "print(\"=== Kontekst użytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"Użytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog jako domyślny\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "\n",
    "# Ścieżki do danych źródłowych\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "\n",
    "print(f\"\\n=== Ścieżki do danych ===\")\n",
    "print(f\"Orders: {ORDERS_JSON}\")\n",
    "print(f\"Customers: {CUSTOMERS_CSV}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723d9642",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 1: Koncepcja Medallion Architecture\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Medallion Architecture dzieli data lakehouse na trzy warstwy o rosnącej jakości danych. Każda warstwa ma określone SLA, retention policy i data quality requirements.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **Bronze (Raw)**: Append-only, immutable landing zone. Dane \"as-is\" z systemu źródłowego + audit metadata (ingestion timestamp, source file, version)\n",
    "- **Silver (Cleansed)**: Validated, deduplicated, standardized. Business rules enforcement, schema evolution, data quality checks\n",
    "- **Gold (Curated)**: Aggregated, denormalized, business-level. KPI models, reporting tables, ML features, star schema\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Separacja odpowiedzialności: data engineers (Bronze/Silver), analytics engineers (Gold)\n",
    "- Incremental processing: tylko nowe/zmienione dane propagowane przez warstwy\n",
    "- Debug-friendly: możliwość reprocessingu Silver/Gold z Bronze bez re-ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d319e",
   "metadata": {},
   "source": [
    "### Przykład 1.1: Bronze Layer - Raw Data Landing\n",
    "\n",
    "**Cel:** Utworzenie Bronze layer z audit metadata\n",
    "\n",
    "**Podejście:**\n",
    "1. Wczytanie surowych danych z JSON\n",
    "2. Dodanie audit columns: ingest_timestamp, source_file, ingested_by\n",
    "3. Zapis do Bronze schema bez transformacji biznesowych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96dc9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 1.1 - Bronze Layer\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Wczytaj surowe dane orders\n",
    "orders_raw = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "print(\"=== Surowe dane (schema) ===\")\n",
    "orders_raw.printSchema()\n",
    "\n",
    "# Dodaj audit metadata (Bronze best practice)\n",
    "orders_bronze = (\n",
    "    orders_raw\n",
    "    .withColumn(\"ingest_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"source_file\", F.input_file_name())\n",
    "    .withColumn(\"ingested_by\", F.lit(raw_user))\n",
    "    .withColumn(\"bronze_version\", F.lit(1))\n",
    ")\n",
    "\n",
    "print(\"\\n=== Bronze layer z audit metadata ===\")\n",
    "display(orders_bronze.limit(3))\n",
    "\n",
    "# Zapisz do Bronze schema\n",
    "bronze_table = f\"{BRONZE_SCHEMA}.orders_bronze\"\n",
    "\n",
    "(\n",
    "    orders_bronze\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(bronze_table)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Utworzono Bronze table: {bronze_table}\")\n",
    "print(f\"Liczba rekordów: {spark.table(bronze_table).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e7e9c1",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Bronze layer:\n",
    "- **Immutable**: Dane zapisane \"as-is\" bez modyfikacji wartości biznesowych\n",
    "- **Audit trail**: Każdy rekord ma metadata: kiedy, skąd, przez kogo został załadowany\n",
    "- **Append-only**: Idealnie nadaje się do incremental loads z COPY INTO lub Auto Loader\n",
    "- **Retention**: Często długa (lata) jako źródło prawdy do reprocessingu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b66494",
   "metadata": {},
   "source": [
    "### Przykład 1.2: Silver Layer - Cleansed & Validated\n",
    "\n",
    "**Cel:** Transformacja Bronze → Silver z data quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aae702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 1.2 - Silver Layer\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {SILVER_SCHEMA}\")\n",
    "\n",
    "# Wczytaj dane z Bronze\n",
    "orders_bronze_df = spark.table(bronze_table)\n",
    "\n",
    "# Silver transformations: cleaning, validation, standardization\n",
    "orders_silver = (\n",
    "    orders_bronze_df\n",
    "    # Deduplikacja po kluczu biznesowym\n",
    "    .dropDuplicates([\"order_id\"])\n",
    "    \n",
    "    # Walidacja: usuń rekordy z NULL w kluczowych kolumnach\n",
    "    .filter(F.col(\"order_id\").isNotNull())\n",
    "    .filter(F.col(\"customer_id\").isNotNull())\n",
    "    \n",
    "    # Walidacja biznesowa: amount > 0\n",
    "    .filter(F.col(\"order_amount\") > 0)\n",
    "    \n",
    "    # Standaryzacja dat\n",
    "    .withColumn(\"order_date\", F.to_date(F.col(\"order_date\")))\n",
    "    \n",
    "    # Standaryzacja tekstów\n",
    "    .withColumn(\"order_status\", F.upper(F.trim(F.col(\"order_status\"))))\n",
    "    \n",
    "    # Dodaj Silver metadata\n",
    "    .withColumn(\"silver_processed_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"data_quality_flag\", F.lit(\"VALID\"))\n",
    ")\n",
    "\n",
    "print(\"=== Silver layer - cleansed data ===\")\n",
    "display(orders_silver.limit(5))\n",
    "\n",
    "# Zapisz do Silver schema\n",
    "silver_table = f\"{SILVER_SCHEMA}.orders_silver\"\n",
    "\n",
    "(\n",
    "    orders_silver\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(silver_table)\n",
    ")\n",
    "\n",
    "bronze_count = orders_bronze_df.count()\n",
    "silver_count = spark.table(silver_table).count()\n",
    "\n",
    "print(f\"\\n✓ Utworzono Silver table: {silver_table}\")\n",
    "print(f\"Bronze records: {bronze_count}\")\n",
    "print(f\"Silver records: {silver_count}\")\n",
    "print(f\"Filtered out: {bronze_count - silver_count} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2983a914",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Silver layer:\n",
    "- **Data Quality**: Walidacja biznesowa (amount > 0), walidacja schematów (NOT NULL)\n",
    "- **Deduplikacja**: Usunięcie duplikatów po kluczu biznesowym\n",
    "- **Standardizacja**: Ujednolicenie formatów (daty, teksty, case sensitivity)\n",
    "- **Incremental friendly**: Można używać MERGE dla slowly changing dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7a4b59",
   "metadata": {},
   "source": [
    "### Przykład 1.3: Gold Layer - Business Aggregates\n",
    "\n",
    "**Cel:** Utworzenie Gold layer z KPI dla analityki i raportowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d24c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 1.3 - Gold Layer: Daily Order Summary\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {GOLD_SCHEMA}\")\n",
    "\n",
    "# Wczytaj dane z Silver\n",
    "orders_silver_df = spark.table(silver_table)\n",
    "\n",
    "# Gold aggregation: Daily order summary\n",
    "daily_summary = (\n",
    "    orders_silver_df\n",
    "    .groupBy(\"order_date\", \"order_status\")\n",
    "    .agg(\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.sum(\"order_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"order_amount\").alias(\"avg_order_value\"),\n",
    "        F.min(\"order_amount\").alias(\"min_order_value\"),\n",
    "        F.max(\"order_amount\").alias(\"max_order_value\"),\n",
    "        F.countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    "    )\n",
    "    .withColumn(\"gold_created_timestamp\", F.current_timestamp())\n",
    "    .orderBy(\"order_date\", \"order_status\")\n",
    ")\n",
    "\n",
    "print(\"=== Gold layer - Daily Order Summary ===\")\n",
    "display(daily_summary)\n",
    "\n",
    "# Zapisz do Gold schema\n",
    "gold_table = f\"{GOLD_SCHEMA}.daily_order_summary\"\n",
    "\n",
    "(\n",
    "    daily_summary\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(gold_table)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Utworzono Gold table: {gold_table}\")\n",
    "print(f\"Liczba agregowanych dni: {spark.table(gold_table).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2eccaf7",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Gold layer:\n",
    "- **Business-level**: KPI i metryki zgodne z business definitions\n",
    "- **Denormalized**: Często szeroka tabela z joinami już wykonanymi (performance dla BI)\n",
    "- **Aggregated**: Dane pre-aggregowane (daily, weekly, monthly) dla szybkich dashboardów\n",
    "- **BI-ready**: Bezpośrednie źródło dla Power BI, Tableau, Looker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e27fbe6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 2: ETL vs ELT Approach\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Medallion architecture wspiera ELT (Extract-Load-Transform) approach, w przeciwieństwie do tradycyjnego ETL. Dane są najpierw ładowane do Bronze (Load), a potem transformowane w Silver/Gold (Transform).\n",
    "\n",
    "**Kluczowe różnice:**\n",
    "- **ETL**: Transform before load - dane są czyszczone poza data warehouse\n",
    "- **ELT**: Load then transform - surowe dane w Bronze, transformacje w lakehouse\n",
    "- **Zalety ELT**: Możliwość reprocessingu, data lineage, audit trail, schema evolution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add2cdb3",
   "metadata": {},
   "source": [
    "### Przykład 2.1: ELT Pipeline - Incremental Processing\n",
    "\n",
    "**Cel:** Demonstracja incremental ELT: nowe dane w Bronze → automatyczna propagacja do Silver/Gold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24434709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 2.1 - Incremental ELT\n",
    "\n",
    "# Symulacja: nowe dane przychodzą do Bronze\n",
    "new_orders_data = [\n",
    "    (9001, 101, \"2025-01-20\", 350.00, \"Pending\"),\n",
    "    (9002, 102, \"2025-01-20\", 120.50, \"Pending\"),\n",
    "    (9003, 103, \"2025-01-21\", 499.99, \"Pending\")\n",
    "]\n",
    "\n",
    "new_orders_df = spark.createDataFrame(\n",
    "    new_orders_data,\n",
    "    [\"order_id\", \"customer_id\", \"order_date\", \"order_amount\", \"order_status\"]\n",
    ")\n",
    "\n",
    "# Dodaj audit metadata (Bronze standard)\n",
    "new_orders_bronze = (\n",
    "    new_orders_df\n",
    "    .withColumn(\"ingest_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"source_file\", F.lit(\"incremental_batch_2\"))\n",
    "    .withColumn(\"ingested_by\", F.lit(raw_user))\n",
    "    .withColumn(\"bronze_version\", F.lit(2))\n",
    ")\n",
    "\n",
    "# Append do Bronze (ELT: Load first)\n",
    "(\n",
    "    new_orders_bronze\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(bronze_table)\n",
    ")\n",
    "\n",
    "print(f\"✓ Dodano {new_orders_df.count()} nowych rekordów do Bronze\")\n",
    "print(f\"Bronze total: {spark.table(bronze_table).count()} records\")\n",
    "\n",
    "# Incremental Silver processing: tylko nowe Bronze records (version 2)\n",
    "new_bronze_records = (\n",
    "    spark.table(bronze_table)\n",
    "    .filter(F.col(\"bronze_version\") == 2)\n",
    ")\n",
    "\n",
    "# Apply Silver transformations\n",
    "new_silver_records = (\n",
    "    new_bronze_records\n",
    "    .dropDuplicates([\"order_id\"])\n",
    "    .filter(F.col(\"order_id\").isNotNull())\n",
    "    .filter(F.col(\"order_amount\") > 0)\n",
    "    .withColumn(\"order_date\", F.to_date(F.col(\"order_date\")))\n",
    "    .withColumn(\"order_status\", F.upper(F.trim(F.col(\"order_status\"))))\n",
    "    .withColumn(\"silver_processed_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"data_quality_flag\", F.lit(\"VALID\"))\n",
    ")\n",
    "\n",
    "# Append do Silver\n",
    "(\n",
    "    new_silver_records\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .saveAsTable(silver_table)\n",
    ")\n",
    "\n",
    "print(f\"✓ Propagowano {new_silver_records.count()} rekordów do Silver\")\n",
    "print(f\"Silver total: {spark.table(silver_table).count()} records\")\n",
    "\n",
    "# Gold: re-aggregate (lub incremental z MERGE)\n",
    "# Dla uproszczenia: pełna re-agregacja\n",
    "updated_daily_summary = (\n",
    "    spark.table(silver_table)\n",
    "    .groupBy(\"order_date\", \"order_status\")\n",
    "    .agg(\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.sum(\"order_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"order_amount\").alias(\"avg_order_value\"),\n",
    "        F.min(\"order_amount\").alias(\"min_order_value\"),\n",
    "        F.max(\"order_amount\").alias(\"max_order_value\"),\n",
    "        F.countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    "    )\n",
    "    .withColumn(\"gold_created_timestamp\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "(\n",
    "    updated_daily_summary\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(gold_table)\n",
    ")\n",
    "\n",
    "print(f\"✓ Zaktualizowano Gold layer\")\n",
    "print(\"\\n=== Updated Gold Summary ===\")\n",
    "display(spark.table(gold_table).orderBy(\"order_date\", \"order_status\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d74784",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Incremental ELT pattern:\n",
    "1. **Bronze**: Append nowych danych z wersjonowaniem (bronze_version)\n",
    "2. **Silver**: Proces tylko nowe Bronze records (watermark lub version)\n",
    "3. **Gold**: Re-aggregate lub MERGE dla affected partitions\n",
    "\n",
    "W produkcji: używamy Delta Live Tables lub Structured Streaming dla automatic incrementality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce651ab0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 3: Partitioning Strategy\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Partycjonowanie to kluczowa decyzja architektoniczna w Medallion. Złe partycjonowanie powoduje small files problem lub inefficient queries.\n",
    "\n",
    "**Zasady partycjonowania:**\n",
    "- **Bronze**: Rzadko partycjonujemy (append-only, bulk operations)\n",
    "- **Silver**: Partycjonowanie po dacie lub region dla incremental MERGE\n",
    "- **Gold**: Partycjonowanie wg wymiarów zapytań (date, region, product_category)\n",
    "- **Reguła**: Partycjonuj tylko jeśli tabela > 1 TB i partition size > 1 GB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9736f0ed",
   "metadata": {},
   "source": [
    "### Przykład 3.1: Partycjonowanie Silver layer po dacie\n",
    "\n",
    "**Cel:** Demonstracja partitioned table dla efektywnych incremental updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67223f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 3.1 - Partitioned Silver table\n",
    "\n",
    "# Utwórz Silver table z partycjonowaniem po order_date\n",
    "silver_partitioned_table = f\"{SILVER_SCHEMA}.orders_silver_partitioned\"\n",
    "\n",
    "(\n",
    "    spark.table(silver_table)\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .partitionBy(\"order_date\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(silver_partitioned_table)\n",
    ")\n",
    "\n",
    "print(f\"✓ Utworzono partycjonowaną tabelę: {silver_partitioned_table}\")\n",
    "\n",
    "# Sprawdź partycje\n",
    "partitions = spark.sql(f\"SHOW PARTITIONS {silver_partitioned_table}\")\n",
    "print(\"\\n=== Partycje ===\")\n",
    "display(partitions)\n",
    "\n",
    "# DESCRIBE DETAIL - sprawdź partitioning columns\n",
    "detail = spark.sql(f\"DESCRIBE DETAIL {silver_partitioned_table}\")\n",
    "print(\"\\n=== Detail (partitionColumns) ===\")\n",
    "display(detail.select(\"name\", \"partitionColumns\", \"numFiles\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e860ca",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Partycjonowanie:\n",
    "- **Partition pruning**: Spark czyta tylko partycje spełniające predicate (WHERE order_date = '2025-01-20')\n",
    "- **Incremental MERGE**: UPDATE/DELETE tylko affected partitions\n",
    "- **Trade-off**: Zbyt dużo partycji (< 1 GB) powoduje small files problem\n",
    "\n",
    "Best practice: Partycjonuj po kolumnie używanej w 80% zapytań (często: date, region)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed13ed12",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "**Projektowanie warstw:**\n",
    "- **Bronze**: Immutable, append-only. Długa retention (lata). Audit metadata obowiązkowe.\n",
    "- **Silver**: Idempotentne transformacje. Możliwość reprocessingu z Bronze. MERGE dla SCD.\n",
    "- **Gold**: Denormalized, aggregated. Partition wg business dimensions. Krótka retention (miesięcy).\n",
    "\n",
    "**Data Quality:**\n",
    "- **Bronze → Silver**: Walidacja schematów, business rules, deduplikacja\n",
    "- **Silver → Gold**: Sprawdzenie completeness (czy wszystkie Bronze records dotarły?)\n",
    "- **Expectations**: Używaj Delta Live Tables expectations (warn/drop/fail)\n",
    "\n",
    "**Performance:**\n",
    "- **Partycjonowanie**: Tylko dla dużych tabel (>1TB), partition size > 1GB\n",
    "- **ZORDER**: Silver/Gold - po kluczu biznesowym lub często filtrowanych kolumnach\n",
    "- **Auto Optimize**: Włącz dla Silver/Gold (częste małe zapisy)\n",
    "\n",
    "**Governance:**\n",
    "- **Unity Catalog**: Bronze/Silver/Gold jako osobne schemas z różnymi permissions\n",
    "- **Lineage**: Używaj Delta Lake lineage do śledzenia Bronze → Silver → Gold\n",
    "- **Retention**: Bronze (3-7 lat), Silver (1-2 lata), Gold (6-12 miesięcy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a803003",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Problem 1: Small files w Bronze**\n",
    "**Objawy:** Setki małych plików Parquet po każdym ingeście\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Włącz Auto Optimize dla Bronze\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {bronze_table} \n",
    "    SET TBLPROPERTIES (\n",
    "        'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "        'delta.autoOptimize.autoCompact' = 'true'\n",
    "    )\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**Problem 2: Silver processing zbyt wolny**\n",
    "**Rozwiązanie:** Użyj incremental processing z watermark zamiast full table scan:\n",
    "```python\n",
    "# Proces tylko rekordy nowsze niż ostatni Silver timestamp\n",
    "max_silver_ts = spark.table(silver_table).agg(F.max(\"ingest_timestamp\")).collect()[0][0]\n",
    "new_bronze = spark.table(bronze_table).filter(F.col(\"ingest_timestamp\") > max_silver_ts)\n",
    "```\n",
    "\n",
    "**Problem 3: Gold re-aggregation trwa zbyt długo**\n",
    "**Rozwiązanie:** Użyj MERGE zamiast overwrite dla incremental Gold:\n",
    "```python\n",
    "# Tylko affected dates\n",
    "affected_dates = new_silver.select(\"order_date\").distinct()\n",
    "# DELETE affected partitions, INSERT new aggregates\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d62c70a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "**W tym notebooku nauczyliśmy się:**\n",
    "\n",
    "✅ **Medallion Architecture:**\n",
    "- Bronze: Raw data landing zone z audit metadata (immutable, append-only)\n",
    "- Silver: Cleansed, validated, deduplicated data (business rules enforcement)\n",
    "- Gold: Business-level aggregates i KPI (BI-ready, denormalized)\n",
    "\n",
    "✅ **ETL vs ELT:**\n",
    "- ELT approach: Load first (Bronze), then Transform (Silver/Gold)\n",
    "- Możliwość reprocessingu bez re-ingestion\n",
    "- Incremental processing dla każdej warstwy\n",
    "\n",
    "✅ **Partitioning Strategy:**\n",
    "- Partycjonuj tylko duże tabele (>1TB)\n",
    "- Partition size > 1GB dla uniknięcia small files\n",
    "- Silver/Gold: partycjonowanie po dacie lub business dimensions\n",
    "\n",
    "**Kluczowe wnioski:**\n",
    "1. Medallion Architecture zapewnia separation of concerns i data quality gates\n",
    "2. Każda warstwa ma określone SLA, retention policy i access patterns\n",
    "3. Bronze jako immutable source of truth umożliwia reprocessing\n",
    "4. Incremental processing jest kluczowy dla performance w dużej skali\n",
    "\n",
    "**Następne kroki:**\n",
    "- **Kolejny notebook**: 03_batch_streaming_load.ipynb - COPY INTO, Auto Loader, Structured Streaming\n",
    "- **Warsztat praktyczny**: 01_delta_medallion_workshop.ipynb\n",
    "- **Delta Live Tables**: Automatyczna implementacja Medallion z deklaratywnym API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d991fb",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Opcjonalnie: usuń utworzone tabele Demo po zakończeniu ćwiczeń:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c14eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcjonalne czyszczenie zasobów testowych\n",
    "# UWAGA: Uruchom tylko jeśli chcesz usunąć wszystkie utworzone dane\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {bronze_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {silver_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {silver_partitioned_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {gold_table}\")\n",
    "\n",
    "# spark.catalog.clearCache()\n",
    "# print(\"Zasoby zostały wyczyszczone\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
