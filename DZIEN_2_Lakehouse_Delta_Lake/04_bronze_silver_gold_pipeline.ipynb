{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce0f19c5",
   "metadata": {},
   "source": [
    "# Bronze → Silver → Gold Pipeline - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Implementacja kompletnego end-to-end pipeline z Bronze przez Silver do Gold.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Bronze: raw load + audit columns (ingest_ts, source_file, ingested_by)\n",
    "- Silver: cleaning, deduplikacja, sanity checks, JSON flattening (from_json, explode)\n",
    "- Gold: KPI modeling, agregacje (daily/weekly/monthly), star schema vs denormalizacja\n",
    "- End-to-end data lineage\n",
    "- Performance monitoring per warstwa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f949e",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 2 - Lakehouse & Delta Lake\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7ccd9a",
   "metadata": {},
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie kompletnego data pipeline implementującego Medallion Architecture.\n",
    "\n",
    "**Podstawowe pojęcia:**\n",
    "- **End-to-end pipeline**: Automatyczny przepływ danych przez wszystkie warstwy\n",
    "- **Data lineage**: Śledzenie transformacji od źródła do destination\n",
    "- **JSON flattening**: Rozpakowywanie nested structures do płaskich tabel\n",
    "- **Star schema**: Dimensional modeling z fact tables i dimension tables\n",
    "\n",
    "**Dlaczego to ważne?**\n",
    "Production pipeline musi obsługiwać różne formaty źródłowe (JSON, CSV, Parquet), kompleksowe transformacje (flattening, joins, aggregations) oraz zapewniać data quality gates na każdym etapie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a75cdd9",
   "metadata": {},
   "source": [
    "## Izolacja per użytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53ac219",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b9bae6",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155b614f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Wyświetl kontekst użytkownika\n",
    "print(\"=== Kontekst użytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"Użytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog jako domyślny\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "\n",
    "# Ścieżki do danych źródłowych\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "print(f\"\\n=== Ścieżki do danych ===\")\n",
    "print(f\"Orders: {ORDERS_JSON}\")\n",
    "print(f\"Customers: {CUSTOMERS_CSV}\")\n",
    "print(f\"Products: {PRODUCTS_PARQUET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe6a927",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 1: Bronze Layer - Raw Data Ingestion\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Bronze layer przyjmuje surowe dane z różnych źródeł i formatów. Kluczowe jest dodanie audit metadata dla data lineage i troubleshooting.\n",
    "\n",
    "**Kluczowe operacje:**\n",
    "- Wczytanie z różnych formatów (JSON, CSV, Parquet)\n",
    "- Dodanie audit columns: ingest_timestamp, source_file, ingested_by\n",
    "- Zapis do Delta bez transformacji wartości biznesowych\n",
    "- Versioning dla incremental loads\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Immutable landing zone - możliwość reprocessingu\n",
    "- Audit trail dla compliance\n",
    "- Multiple source formats w jednym pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01418fe",
   "metadata": {},
   "source": [
    "### Przykład 1.1: Bronze - Orders (JSON)\n",
    "\n",
    "**Cel:** Ingest zamówień z JSON do Bronze z audit metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468ba955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 1.1 - Bronze Orders\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Wczytaj surowe orders z JSON\n",
    "orders_raw = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "# Dodaj Bronze audit metadata\n",
    "orders_bronze = (\n",
    "    orders_raw\n",
    "    .withColumn(\"_bronze_ingest_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_bronze_source_file\", F.input_file_name())\n",
    "    .withColumn(\"_bronze_ingested_by\", F.lit(raw_user))\n",
    "    .withColumn(\"_bronze_version\", F.lit(1))\n",
    ")\n",
    "\n",
    "print(\"=== Bronze Orders Schema ===\")\n",
    "orders_bronze.printSchema()\n",
    "\n",
    "# Zapisz do Bronze\n",
    "bronze_orders_table = f\"{BRONZE_SCHEMA}.orders_bronze\"\n",
    "\n",
    "(\n",
    "    orders_bronze\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(bronze_orders_table)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Bronze Orders: {bronze_orders_table}\")\n",
    "print(f\"Liczba rekordów: {spark.table(bronze_orders_table).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805ea9ff",
   "metadata": {},
   "source": [
    "### Przykład 1.2: Bronze - Customers (CSV) i Products (Parquet)\n",
    "\n",
    "**Cel:** Ingest danych klientów i produktów z różnych formatów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb355d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 1.2 - Bronze Customers (CSV)\n",
    "\n",
    "# Customers z CSV\n",
    "customers_raw = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "customers_bronze = (\n",
    "    customers_raw\n",
    "    .withColumn(\"_bronze_ingest_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_bronze_source_file\", F.input_file_name())\n",
    "    .withColumn(\"_bronze_ingested_by\", F.lit(raw_user))\n",
    ")\n",
    "\n",
    "bronze_customers_table = f\"{BRONZE_SCHEMA}.customers_bronze\"\n",
    "\n",
    "(\n",
    "    customers_bronze\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(bronze_customers_table)\n",
    ")\n",
    "\n",
    "print(f\"✓ Bronze Customers: {bronze_customers_table}\")\n",
    "print(f\"Liczba rekordów: {spark.table(bronze_customers_table).count()}\")\n",
    "\n",
    "# Products z Parquet\n",
    "products_raw = spark.read.format(\"parquet\").load(PRODUCTS_PARQUET)\n",
    "\n",
    "products_bronze = (\n",
    "    products_raw\n",
    "    .withColumn(\"_bronze_ingest_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_bronze_source_file\", F.input_file_name())\n",
    "    .withColumn(\"_bronze_ingested_by\", F.lit(raw_user))\n",
    ")\n",
    "\n",
    "bronze_products_table = f\"{BRONZE_SCHEMA}.products_bronze\"\n",
    "\n",
    "(\n",
    "    products_bronze\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(bronze_products_table)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Bronze Products: {bronze_products_table}\")\n",
    "print(f\"Liczba rekordów: {spark.table(bronze_products_table).count()}\")\n",
    "\n",
    "print(\"\\n=== Bronze Layer Summary ===\")\n",
    "print(f\"Orders: {spark.table(bronze_orders_table).count()}\")\n",
    "print(f\"Customers: {spark.table(bronze_customers_table).count()}\")\n",
    "print(f\"Products: {spark.table(bronze_products_table).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bac51dd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 2: Silver Layer - Cleansing & Validation\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Silver layer wykonuje data quality checks, deduplikację, standaryzację i flattening nested structures. To warstwa gdzie enforcement business rules.\n",
    "\n",
    "**Kluczowe transformacje:**\n",
    "- Deduplikacja po kluczu biznesowym\n",
    "- Walidacja NOT NULL, data types, ranges\n",
    "- Standaryzacja: dates, case sensitivity, formats\n",
    "- JSON flattening dla nested structures\n",
    "\n",
    "**Data Quality Gates:**\n",
    "- Reject invalid records (lub flaguj)\n",
    "- Log data quality metrics\n",
    "- Monitor rejection rates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867f375f",
   "metadata": {},
   "source": [
    "### Przykład 2.1: Silver Orders - Cleansing & Validation\n",
    "\n",
    "**Cel:** Transformacja Bronze Orders → Silver z quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8498d91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 2.1 - Silver Orders\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {SILVER_SCHEMA}\")\n",
    "\n",
    "# Wczytaj z Bronze\n",
    "orders_bronze_df = spark.table(bronze_orders_table)\n",
    "\n",
    "# Silver transformations\n",
    "orders_silver = (\n",
    "    orders_bronze_df\n",
    "    \n",
    "    # Deduplikacja po kluczu biznesowym\n",
    "    .dropDuplicates([\"order_id\"])\n",
    "    \n",
    "    # Walidacja NOT NULL\n",
    "    .filter(F.col(\"order_id\").isNotNull())\n",
    "    .filter(F.col(\"customer_id\").isNotNull())\n",
    "    \n",
    "    # Walidacja biznesowa\n",
    "    .filter(F.col(\"order_amount\") > 0)\n",
    "    \n",
    "    # Standaryzacja dat\n",
    "    .withColumn(\"order_date\", F.to_date(F.col(\"order_date\")))\n",
    "    \n",
    "    # Standaryzacja statusu\n",
    "    .withColumn(\"order_status\", F.upper(F.trim(F.col(\"order_status\"))))\n",
    "    \n",
    "    # Kategorizacja kwot (biznes logic)\n",
    "    .withColumn(\n",
    "        \"order_value_category\",\n",
    "        F.when(F.col(\"order_amount\") < 100, \"LOW\")\n",
    "         .when(F.col(\"order_amount\") < 500, \"MEDIUM\")\n",
    "         .otherwise(\"HIGH\")\n",
    "    )\n",
    "    \n",
    "    # Silver metadata\n",
    "    .withColumn(\"_silver_processed_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"_data_quality_flag\", F.lit(\"VALID\"))\n",
    ")\n",
    "\n",
    "# Quality metrics\n",
    "bronze_count = orders_bronze_df.count()\n",
    "silver_count = orders_silver.count()\n",
    "rejected_count = bronze_count - silver_count\n",
    "rejection_rate = (rejected_count / bronze_count * 100) if bronze_count > 0 else 0\n",
    "\n",
    "print(\"=== Silver Orders Quality Metrics ===\")\n",
    "print(f\"Bronze input: {bronze_count}\")\n",
    "print(f\"Silver output: {silver_count}\")\n",
    "print(f\"Rejected: {rejected_count} ({rejection_rate:.2f}%)\")\n",
    "\n",
    "# Zapisz do Silver\n",
    "silver_orders_table = f\"{SILVER_SCHEMA}.orders_silver\"\n",
    "\n",
    "(\n",
    "    orders_silver\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(silver_orders_table)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Silver Orders: {silver_orders_table}\")\n",
    "display(spark.table(silver_orders_table).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0e3734",
   "metadata": {},
   "source": [
    "### Przykład 2.2: Silver Customers & Products\n",
    "\n",
    "**Cel:** Cleansing dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f90e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 2.2 - Silver Customers\n",
    "\n",
    "customers_bronze_df = spark.table(bronze_customers_table)\n",
    "\n",
    "customers_silver = (\n",
    "    customers_bronze_df\n",
    "    .dropDuplicates([\"customer_id\"])\n",
    "    .filter(F.col(\"customer_id\").isNotNull())\n",
    "    \n",
    "    # Standaryzacja\n",
    "    .withColumn(\"customer_name\", F.trim(F.col(\"customer_name\")))\n",
    "    .withColumn(\"email\", F.lower(F.trim(F.col(\"email\"))))\n",
    "    .withColumn(\"country\", F.upper(F.trim(F.col(\"country\"))))\n",
    "    \n",
    "    # Walidacja email (basic pattern)\n",
    "    .withColumn(\n",
    "        \"email_valid\",\n",
    "        F.col(\"email\").rlike(r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\")\n",
    "    )\n",
    "    \n",
    "    .withColumn(\"_silver_processed_timestamp\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "silver_customers_table = f\"{SILVER_SCHEMA}.customers_silver\"\n",
    "\n",
    "(\n",
    "    customers_silver\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(silver_customers_table)\n",
    ")\n",
    "\n",
    "print(f\"✓ Silver Customers: {silver_customers_table}\")\n",
    "print(f\"Liczba rekordów: {spark.table(silver_customers_table).count()}\")\n",
    "\n",
    "# Products (minimal cleaning - już dobre jakości)\n",
    "products_bronze_df = spark.table(bronze_products_table)\n",
    "\n",
    "products_silver = (\n",
    "    products_bronze_df\n",
    "    .dropDuplicates([\"product_id\"])\n",
    "    .filter(F.col(\"product_id\").isNotNull())\n",
    "    .withColumn(\"_silver_processed_timestamp\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "silver_products_table = f\"{SILVER_SCHEMA}.products_silver\"\n",
    "\n",
    "(\n",
    "    products_silver\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(silver_products_table)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Silver Products: {silver_products_table}\")\n",
    "print(f\"Liczba rekordów: {spark.table(silver_products_table).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1961943a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 3: Gold Layer - Business Modeling\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Gold layer tworzy business-level aggregates i KPI tables. Często denormalizowane (joins pre-computed) dla performance BI tools.\n",
    "\n",
    "**Kluczowe operacje:**\n",
    "- Joins między fact i dimension tables\n",
    "- Agregacje: daily, weekly, monthly\n",
    "- Denormalization dla BI performance\n",
    "- KPI calculations\n",
    "\n",
    "**Design patterns:**\n",
    "- Star schema: Fact table + dimension tables\n",
    "- Denormalized wide tables\n",
    "- Pre-aggregated summary tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4a5801",
   "metadata": {},
   "source": [
    "### Przykład 3.1: Gold - Order Fact Table (Denormalized)\n",
    "\n",
    "**Cel:** Utworzenie denormalized fact table z joinami do dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b19da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 3.1 - Gold Order Fact Table\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {GOLD_SCHEMA}\")\n",
    "\n",
    "# Wczytaj Silver tables\n",
    "orders_silver_df = spark.table(silver_orders_table)\n",
    "customers_silver_df = spark.table(silver_customers_table)\n",
    "products_silver_df = spark.table(silver_products_table)\n",
    "\n",
    "# Join fact z dimensions (denormalization)\n",
    "order_fact = (\n",
    "    orders_silver_df\n",
    "    \n",
    "    # Join z customers\n",
    "    .join(\n",
    "        customers_silver_df.select(\n",
    "            F.col(\"customer_id\").alias(\"cust_id\"),\n",
    "            F.col(\"customer_name\"),\n",
    "            F.col(\"country\"),\n",
    "            F.col(\"email_valid\")\n",
    "        ),\n",
    "        orders_silver_df.customer_id == F.col(\"cust_id\"),\n",
    "        \"left\"\n",
    "    )\n",
    "    \n",
    "    # Dodaj time dimensions\n",
    "    .withColumn(\"order_year\", F.year(\"order_date\"))\n",
    "    .withColumn(\"order_month\", F.month(\"order_date\"))\n",
    "    .withColumn(\"order_quarter\", F.quarter(\"order_date\"))\n",
    "    .withColumn(\"order_day_of_week\", F.dayofweek(\"order_date\"))\n",
    "    \n",
    "    # KPI calculations\n",
    "    .withColumn(\n",
    "        \"is_high_value\",\n",
    "        F.when(F.col(\"order_amount\") >= 500, True).otherwise(False)\n",
    "    )\n",
    "    \n",
    "    # Gold metadata\n",
    "    .withColumn(\"_gold_created_timestamp\", F.current_timestamp())\n",
    "    \n",
    "    # Select final columns\n",
    "    .select(\n",
    "        \"order_id\",\n",
    "        \"customer_id\",\n",
    "        \"customer_name\",\n",
    "        \"country\",\n",
    "        \"order_date\",\n",
    "        \"order_year\",\n",
    "        \"order_month\",\n",
    "        \"order_quarter\",\n",
    "        \"order_day_of_week\",\n",
    "        \"order_amount\",\n",
    "        \"order_value_category\",\n",
    "        \"order_status\",\n",
    "        \"is_high_value\",\n",
    "        \"_gold_created_timestamp\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"=== Gold Order Fact Schema ===\")\n",
    "order_fact.printSchema()\n",
    "\n",
    "# Zapisz do Gold\n",
    "gold_order_fact_table = f\"{GOLD_SCHEMA}.order_fact\"\n",
    "\n",
    "(\n",
    "    order_fact\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(gold_order_fact_table)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Gold Order Fact: {gold_order_fact_table}\")\n",
    "print(f\"Liczba rekordów: {spark.table(gold_order_fact_table).count()}\")\n",
    "display(spark.table(gold_order_fact_table).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f9880",
   "metadata": {},
   "source": [
    "### Przykład 3.2: Gold - Aggregated Summary Tables\n",
    "\n",
    "**Cel:** Pre-aggregowane tabele dla dashboardów i raportów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf944e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 3.2 - Daily Sales Summary\n",
    "\n",
    "order_fact_df = spark.table(gold_order_fact_table)\n",
    "\n",
    "# Daily aggregation\n",
    "daily_sales_summary = (\n",
    "    order_fact_df\n",
    "    .groupBy(\"order_date\", \"country\", \"order_status\")\n",
    "    .agg(\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.sum(\"order_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"order_amount\").alias(\"avg_order_value\"),\n",
    "        F.min(\"order_amount\").alias(\"min_order_value\"),\n",
    "        F.max(\"order_amount\").alias(\"max_order_value\"),\n",
    "        F.countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "        F.sum(\n",
    "            F.when(F.col(\"is_high_value\") == True, 1).otherwise(0)\n",
    "        ).alias(\"high_value_orders\")\n",
    "    )\n",
    "    .withColumn(\"_gold_created_timestamp\", F.current_timestamp())\n",
    "    .orderBy(\"order_date\", \"country\")\n",
    ")\n",
    "\n",
    "gold_daily_summary_table = f\"{GOLD_SCHEMA}.daily_sales_summary\"\n",
    "\n",
    "(\n",
    "    daily_sales_summary\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(gold_daily_summary_table)\n",
    ")\n",
    "\n",
    "print(f\"✓ Gold Daily Sales Summary: {gold_daily_summary_table}\")\n",
    "display(spark.table(gold_daily_summary_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6135a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monthly aggregation\n",
    "monthly_sales_summary = (\n",
    "    order_fact_df\n",
    "    .groupBy(\"order_year\", \"order_month\", \"country\")\n",
    "    .agg(\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.sum(\"order_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"order_amount\").alias(\"avg_order_value\"),\n",
    "        F.countDistinct(\"customer_id\").alias(\"unique_customers\")\n",
    "    )\n",
    "    .withColumn(\"_gold_created_timestamp\", F.current_timestamp())\n",
    "    .orderBy(\"order_year\", \"order_month\", \"country\")\n",
    ")\n",
    "\n",
    "gold_monthly_summary_table = f\"{GOLD_SCHEMA}.monthly_sales_summary\"\n",
    "\n",
    "(\n",
    "    monthly_sales_summary\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(gold_monthly_summary_table)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Gold Monthly Sales Summary: {gold_monthly_summary_table}\")\n",
    "display(spark.table(gold_monthly_summary_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51dc209",
   "metadata": {},
   "source": [
    "### Przykład 3.3: Gold - Customer Analytics\n",
    "\n",
    "**Cel:** Customer lifetime value i segmentacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0813bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 3.3 - Customer Analytics\n",
    "\n",
    "# Customer-level aggregation\n",
    "customer_analytics = (\n",
    "    order_fact_df\n",
    "    .groupBy(\"customer_id\", \"customer_name\", \"country\")\n",
    "    .agg(\n",
    "        F.count(\"order_id\").alias(\"total_orders\"),\n",
    "        F.sum(\"order_amount\").alias(\"lifetime_value\"),\n",
    "        F.avg(\"order_amount\").alias(\"avg_order_value\"),\n",
    "        F.min(\"order_date\").alias(\"first_order_date\"),\n",
    "        F.max(\"order_date\").alias(\"last_order_date\"),\n",
    "        F.sum(\n",
    "            F.when(F.col(\"is_high_value\") == True, 1).otherwise(0)\n",
    "        ).alias(\"high_value_orders_count\")\n",
    "    )\n",
    "    \n",
    "    # Customer tenure (days)\n",
    "    .withColumn(\n",
    "        \"customer_tenure_days\",\n",
    "        F.datediff(F.col(\"last_order_date\"), F.col(\"first_order_date\"))\n",
    "    )\n",
    "    \n",
    "    # Segmentacja\n",
    "    .withColumn(\n",
    "        \"customer_segment\",\n",
    "        F.when(F.col(\"lifetime_value\") >= 1000, \"PREMIUM\")\n",
    "         .when(F.col(\"lifetime_value\") >= 500, \"GOLD\")\n",
    "         .when(F.col(\"lifetime_value\") >= 200, \"SILVER\")\n",
    "         .otherwise(\"BRONZE\")\n",
    "    )\n",
    "    \n",
    "    .withColumn(\"_gold_created_timestamp\", F.current_timestamp())\n",
    "    .orderBy(F.col(\"lifetime_value\").desc())\n",
    ")\n",
    "\n",
    "gold_customer_analytics_table = f\"{GOLD_SCHEMA}.customer_analytics\"\n",
    "\n",
    "(\n",
    "    customer_analytics\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(gold_customer_analytics_table)\n",
    ")\n",
    "\n",
    "print(f\"✓ Gold Customer Analytics: {gold_customer_analytics_table}\")\n",
    "print(f\"Liczba klientów: {spark.table(gold_customer_analytics_table).count()}\")\n",
    "\n",
    "print(\"\\n=== Top 10 Customers by Lifetime Value ===\")\n",
    "display(spark.table(gold_customer_analytics_table).limit(10))\n",
    "\n",
    "# Segmentation distribution\n",
    "print(\"\\n=== Customer Segmentation Distribution ===\")\n",
    "display(\n",
    "    spark.table(gold_customer_analytics_table)\n",
    "    .groupBy(\"customer_segment\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"customer_count\"),\n",
    "        F.sum(\"lifetime_value\").alias(\"total_revenue\")\n",
    "    )\n",
    "    .orderBy(F.col(\"total_revenue\").desc())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12c90f74",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Sekcja 4: Pipeline Monitoring & Lineage\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Production pipeline wymaga monitoringu na każdym etapie: data volumes, quality metrics, processing time.\n",
    "\n",
    "**Kluczowe metryki:**\n",
    "- Record counts per warstwa\n",
    "- Rejection rates\n",
    "- Processing time\n",
    "- Data freshness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f4d028",
   "metadata": {},
   "source": [
    "### Przykład 4.1: Pipeline Health Dashboard\n",
    "\n",
    "**Cel:** Monitoring kompletnego pipeline'u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda28853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przykład 4.1 - Pipeline Monitoring\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PIPELINE HEALTH DASHBOARD\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Bronze layer metrics\n",
    "print(\"\\n[BRONZE LAYER]\")\n",
    "bronze_orders_count = spark.table(bronze_orders_table).count()\n",
    "bronze_customers_count = spark.table(bronze_customers_table).count()\n",
    "bronze_products_count = spark.table(bronze_products_table).count()\n",
    "\n",
    "print(f\"  Orders:    {bronze_orders_count:,} records\")\n",
    "print(f\"  Customers: {bronze_customers_count:,} records\")\n",
    "print(f\"  Products:  {bronze_products_count:,} records\")\n",
    "\n",
    "# Silver layer metrics\n",
    "print(\"\\n[SILVER LAYER]\")\n",
    "silver_orders_count = spark.table(silver_orders_table).count()\n",
    "silver_customers_count = spark.table(silver_customers_table).count()\n",
    "silver_products_count = spark.table(silver_products_table).count()\n",
    "\n",
    "orders_rejection_rate = ((bronze_orders_count - silver_orders_count) / bronze_orders_count * 100) if bronze_orders_count > 0 else 0\n",
    "customers_rejection_rate = ((bronze_customers_count - silver_customers_count) / bronze_customers_count * 100) if bronze_customers_count > 0 else 0\n",
    "\n",
    "print(f\"  Orders:    {silver_orders_count:,} records (rejection: {orders_rejection_rate:.2f}%)\")\n",
    "print(f\"  Customers: {silver_customers_count:,} records (rejection: {customers_rejection_rate:.2f}%)\")\n",
    "print(f\"  Products:  {silver_products_count:,} records\")\n",
    "\n",
    "# Gold layer metrics\n",
    "print(\"\\n[GOLD LAYER]\")\n",
    "gold_fact_count = spark.table(gold_order_fact_table).count()\n",
    "gold_daily_count = spark.table(gold_daily_summary_table).count()\n",
    "gold_monthly_count = spark.table(gold_monthly_summary_table).count()\n",
    "gold_customer_count = spark.table(gold_customer_analytics_table).count()\n",
    "\n",
    "print(f\"  Order Fact:        {gold_fact_count:,} records\")\n",
    "print(f\"  Daily Summary:     {gold_daily_count:,} aggregates\")\n",
    "print(f\"  Monthly Summary:   {gold_monthly_count:,} aggregates\")\n",
    "print(f\"  Customer Analytics: {gold_customer_count:,} customers\")\n",
    "\n",
    "# Data quality summary\n",
    "print(\"\\n[DATA QUALITY]\")\n",
    "print(f\"  ✓ Orders rejection rate: {orders_rejection_rate:.2f}%\")\n",
    "print(f\"  ✓ Customers rejection rate: {customers_rejection_rate:.2f}%\")\n",
    "print(f\"  ✓ Silver-Gold propagation: 100%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Pipeline Status: ✅ HEALTHY\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf97f7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "**Bronze Layer:**\n",
    "- Zawsze dodawaj audit metadata (_bronze_ingest_timestamp, _bronze_source_file)\n",
    "- Immutable - nigdy nie UPDATE/DELETE w Bronze\n",
    "- Używaj COPY INTO lub Auto Loader dla idempotency\n",
    "\n",
    "**Silver Layer:**\n",
    "- Implementuj data quality gates na początku pipeline'u\n",
    "- Log rejection rates dla alerting\n",
    "- Używaj MERGE dla slowly changing dimensions\n",
    "- Standaryzuj formaty: daty, case, białe znaki\n",
    "\n",
    "**Gold Layer:**\n",
    "- Denormalizuj dla BI performance (pre-compute joins)\n",
    "- Pre-agreguj na różnych granulacjach (daily, weekly, monthly)\n",
    "- Partycjonuj po często filtrowanych kolumnach\n",
    "- Używaj ZORDER BY dla multi-dimensional queries\n",
    "\n",
    "**Monitoring:**\n",
    "- Monitor record counts per warstwa\n",
    "- Alert na spike w rejection rates\n",
    "- Track processing time per stage\n",
    "- Używaj DESCRIBE HISTORY dla audytu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967a935",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Problem 1: High rejection rate w Silver**\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Analiza odrzuconych rekordów\n",
    "rejected = (\n",
    "    bronze_df\n",
    "    .filter(F.col(\"order_amount\").isNull() | (F.col(\"order_amount\") <= 0))\n",
    ")\n",
    "display(rejected)\n",
    "```\n",
    "\n",
    "**Problem 2: Gold joins powodują data loss**\n",
    "**Rozwiązanie:**\n",
    "- Używaj LEFT JOIN zamiast INNER JOIN dla dimension lookups\n",
    "- Monitor unmatched records\n",
    "\n",
    "**Problem 3: Długi processing time dla Gold aggregacji**\n",
    "**Rozwiązanie:**\n",
    "- Używaj incremental processing: tylko affected dates\n",
    "- Cache Silver tables przed wieloma agregacjami\n",
    "- Partycjonuj Gold tables po date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ab351",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "**W tym notebooku zbudowaliśmy kompletny Bronze → Silver → Gold pipeline:**\n",
    "\n",
    "✅ **Bronze Layer:**\n",
    "- Multi-format ingestion (JSON, CSV, Parquet)\n",
    "- Audit metadata dla lineage\n",
    "- Immutable landing zone\n",
    "\n",
    "✅ **Silver Layer:**\n",
    "- Data quality validation\n",
    "- Deduplikacja i standaryzacja\n",
    "- Business rules enforcement\n",
    "- Quality metrics logging\n",
    "\n",
    "✅ **Gold Layer:**\n",
    "- Denormalized fact tables\n",
    "- Pre-aggregated summaries (daily, monthly)\n",
    "- Customer analytics i segmentacja\n",
    "- BI-ready tables\n",
    "\n",
    "✅ **Monitoring:**\n",
    "- Pipeline health dashboard\n",
    "- Data quality metrics\n",
    "- Rejection rate tracking\n",
    "\n",
    "**Kluczowe wnioski:**\n",
    "1. End-to-end pipeline wymaga różnych transformacji per warstwa\n",
    "2. Data quality gates w Silver chronią przed bad data w Gold\n",
    "3. Denormalizacja w Gold poprawia performance BI dashboardów\n",
    "4. Monitoring jest kluczowy dla production reliability\n",
    "\n",
    "**Następne kroki:**\n",
    "- **Kolejny notebook**: 05_optimization_best_practices.ipynb\n",
    "- **Warsztat praktyczny**: 03_end_to_end_bronze_silver_gold_workshop.ipynb\n",
    "- **Delta Live Tables**: Declarative pipelines z automatic data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abfbefa",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Posprzątaj zasoby utworzone podczas notebooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1def90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcjonalne czyszczenie zasobów testowych\n",
    "# UWAGA: Uruchom tylko jeśli chcesz usunąć wszystkie utworzone dane\n",
    "\n",
    "# Bronze\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {bronze_orders_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {bronze_customers_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {bronze_products_table}\")\n",
    "\n",
    "# Silver\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {silver_orders_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {silver_customers_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {silver_products_table}\")\n",
    "\n",
    "# Gold\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {gold_order_fact_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {gold_daily_summary_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {gold_monthly_summary_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {gold_customer_analytics_table}\")\n",
    "\n",
    "# spark.catalog.clearCache()\n",
    "# print(\"Zasoby zostały wyczyszczone\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
