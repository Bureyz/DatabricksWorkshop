{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7952b5f9",
   "metadata": {},
   "source": [
    "# Streaming Data Ingestion - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Opanowanie technik streaming data ingestion do Delta Lake z u≈ºyciem Structured Streaming i Auto Loader.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Structured Streaming fundamentals\n",
    "- Auto Loader (cloudFiles) deep dive\n",
    "- readStream & writeStream API\n",
    "- Watermarking & late data handling\n",
    "- Checkpoint management\n",
    "- Trigger modes (once, continuous, availableNow, processingTime)\n",
    "- Schema evolution w streaming\n",
    "- Stream-to-Delta patterns\n",
    "- Exactly-once semantics\n",
    "- Monitoring & troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2bb19",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzie≈Ñ szkolenia**: Dzie≈Ñ 2 - Delta Lake & Lakehouse Architecture\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog w≈ÇƒÖczony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers\n",
    "- **Zale≈ºno≈õci**: \n",
    "  - Wykonany notebook 01_delta_lake_operations.ipynb\n",
    "  - Wykonany notebook 02_batch_data_ingestion.ipynb\n",
    "- **Czas realizacji**: ~60 minut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef52f6",
   "metadata": {},
   "source": [
    "## Wstƒôp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie fundament√≥w Structured Streaming i kiedy stosowaƒá streaming vs batch ingestion.\n",
    "\n",
    "### Structured Streaming - Kluczowe Koncepty\n",
    "\n",
    "**Co to jest Structured Streaming?**\n",
    "- Streaming engine zbudowany na Spark SQL\n",
    "- Traktuje stream jako \"unbounded table\" (niesko≈ÑczonƒÖ tabelƒô)\n",
    "- Micro-batch processing (domy≈õlnie) lub continuous processing\n",
    "- Exactly-once semantics z idempotent writes\n",
    "- Fault-tolerant z checkpoint recovery\n",
    "\n",
    "**Micro-batch Architecture:**\n",
    "```\n",
    "Input Stream ‚Üí Micro-batch ‚Üí Processing ‚Üí Output Sink\n",
    "     ‚Üì              ‚Üì             ‚Üì            ‚Üì\n",
    "  (files)      (trigger)    (DataFrame)   (Delta)\n",
    "                              API\n",
    "```\n",
    "\n",
    "**Dlaczego Streaming?**\n",
    "- **Low latency**: Sekundy/minuty zamiast godzin\n",
    "- **Real-time insights**: Dashboards, alerts, ML inference\n",
    "- **Continuous processing**: Nie czeka na batch window\n",
    "- **Event-driven**: React to data as it arrives\n",
    "\n",
    "### Batch vs Streaming Decision Matrix\n",
    "\n",
    "| Cecha | Batch (COPY INTO) | Streaming (Auto Loader) |\n",
    "|-------|-------------------|-------------------------|\n",
    "| **Latency** | Minutes-Hours | Seconds-Minutes |\n",
    "| **File arrival** | Large files, scheduled | Continuous small files |\n",
    "| **Complexity** | Low | Medium |\n",
    "| **Cost** | Lower (runs on schedule) | Higher (always on) |\n",
    "| **Use Case** | Daily ETL, reports | Real-time dashboards, CDC |\n",
    "| **Idempotency** | Built-in (file tracking) | Built-in (checkpoint) |\n",
    "| **Schema evolution** | Manual | Automatic (rescue mode) |\n",
    "\n",
    "**Kiedy u≈ºywaƒá Streaming:**\n",
    "- ‚úÖ Dane przychodzƒÖ kontinously (< 1h intervals)\n",
    "- ‚úÖ Potrzebujesz low latency (< 5 min)\n",
    "- ‚úÖ Ma≈Çe pliki (< 100MB each)\n",
    "- ‚úÖ Event-driven applications\n",
    "- ‚úÖ Real-time dashboards/analytics\n",
    "\n",
    "**Kiedy u≈ºywaƒá Batch:**\n",
    "- ‚úÖ Dane przychodzƒÖ w scheduled intervals (hourly/daily)\n",
    "- ‚úÖ Du≈ºe pliki (> 1GB)\n",
    "- ‚úÖ Latency nie jest krytyczna\n",
    "- ‚úÖ Lower cost requirement\n",
    "- ‚úÖ Simple operational model\n",
    "\n",
    "### Auto Loader (cloudFiles) - The Game Changer\n",
    "\n",
    "**Co to jest Auto Loader?**\n",
    "- Databricks-managed streaming source (`cloudFiles`)\n",
    "- Automatyczne file discovery (nie trzeba manually list files)\n",
    "- Incremental processing (tylko nowe pliki)\n",
    "- Schema inference & evolution\n",
    "- File notification (nie trzeba skanowaƒá folderu)\n",
    "- Checkpoint management\n",
    "\n",
    "**Dlaczego Auto Loader > readStream.format(\"json\")?**\n",
    "\n",
    "| Feature | Auto Loader | Standard readStream |\n",
    "|---------|-------------|---------------------|\n",
    "| File discovery | Automatic (notifications) | Manual (directory listing) |\n",
    "| Schema inference | Built-in + evolution | Manual definition |\n",
    "| Small files | Optimized | Slow (many tasks) |\n",
    "| Scalability | Millions of files | Struggles at 100k+ |\n",
    "| Cost | Lower (notifications) | Higher (continuous scan) |\n",
    "\n",
    "**Auto Loader Architecture:**\n",
    "```\n",
    "Cloud Storage ‚Üí File Notification ‚Üí Databricks ‚Üí Processing ‚Üí Delta Lake\n",
    "    (S3)           (SQS/EventGrid)      (Auto        (Spark)      (Target)\n",
    "                                        Loader)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb98ae5",
   "metadata": {},
   "source": [
    "## Izolacja per u≈ºytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalog√≥w i schemat√≥w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d17cacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e138d6",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych ≈õrodowiskowych dla streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2934dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Wy≈õwietl kontekst u≈ºytkownika\n",
    "print(\"=== Kontekst u≈ºytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"U≈ºytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog i schemat jako domy≈õlne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# ≈öcie≈ºki do danych streaming\n",
    "ORDERS_STREAMING_PATH = f\"{DATASET_BASE_PATH}/orders\"  # Folder z plikami streaming\n",
    "CHECKPOINT_BASE = f\"/tmp/{raw_user}/streaming_checkpoints\"\n",
    "\n",
    "# Cleanup previous checkpoints (dla demo)\n",
    "try:\n",
    "    dbutils.fs.rm(CHECKPOINT_BASE, True)\n",
    "    print(f\"‚úì Wyczy≈õcono poprzednie checkpoints\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"\\n=== ≈öcie≈ºki dla Streaming ===\")\n",
    "print(f\"Streaming source: {ORDERS_STREAMING_PATH}\")\n",
    "print(f\"Checkpoint base: {CHECKPOINT_BASE}\")\n",
    "\n",
    "# Wy≈õwietl dostƒôpne pliki streaming\n",
    "print(f\"\\n=== Dostƒôpne pliki streaming ===\")\n",
    "try:\n",
    "    files = dbutils.fs.ls(ORDERS_STREAMING_PATH)\n",
    "    stream_files = [f for f in files if f.name.startswith(\"orders_stream_\")]\n",
    "    print(f\"Znaleziono {len(stream_files)} plik√≥w streaming:\")\n",
    "    for f in stream_files[:5]:  # Poka≈º pierwsze 5\n",
    "        print(f\"  - {f.name} ({f.size} bytes)\")\n",
    "    if len(stream_files) > 5:\n",
    "        print(f\"  ... i {len(stream_files) - 5} wiƒôcej\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Nie mo≈ºna wy≈õwietliƒá plik√≥w: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e057799",
   "metadata": {},
   "source": [
    "## Sekcja 1: Structured Streaming Basics - readStream & writeStream\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Structured Streaming opiera siƒô na dw√≥ch podstawowych operacjach:\n",
    "- **`readStream`**: Czyta dane jako stream (unbounded DataFrame)\n",
    "- **`writeStream`**: Zapisuje stream do sink (Delta, Parquet, console)\n",
    "\n",
    "**Podstawowa sk≈Çadnia:**\n",
    "```python\n",
    "# Read stream\n",
    "df_stream = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .load(\"/path/to/files\")\n",
    "\n",
    "# Write stream\n",
    "query = df_stream.writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"/path/to/checkpoint\") \\\n",
    "    .toTable(\"target_table\")\n",
    "```\n",
    "\n",
    "**Kluczowe koncepty:**\n",
    "\n",
    "**1. Output Modes:**\n",
    "- `append`: Tylko nowe rekordy (most common)\n",
    "- `complete`: Ca≈Ça tabela wynikowa (tylko dla agregacji)\n",
    "- `update`: Tylko zmienione rekordy (dla agregacji z watermark)\n",
    "\n",
    "**2. Checkpoint Location:**\n",
    "- ObowiƒÖzkowy dla production streams\n",
    "- Przechowuje offset/progress dla fault tolerance\n",
    "- Umo≈ºliwia restart bez duplikacji/utraty danych\n",
    "\n",
    "**3. Trigger Modes:**\n",
    "- `once`: Jednorazowe przetworzenie (batch-like)\n",
    "- `availableNow`: Przetworz wszystko co jest dostƒôpne, potem zatrzymaj\n",
    "- `processingTime`: Micro-batch co X sekund/minut\n",
    "- `continuous`: Low-latency continuous processing (experimental)\n",
    "\n",
    "**Dlaczego to wa≈ºne:**\n",
    "- Exactly-once semantics z checkpoint\n",
    "- Fault tolerance (restart bez duplikacji)\n",
    "- Kontrola nad throughput (maxFilesPerTrigger)\n",
    "- Monitoring & observability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8848162b",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 1.1: Basic readStream z JSON files\n",
    "\n",
    "**Cel:** Utworzyƒá prosty streaming pipeline czytajƒÖcy JSON files i zapisujƒÖcy do Delta table.\n",
    "\n",
    "**Podej≈õcie:**\n",
    "1. Przygotuj target Delta table\n",
    "2. U≈ºyj readStream do czytania JSON\n",
    "3. U≈ºyj writeStream do zapisu do Delta\n",
    "4. Monitor streaming query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4a2fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.1 - Basic Streaming with readStream/writeStream\n",
    "\n",
    "TARGET_TABLE = f\"{BRONZE_SCHEMA}.orders_streaming_basic\"\n",
    "CHECKPOINT_PATH_BASIC = f\"{CHECKPOINT_BASE}/basic_stream\"\n",
    "\n",
    "print(f\"=== Przyk≈Çad 1.1: Basic Streaming ===\\n\")\n",
    "print(f\"Target table: {TARGET_TABLE}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_PATH_BASIC}\\n\")\n",
    "\n",
    "# Krok 1: Utw√≥rz target Delta table (opcjonalne - writeStream mo≈ºe auto-create)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TARGET_TABLE} (\n",
    "  order_id STRING,\n",
    "  customer_id STRING,\n",
    "  order_date STRING,\n",
    "  total_amount DOUBLE,\n",
    "  payment_method STRING,\n",
    "  product_id STRING,\n",
    "  quantity INT,\n",
    "  _processing_timestamp TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úì Tabela {TARGET_TABLE} gotowa\\n\")\n",
    "\n",
    "# Krok 2: Zdefiniuj explicit schema (zalecane dla production)\n",
    "orders_schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"total_amount\", DoubleType(), True),\n",
    "    StructField(\"payment_method\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Krok 3: readStream - czytaj JSON files jako stream\n",
    "df_stream = (spark.readStream\n",
    "    .format(\"json\")\n",
    "    .schema(orders_schema)  # Explicit schema\n",
    "    .option(\"maxFilesPerTrigger\", 2)  # Przetw. max 2 pliki per trigger\n",
    "    .load(ORDERS_STREAMING_PATH)\n",
    ")\n",
    "\n",
    "print(\"‚úì Stream reader skonfigurowany\")\n",
    "print(f\"  Format: JSON\")\n",
    "print(f\"  Max files per trigger: 2\")\n",
    "print(f\"  Schema: Explicit ({len(orders_schema.fields)} kolumn)\\n\")\n",
    "\n",
    "# Krok 4: Dodaj processing timestamp\n",
    "df_stream_with_ts = df_stream.withColumn(\n",
    "    \"_processing_timestamp\", \n",
    "    F.current_timestamp()\n",
    ")\n",
    "\n",
    "# Krok 5: writeStream - zapisz do Delta table\n",
    "print(\"üöÄ Uruchamianie streaming query...\\n\")\n",
    "\n",
    "query = (df_stream_with_ts.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")  # Tylko nowe rekordy\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH_BASIC)\n",
    "    .trigger(availableNow=True)  # Przetworz wszystko i stop\n",
    "    .toTable(TARGET_TABLE)\n",
    ")\n",
    "\n",
    "# Czekaj na zako≈Ñczenie (availableNow auto-stops)\n",
    "query.awaitTermination()\n",
    "\n",
    "print(\"‚úÖ Stream zako≈Ñczony\\n\")\n",
    "\n",
    "# Krok 6: Sprawd≈∫ wyniki\n",
    "count = spark.table(TARGET_TABLE).count()\n",
    "print(f\"=== Wyniki ===\")\n",
    "print(f\"Za≈Çadowano rekord√≥w: {count}\\n\")\n",
    "\n",
    "# Poka≈º przyk≈Çadowe dane\n",
    "print(\"Przyk≈Çadowe dane:\")\n",
    "display(spark.table(TARGET_TABLE).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0584a28",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "**`readStream` options:**\n",
    "- `schema`: Explicit schema (zalecane) - szybsze ni≈º inference\n",
    "- `maxFilesPerTrigger`: Kontrola throughput - nie przeciƒÖ≈ºaj klastra\n",
    "- `format(\"json\")`: Wspiera JSON, CSV, Parquet, Avro, ORC\n",
    "\n",
    "**`writeStream` options:**\n",
    "- `outputMode(\"append\")`: Tylko nowe rekordy (najbardziej efektywne)\n",
    "- `checkpointLocation`: OBOWIƒÑZKOWE dla production - fault tolerance\n",
    "- `trigger(availableNow=True)`: Przetworz wszystko co jest + stop (batch-like)\n",
    "\n",
    "**`trigger` modes:**\n",
    "- `availableNow=True`: One-time processing (zalecane dla scheduled jobs)\n",
    "- `once=True`: Legacy version of availableNow\n",
    "- `processingTime=\"10 seconds\"`: Micro-batch co 10s (always-on)\n",
    "- `continuous=\"1 second\"`: Ultra-low latency (experimental)\n",
    "\n",
    "**Checkpoint:**\n",
    "- Przechowuje offset (kt√≥re pliki przetworzone)\n",
    "- Umo≈ºliwia restart bez duplikacji\n",
    "- Nie usuwaj checkpoint location je≈õli chcesz incremental processing!\n",
    "\n",
    "**üí° Best Practice**: Zawsze u≈ºywaj `availableNow=True` dla scheduled jobs (batch-like streaming)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99c0fe6",
   "metadata": {},
   "source": [
    "## Sekcja 2: Auto Loader (cloudFiles) - Deep Dive\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Auto Loader to Databricks-managed streaming source zoptymalizowany dla incremental file ingestion z cloud storage.\n",
    "\n",
    "**Kluczowe zalety Auto Loader:**\n",
    "\n",
    "**1. Automatic File Discovery:**\n",
    "- Nie trzeba manually list files w folderze\n",
    "- File notification (SQS/EventGrid) zamiast directory listing\n",
    "- Skaluje do millions of files\n",
    "\n",
    "**2. Schema Inference & Evolution:**\n",
    "- Automatyczne wykrywanie schema z sample files\n",
    "- `cloudFiles.schemaEvolutionMode`: addNewColumns, rescue, failOnNewColumns\n",
    "- Rescue columns dla unexpected data\n",
    "\n",
    "**3. File Notification Modes:**\n",
    "- **Directory listing** (default < 10k files): Skanuje folder\n",
    "- **File notification** (> 10k files): Event-driven (SQS/EventGrid/Event Hub)\n",
    "- Automatyczny wyb√≥r based on scale\n",
    "\n",
    "**4. Performance Optimizations:**\n",
    "- Batch small files together\n",
    "- Parallel processing\n",
    "- Efficient checkpointing\n",
    "\n",
    "**Auto Loader Syntax:**\n",
    "```python\n",
    "df = spark.readStream \\\n",
    "    .format(\"cloudFiles\") \\  # Magic format!\n",
    "    .option(\"cloudFiles.format\", \"json\") \\  # Source format\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/path\") \\  # Schema persistence\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\") \\  # Type inference\n",
    "    .load(\"/path/to/files\")\n",
    "```\n",
    "\n",
    "**Por√≥wnanie: Standard readStream vs Auto Loader:**\n",
    "\n",
    "| Feature | readStream.format(\"json\") | readStream.format(\"cloudFiles\") |\n",
    "|---------|---------------------------|----------------------------------|\n",
    "| File discovery | Manual listing | Automatic notifications |\n",
    "| Performance | Slow for 10k+ files | Fast for millions |\n",
    "| Schema inference | On each start | Cached, incremental |\n",
    "| Schema evolution | Manual | Automatic |\n",
    "| Small files | Many small tasks | Optimized batching |\n",
    "| Cost | Higher (scan overhead) | Lower (event-driven) |\n",
    "| Setup | Simple | Simple + notifications |\n",
    "\n",
    "**Kiedy u≈ºywaƒá Auto Loader:**\n",
    "- ‚úÖ > 1000 files w folderze\n",
    "- ‚úÖ Files przychodzƒÖ continuously\n",
    "- ‚úÖ Potrzebujesz schema evolution\n",
    "- ‚úÖ Small files (< 10MB each)\n",
    "- ‚úÖ Production pipelines (scale, reliability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f8c5d9",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 2.1: Auto Loader z Schema Inference\n",
    "\n",
    "**Cel:** U≈ºyƒá Auto Loader (cloudFiles) z automatycznym schema inference - najbardziej praktyczne podej≈õcie.\n",
    "\n",
    "**Podej≈õcie:**\n",
    "1. U≈ºyj `format(\"cloudFiles\")` zamiast `format(\"json\")`\n",
    "2. W≈ÇƒÖcz schema inference i evolution\n",
    "3. Zapisz inferred schema do location\n",
    "4. Przetestuj auto-discovery nowych plik√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf70fc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 2.1 - Auto Loader with Schema Inference\n",
    "\n",
    "TARGET_TABLE_AL = f\"{BRONZE_SCHEMA}.orders_autoloader\"\n",
    "CHECKPOINT_PATH_AL = f\"{CHECKPOINT_BASE}/autoloader\"\n",
    "SCHEMA_LOCATION_AL = f\"{CHECKPOINT_BASE}/autoloader_schema\"\n",
    "\n",
    "print(f\"=== Przyk≈Çad 2.1: Auto Loader ===\\n\")\n",
    "print(f\"Target table: {TARGET_TABLE_AL}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_PATH_AL}\")\n",
    "print(f\"Schema location: {SCHEMA_LOCATION_AL}\\n\")\n",
    "\n",
    "# Cleanup previous run (dla demo)\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE_AL}\")\n",
    "\n",
    "# Krok 1: readStream z Auto Loader (cloudFiles)\n",
    "df_autoloader = (spark.readStream\n",
    "    .format(\"cloudFiles\")  # üåü Auto Loader magic!\n",
    "    .option(\"cloudFiles.format\", \"json\")  # Source format\n",
    "    .option(\"cloudFiles.schemaLocation\", SCHEMA_LOCATION_AL)  # Persist schema\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")  # Infer types (not just STRING)\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")  # Auto-add new columns\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\", 3)  # Throttle processing\n",
    "    .load(ORDERS_STREAMING_PATH)\n",
    ")\n",
    "\n",
    "print(\"‚úì Auto Loader reader skonfigurowany\")\n",
    "print(f\"  Format: cloudFiles (JSON)\")\n",
    "print(f\"  Schema inference: ENABLED\")\n",
    "print(f\"  Schema evolution: addNewColumns\")\n",
    "print(f\"  Max files per trigger: 3\\n\")\n",
    "\n",
    "# Krok 2: Dodaj metadata columns\n",
    "df_autoloader_enriched = (df_autoloader\n",
    "    .withColumn(\"_processing_time\", F.current_timestamp())\n",
    "    .withColumn(\"_source_file\", F.input_file_name())\n",
    ")\n",
    "\n",
    "# Krok 3: writeStream do Delta\n",
    "print(\"üöÄ Uruchamianie Auto Loader stream...\\n\")\n",
    "\n",
    "query_al = (df_autoloader_enriched.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_PATH_AL)\n",
    "    .trigger(availableNow=True)  # Process all available + stop\n",
    "    .toTable(TARGET_TABLE_AL)\n",
    ")\n",
    "\n",
    "# Czekaj na zako≈Ñczenie\n",
    "query_al.awaitTermination()\n",
    "\n",
    "print(\"‚úÖ Auto Loader stream zako≈Ñczony\\n\")\n",
    "\n",
    "# Krok 4: Analiza wynik√≥w\n",
    "count = spark.table(TARGET_TABLE_AL).count()\n",
    "print(f\"=== Wyniki ===\")\n",
    "print(f\"Za≈Çadowano rekord√≥w: {count}\")\n",
    "\n",
    "# Sprawd≈∫ inferred schema\n",
    "print(\"\\n=== Inferred Schema ===\")\n",
    "spark.table(TARGET_TABLE_AL).printSchema()\n",
    "\n",
    "# Sprawd≈∫ unikalne source files\n",
    "print(\"\\n=== Source Files ===\")\n",
    "source_files = spark.table(TARGET_TABLE_AL).select(\"_source_file\").distinct().count()\n",
    "print(f\"Przetworzone pliki: {source_files}\")\n",
    "\n",
    "# Poka≈º przyk≈Çadowe dane\n",
    "print(\"\\n=== Przyk≈Çadowe dane ===\")\n",
    "display(spark.table(TARGET_TABLE_AL).limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3280b0e4",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie:**\n",
    "\n",
    "**cloudFiles options:**\n",
    "\n",
    "`cloudFiles.format`:\n",
    "- Source format (json, csv, parquet, avro)\n",
    "- Auto Loader handle parsing\n",
    "\n",
    "`cloudFiles.schemaLocation`:\n",
    "- Persists inferred schema\n",
    "- Checkpoint-like dla schema\n",
    "- Umo≈ºliwia fast restarts (no re-inference)\n",
    "\n",
    "`cloudFiles.inferColumnTypes`:\n",
    "- `true`: Wykrywa INT, DOUBLE, DATE, etc.\n",
    "- `false`: Wszystko jako STRING (szybsze)\n",
    "\n",
    "`cloudFiles.schemaEvolutionMode`:\n",
    "- `addNewColumns`: Auto-add new columns (most flexible)\n",
    "- `rescue`: New columns ‚Üí `_rescued_data` JSON\n",
    "- `failOnNewColumns`: Fail if schema changes (strict)\n",
    "- `none`: No evolution (default)\n",
    "\n",
    "**Auto Loader File Notification:**\n",
    "- < 10k files: Directory listing mode (default)\n",
    "- \\> 10k files: File notification mode (auto-setup)\n",
    "- AWS: SQS queue\n",
    "- Azure: Event Grid\n",
    "- GCP: Pub/Sub\n",
    "\n",
    "**üí° Best Practice:** Zawsze u≈ºywaj Auto Loader zamiast standard readStream dla file-based sources!\n",
    "\n",
    "---\n",
    "\n",
    "## Sekcja 3: Trigger Modes - Kontrola Wykonania Streamu\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Trigger okre≈õla **jak czƒôsto** streaming query wykonuje micro-batches. R√≥≈ºne modes dla r√≥≈ºnych use cases.\n",
    "\n",
    "### Trigger Modes - Szczeg√≥≈Çowy PrzeglƒÖd\n",
    "\n",
    "**1. `trigger(availableNow=True)` - Batch-like Streaming** ‚≠ê ZALECANE\n",
    "\n",
    "```python\n",
    ".trigger(availableNow=True)\n",
    "```\n",
    "\n",
    "**Zachowanie:**\n",
    "- Przetwarza wszystkie dostƒôpne dane\n",
    "- Zatrzymuje siƒô automatycznie po zako≈Ñczeniu\n",
    "- Incremental (u≈ºywa checkpoint)\n",
    "- Idempotent (mo≈ºna uruchomiƒá wielokrotnie)\n",
    "\n",
    "**Use Cases:**\n",
    "- ‚úÖ Scheduled jobs (hourly, daily)\n",
    "- ‚úÖ Backfilling historical data\n",
    "- ‚úÖ Cost optimization (nie always-on)\n",
    "- ‚úÖ Databricks Workflows scheduled runs\n",
    "\n",
    "**2. `trigger(once=True)` - Legacy One-Time**\n",
    "\n",
    "```python\n",
    ".trigger(once=True)\n",
    "```\n",
    "\n",
    "**Zachowanie:**\n",
    "- Legacy version of `availableNow`\n",
    "- Przetwarza jeden micro-batch\n",
    "- Mo≈ºe nie przetworzyƒá wszystkich danych\n",
    "\n",
    "**Use Cases:**\n",
    "- ‚ùå Deprecated, use `availableNow` instead\n",
    "\n",
    "**3. `trigger(processingTime=\"X seconds\")` - Always-On Streaming**\n",
    "\n",
    "```python\n",
    ".trigger(processingTime=\"10 seconds\")\n",
    "```\n",
    "\n",
    "**Zachowanie:**\n",
    "- Uruchamia micro-batch co X sekund/minut\n",
    "- Always-on (nigdy siƒô nie zatrzymuje)\n",
    "- Continuous monitoring\n",
    "\n",
    "**Use Cases:**\n",
    "- ‚úÖ Real-time dashboards (low latency)\n",
    "- ‚úÖ Monitoring & alerting\n",
    "- ‚úÖ CDC pipelines\n",
    "- ‚ö†Ô∏è Higher cost (always running)\n",
    "\n",
    "**4. `trigger(continuous=\"X seconds\")` - Ultra-Low Latency** ‚ö†Ô∏è Experimental\n",
    "\n",
    "```python\n",
    ".trigger(continuous=\"1 second\")\n",
    "```\n",
    "\n",
    "**Zachowanie:**\n",
    "- Continuous processing (nie micro-batches)\n",
    "- Sub-second latency\n",
    "- At-least-once semantics (nie exactly-once!)\n",
    "\n",
    "**Use Cases:**\n",
    "- ‚ö†Ô∏è Experimental - nie u≈ºywaj w production\n",
    "- Research, POCs\n",
    "\n",
    "**5. Default (no trigger specified)**\n",
    "\n",
    "```python\n",
    ".writeStream  # No trigger specified\n",
    "```\n",
    "\n",
    "**Zachowanie:**\n",
    "- Micro-batch ASAP (jak najszybciej)\n",
    "- Similar to `processingTime=\"0 seconds\"`\n",
    "- Always-on\n",
    "\n",
    "### Trigger Modes - Decision Matrix\n",
    "\n",
    "| Trigger Mode | Latency | Cost | Use Case | Production Ready |\n",
    "|--------------|---------|------|----------|------------------|\n",
    "| `availableNow=True` | Minutes | Low | Scheduled jobs | ‚úÖ YES |\n",
    "| `processingTime=\"10s\"` | Seconds | High | Real-time | ‚úÖ YES |\n",
    "| `once=True` | Minutes | Low | Legacy | ‚ö†Ô∏è Use availableNow |\n",
    "| `continuous=\"1s\"` | Milliseconds | High | Ultra low-latency | ‚ùå Experimental |\n",
    "| Default (none) | Seconds | High | Always-on | ‚ö†Ô∏è Rare |\n",
    "\n",
    "**üí° Best Practice:**\n",
    "- **Scheduled jobs**: `availableNow=True`\n",
    "- **Real-time dashboards**: `processingTime=\"30 seconds\"` lub `processingTime=\"1 minute\"`\n",
    "- **Cost optimization**: Zawsze preferuj `availableNow` je≈õli mo≈ºesz tolerowaƒá minutes latency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3f60b1",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 3.1: Por√≥wnanie Trigger Modes\n",
    "\n",
    "**Cel:** Por√≥wnaƒá r√≥≈ºne trigger modes i zobaczyƒá ich wp≈Çyw na execution.\n",
    "\n",
    "**Podej≈õcie:**\n",
    "1. Uruchom stream z `availableNow=True`\n",
    "2. Por√≥wnaj z `processingTime`\n",
    "3. Sprawd≈∫ monitoring metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80d891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 3.1 - Por√≥wnanie Trigger Modes\n",
    "\n",
    "print(\"=== Przyk≈Çad 3.1: Trigger Modes ===\\n\")\n",
    "\n",
    "# Test 1: availableNow (batch-like)\n",
    "print(\"üìä Test 1: trigger(availableNow=True)\")\n",
    "print(\"  Typ: Batch-like streaming\")\n",
    "print(\"  Zachowanie: Przetworz wszystko ‚Üí zatrzymaj\\n\")\n",
    "\n",
    "TARGET_TABLE_TRIGGER1 = f\"{BRONZE_SCHEMA}.orders_trigger_availablenow\"\n",
    "CHECKPOINT_TRIGGER1 = f\"{CHECKPOINT_BASE}/trigger_availablenow\"\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE_TRIGGER1}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df_stream_trigger1 = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_TRIGGER1}_schema\")\n",
    "    .load(ORDERS_STREAMING_PATH)\n",
    ")\n",
    "\n",
    "query_trigger1 = (df_stream_trigger1.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_TRIGGER1)\n",
    "    .trigger(availableNow=True)  # üéØ Batch-like\n",
    "    .toTable(TARGET_TABLE_TRIGGER1)\n",
    ")\n",
    "\n",
    "query_trigger1.awaitTermination()\n",
    "elapsed1 = time.time() - start_time\n",
    "\n",
    "count1 = spark.table(TARGET_TABLE_TRIGGER1).count()\n",
    "print(f\"‚úÖ Zako≈Ñczony w {elapsed1:.2f}s\")\n",
    "print(f\"   Za≈Çadowano: {count1} rekord√≥w\\n\")\n",
    "\n",
    "print(\"-\" * 60 + \"\\n\")\n",
    "\n",
    "# Test 2: processingTime (always-on simulation)\n",
    "print(\"üìä Test 2: trigger(processingTime='5 seconds')\")\n",
    "print(\"  Typ: Always-on streaming\")\n",
    "print(\"  Zachowanie: Micro-batch co 5s (symulacja)\\n\")\n",
    "print(\"  ‚ö†Ô∏è  Uwaga: To uruchomi always-on stream!\")\n",
    "print(\"  ‚ö†Ô∏è  Trzeba bƒôdzie go rƒôcznie zatrzymaƒá\\n\")\n",
    "\n",
    "print(\"  Kod przyk≈Çadowy (NIE URUCHAMIAJ teraz):\")\n",
    "print(\"\"\"\n",
    "    query = (df.writeStream\n",
    "        .format(\"delta\")\n",
    "        .outputMode(\"append\")\n",
    "        .option(\"checkpointLocation\", \"/path\")\n",
    "        .trigger(processingTime=\"5 seconds\")  # Co 5s\n",
    "        .toTable(\"target\")\n",
    "    )\n",
    "    \n",
    "    # Stream dzia≈Ça w tle\n",
    "    # Zatrzymaj: query.stop()\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "\n",
    "# Podsumowanie\n",
    "print(\"=== Por√≥wnanie ===\\n\")\n",
    "\n",
    "print(\"| Metric | availableNow | processingTime |\")\n",
    "print(\"|--------|--------------|----------------|\")\n",
    "print(f\"| Execution time | {elapsed1:.2f}s | Infinite (always-on) |\")\n",
    "print(f\"| Records | {count1} | Continuous |\")\n",
    "print(\"| Cost | Low (one-time) | High (always running) |\")\n",
    "print(\"| Latency | Minutes | Seconds |\")\n",
    "print(\"| Use Case | Scheduled jobs | Real-time |\")\n",
    "\n",
    "print(\"\\nüí° Zalecenie: U≈ºywaj availableNow dla scheduled jobs (cost-effective)\")\n",
    "print(\"üí° U≈ºywaj processingTime tylko gdy potrzebujesz real-time (<5min latency)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb799071",
   "metadata": {},
   "source": [
    "## Sekcja 4: Watermarking & Late Data Handling\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Watermarking to mechanizm obs≈Çugi **late-arriving data** (dane sp√≥≈∫nione) w streaming aggregations.\n",
    "\n",
    "### Problem: Late Data\n",
    "\n",
    "W real-world streaming, dane nie zawsze przychodzƒÖ w kolejno≈õci:\n",
    "```\n",
    "Event Time: 10:00 ‚Üí 10:01 ‚Üí 10:02 ‚Üí 10:00 (LATE!)\n",
    "Arrival Time: 10:05 ‚Üí 10:06 ‚Üí 10:07 ‚Üí 10:08\n",
    "```\n",
    "\n",
    "**Pytanie:** Jak d≈Çugo czekaƒá na sp√≥≈∫nione dane przed finalizacjƒÖ agregacji?\n",
    "\n",
    "### Watermark - RozwiƒÖzanie\n",
    "\n",
    "**Watermark** = threshold for late data tolerance\n",
    "\n",
    "```python\n",
    "df.withWatermark(\"event_time\", \"10 minutes\")\n",
    "```\n",
    "\n",
    "**Znaczenie:**\n",
    "- Czekaj do 10 minut na sp√≥≈∫nione dane\n",
    "- Dane starsze ni≈º watermark sƒÖ **odrzucane**\n",
    "- Finalizuj agregacje gdy watermark przekroczy window\n",
    "\n",
    "### Watermark Behavior\n",
    "\n",
    "**Przyk≈Çad:** Watermark \"10 minutes\"\n",
    "\n",
    "```\n",
    "Current max event_time: 12:00\n",
    "Watermark: 12:00 - 10min = 11:50\n",
    "\n",
    "Incoming event @ 11:55 ‚Üí ACCEPTED (> watermark)\n",
    "Incoming event @ 11:45 ‚Üí DROPPED (< watermark)\n",
    "```\n",
    "\n",
    "**Watermark Movement:**\n",
    "- Watermark ro≈õnie tylko w g√≥rƒô (monotonic)\n",
    "- Based on max observed event_time\n",
    "- Never moves backward\n",
    "\n",
    "### Watermark + Windows\n",
    "\n",
    "**Tumbling Window Example:**\n",
    "```python\n",
    "df.withWatermark(\"event_time\", \"10 minutes\") \\\n",
    "  .groupBy(\n",
    "      F.window(\"event_time\", \"5 minutes\")\n",
    "  ).count()\n",
    "```\n",
    "\n",
    "**Output:**\n",
    "- Window [10:00-10:05] finalized gdy watermark > 10:05\n",
    "- Late data < watermark ‚Üí dropped\n",
    "- Late data > watermark ‚Üí included\n",
    "\n",
    "### Output Modes z Watermark\n",
    "\n",
    "**1. `append` mode:** (most common)\n",
    "- Wypuszcza windows tylko gdy sƒÖ finalized (watermark passed)\n",
    "- Once outputted, never updated\n",
    "- Best for late data tolerance\n",
    "\n",
    "**2. `update` mode:**\n",
    "- Wypuszcza updates dla windows\n",
    "- Can update same window multiple times\n",
    "- More output, less complete\n",
    "\n",
    "**3. `complete` mode:**\n",
    "- Outputs entire result table\n",
    "- Not recommended for streaming (too much data)\n",
    "\n",
    "### Kiedy u≈ºywaƒá Watermark?\n",
    "\n",
    "**Potrzebujesz watermark gdy:**\n",
    "- ‚úÖ Streaming aggregations (groupBy + window)\n",
    "- ‚úÖ Joins with event-time\n",
    "- ‚úÖ Late data tolerance required\n",
    "- ‚úÖ Need to finalize windows\n",
    "\n",
    "**NIE potrzebujesz watermark gdy:**\n",
    "- ‚ùå No aggregations (simple append)\n",
    "- ‚ùå No event-time (only processing-time)\n",
    "- ‚ùå No late data concerns\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "**1. Wyb√≥r watermark threshold:**\n",
    "- Zbyt ma≈Çy (1 min): Du≈ºo dropped late data\n",
    "- Zbyt du≈ºy (1 day): State grows, memory issues\n",
    "- Sweet spot: 10-30 minutes dla most use cases\n",
    "\n",
    "**2. Event-time column:**\n",
    "- Musi byƒá TIMESTAMP\n",
    "- Powinien reprezentowaƒá event creation time (nie arrival)\n",
    "- Mieƒá timezone awareness\n",
    "\n",
    "**3. Monitoring:**\n",
    "- Track dropped late data metrics\n",
    "- Monitor watermark lag\n",
    "- Adjust threshold based on observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b139a7",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 4.1: Watermarking w praktyce\n",
    "\n",
    "**Cel:** Zaimplementowaƒá watermarking dla streaming aggregation z oknem czasowym.\n",
    "\n",
    "**Podej≈õcie:**\n",
    "1. Parsuj event_time z danych\n",
    "2. Dodaj watermark (10 minutes tolerance)\n",
    "3. Window aggregation (5-minute tumbling windows)\n",
    "4. Obserwuj finalization behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961aa715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 4.1 - Watermarking for Late Data\n",
    "\n",
    "TARGET_TABLE_WM = f\"{SILVER_SCHEMA}.orders_windowed_aggregates\"\n",
    "CHECKPOINT_WM = f\"{CHECKPOINT_BASE}/watermark_agg\"\n",
    "\n",
    "print(\"=== Przyk≈Çad 4.1: Watermarking & Windowing ===\\n\")\n",
    "print(f\"Target table: {TARGET_TABLE_WM}\")\n",
    "print(f\"Checkpoint: {CHECKPOINT_WM}\\n\")\n",
    "\n",
    "# Cleanup\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE_WM}\")\n",
    "\n",
    "# Krok 1: readStream source data\n",
    "df_stream_wm = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_WM}_schema\")\n",
    "    .load(ORDERS_STREAMING_PATH)\n",
    ")\n",
    "\n",
    "# Krok 2: Parse event_time (z order_date string ‚Üí timestamp)\n",
    "df_with_event_time = df_stream_wm.withColumn(\n",
    "    \"event_time\",\n",
    "    F.to_timestamp(F.col(\"order_date\"), \"yyyy-MM-dd HH:mm:ss\")\n",
    ")\n",
    "\n",
    "print(\"‚úì Event time column dodany\")\n",
    "print(\"  Format: yyyy-MM-dd HH:mm:ss ‚Üí TIMESTAMP\\n\")\n",
    "\n",
    "# Krok 3: Apply Watermark + Window Aggregation\n",
    "df_windowed = (df_with_event_time\n",
    "    .withWatermark(\"event_time\", \"10 minutes\")  # üåä Watermark!\n",
    "    .groupBy(\n",
    "        F.window(\"event_time\", \"5 minutes\"),  # 5-min tumbling windows\n",
    "        \"payment_method\"\n",
    "    )\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"order_count\"),\n",
    "        F.sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_order_value\")\n",
    "    )\n",
    "    .select(\n",
    "        F.col(\"window.start\").alias(\"window_start\"),\n",
    "        F.col(\"window.end\").alias(\"window_end\"),\n",
    "        \"payment_method\",\n",
    "        \"order_count\",\n",
    "        \"total_revenue\",\n",
    "        \"avg_order_value\"\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úì Watermark & Windowing skonfigurowane\")\n",
    "print(\"  Watermark: 10 minutes (late data tolerance)\")\n",
    "print(\"  Window: 5 minutes tumbling\")\n",
    "print(\"  Aggregates: COUNT, SUM, AVG per payment_method\\n\")\n",
    "\n",
    "# Krok 4: writeStream z append mode (finalized windows only)\n",
    "print(\"üöÄ Uruchamianie windowed aggregation stream...\\n\")\n",
    "\n",
    "query_wm = (df_windowed.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")  # Only finalized windows\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_WM)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(TARGET_TABLE_WM)\n",
    ")\n",
    "\n",
    "query_wm.awaitTermination()\n",
    "\n",
    "print(\"‚úÖ Windowed aggregation zako≈Ñczona\\n\")\n",
    "\n",
    "# Krok 5: Analiza wynik√≥w\n",
    "print(\"=== Wyniki Agregacji ===\\n\")\n",
    "\n",
    "result_df = spark.table(TARGET_TABLE_WM).orderBy(\"window_start\", \"payment_method\")\n",
    "window_count = result_df.select(\"window_start\").distinct().count()\n",
    "\n",
    "print(f\"Liczba okien czasowych: {window_count}\")\n",
    "print(f\"≈ÅƒÖczna liczba rekord√≥w agregacji: {result_df.count()}\\n\")\n",
    "\n",
    "print(\"Przyk≈Çadowe wyniki (per window + payment method):\")\n",
    "display(result_df.limit(20))\n",
    "\n",
    "# Krok 6: Visualize windows\n",
    "print(\"\\n=== Wizualizacja Okien Czasowych ===\")\n",
    "windows_summary = result_df.groupBy(\"window_start\", \"window_end\").agg(\n",
    "    F.sum(\"order_count\").alias(\"total_orders\"),\n",
    "    F.sum(\"total_revenue\").alias(\"total_revenue\")\n",
    ").orderBy(\"window_start\")\n",
    "\n",
    "display(windows_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cddd46d",
   "metadata": {},
   "source": [
    "**Wyja≈õnienie Watermarking:**\n",
    "\n",
    "**Co siƒô sta≈Ço:**\n",
    "1. Event time parsed z `order_date` string\n",
    "2. Watermark ustawiony na 10 minut\n",
    "3. 5-minutowe okna (tumbling windows)\n",
    "4. Agregacje per payment_method\n",
    "\n",
    "**Watermark Behavior:**\n",
    "- Window [10:00-10:05] finalized gdy max event_time > 10:15 (10:05 + 10min watermark)\n",
    "- Late data > watermark ‚Üí included\n",
    "- Late data < watermark ‚Üí dropped (nie pokazane w tym demo)\n",
    "\n",
    "**üí° Debugging Late Data:**\n",
    "```python\n",
    "# Monitor dropped records\n",
    "spark.conf.set(\"spark.sql.streaming.metricsEnabled\", \"true\")\n",
    "\n",
    "# Check watermark delays in Spark UI\n",
    "# Streaming tab ‚Üí Query details ‚Üí Watermark\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Sekcja 5: Checkpoint Management\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Checkpoint to **krityczny komponent** streaming pipelines - zapewnia fault tolerance i exactly-once semantics.\n",
    "\n",
    "### Co Jest w Checkpoint?\n",
    "\n",
    "**Checkpoint location przechowuje:**\n",
    "\n",
    "**1. Offsets:**\n",
    "- Kt√≥re pliki/partycje przetworzone\n",
    "- W jakiej kolejno≈õci\n",
    "- Do kt√≥rej pozycji w stream\n",
    "\n",
    "**2. Metadata:**\n",
    "- Query configuration\n",
    "- Schema information\n",
    "- State information (dla stateful operations)\n",
    "\n",
    "**3. State Store** (dla stateful ops):\n",
    "- Aggregation state\n",
    "- Join state\n",
    "- Watermark state\n",
    "\n",
    "### Checkpoint Structure\n",
    "\n",
    "```\n",
    "checkpoint_location/\n",
    "‚îú‚îÄ‚îÄ commits/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 0         # Batch 0 completion marker\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 1\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 2\n",
    "‚îú‚îÄ‚îÄ offsets/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 0         # Batch 0 offsets\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ 1\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 2\n",
    "‚îú‚îÄ‚îÄ sources/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ 0/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ [source-specific data]\n",
    "‚îú‚îÄ‚îÄ state/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ [state store files for stateful ops]\n",
    "‚îî‚îÄ‚îÄ metadata\n",
    "```\n",
    "\n",
    "### Checkpoint Behavior\n",
    "\n",
    "**First Run:**\n",
    "- Creates checkpoint location\n",
    "- Starts from beginning (or latest based on `startingOffsets`)\n",
    "- Writes offset after each batch\n",
    "\n",
    "**Restart:**\n",
    "- Reads last committed offset\n",
    "- Resumes from that point\n",
    "- No duplicate processing (exactly-once)\n",
    "- No data loss\n",
    "\n",
    "**Schema Evolution:**\n",
    "- Checkpoint validates schema compatibility\n",
    "- Incompatible changes ‚Üí fail (protection)\n",
    "- Use `cloudFiles.schemaEvolutionMode` for flexibility\n",
    "\n",
    "### Common Checkpoint Issues\n",
    "\n",
    "**Problem 1: Incompatible schema change**\n",
    "```\n",
    "AnalysisException: Incompatible format/schema\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "- Option A: Delete checkpoint (reprocess all data)\n",
    "- Option B: Use new checkpoint location\n",
    "- Option C: Use schema evolution features\n",
    "\n",
    "**Problem 2: Checkpoint corruption**\n",
    "```\n",
    "StreamingQueryException: Unable to read offsets\n",
    "```\n",
    "\n",
    "**Solution:**\n",
    "- Backup important checkpoints\n",
    "- Delete corrupted checkpoint\n",
    "- Reprocess from beginning\n",
    "\n",
    "**Problem 3: Checkpoint location full**\n",
    "\n",
    "**Solution:**\n",
    "- Monitor checkpoint size\n",
    "- Clean old state (automatic with retention)\n",
    "- Use appropriate checkpoint location (not /tmp)\n",
    "\n",
    "### Checkpoint Best Practices\n",
    "\n",
    "**1. Location Choice:**\n",
    "```python\n",
    "# ‚ùå BAD - ephemeral, mo≈ºe zniknƒÖƒá\n",
    ".option(\"checkpointLocation\", \"/tmp/checkpoint\")\n",
    "\n",
    "# ‚úÖ GOOD - persistent storage\n",
    ".option(\"checkpointLocation\", \"s3://bucket/checkpoints/job1\")\n",
    ".option(\"checkpointLocation\", \"/dbfs/mnt/storage/checkpoints/job1\")\n",
    "```\n",
    "\n",
    "**2. One Checkpoint Per Query:**\n",
    "```python\n",
    "# ‚ùå BAD - reusing checkpoint\n",
    "query1.option(\"checkpointLocation\", \"/path/shared\")\n",
    "query2.option(\"checkpointLocation\", \"/path/shared\")  # B≈ÅƒÑD!\n",
    "\n",
    "# ‚úÖ GOOD - unique per query\n",
    "query1.option(\"checkpointLocation\", \"/path/query1\")\n",
    "query2.option(\"checkpointLocation\", \"/path/query2\")\n",
    "```\n",
    "\n",
    "**3. Checkpoint Cleanup:**\n",
    "```python\n",
    "# Delete checkpoint dla fresh start\n",
    "dbutils.fs.rm(\"/path/to/checkpoint\", recurse=True)\n",
    "\n",
    "# Tylko dla development/testing!\n",
    "# W production: zachowaj checkpoint dla incremental processing\n",
    "```\n",
    "\n",
    "**4. Monitoring:**\n",
    "```python\n",
    "# Check checkpoint size\n",
    "dbutils.fs.ls(\"/path/to/checkpoint\")\n",
    "\n",
    "# Monitor in Spark UI\n",
    "# Streaming tab ‚Üí Active Streams ‚Üí Query Details\n",
    "```\n",
    "\n",
    "**5. Backup Critical Checkpoints:**\n",
    "```bash\n",
    "# Before major changes\n",
    "aws s3 sync s3://bucket/checkpoint s3://bucket/checkpoint_backup\n",
    "```\n",
    "\n",
    "### Checkpoint vs Schema Location\n",
    "\n",
    "**Auto Loader ma DWA persistence locations:**\n",
    "\n",
    "```python\n",
    ".option(\"checkpointLocation\", \"/path/checkpoint\")      # Stream offsets\n",
    ".option(\"cloudFiles.schemaLocation\", \"/path/schema\")   # Inferred schema\n",
    "```\n",
    "\n",
    "**Oba sƒÖ potrzebne:**\n",
    "- `checkpointLocation`: Which files processed\n",
    "- `schemaLocation`: What schema to use\n",
    "\n",
    "**Lifecycle:**\n",
    "- Delete both ‚Üí reprocess all + re-infer schema\n",
    "- Delete checkpoint only ‚Üí reprocess all + use existing schema\n",
    "- Delete schema only ‚Üí keep offsets + re-infer schema"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3830d754",
   "metadata": {},
   "source": [
    "### Przyk≈Çad 5.1: Checkpoint Management & Recovery\n",
    "\n",
    "**Cel:** Zademonstrowaƒá checkpoint behavior - restart bez duplikacji.\n",
    "\n",
    "**Podej≈õcie:**\n",
    "1. Uruchom stream z checkpoint\n",
    "2. Stop in middle\n",
    "3. Restart - observe incremental processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cf478d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przyk≈Çad 5.1 - Checkpoint Management & Recovery\n",
    "\n",
    "TARGET_TABLE_CP = f\"{BRONZE_SCHEMA}.orders_checkpoint_demo\"\n",
    "CHECKPOINT_CP = f\"{CHECKPOINT_BASE}/checkpoint_demo\"\n",
    "\n",
    "print(\"=== Przyk≈Çad 5.1: Checkpoint & Recovery ===\\n\")\n",
    "\n",
    "# Cleanup dla demo\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {TARGET_TABLE_CP}\")\n",
    "try:\n",
    "    dbutils.fs.rm(CHECKPOINT_CP, True)\n",
    "    print(\"‚úì Wyczy≈õcono poprzedni checkpoint\\n\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Run 1: Initial stream (przetworzy tylko czƒô≈õƒá danych)\n",
    "print(\"üìä RUN 1: Initial streaming query\\n\")\n",
    "\n",
    "df_stream_cp = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_CP}_schema\")\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\", 3)  # Tylko 3 pliki\n",
    "    .load(ORDERS_STREAMING_PATH)\n",
    ")\n",
    "\n",
    "query_cp1 = (df_stream_cp.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_CP)\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(TARGET_TABLE_CP)\n",
    ")\n",
    "\n",
    "query_cp1.awaitTermination()\n",
    "\n",
    "count_run1 = spark.table(TARGET_TABLE_CP).count()\n",
    "print(f\"‚úÖ RUN 1 zako≈Ñczony\")\n",
    "print(f\"   Za≈Çadowano: {count_run1} rekord√≥w\\n\")\n",
    "\n",
    "# Sprawd≈∫ checkpoint content\n",
    "print(\"=== Checkpoint Structure ===\")\n",
    "checkpoint_files = dbutils.fs.ls(CHECKPOINT_CP)\n",
    "print(f\"Foldery w checkpoint:\")\n",
    "for f in checkpoint_files:\n",
    "    print(f\"  - {f.name}\")\n",
    "\n",
    "print(\"\\n\" + \"-\" * 60 + \"\\n\")\n",
    "\n",
    "# Run 2: Restart z tym samym checkpoint (incremental)\n",
    "print(\"üìä RUN 2: Restart z existing checkpoint\\n\")\n",
    "print(\"  Checkpoint istnieje - stream resume from last offset\")\n",
    "print(\"  Tylko NOWE pliki bƒôdƒÖ przetworzone (no duplicates)\\n\")\n",
    "\n",
    "df_stream_cp2 = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_CP}_schema\")\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\", 3)\n",
    "    .load(ORDERS_STREAMING_PATH)\n",
    ")\n",
    "\n",
    "query_cp2 = (df_stream_cp2.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", CHECKPOINT_CP)  # Ten sam checkpoint!\n",
    "    .trigger(availableNow=True)\n",
    "    .toTable(TARGET_TABLE_CP)\n",
    ")\n",
    "\n",
    "query_cp2.awaitTermination()\n",
    "\n",
    "count_run2 = spark.table(TARGET_TABLE_CP).count()\n",
    "new_records = count_run2 - count_run1\n",
    "\n",
    "print(f\"‚úÖ RUN 2 zako≈Ñczony\")\n",
    "print(f\"   Total records: {count_run2}\")\n",
    "print(f\"   New records: {new_records}\")\n",
    "print(f\"   Previous records: {count_run1}\\n\")\n",
    "\n",
    "# Verify: check distinct source files\n",
    "source_files = spark.table(TARGET_TABLE_CP) \\\n",
    "    .select(F.input_file_name().alias(\"file\")) \\\n",
    "    .distinct() \\\n",
    "    .count()\n",
    "\n",
    "print(f\"=== Verification ===\")\n",
    "print(f\"Unique source files processed: {source_files}\")\n",
    "print(f\"Total records: {count_run2}\")\n",
    "print(f\"\\nüí° Checkpoint zapewni≈Ç:\")\n",
    "print(f\"   ‚úÖ No duplicates (exact-once semantics)\")\n",
    "print(f\"   ‚úÖ Incremental processing\")\n",
    "print(f\"   ‚úÖ Resume from last offset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f65226b",
   "metadata": {},
   "source": [
    "## Sekcja 6: Schema Evolution w Streaming\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "W production streaming pipelines, **schema changes** sƒÖ nieuniknione:\n",
    "- Nowe kolumny w source data\n",
    "- Zmienione typy danych\n",
    "- Usuniƒôte kolumny\n",
    "\n",
    "Auto Loader oferuje r√≥≈ºne strategie obs≈Çugi schema evolution.\n",
    "\n",
    "### Schema Evolution Modes\n",
    "\n",
    "**1. `addNewColumns` (ZALECANE)**\n",
    "```python\n",
    ".option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "```\n",
    "\n",
    "**Zachowanie:**\n",
    "- Nowe kolumny ‚Üí automatycznie dodane do tabeli\n",
    "- IstniejƒÖce kolumny ‚Üí unchanged\n",
    "- Deleted columns ‚Üí NULL w nowych danych\n",
    "\n",
    "**Use Case:** Production pipelines z flexible schema\n",
    "\n",
    "**2. `rescue`**\n",
    "```python\n",
    ".option(\"cloudFiles.schemaEvolutionMode\", \"rescue\")\n",
    ".option(\"rescuedDataColumn\", \"_rescued_data\")\n",
    "```\n",
    "\n",
    "**Zachowanie:**\n",
    "- Nowe/unexpected columns ‚Üí zapisane w `_rescued_data` (JSON)\n",
    "- Schema tabeli ‚Üí unchanged\n",
    "- Manual inspection & processing later\n",
    "\n",
    "**Use Case:** Strict schema enforcement + monitoring\n",
    "\n",
    "**3. `failOnNewColumns`**\n",
    "```python\n",
    ".option(\"cloudFiles.schemaEvolutionMode\", \"failOnNewColumns\")\n",
    "```\n",
    "\n",
    "**Zachowanie:**\n",
    "- Nowe kolumny ‚Üí FAIL streaming query\n",
    "- Forces manual intervention\n",
    "- No automatic changes\n",
    "\n",
    "**Use Case:** Critical pipelines, strict governance\n",
    "\n",
    "**4. `none` (default)**\n",
    "```python\n",
    "# No schema evolution mode specified\n",
    "```\n",
    "\n",
    "**Zachowanie:**\n",
    "- Schema fixed at first inference\n",
    "- New columns ‚Üí ignored (nie w rescue)\n",
    "- Can cause data loss silently\n",
    "\n",
    "**Use Case:** ‚ùå Nie u≈ºywaj (dangerous)\n",
    "\n",
    "### Schema Evolution Decision Matrix\n",
    "\n",
    "| Mode | New Columns | Type Changes | Failures | Use Case |\n",
    "|------|-------------|--------------|----------|----------|\n",
    "| `addNewColumns` | Auto-add | Fail | Rare | Production (flexible) |\n",
    "| `rescue` | To JSON | Ignore | No | Strict + monitor |\n",
    "| `failOnNewColumns` | Fail | Fail | Yes | Critical |\n",
    "| `none` | Ignored | Ignored | No | ‚ùå Avoid |\n",
    "\n",
    "### mergeSchema dla Delta\n",
    "\n",
    "Opr√≥cz Auto Loader evolution, Delta Lake ma w≈Çasne `mergeSchema`:\n",
    "\n",
    "```python\n",
    "df.write.format(\"delta\") \\\n",
    "  .mode(\"append\") \\\n",
    "  .option(\"mergeSchema\", \"true\") \\  # Delta schema evolution\n",
    "  .saveAsTable(\"table\")\n",
    "```\n",
    "\n",
    "**Kombinacja:**\n",
    "- Auto Loader evolution: Source files ‚Üí DataFrame\n",
    "- Delta mergeSchema: DataFrame ‚Üí Delta table\n",
    "\n",
    "**Best Practice:** U≈ºywaj obu dla full pipeline evolution\n",
    "\n",
    "### Monitoring Schema Changes\n",
    "\n",
    "```python\n",
    "# Read schema location to see inferred schema\n",
    "schema_files = dbutils.fs.ls(\"/path/to/schemaLocation\")\n",
    "\n",
    "# Check Delta table schema history\n",
    "spark.sql(\"DESCRIBE HISTORY table_name\")\n",
    "\n",
    "# Monitor _rescued_data column (if using rescue mode)\n",
    "spark.table(\"table\").filter(col(\"_rescued_data\").isNotNull()).count()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Sekcja 7: Best Practices - Streaming Production Pipelines\n",
    "\n",
    "### 1. Zawsze u≈ºywaj Auto Loader (cloudFiles)\n",
    "\n",
    "```python\n",
    "# ‚ùå DON'T - standard readStream\n",
    "spark.readStream.format(\"json\").load(\"/path\")\n",
    "\n",
    "# ‚úÖ DO - Auto Loader\n",
    "spark.readStream.format(\"cloudFiles\") \\\n",
    "  .option(\"cloudFiles.format\", \"json\") \\\n",
    "  .load(\"/path\")\n",
    "```\n",
    "\n",
    "**Dlaczego:** Scalability, performance, schema evolution\n",
    "\n",
    "### 2. Explicit Checkpoint Locations\n",
    "\n",
    "```python\n",
    "# ‚ùå DON'T - ephemeral location\n",
    ".option(\"checkpointLocation\", \"/tmp/checkpoint\")\n",
    "\n",
    "# ‚úÖ DO - persistent storage\n",
    ".option(\"checkpointLocation\", \"s3://bucket/checkpoints/pipeline_v1\")\n",
    "```\n",
    "\n",
    "**Dlaczego:** Fault tolerance, no data loss\n",
    "\n",
    "### 3. U≈ºywaj availableNow dla Scheduled Jobs\n",
    "\n",
    "```python\n",
    "# ‚ùå DON'T - always-on unless necessary\n",
    ".trigger(processingTime=\"10 seconds\")\n",
    "\n",
    "# ‚úÖ DO - batch-like dla scheduled\n",
    ".trigger(availableNow=True)\n",
    "```\n",
    "\n",
    "**Dlaczego:** Cost optimization (pay only when running)\n",
    "\n",
    "### 4. Schema Evolution Strategy\n",
    "\n",
    "```python\n",
    "# ‚úÖ DO - flexible schema evolution\n",
    ".option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    ".option(\"cloudFiles.schemaLocation\", \"/path/schema\")\n",
    "\n",
    "# Combined with Delta mergeSchema\n",
    ".option(\"mergeSchema\", \"true\")\n",
    "```\n",
    "\n",
    "**Dlaczego:** Handle schema changes gracefully\n",
    "\n",
    "### 5. Throttle Processing (maxFilesPerTrigger)\n",
    "\n",
    "```python\n",
    "# ‚ùå DON'T - process unlimited files\n",
    "spark.readStream.format(\"cloudFiles\").load(\"/path\")\n",
    "\n",
    "# ‚úÖ DO - throttle for stability\n",
    ".option(\"cloudFiles.maxFilesPerTrigger\", 1000)\n",
    "```\n",
    "\n",
    "**Dlaczego:** Prevent cluster overload, stable processing\n",
    "\n",
    "### 6. Monitoring & Alerting\n",
    "\n",
    "```python\n",
    "# Enable metrics\n",
    "spark.conf.set(\"spark.sql.streaming.metricsEnabled\", \"true\")\n",
    "\n",
    "# Log query progress\n",
    "query.lastProgress  # JSON with metrics\n",
    "\n",
    "# Monitor in Spark UI\n",
    "# Streaming tab ‚Üí Active/Completed Queries\n",
    "```\n",
    "\n",
    "**Metrics to monitor:**\n",
    "- `inputRowsPerSecond`: Incoming rate\n",
    "- `processedRowsPerSecond`: Processing rate\n",
    "- `batchDuration`: Time per micro-batch\n",
    "- `numInputRows`: Rows in batch\n",
    "- `stateMemory`: State size (for stateful ops)\n",
    "\n",
    "### 7. Error Handling\n",
    "\n",
    "```python\n",
    "# ‚ùå DON'T - let stream fail silently\n",
    "query.start()\n",
    "\n",
    "# ‚úÖ DO - monitor status\n",
    "query = stream.start()\n",
    "try:\n",
    "    query.awaitTermination()\n",
    "except Exception as e:\n",
    "    # Log error, send alert\n",
    "    print(f\"Stream failed: {e}\")\n",
    "    # query.stop()\n",
    "```\n",
    "\n",
    "### 8. Idempotency & Reprocessing\n",
    "\n",
    "```python\n",
    "# Design for idempotency\n",
    "# - Checkpoint enables exactly-once\n",
    "# - Delta MERGE for upserts\n",
    "# - Unique keys for deduplication\n",
    "\n",
    "# Safe to rerun:\n",
    "dbutils.fs.rm(checkpoint_path, True)  # Fresh start\n",
    "query.start()  # Reprocess all data\n",
    "```\n",
    "\n",
    "### 9. Partitioning Strategy\n",
    "\n",
    "```python\n",
    "# Partition Bronze by date for efficient queries\n",
    ".partitionBy(\"_processing_date\")\n",
    "\n",
    "# Don't over-partition (< 1GB per partition)\n",
    "# Don't under-partition (> 10GB per partition)\n",
    "```\n",
    "\n",
    "### 10. Testing & Validation\n",
    "\n",
    "```python\n",
    "# Test with small dataset first\n",
    ".option(\"cloudFiles.maxFilesPerTrigger\", 1)\n",
    "\n",
    "# Validate row counts\n",
    "source_count = # from source\n",
    "target_count = spark.table(\"target\").count()\n",
    "assert source_count == target_count\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = spark.table(\"target\") \\\n",
    "  .groupBy(\"id\").count() \\\n",
    "  .filter(\"count > 1\").count()\n",
    "assert duplicates == 0\n",
    "```\n",
    "\n",
    "### Quick Reference Card\n",
    "\n",
    "**Production Streaming Checklist:**\n",
    "\n",
    "```python\n",
    "df = (spark.readStream\n",
    "    .format(\"cloudFiles\")  # ‚úÖ Auto Loader\n",
    "    .option(\"cloudFiles.format\", \"json\")\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/schema\")  # ‚úÖ Schema persistence\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")  # ‚úÖ Evolution\n",
    "    .option(\"cloudFiles.maxFilesPerTrigger\", 1000)  # ‚úÖ Throttle\n",
    "    .option(\"cloudFiles.inferColumnTypes\", \"true\")  # ‚úÖ Type inference\n",
    "    .load(\"/data\")\n",
    ")\n",
    "\n",
    "query = (df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"/checkpoint\")  # ‚úÖ Persistent\n",
    "    .option(\"mergeSchema\", \"true\")  # ‚úÖ Delta evolution\n",
    "    .trigger(availableNow=True)  # ‚úÖ Cost-effective\n",
    "    .toTable(\"target\")\n",
    ")\n",
    "```\n",
    "\n",
    "**Monitoring:**\n",
    "- Spark UI ‚Üí Streaming tab\n",
    "- CloudWatch/Log Analytics metrics\n",
    "- Alert on failures\n",
    "- Track watermark lag (stateful ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e31e10",
   "metadata": {},
   "source": [
    "## Sekcja 8: Troubleshooting Streaming Queries\n",
    "\n",
    "### Problem 1: Stream jest powolny (low throughput)\n",
    "\n",
    "**Objawy:**\n",
    "- `processedRowsPerSecond` << `inputRowsPerSecond`\n",
    "- Batch duration > 1 minute\n",
    "- Growing input backlog\n",
    "\n",
    "**Przyczyny & RozwiƒÖzania:**\n",
    "\n",
    "**1. Too many small files**\n",
    "```python\n",
    "# Solution: Increase maxFilesPerTrigger\n",
    ".option(\"cloudFiles.maxFilesPerTrigger\", 1000)  # by≈Ço: 10\n",
    "```\n",
    "\n",
    "**2. Insufficient cluster resources**\n",
    "```python\n",
    "# Solution: Scale up cluster\n",
    "# Dodaj workers lub zwiƒôksz executor memory\n",
    "```\n",
    "\n",
    "**3. Expensive transformations**\n",
    "```python\n",
    "# Solution: Optimize queries\n",
    "# - Avoid UDFs\n",
    "# - Use built-in functions\n",
    "# - Cache intermediate results (ostro≈ºnie w streaming!)\n",
    "```\n",
    "\n",
    "**4. Small micro-batches**\n",
    "```python\n",
    "# Solution: Increase batch size\n",
    ".option(\"maxBytesPerTrigger\", \"10g\")  # by≈Ço: 1g\n",
    "```\n",
    "\n",
    "### Problem 2: Checkpoint incompatible schema\n",
    "\n",
    "**Objawy:**\n",
    "```\n",
    "StreamingQueryException: Incompatible checkpoint schema\n",
    "```\n",
    "\n",
    "**Przyczyny:**\n",
    "- Schema changed drastically (type changes)\n",
    "- Different checkpoint dla different query\n",
    "\n",
    "**RozwiƒÖzania:**\n",
    "\n",
    "**Option 1: Delete checkpoint (reprocess all)**\n",
    "```python\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "# Start query ‚Üí reprocesses from beginning\n",
    "```\n",
    "\n",
    "**Option 2: New checkpoint location**\n",
    "```python\n",
    "# Use different checkpoint\n",
    ".option(\"checkpointLocation\", \"/checkpoint_v2\")\n",
    "```\n",
    "\n",
    "**Option 3: Schema evolution mode**\n",
    "```python\n",
    "# Enable flexible schema\n",
    ".option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "```\n",
    "\n",
    "### Problem 3: OOM (Out of Memory) in Streaming\n",
    "\n",
    "**Objawy:**\n",
    "```\n",
    "OutOfMemoryError: Java heap space\n",
    "Container killed: exceeding memory limits\n",
    "```\n",
    "\n",
    "**Przyczyny & RozwiƒÖzania:**\n",
    "\n",
    "**1. Large state (stateful aggregations)**\n",
    "```python\n",
    "# Solution: Add watermark to limit state\n",
    ".withWatermark(\"event_time\", \"1 hour\")  # Trim old state\n",
    "```\n",
    "\n",
    "**2. Too many partitions in memory**\n",
    "```python\n",
    "# Solution: Reduce shuffle partitions\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)  # by≈Ço: 2000\n",
    "```\n",
    "\n",
    "**3. Large micro-batches**\n",
    "```python\n",
    "# Solution: Throttle input\n",
    ".option(\"maxBytesPerTrigger\", \"1g\")\n",
    ".option(\"maxFilesPerTrigger\", 100)\n",
    "```\n",
    "\n",
    "### Problem 4: Data duplicates w output\n",
    "\n",
    "**Objawy:**\n",
    "- Duplicate records w Delta table\n",
    "- Primary key violations\n",
    "\n",
    "**Przyczyny & RozwiƒÖzania:**\n",
    "\n",
    "**1. No checkpoint location**\n",
    "```python\n",
    "# ‚ùå Missing checkpoint\n",
    ".writeStream.toTable(\"target\")  # NO checkpoint!\n",
    "\n",
    "# ‚úÖ Add checkpoint\n",
    ".option(\"checkpointLocation\", \"/checkpoint\")\n",
    "```\n",
    "\n",
    "**2. Checkpoint deleted between runs**\n",
    "```python\n",
    "# Don't delete checkpoint unless intended reprocessing\n",
    "# Checkpoint = exactly-once semantics\n",
    "```\n",
    "\n",
    "**3. Multiple streams writing to same table**\n",
    "```python\n",
    "# Solution: Use MERGE instead of append\n",
    "# Or ensure unique checkpoint per query\n",
    "```\n",
    "\n",
    "### Problem 5: Stream stuck (no progress)\n",
    "\n",
    "**Objawy:**\n",
    "- No new batches processed\n",
    "- `numInputRows = 0` consistently\n",
    "- Stream \"running\" but idle\n",
    "\n",
    "**Przyczyny & RozwiƒÖzania:**\n",
    "\n",
    "**1. No new files**\n",
    "```python\n",
    "# Check source location\n",
    "dbutils.fs.ls(\"/source/path\")\n",
    "\n",
    "# Verify file notification (Auto Loader)\n",
    "# Check SQS/EventGrid configuration\n",
    "```\n",
    "\n",
    "**2. Watermark blocking output**\n",
    "```python\n",
    "# Stateful query may wait for watermark\n",
    "# Check watermark progress in Spark UI\n",
    "# May need to adjust watermark threshold\n",
    "```\n",
    "\n",
    "**3. Trigger condition not met**\n",
    "```python\n",
    "# processingTime trigger may wait for next interval\n",
    "# Solution: Use availableNow for immediate processing\n",
    "```\n",
    "\n",
    "### Problem 6: Schema mismatch errors\n",
    "\n",
    "**Objawy:**\n",
    "```\n",
    "AnalysisException: Cannot resolve column 'xyz'\n",
    "Schema mismatch: expected INT, found STRING\n",
    "```\n",
    "\n",
    "**RozwiƒÖzania:**\n",
    "\n",
    "**1. Use explicit schema**\n",
    "```python\n",
    "# Instead of inference\n",
    ".schema(explicit_schema)\n",
    "```\n",
    "\n",
    "**2. Enable schema evolution**\n",
    "```python\n",
    ".option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    ".option(\"mergeSchema\", \"true\")\n",
    "```\n",
    "\n",
    "**3. CAST problematic columns**\n",
    "```python\n",
    "df.withColumn(\"amount\", col(\"amount\").cast(\"decimal(10,2)\"))\n",
    "```\n",
    "\n",
    "### Debugging Checklist\n",
    "\n",
    "When stream fails, check in order:\n",
    "\n",
    "**1. Spark UI ‚Üí Streaming Tab**\n",
    "- Query status (Active/Failed)\n",
    "- Last error message\n",
    "- Batch statistics\n",
    "\n",
    "**2. Check Logs**\n",
    "```python\n",
    "# Driver logs\n",
    "# Databricks: Clusters ‚Üí Driver Logs\n",
    "\n",
    "# Query progress\n",
    "query.lastProgress\n",
    "\n",
    "# Query status\n",
    "query.status\n",
    "```\n",
    "\n",
    "**3. Checkpoint Location**\n",
    "```python\n",
    "# Verify checkpoint exists & accessible\n",
    "dbutils.fs.ls(checkpoint_path)\n",
    "\n",
    "# Check checkpoint size (may indicate issues)\n",
    "```\n",
    "\n",
    "**4. Source Data**\n",
    "```python\n",
    "# Verify files exist\n",
    "dbutils.fs.ls(source_path)\n",
    "\n",
    "# Sample read (non-streaming)\n",
    "spark.read.format(\"json\").load(source_path).show()\n",
    "```\n",
    "\n",
    "**5. Cluster Resources**\n",
    "- CPU utilization\n",
    "- Memory usage\n",
    "- Disk space\n",
    "\n",
    "### Monitoring Commands\n",
    "\n",
    "```python\n",
    "# 1. Query Status\n",
    "print(query.status)\n",
    "\n",
    "# 2. Last Progress (metrics)\n",
    "import json\n",
    "print(json.dumps(query.lastProgress, indent=2))\n",
    "\n",
    "# 3. Recent Progress\n",
    "for progress in query.recentProgress:\n",
    "    print(f\"Batch {progress['batchId']}: {progress['numInputRows']} rows\")\n",
    "\n",
    "# 4. Exception (if failed)\n",
    "if query.exception():\n",
    "    print(query.exception())\n",
    "\n",
    "# 5. Checkpoint contents\n",
    "dbutils.fs.ls(checkpoint_path)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Sekcja 9: Podsumowanie & Nastƒôpne Kroki\n",
    "\n",
    "### Co zosta≈Ço osiƒÖgniƒôte w tym notebooku:\n",
    "\n",
    "‚úÖ **1. Structured Streaming Fundamentals**\n",
    "- readStream & writeStream API\n",
    "- Output modes (append, update, complete)\n",
    "- Micro-batch architecture\n",
    "- Exactly-once semantics\n",
    "\n",
    "‚úÖ **2. Auto Loader (cloudFiles) Deep Dive**\n",
    "- cloudFiles format vs standard readStream\n",
    "- Automatic file discovery & notification\n",
    "- Schema inference & caching\n",
    "- Performance optimizations dla millions of files\n",
    "\n",
    "‚úÖ **3. Trigger Modes**\n",
    "- `availableNow=True` dla batch-like (ZALECANE)\n",
    "- `processingTime` dla always-on real-time\n",
    "- `once` (legacy)\n",
    "- Cost vs latency trade-offs\n",
    "\n",
    "‚úÖ **4. Watermarking & Late Data**\n",
    "- Event-time processing\n",
    "- Watermark threshold configuration\n",
    "- Window aggregations\n",
    "- Late data handling strategies\n",
    "\n",
    "‚úÖ **5. Checkpoint Management**\n",
    "- Fault tolerance mechanism\n",
    "- Exactly-once delivery guarantees\n",
    "- Checkpoint structure & lifecycle\n",
    "- Recovery patterns\n",
    "\n",
    "‚úÖ **6. Schema Evolution**\n",
    "- `addNewColumns` mode (flexible)\n",
    "- `rescue` mode (strict + monitoring)\n",
    "- `failOnNewColumns` (critical systems)\n",
    "- mergeSchema integration z Delta\n",
    "\n",
    "‚úÖ **7. Best Practices**\n",
    "- Production-ready patterns\n",
    "- Monitoring & alerting\n",
    "- Performance optimization\n",
    "- Error handling\n",
    "\n",
    "‚úÖ **8. Troubleshooting**\n",
    "- Common issues & solutions\n",
    "- Debugging workflow\n",
    "- Performance tuning\n",
    "- Monitoring commands\n",
    "\n",
    "### Kluczowe Wnioski:\n",
    "\n",
    "üí° **1. Always Use Auto Loader (cloudFiles)**\n",
    "```\n",
    "Auto Loader > standard readStream:\n",
    "- Scalability (millions of files)\n",
    "- Schema evolution (automatic)\n",
    "- Performance (file notifications)\n",
    "- Lower cost (event-driven)\n",
    "```\n",
    "\n",
    "üí° **2. availableNow dla Scheduled Jobs**\n",
    "```\n",
    "Cost optimization:\n",
    "- availableNow: batch-like, pay per run\n",
    "- processingTime: always-on, continuous cost\n",
    "- 90% use cases: availableNow is sufficient\n",
    "```\n",
    "\n",
    "üí° **3. Checkpoint = Fault Tolerance**\n",
    "```\n",
    "Always specify checkpoint:\n",
    "- Exactly-once semantics\n",
    "- Incremental processing\n",
    "- Restart without duplicates\n",
    "- NO checkpoint = NO guarantees\n",
    "```\n",
    "\n",
    "üí° **4. Schema Evolution Strategy**\n",
    "```\n",
    "Production pipelines need:\n",
    "- cloudFiles.schemaEvolutionMode: addNewColumns\n",
    "- cloudFiles.schemaLocation: persist schema\n",
    "- mergeSchema: true (Delta compatibility)\n",
    "```\n",
    "\n",
    "üí° **5. Watermark dla Stateful Operations**\n",
    "```\n",
    "Aggregations require:\n",
    "- withWatermark() for late data\n",
    "- Threshold: 10-30min typical\n",
    "- Monitor dropped data\n",
    "- Balance latency vs completeness\n",
    "```\n",
    "\n",
    "### Decision Tree - Streaming Setup\n",
    "\n",
    "```\n",
    "Potrzebujƒô streaming pipeline...\n",
    "\n",
    "‚îú‚îÄ Source type?\n",
    "‚îÇ  ‚îú‚îÄ Files (S3/ADLS/GCS)\n",
    "‚îÇ  ‚îÇ  ‚îî‚îÄ Use: readStream.format(\"cloudFiles\")\n",
    "‚îÇ  ‚îÇ     Options: schemaEvolutionMode, maxFilesPerTrigger\n",
    "‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Kafka/EventHub\n",
    "‚îÇ     ‚îî‚îÄ Use: readStream.format(\"kafka\")\n",
    "‚îÇ\n",
    "‚îú‚îÄ Execution frequency?\n",
    "‚îÇ  ‚îú‚îÄ Scheduled (hourly/daily)\n",
    "‚îÇ  ‚îÇ  ‚îî‚îÄ Use: trigger(availableNow=True)\n",
    "‚îÇ  ‚îÇ     Lower cost, batch-like\n",
    "‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ Real-time (< 5min latency)\n",
    "‚îÇ     ‚îî‚îÄ Use: trigger(processingTime=\"30 seconds\")\n",
    "‚îÇ        Always-on, higher cost\n",
    "‚îÇ\n",
    "‚îú‚îÄ Schema changes expected?\n",
    "‚îÇ  ‚îú‚îÄ YES (flexible)\n",
    "‚îÇ  ‚îÇ  ‚îî‚îÄ schemaEvolutionMode: addNewColumns\n",
    "‚îÇ  ‚îÇ\n",
    "‚îÇ  ‚îî‚îÄ NO (strict)\n",
    "‚îÇ     ‚îî‚îÄ schemaEvolutionMode: failOnNewColumns\n",
    "‚îÇ\n",
    "‚îî‚îÄ Aggregations needed?\n",
    "   ‚îú‚îÄ YES\n",
    "   ‚îÇ  ‚îî‚îÄ Add watermark + window\n",
    "   ‚îÇ     Handle late data\n",
    "   ‚îÇ\n",
    "   ‚îî‚îÄ NO\n",
    "      ‚îî‚îÄ Simple append mode\n",
    "         No watermark needed\n",
    "```\n",
    "\n",
    "### Comparison Matrix - Final Reference\n",
    "\n",
    "| Feature | Batch (COPY INTO) | Streaming (Auto Loader) |\n",
    "|---------|-------------------|-------------------------|\n",
    "| **Latency** | Hours | Seconds-Minutes |\n",
    "| **Cost** | Low | Medium-High |\n",
    "| **Complexity** | Low | Medium |\n",
    "| **Use Case** | Daily ETL | Real-time CDC |\n",
    "| **File Discovery** | Manual | Automatic |\n",
    "| **Schema Evolution** | Manual | Automatic |\n",
    "| **Exactly-Once** | Built-in (file tracking) | Built-in (checkpoint) |\n",
    "| **Late Data** | N/A | Watermark |\n",
    "| **Stateful Ops** | No | Yes (aggregations) |\n",
    "| **Best For** | Large files, scheduled | Small files, continuous |\n",
    "\n",
    "### Nastƒôpne Kroki w Szkoleniu:\n",
    "\n",
    "**üìö Kolejny Notebook:**\n",
    "- **04_bronze_silver_gold_pipeline.ipynb**\n",
    "  - Medallion Architecture implementation\n",
    "  - Multi-hop transformations\n",
    "  - Data quality checks\n",
    "  - Complete streaming pipeline\n",
    "\n",
    "**üõ†Ô∏è Warsztat Praktyczny:**\n",
    "- **02_ingestion_pipeline_workshop.ipynb**\n",
    "  - Build end-to-end streaming pipeline\n",
    "  - Handle schema changes\n",
    "  - Implement monitoring\n",
    "  - Production deployment patterns\n",
    "\n",
    "**üìñ Materia≈Çy Dodatkowe:**\n",
    "- Databricks Auto Loader documentation\n",
    "- Structured Streaming Programming Guide\n",
    "- Delta Lake Streaming integration\n",
    "\n",
    "### Zadanie Domowe (Optional):\n",
    "\n",
    "**Zadanie:** Zbuduj production-ready streaming pipeline\n",
    "\n",
    "**Requirements:**\n",
    "1. ‚úÖ Use Auto Loader (cloudFiles)\n",
    "2. ‚úÖ Enable schema evolution (addNewColumns)\n",
    "3. ‚úÖ Checkpoint configuration\n",
    "4. ‚úÖ Trigger: availableNow for cost optimization\n",
    "5. ‚úÖ Add watermark (if aggregations)\n",
    "6. ‚úÖ Monitoring metrics logging\n",
    "7. ‚úÖ Error handling & alerting\n",
    "8. ‚úÖ Unity Catalog integration\n",
    "\n",
    "**Bonus:**\n",
    "- Schedule jako Databricks Workflow (hourly)\n",
    "- Add data quality checks\n",
    "- Implement Bronze ‚Üí Silver transformation\n",
    "- Dashboard w PowerBI/Tableau\n",
    "\n",
    "### Production Deployment Checklist:\n",
    "\n",
    "Before deploying streaming pipeline:\n",
    "\n",
    "- [ ] Checkpoint location persistent (not /tmp)\n",
    "- [ ] Schema evolution mode configured\n",
    "- [ ] maxFilesPerTrigger set (throttling)\n",
    "- [ ] Monitoring enabled (metrics, logs)\n",
    "- [ ] Alerting configured (failures)\n",
    "- [ ] Resource sizing validated (cluster)\n",
    "- [ ] Testing completed (small dataset)\n",
    "- [ ] Documentation updated (runbook)\n",
    "- [ ] Backup strategy (checkpoint, code)\n",
    "- [ ] Rollback plan (if deployment fails)\n",
    "\n",
    "---\n",
    "\n",
    "**Gratulacje!** üéâ \n",
    "Uko≈Ñczy≈Çe≈õ notebook o Streaming Data Ingestion. \n",
    "Jeste≈õ gotowy do budowania production-grade real-time data pipelines w Delta Lake!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe12c51",
   "metadata": {},
   "source": [
    "## Sekcja 10: Czyszczenie Zasob√≥w\n",
    "\n",
    "**Uwaga:** Ta sekcja jest opcjonalna. Uruchom tylko je≈õli chcesz usunƒÖƒá wszystkie dane utworzone podczas notebooka.\n",
    "\n",
    "W ≈õrodowisku szkoleniowym zazwyczaj chcemy **zachowaƒá** dane dla kolejnych notebook√≥w."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4b776a",
   "metadata": {},
   "source": [
    "### Opcja 1: Sprawd≈∫ utworzone zasoby (zalecane)\n",
    "\n",
    "Zostaw tabele i checkpoints dla kolejnych notebook√≥w:\n",
    "- `04_bronze_silver_gold_pipeline.ipynb` u≈ºyje tych danych\n",
    "- Warsztaty praktyczne wykorzystajƒÖ streaming tables\n",
    "- Checkpoints sƒÖ potrzebne dla incremental processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1accc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcja 1: Sprawd≈∫ utworzone zasoby (bez usuwania)\n",
    "\n",
    "print(\"=== Utworzone tabele streaming w tym notebooku ===\\n\")\n",
    "\n",
    "streaming_tables = [\n",
    "    f\"{BRONZE_SCHEMA}.orders_streaming_basic\",\n",
    "    f\"{BRONZE_SCHEMA}.orders_autoloader\",\n",
    "    f\"{BRONZE_SCHEMA}.orders_trigger_availablenow\",\n",
    "    f\"{BRONZE_SCHEMA}.orders_checkpoint_demo\",\n",
    "    f\"{SILVER_SCHEMA}.orders_windowed_aggregates\"\n",
    "]\n",
    "\n",
    "total_records = 0\n",
    "total_size_bytes = 0\n",
    "\n",
    "for table in streaming_tables:\n",
    "    full_table = f\"{CATALOG}.{table}\"\n",
    "    try:\n",
    "        if spark.catalog.tableExists(full_table):\n",
    "            count = spark.table(full_table).count()\n",
    "            total_records += count\n",
    "            \n",
    "            # Pobierz rozmiar\n",
    "            detail = spark.sql(f\"DESCRIBE DETAIL {full_table}\").collect()[0]\n",
    "            size_bytes = detail['sizeInBytes']\n",
    "            size_mb = size_bytes / (1024 * 1024)\n",
    "            total_size_bytes += size_bytes\n",
    "            \n",
    "            print(f\"‚úÖ {table}\")\n",
    "            print(f\"   Rekordy: {count:,}\")\n",
    "            print(f\"   Rozmiar: {size_mb:.2f} MB\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  {table} - nie istnieje\")\n",
    "            print()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  {table} - b≈ÇƒÖd: {str(e)}\")\n",
    "        print()\n",
    "\n",
    "# Sprawd≈∫ checkpoints\n",
    "print(\"=== Checkpoints ===\\n\")\n",
    "try:\n",
    "    checkpoint_dirs = dbutils.fs.ls(CHECKPOINT_BASE)\n",
    "    print(f\"Checkpoints w {CHECKPOINT_BASE}:\")\n",
    "    for cp in checkpoint_dirs:\n",
    "        size = sum([f.size for f in dbutils.fs.ls(cp.path)])\n",
    "        size_mb = size / (1024 * 1024)\n",
    "        print(f\"  - {cp.name}: {size_mb:.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Checkpoints: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"≈ÅƒÖczna liczba rekord√≥w: {total_records:,}\")\n",
    "print(f\"≈ÅƒÖczny rozmiar tabel: {total_size_bytes / (1024 * 1024):.2f} MB\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\nüí° Dane sƒÖ zachowane dla kolejnych notebook√≥w\")\n",
    "print(\"üí° Checkpoints umo≈ºliwiajƒÖ incremental processing\")\n",
    "print(\"üí° Aby usunƒÖƒá, uruchom kom√≥rkƒô poni≈ºej (Opcja 2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd191501",
   "metadata": {},
   "source": [
    "### Opcja 2: Usu≈Ñ wszystkie zasoby (tylko je≈õli naprawdƒô chcesz)\n",
    "\n",
    "**UWAGA:** To usunie wszystkie tabele, checkpoints i schema locations utworzone w tym notebooku!\n",
    "\n",
    "Uruchom kom√≥rkƒô poni≈ºej tylko je≈õli:\n",
    "- Sko≈Ñczy≈Çe≈õ szkolenie i chcesz posprzƒÖtaƒá\n",
    "- Chcesz zaczƒÖƒá od nowa (fresh start)\n",
    "- Testujesz notebook i potrzebujesz clean slate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc1c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcja 2: Usu≈Ñ wszystkie zasoby streaming (TYLKO JE≈öLI JESTE≈ö PEWIEN!)\n",
    "\n",
    "# ‚ö†Ô∏è  UWAGA: Odkomentuj poni≈ºszy kod tylko je≈õli chcesz usunƒÖƒá wszystko!\n",
    "\n",
    "\"\"\"\n",
    "print(\"=== üóëÔ∏è  USUWANIE ZASOB√ìW STREAMING ===\\n\")\n",
    "print(\"‚ö†Ô∏è  To usunie wszystkie tabele i checkpoints!\\n\")\n",
    "\n",
    "# Lista tabel do usuniƒôcia\n",
    "streaming_tables = [\n",
    "    f\"{BRONZE_SCHEMA}.orders_streaming_basic\",\n",
    "    f\"{BRONZE_SCHEMA}.orders_autoloader\",\n",
    "    f\"{BRONZE_SCHEMA}.orders_trigger_availablenow\",\n",
    "    f\"{BRONZE_SCHEMA}.orders_checkpoint_demo\",\n",
    "    f\"{SILVER_SCHEMA}.orders_windowed_aggregates\"\n",
    "]\n",
    "\n",
    "# Usu≈Ñ tabele\n",
    "print(\"Usuwanie tabel...\")\n",
    "for table in streaming_tables:\n",
    "    full_table = f\"{CATALOG}.{table}\"\n",
    "    try:\n",
    "        spark.sql(f\"DROP TABLE IF EXISTS {full_table}\")\n",
    "        print(f\"  ‚úì Usuniƒôto: {table}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ‚ö†Ô∏è  B≈ÇƒÖd przy {table}: {e}\")\n",
    "\n",
    "# Usu≈Ñ wszystkie checkpoints\n",
    "print(\"\\nUsuwanie checkpoints...\")\n",
    "try:\n",
    "    dbutils.fs.rm(CHECKPOINT_BASE, True)\n",
    "    print(f\"  ‚úì Usuniƒôto wszystkie checkpoints z: {CHECKPOINT_BASE}\")\n",
    "except Exception as e:\n",
    "    print(f\"  ‚ö†Ô∏è  B≈ÇƒÖd: {e}\")\n",
    "\n",
    "print(\"\\n‚úÖ Czyszczenie zako≈Ñczone!\")\n",
    "print(\"üí° Wszystkie streaming tables i checkpoints zosta≈Çy usuniƒôte\")\n",
    "print(\"üí° Mo≈ºesz teraz uruchomiƒá notebook od nowa\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚ö†Ô∏è  KOD CZYSZCZENIA JEST ZAKOMENTOWANY\")\n",
    "print(\"‚ö†Ô∏è  Odkomentuj powy≈ºszy kod tylko je≈õli chcesz usunƒÖƒá wszystkie zasoby\")\n",
    "print(\"\\nüí° Zalecenie: Zostaw dane dla kolejnych notebook√≥w!\")\n",
    "print(\"üí° Nastƒôpny notebook: 04_bronze_silver_gold_pipeline.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
