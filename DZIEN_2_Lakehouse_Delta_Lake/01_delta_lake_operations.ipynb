{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a373df9",
   "metadata": {},
   "source": [
    "# Delta Lake Operations - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Zrozumienie fundamentów Delta Lake i praktyczne zastosowanie operacji CRUD, Time Travel, optymalizacji i Change Data Feed\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Delta Lake core features: ACID, Delta Log, Schema enforcement\n",
    "- Schema evolution (additive, automatic)\n",
    "- Time Travel i Copy-on-write\n",
    "- CRUD operations: CREATE TABLE, INSERT, UPDATE, DELETE, MERGE INTO\n",
    "- Optymalizacja: OPTIMIZE, ZORDER BY, VACUUM\n",
    "- Change Data Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7fab7c",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 2 - Lakehouse & Delta Lake\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z 2-4 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b9b43d",
   "metadata": {},
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Wprowadzenie do Delta Lake jako transakcyjnej warstwy storage nad Data Lake\n",
    "\n",
    "**Podstawowe pojęcia:**\n",
    "- **Delta Lake**: Open-source storage layer zapewniający ACID transactions dla Apache Spark\n",
    "- **Delta Log**: Transakcyjny log przechowujący metadane o wszystkich zmianach w tabeli\n",
    "- **Schema Enforcement**: Automatyczna walidacja zgodności schematów przy zapisie\n",
    "- **Time Travel**: Możliwość dostępu do poprzednich wersji danych\n",
    "\n",
    "**Dlaczego to ważne?**\n",
    "Delta Lake rozwiązuje fundamentalne problemy Data Lake: brak transakcji, schema drift, trudności z aktualizacjami i quality assurance. Zapewnia niezawodność Data Warehouse z elastycznością Data Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c0982",
   "metadata": {},
   "source": [
    "## Izolacja per użytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f8af6",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95619fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Wyświetl kontekst użytkownika\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (CATALOG, BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA)\n",
    "    ], ['catalog', 'bronze_schema', 'silver_schema', 'gold_schema'])\n",
    ")\n",
    "\n",
    "# Ustaw katalog i schemat jako domyślne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31dea81",
   "metadata": {},
   "source": [
    "## Sekcja 1: Delta Lake Core Features\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Delta Lake to warstwa transakcyjna nad Parquet, która zapewnia ACID properties (Atomicity, Consistency, Isolation, Durability). Każda operacja na tabeli Delta jest rejestrowana w Delta Log - JSON pliku zawierającym metadane o zmianach.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **ACID Transactions**: Wszystkie operacje są atomowe i spójne\n",
    "- **Delta Log**: `_delta_log/` folder z JSON plikami opisującymi każdą transakcję\n",
    "- **Schema Enforcement**: Automatyczna walidacja zgodności schematów\n",
    "- **Unified Batch and Streaming**: Jedna tabela obsługuje zarówno batch jak i streaming\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Transakcyjne aktualizacje w Data Lake\n",
    "- Zapewnienie jakości danych poprzez schema validation\n",
    "- Jednolity dostęp do danych dla batch i streaming workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcadceb9",
   "metadata": {},
   "source": [
    "### Przykład 1.1: Utworzenie pierwszej tabeli Delta\n",
    "\n",
    "**Cel:** Demonstracja tworzenia tabeli Delta i podstawowych właściwości\n",
    "\n",
    "**Podejście:**\n",
    "1. Wczytanie danych z Unity Catalog Volume\n",
    "2. Utworzenie managed table w formacie Delta\n",
    "3. Eksploracja Delta Log i metadanych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ffd89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytaj dane klientów z Unity Catalog Volume\n",
    "customers_df = (spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(f\"{DATASET_BASE_PATH}/customers/customers.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf04c700",
   "metadata": {},
   "source": [
    "**Utwórz managed Delta table:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a24702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utwórz managed Delta table\n",
    "customers_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea97ac2",
   "metadata": {},
   "source": [
    "**Wyświetl wynik:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4340d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2ba69",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Utworzono managed Delta table w Unity Catalog. Format Delta automatycznie:\n",
    "- Stworzył `_delta_log/` folder z metadanymi transakcji\n",
    "- Zarejestrował schemat tabeli w Unity Catalog\n",
    "- Zastosował kompresję Parquet z dodatkowymi Delta features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a67da9b",
   "metadata": {},
   "source": [
    "### Przykład 1.2: Schema Enforcement w akcji\n",
    "\n",
    "**Cel:** Demonstracja automatycznej walidacji schematów przy wstawianiu danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d17904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź aktualny schemat tabeli\n",
    "spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Próba wstawienia danych z niepoprawnym schematem\n",
    "invalid_data = spark.createDataFrame([\n",
    "    (999, \"Test Customer\", 25.5, \"invalid_email\", \"2023-01-01\")  # age jako float zamiast int\n",
    "], [\"customer_id\", \"name\", \"age\", \"email\", \"registration_date\"])\n",
    "\n",
    "try:\n",
    "    invalid_data.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "except Exception as e:\n",
    "    display(\n",
    "        spark.createDataFrame([\n",
    "            (\"Schema enforcement w działaniu\", str(e)[:100] + \"...\")\n",
    "        ], [\"message\", \"error\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731631ff",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Schema enforcement automatycznie odrzucił dane z niepoprawnym typem. Delta Lake porównuje schemat nowych danych ze schematem tabeli i blokuje niezgodne wstawienia, zapewniając consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2cb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utwórz tabelę z Identity Column i Generated Column\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {CATALOG}.{BRONZE_SCHEMA}.orders_modern (\n",
    "    order_sk BIGINT GENERATED ALWAYS AS IDENTITY,  -- Surrogate Key\n",
    "    order_id STRING,\n",
    "    total_amount DOUBLE,\n",
    "    order_timestamp TIMESTAMP,\n",
    "    order_date DATE GENERATED ALWAYS AS (CAST(order_timestamp AS DATE)) -- Auto-calculated\n",
    ") USING DELTA\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c610907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.orders_modern\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97953f34",
   "metadata": {},
   "source": [
    "Sprawdźmy wynik. Zwróć uwagę na automatycznie wypełnione kolumny.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f72bfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wstaw dane bez podawania kolumn generowanych\n",
    "spark.sql(f\"\"\"\n",
    "INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.orders_modern (order_id, total_amount, order_timestamp)\n",
    "VALUES \n",
    "    ('ORD-001', 150.50, current_timestamp()),\n",
    "    ('ORD-002', 200.00, current_timestamp())\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c0ef92",
   "metadata": {},
   "source": [
    "Teraz wstawimy dane. Zauważ, że w zapytaniu `INSERT` pomijamy kolumny `order_sk` oraz `order_date`.\n",
    "- `order_sk`: zostanie wygenerowane automatycznie (unikalny numer).\n",
    "- `order_date`: zostanie wyliczone na podstawie `order_timestamp`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c17789",
   "metadata": {},
   "source": [
    "### Przykład 1.3: Modern Modeling - Identity & Generated Columns\n",
    "\n",
    "**Cel:** Wykorzystanie nowoczesnych funkcji Delta Lake do automatyzacji modelu danych.\n",
    "\n",
    "**Funkcje:**\n",
    "- **Identity Columns**: Automatyczne generowanie unikalnych kluczy (Surrogate Keys).\n",
    "- **Generated Columns**: Automatyczne wyliczanie wartości kolumn na podstawie innych (np. data z timestamp).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc3fcd",
   "metadata": {},
   "source": [
    "## Sekcja 2: Schema Evolution\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Schema Evolution pozwala na kontrolowane dodawanie nowych kolumn do istniejących tabel Delta bez przerywania działania aplikacji. Delta Lake wspiera additive schema changes automatycznie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b97fb11",
   "metadata": {},
   "source": [
    "### Przykład 2.1: Automatyczne dodawanie kolumn\n",
    "\n",
    "**Cel:** Demonstracja automatycznej ewolucji schematu przy dodawaniu nowych kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215dabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dane z dodatkową kolumną\n",
    "extended_customers = spark.createDataFrame([\n",
    "    (1001, \"New Customer\", 30, \"new@example.com\", \"2023-12-01\", \"Premium\"),\n",
    "    (1002, \"Another Customer\", 25, \"another@example.com\", \"2023-12-02\", \"Standard\")\n",
    "], [\"customer_id\", \"name\", \"age\", \"email\", \"registration_date\", \"customer_tier\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dacbfb",
   "metadata": {},
   "source": [
    "**Włącz automatic schema evolution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7306c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Włącz automatic schema evolution\n",
    "extended_customers.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b618e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź nowy schemat\n",
    "spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eef7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikuj dane - nowa kolumna ma NULL dla starych rekordów\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .select(\"customer_id\", \"name\", \"customer_tier\")\n",
    "    .orderBy(\"customer_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606b4424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dodaj CHECK constraint: Wiek musi być dodatni\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        ALTER TABLE {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "        ADD CONSTRAINT valid_age CHECK (age > 0)\n",
    "    \"\"\")\n",
    "    print(\"Constraint 'valid_age' dodany pomyślnie.\")\n",
    "except Exception as e:\n",
    "    print(f\"Informacja: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237bfe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Próba wstawienia błędnych danych (wiek = -5)\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta (customer_id, name, age)\n",
    "        VALUES (9999, 'Invalid Age Customer', -5)\n",
    "    \"\"\")\n",
    "except Exception as e:\n",
    "    print(f\"Oczekiwany błąd Data Quality:\\n{str(e)[:300]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af585f2f",
   "metadata": {},
   "source": [
    "Constraint został dodany. Teraz spróbujmy wstawić dane, które go naruszają (wiek = -5). \n",
    "Oczekujemy, że Delta Lake zablokuje tę operację i zwróci błąd `InvariantViolationException` lub `CheckConstraintViolationException`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b223861",
   "metadata": {},
   "source": [
    "## Sekcja 2.5: Data Quality & Constraints\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Delta Lake pozwala na definiowanie **Constraints** (ograniczeń), które gwarantują jakość danych na poziomie tabeli. Działa to podobnie jak w tradycyjnych bazach danych SQL.\n",
    "\n",
    "**Typy Constraints:**\n",
    "- `NOT NULL`: Wymusza obecność wartości.\n",
    "- `CHECK`: Wymusza spełnienie dowolnego warunku logicznego (np. `age > 0`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9287bda6",
   "metadata": {},
   "source": [
    "## Sekcja 3: Time Travel i Disaster Recovery\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Time Travel to kluczowa funkcjonalność Delta Lake umożliwiająca dostęp do poprzednich wersji danych. Bazuje na Copy-on-Write mechanizmie - każda zmiana tworzy nową wersję plików, a stare wersje pozostają dostępne.\n",
    "\n",
    "**Disaster Recovery:**\n",
    "Dzięki Time Travel możemy nie tylko czytać stare dane, ale także **przywracać** tabelę do poprzedniego stanu za pomocą polecenia `RESTORE`. To kluczowe w przypadku przypadkowego usunięcia danych lub błędnych aktualizacji."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24614da6",
   "metadata": {},
   "source": [
    "### Przykład 3.1: Eksploracja historii tabeli\n",
    "\n",
    "**Cel:** Użycie DESCRIBE HISTORY do analizy wszystkich operacji na tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78369254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pokaż historię wszystkich operacji na tabeli\n",
    "display(\n",
    "    spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3076d",
   "metadata": {},
   "source": [
    "### Przykład 3.2: Time Travel queries\n",
    "\n",
    "**Cel:** Dostęp do poprzednich wersji danych używając VERSION AS OF i TIMESTAMP AS OF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b7bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dostęp do danych z wersji 0 (przed schema evolution)\n",
    "version_0_data = spark.sql(f\"\"\"\n",
    "    SELECT customer_id, name, age, email \n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.customers_delta VERSION AS OF 0\n",
    "    ORDER BY customer_id\n",
    "\"\"\")\n",
    "\n",
    "display(version_0_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e025209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porównaj liczbę rekordów między wersjami\n",
    "current_count = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").count()\n",
    "version_0_count = spark.sql(f\"SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.customers_delta VERSION AS OF 0\").count()\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Current version\", current_count),\n",
    "        (\"Version 0\", version_0_count)\n",
    "    ], [\"version\", \"record_count\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cfa052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Symulacja błędu: Przypadkowe usunięcie wszystkich danych\n",
    "spark.sql(f\"DELETE FROM {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd489ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Liczba rekordów po RESTORE:\", spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55536967",
   "metadata": {},
   "source": [
    "Tabela została przywrócona. Zweryfikujmy liczbę rekordów.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2098e89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Naprawa: RESTORE do wersji sprzed usunięcia\n",
    "# Pobieramy ostatnią dobrą wersję (przed DELETE)\n",
    "last_good_version = spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers_delta\").select(\"version\").limit(2).collect()[1][0]\n",
    "\n",
    "print(f\"Przywracanie do wersji: {last_good_version}\")\n",
    "\n",
    "spark.sql(f\"RESTORE TABLE {CATALOG}.{BRONZE_SCHEMA}.customers_delta TO VERSION AS OF {last_good_version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe46231",
   "metadata": {},
   "source": [
    "Teraz użyjemy **Time Travel**, aby znaleźć ostatnią poprawną wersję (sprzed usunięcia) i przywrócić tabelę poleceniem `RESTORE`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282f6925",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Liczba rekordów po awarii:\", spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcd7aa8",
   "metadata": {},
   "source": [
    "Ups! Usunęliśmy wszystkie dane. Sprawdźmy, czy tabela jest faktycznie pusta.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2274e1",
   "metadata": {},
   "source": [
    "### Przykład 3.3: Disaster Recovery - RESTORE TABLE\n",
    "\n",
    "**Cel:** Przywrócenie tabeli do stanu sprzed błędnej operacji (symulacja awarii).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c220416b",
   "metadata": {},
   "source": [
    "## Sekcja 4: CRUD Operations\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Delta Lake wspiera pełen zakres operacji CRUD (Create, Read, Update, Delete), co czyni go idealnym dla transakcyjnych workloadów w Data Lake. Wszystkie operacje są atomowe i ACID-compliant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17433d1",
   "metadata": {},
   "source": [
    "### Przykład 4.1: INSERT operation\n",
    "\n",
    "**Cel:** Dodawanie nowych rekordów do istniejącej tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf6eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT nowych klientów\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    VALUES \n",
    "        (2001, 'Insert Customer 1', 28, 'insert1@example.com', '2023-12-10', 'Gold'),\n",
    "        (2002, 'Insert Customer 2', 35, 'insert2@example.com', '2023-12-11', 'Silver')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf99a1f2",
   "metadata": {},
   "source": [
    "**Weryfikuj wstawienie:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52567b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikuj wstawienie\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .filter(F.col(\"customer_id\") >= 2000)\n",
    "    .orderBy(\"customer_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90275d6d",
   "metadata": {},
   "source": [
    "### Przykład 4.2: UPDATE operation\n",
    "\n",
    "**Cel:** Aktualizacja istniejących rekordów w tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85312ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE customer tier dla specific customers\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    SET customer_tier = 'Platinum'\n",
    "    WHERE customer_id IN (1001, 2001)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0a0701",
   "metadata": {},
   "source": [
    "**Weryfikuj aktualizację:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikuj aktualizację\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .filter(F.col(\"customer_tier\") == \"Platinum\")\n",
    "    .select(\"customer_id\", \"name\", \"customer_tier\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5fd1d6",
   "metadata": {},
   "source": [
    "### Przykład 4.3: DELETE operation\n",
    "\n",
    "**Cel:** Usuwanie rekordów z tabeli Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce35b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE specific customer\n",
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    WHERE customer_id = 2002\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82007b8",
   "metadata": {},
   "source": [
    "**Weryfikuj usunięcie:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755b5460",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikuj usunięcie\n",
    "deleted_check = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\") \\\n",
    "    .filter(F.col(\"customer_id\") == 2002) \\\n",
    "    .count()\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Records with customer_id 2002\", deleted_check)\n",
    "    ], [\"description\", \"count\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47731f5",
   "metadata": {},
   "source": [
    "## Sekcja 5: MERGE INTO Operations\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "MERGE INTO to potężna operacja umożliwiająca upsert (update + insert) w jednej transakcji. Szczególnie przydatna przy przetwarzaniu zmian z systemów transakcyjnych (CDC - Change Data Capture)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f21549",
   "metadata": {},
   "source": [
    "### Przykład 5.1: Podstawowy MERGE INTO\n",
    "\n",
    "**Cel:** Demonstracja upsert operation - update istniejących i insert nowych rekordów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5782b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przygotuj dane do merge (mix updates i nowych rekordów)\n",
    "merge_data = spark.createDataFrame([\n",
    "    (1001, \"Updated Customer Name\", 31, \"updated@example.com\", \"2023-12-01\", \"Diamond\"),  # Update\n",
    "    (3001, \"Brand New Customer\", 29, \"brand.new@example.com\", \"2023-12-15\", \"Bronze\"),   # Insert\n",
    "    (3002, \"Another New Customer\", 33, \"another.new@example.com\", \"2023-12-16\", \"Silver\") # Insert\n",
    "], [\"customer_id\", \"name\", \"age\", \"email\", \"registration_date\", \"customer_tier\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eccb030",
   "metadata": {},
   "source": [
    "**Utwórz temporary view dla merge operation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5383d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utwórz temporary view dla merge operation\n",
    "merge_data.createOrReplaceTempView(\"customer_updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58692528",
   "metadata": {},
   "source": [
    "**Wykonaj operację MERGE (Upsert):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE INTO operation\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta AS target\n",
    "    USING customer_updates AS source\n",
    "    ON target.customer_id = source.customer_id\n",
    "    \n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            name = source.name,\n",
    "            age = source.age,\n",
    "            email = source.email,\n",
    "            customer_tier = source.customer_tier\n",
    "    \n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (customer_id, name, age, email, registration_date, customer_tier)\n",
    "        VALUES (source.customer_id, source.name, source.age, source.email, source.registration_date, source.customer_tier)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d85cec4",
   "metadata": {},
   "source": [
    "**Zweryfikuj wyniki MERGE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikuj wyniki MERGE\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .filter(F.col(\"customer_id\").isin([1001, 3001, 3002]))\n",
    "    .orderBy(\"customer_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999adf45",
   "metadata": {},
   "source": [
    "## Sekcja 6: Metadane i Analytics\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Delta Lake oferuje bogate metadane o tabelach i operacjach. DESCRIBE DETAIL dostarcza informacji o strukturze plików, partitioning, i właściwościach tabeli."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133735b",
   "metadata": {},
   "source": [
    "### Przykład 6.1: DESCRIBE DETAIL\n",
    "\n",
    "**Cel:** Analiza metadanych tabeli Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c239525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Szczegółowe informacje o tabeli\n",
    "display(\n",
    "    spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e5b2e7",
   "metadata": {},
   "source": [
    "### Przykład 6.2: Analiza historii operacji\n",
    "\n",
    "**Cel:** Głębsza analiza historii i metryk operacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d63af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historia z dodatkowymi metrykami\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "\n",
    "display(\n",
    "    history_df.select(\n",
    "        \"version\", \n",
    "        \"timestamp\", \n",
    "        \"operation\", \n",
    "        \"operationMetrics.numTargetRowsInserted\",\n",
    "        \"operationMetrics.numTargetRowsUpdated\",\n",
    "        \"operationMetrics.numTargetRowsDeleted\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10bfaaf",
   "metadata": {},
   "source": [
    "**Pobierz ścieżkę do tabeli i _delta_log:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2da9cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pobierz ścieżkę do tabeli\n",
    "table_path = spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.customers_delta\").select(\"location\").collect()[0][0]\n",
    "delta_log_path = f\"{table_path}/_delta_log\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2772cae3",
   "metadata": {},
   "source": [
    "**Wyświetl pliki w _delta_log:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969ce360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyświetl pliki w _delta_log\n",
    "display(dbutils.fs.ls(delta_log_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb611698",
   "metadata": {},
   "source": [
    "**Analiza ostatniego pliku transakcji (JSON):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb80a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyświetl zawartość ostatniego pliku JSON (transakcji)\n",
    "last_json = sorted([f.name for f in dbutils.fs.ls(delta_log_path) if f.name.endswith(\".json\")])[-1]\n",
    "print(f\"Analiza pliku transakcji: {last_json}\")\n",
    "print(dbutils.fs.head(f\"{delta_log_path}/{last_json}\", 1000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "829a87b4",
   "metadata": {},
   "source": [
    "Poniżej podgląd zawartości ostatniego pliku transakcyjnego JSON. \n",
    "Zawiera on metadane o operacjach, takich jak:\n",
    "- `add`: dodanie nowego pliku Parquet z danymi.\n",
    "- `remove`: logiczne usunięcie pliku (np. przy operacji DELETE lub OPTIMIZE).\n",
    "- `commitInfo`: metadane o samej transakcji (kto, kiedy, jaka operacja).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e431d1",
   "metadata": {},
   "source": [
    "### Przykład 6.3: Delta Log Internals (Deep Dive)\n",
    "\n",
    "**Cel:** Zrozumienie jak Delta Lake zapewnia ACID, zaglądając \"pod maskę\" do plików JSON w `_delta_log`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af958e",
   "metadata": {},
   "source": [
    "## Sekcja 7: Optymalizacja (Wstęp)\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Delta Lake oferuje szereg mechanizmów optymalizacyjnych. W tym notebooku skupimy się na podstawowej operacji **OPTIMIZE** (kompaktowanie plików) oraz **VACUUM** (czyszczenie).\n",
    "\n",
    "> **Deep Dive:** Zaawansowane techniki takie jak **ZORDER BY**, **Partycjonowanie** oraz **Liquid Clustering** są szczegółowo omówione w notebooku **05_optimization_best_practices.ipynb**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d714b5fd",
   "metadata": {},
   "source": [
    "### Przykład 7.1: OPTIMIZE (File Compaction)\n",
    "\n",
    "**Cel:** Kompaktowanie małych plików (small files problem) w większe, co poprawia wydajność odczytu.\n",
    "Możemy opcjonalnie dodać klauzulę `ZORDER BY` (omówioną w notebooku 05), aby dodatkowo posortować dane.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f4a0f0",
   "metadata": {},
   "source": [
    "**Wykonaj kompaktowanie plików:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b204d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZE tabeli\n",
    "optimize_result = spark.sql(f\"\"\"\n",
    "    OPTIMIZE {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "\"\"\")\n",
    "\n",
    "display(optimize_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc32a1d",
   "metadata": {},
   "source": [
    "### Przykład 7.3: Liquid Clustering (Wzmianka)\n",
    "\n",
    "**Nowoczesna alternatywa:**\n",
    "Databricks wprowadził **Liquid Clustering** - nową technikę, która zastępuje tradycyjne partycjonowanie i ZORDER. \n",
    "Liquid Clustering automatycznie zarządza układem danych, dostosowując się do wzorców zapytań.\n",
    "\n",
    "> **Deep Dive:** Szczegółowe omówienie i przykłady Liquid Clustering znajdują się w notebooku **05_optimization_best_practices.ipynb**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2827b",
   "metadata": {},
   "source": [
    "### Przykład 7.2: VACUUM operation\n",
    "\n",
    "**Cel:** Usunięcie starych plików (starszych niż retention period), które nie są już potrzebne do Time Travel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565f4ba1",
   "metadata": {},
   "source": [
    "**Wyłącz sprawdzenie retencji (tylko dla demo):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c18b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VACUUM - usuń pliki starsze niż 0 godzin (tylko dla demo!)\n",
    "# W produkcji: domyślnie 7 dni, minimum 0 godzin z flagą\n",
    "spark.sql(\"SET spark.databricks.delta.retentionDurationCheck.enabled = false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5aeae1",
   "metadata": {},
   "source": [
    "**Uruchom VACUUM (usuń stare pliki):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8f18c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "vacuum_result = spark.sql(f\"\"\"\n",
    "    VACUUM {CATALOG}.{BRONZE_SCHEMA}.customers_delta RETAIN 0 HOURS\n",
    "\"\"\")\n",
    "\n",
    "display(vacuum_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c00819",
   "metadata": {},
   "source": [
    "## Sekcja 8: Change Data Feed\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Change Data Feed (CDF) to feature Delta Lake umożliwiający tracking wszystkich zmian w tabeli. Każda operacja INSERT, UPDATE, DELETE jest rejestrowana z dodatkowymi metadanymi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b5fe89",
   "metadata": {},
   "source": [
    "### Przykład 8.1: Włączenie Change Data Feed\n",
    "\n",
    "**Cel:** Aktywacja CDF dla istniejącej tabeli"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e21c7c",
   "metadata": {},
   "source": [
    "**Włącz Change Data Feed:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c964c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Włącz Change Data Feed\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {CATALOG}.{BRONZE_SCHEMA}.customers_delta \n",
    "    SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c67aa",
   "metadata": {},
   "source": [
    "### Przykład 8.2: Generowanie zmian dla CDF\n",
    "\n",
    "**Cel:** Wykonanie operacji które będą śledzzone przez CDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0be7a6",
   "metadata": {},
   "source": [
    "**Wstaw nowy rekord (INSERT):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf14dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wykonaj więcej zmian po włączeniu CDF\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    VALUES (4001, 'CDF Test Customer', 27, 'cdf@example.com', '2023-12-20', 'Bronze')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c16f14",
   "metadata": {},
   "source": [
    "**Zaktualizuj rekord (UPDATE):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18585d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    UPDATE {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    SET customer_tier = 'Gold'\n",
    "    WHERE customer_id = 4001\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13549b59",
   "metadata": {},
   "source": [
    "### Przykład 8.3: Odczyt Change Data Feed\n",
    "\n",
    "**Cel:** Analiza wszystkich zmian zarejestrowanych przez CDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee2c2a80",
   "metadata": {},
   "source": [
    "**Odczytaj zmiany (CDF):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch read change data feed od określonej wersji\n",
    "changes_batch = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", 5) \\\n",
    "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "\n",
    "display(\n",
    "    changes_batch.select(\n",
    "        \"customer_id\", \"name\", \"customer_tier\", \n",
    "        \"_change_type\", \"_commit_version\", \"_commit_timestamp\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b3a52",
   "metadata": {},
   "source": [
    "**Utwórz SHALLOW CLONE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67c36f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utwórz SHALLOW CLONE do testów\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE {CATALOG}.{BRONZE_SCHEMA}.customers_test_clone\n",
    "SHALLOW CLONE {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa58f118",
   "metadata": {},
   "source": [
    "**Wykonaj operację DELETE na klonie:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7641697e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wykonaj destrukcyjną operację na klonie (nie wpływa na oryginał!)\n",
    "spark.sql(f\"DELETE FROM {CATALOG}.{BRONZE_SCHEMA}.customers_test_clone WHERE age < 30\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd36bb8",
   "metadata": {},
   "source": [
    "**Porównaj liczniki (izolacja):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a31d750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porównaj liczniki\n",
    "orig_count = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").count()\n",
    "clone_count = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_test_clone\").count()\n",
    "\n",
    "print(f\"Oryginał: {orig_count}\")\n",
    "print(f\"Klon (po delete): {clone_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf57ebd7",
   "metadata": {},
   "source": [
    "Porównanie liczby rekordów potwierdza izolację. Oryginał powinien mieć więcej rekordów niż klon po usunięciu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f69722a",
   "metadata": {},
   "source": [
    "## Sekcja 9: Zero-Copy Cloning\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Delta Lake umożliwia tworzenie kopii tabel (Clones).\n",
    "- **SHALLOW CLONE**: Kopiuje tylko metadane (Delta Log), a dane fizyczne pozostają te same. Idealne do testów, QA, eksperymentów. Koszt storage = prawie zero.\n",
    "- **DEEP CLONE**: Kopiuje metadane i dane fizyczne. Idealne do archiwizacji lub migracji.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67769b73",
   "metadata": {},
   "source": [
    "## Porównanie PySpark vs SQL\n",
    "\n",
    "**DataFrame API (PySpark):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27631571",
   "metadata": {},
   "source": [
    "**Przygotuj dane (PySpark):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark approach - MERGE simulation\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forName(spark, f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "\n",
    "new_data = spark.createDataFrame([\n",
    "    (5001, \"PySpark Customer\", 32, \"pyspark@example.com\", \"2023-12-25\", \"Silver\")\n",
    "], [\"customer_id\", \"name\", \"age\", \"email\", \"registration_date\", \"customer_tier\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ee1bfd",
   "metadata": {},
   "source": [
    "**Wykonaj MERGE (PySpark):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a887130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table.alias(\"target\") \\\n",
    "    .merge(\n",
    "        new_data.alias(\"source\"),\n",
    "        \"target.customer_id = source.customer_id\"\n",
    "    ) \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d42518",
   "metadata": {},
   "source": [
    "**SQL Equivalent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adfd093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL approach\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta AS target\n",
    "    USING (SELECT 5002 as customer_id, 'SQL Customer' as name, 30 as age, \n",
    "                  'sql@example.com' as email, '2023-12-26' as registration_date, \n",
    "                  'Gold' as customer_tier) AS source\n",
    "    ON target.customer_id = source.customer_id\n",
    "    WHEN MATCHED THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f14c89f",
   "metadata": {},
   "source": [
    "**Porównanie:**\n",
    "- **Wydajność**: Identyczna - obydwa używają Catalyst optimizer\n",
    "- **Kiedy używać PySpark**: Programatic ETL, complex business logic, integration z ML pipelines\n",
    "- **Kiedy używać SQL**: Ad-hoc analysis, reporting, BI tools integration, łatwiejsze dla analityków"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54858520",
   "metadata": {},
   "source": [
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist - Co powinieneś uzyskać:\n",
    "- [ ] Tabela Delta utworzona z automatycznym schema enforcement\n",
    "- [ ] Schema evolution - dodana kolumna customer_tier\n",
    "- [ ] Time Travel queries działają dla poprzednich wersji\n",
    "- [ ] CRUD operations (INSERT, UPDATE, DELETE) wykonane poprawnie\n",
    "- [ ] MERGE INTO zaimplementowane z upsert logic\n",
    "- [ ] Optymalizacja OPTIMIZE i ZORDER zastosowana\n",
    "- [ ] Change Data Feed włączony i rejestruje zmiany\n",
    "\n",
    "### Komendy weryfikacyjne:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87feb87",
   "metadata": {},
   "source": [
    "**Weryfikacja podstawowych metryk:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3e28d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja wyników\n",
    "final_count = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").count()\n",
    "final_schema_cols = len(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").columns)\n",
    "history_count = spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers_delta\").count()\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Total records\", final_count),\n",
    "        (\"Schema columns\", final_schema_cols),\n",
    "        (\"History versions\", history_count)\n",
    "    ], [\"metric\", \"value\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2f88f3",
   "metadata": {},
   "source": [
    "**Weryfikacja ustawień CDF:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a935f18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź CDF properties\n",
    "table_properties = spark.sql(f\"SHOW TBLPROPERTIES {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "cdf_enabled = table_properties.filter(F.col(\"key\") == \"delta.enableChangeDataFeed\").count() > 0\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Change Data Feed enabled\", cdf_enabled)\n",
    "    ], [\"property\", \"status\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c4541",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Problem 1: Schema enforcement błąd\n",
    "**Objawy:**\n",
    "- AnalysisException przy INSERT/MERGE z incompatible schema\n",
    "- \"Cannot write incompatible datatype\" message\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Użyj mergeSchema option dla schema evolution\n",
    "df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\")\n",
    "```\n",
    "\n",
    "### Problem 2: Time Travel - version not found\n",
    "**Objawy:** \n",
    "File not found dla określonej wersji po VACUUM\n",
    "\n",
    "**Rozwiązanie:** \n",
    "Sprawdź retention period i dostępne wersje przez DESCRIBE HISTORY\n",
    "\n",
    "### Problem 3: VACUUM usuwa zbyt dużo plików\n",
    "**Objawy:** Time Travel queries failują po VACUUM\n",
    "\n",
    "**Rozwiązanie:** \n",
    "Ustaw odpowiedni retention period (domyślnie 7 dni minimum)\n",
    "\n",
    "### Debugging tips:\n",
    "- Użyj `DESCRIBE HISTORY` aby zrozumieć operacje na tabeli\n",
    "- Sprawdź `DESCRIBE DETAIL` dla metadanych o plikach\n",
    "- Weryfikuj table properties przez `SHOW TBLPROPERTIES`\n",
    "- Monitoruj `_delta_log/` folder dla troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff65565",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### Wydajność:\n",
    "- Używaj ZORDER BY dla kolumn często występujących w WHERE clauses\n",
    "- Regularnie uruchamiaj OPTIMIZE dla kompaktowania small files\n",
    "- Partitioning tylko dla bardzo dużych tabel (TB+) ze skewed data\n",
    "\n",
    "### Jakość kodu:\n",
    "- Zawsze używaj explicit schema zamiast inferSchema w production\n",
    "- Implementuj schema evolution strategy dla backward compatibility\n",
    "- Używaj MERGE INTO zamiast separate DELETE + INSERT operations\n",
    "\n",
    "### Data Quality:\n",
    "- Włącz Change Data Feed dla audit trails i compliance\n",
    "- Regularne backup przez Time Travel snapshots\n",
    "- Implement data validation rules w Delta constraints\n",
    "\n",
    "### Governance:\n",
    "- Ustaw odpowiednie retention periods dla compliance requirements\n",
    "- Używaj Unity Catalog permissions dla row/column level security\n",
    "- Dokumentuj schema changes i business logic w table comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27a544d",
   "metadata": {},
   "source": [
    "## Podsumowanie\n",
    "\n",
    "### Co zostało osiągnięte:\n",
    "- Demonstracja Delta Lake ACID properties i schema enforcement\n",
    "- Hands-on Schema Evolution z automatic column addition\n",
    "- Time Travel queries dla historical data access\n",
    "- Kompletne CRUD operations (CREATE, READ, UPDATE, DELETE)\n",
    "- Advanced MERGE INTO dla upsert scenarios\n",
    "- Performance optimization z OPTIMIZE, ZORDER, VACUUM\n",
    "- Change Data Feed dla comprehensive audit trails\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "1. **Delta Lake = Data Lake + ACID**: Łączy elastyczność Data Lake z niezawodnością transakcyjną\n",
    "2. **Schema Evolution bezpiecznie**: Additive changes są automatyczne, breaking changes wymagają planowania\n",
    "3. **Time Travel + Copy-on-Write**: Każda wersja jest preserved, umożliwiając rollback i audit\n",
    "\n",
    "### Quick Reference - Najważniejsze komendy:\n",
    "\n",
    "| Operacja | PySpark | SQL |\n",
    "|----------|---------|-----|\n",
    "| Create Delta Table | `df.write.format(\"delta\").saveAsTable()` | `CREATE TABLE USING DELTA` |\n",
    "| Time Travel | `spark.read.format(\"delta\").option(\"versionAsOf\", 1)` | `SELECT * FROM table VERSION AS OF 1` |\n",
    "| MERGE | `DeltaTable.forName().merge().execute()` | `MERGE INTO target USING source` |\n",
    "| Optimize | N/A | `OPTIMIZE table ZORDER BY col` |\n",
    "| History | N/A | `DESCRIBE HISTORY table` |\n",
    "\n",
    "### Następne kroki:\n",
    "- **Kolejny notebook**: 02_medallion_architecture.ipynb\n",
    "- **Warsztat praktyczny**: 01_delta_medallion_workshop.ipynb\n",
    "- **Materiały dodatkowe**: Delta Lake documentation, best practices guides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf38219",
   "metadata": {},
   "source": [
    "## Czyszczenie zasobów\n",
    "\n",
    "Posprzątaj zasoby utworzone podczas notebooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabadeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcjonalne czyszczenie zasobów testowych\n",
    "# UWAGA: Uruchom tylko jeśli chcesz usunąć wszystkie utworzone dane\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "# spark.sql(\"DROP VIEW IF EXISTS customer_updates\")\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "# display(spark.createDataFrame([(\"Zasoby zostały wyczyszczone\", \"✓\")], [\"status\", \"result\"]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
