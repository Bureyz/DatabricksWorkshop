{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1b8a314b-58a6-4d37-b92c-772039a6c6a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Delta Lake Operations - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Opanowanie podstawowych i zaawansowanych operacji Delta Lake, w tym CRUD, MERGE, Time Travel i optymalizacji.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Delta Lake core features: ACID, Delta Log, Schema enforcement\n",
    "- Schema evolution (additive, automatic)\n",
    "- Time Travel i Copy-on-write\n",
    "- CRUD operations: CREATE TABLE, INSERT, UPDATE, DELETE\n",
    "- MERGE INTO - logika zmian na kluczach\n",
    "- DESCRIBE DETAIL, DESCRIBE HISTORY\n",
    "- Optymalizacja: OPTIMIZE, ZORDER BY, VACUUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d92e4e24-a75b-495e-9f13-63add1e0d834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 2 - Lakehouse & Delta Lake\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1f0a0485-0e9d-4fbc-b4a4-2188c110a705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie fundamentów Delta Lake jako formatu tabel transakcyjnych dla data lakehouse.\n",
    "\n",
    "**Podstawowe pojęcia:**\n",
    "- **Delta Lake**: Format tabel oparty na Parquet z warstwą transakcyjną (ACID)\n",
    "- **Delta Log**: Dziennik transakcji przechowujący metadane wszystkich operacji\n",
    "- **ACID**: Atomicity, Consistency, Isolation, Durability - gwarancje transakcyjne\n",
    "- **Time Travel**: Możliwość odczytania danych z dowolnego punktu w historii\n",
    "- **Schema Evolution**: Automatyczne lub kontrolowane zmiany w schemacie tabeli\n",
    "\n",
    "**Dlaczego to ważne?**\n",
    "Delta Lake rozwiązuje fundamentalne problemy tradycyjnych data lake'ów: brak transakcji, trudności z aktualizacją danych, brak schema enforcement. Pozwala na reliable data pipelines z gwarancjami ACID i możliwością audytu zmian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5625466e-48b2-40d4-90d4-f64ec704109b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Izolacja per użytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b7d87ba-e270-4254-b6c5-963cf5706de3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f251543a-7c90-47e3-9cbf-643b5581b5d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbbf73e0-af1c-414b-ae25-36f75034a142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Wyświetl kontekst użytkownika\n",
    "print(\"=== Kontekst użytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"Użytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog i schemat jako domyślne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Ścieżki do danych źródłowych\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "\n",
    "# Nazwy tabel Delta, które będziemy tworzyć\n",
    "CUSTOMERS_DELTA = f\"{BRONZE_SCHEMA}.customers_delta\"\n",
    "ORDERS_DELTA = f\"{BRONZE_SCHEMA}.orders_delta\"\n",
    "\n",
    "print(f\"\\n=== Tabele Delta ===\")\n",
    "print(f\"Customers: {CUSTOMERS_DELTA}\")\n",
    "print(f\"Orders: {ORDERS_DELTA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14da3d4a-ecab-4c74-81d6-3bd560de98ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 1: Delta Lake Core Features\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Delta Lake dodaje warstwę transakcyjną na top of Parquet files. Każda operacja zapisu (INSERT, UPDATE, DELETE, MERGE) jest zapisywana w Delta Log jako transakcja. Delta Log to seria JSON files w folderze `_delta_log/` zawierająca metadane wszystkich zmian.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **Transaction Log**: Pojedynczy wpis w Delta Log reprezentujący jedną transakcję\n",
    "- **Checkpoint**: Snapshots stanu tabeli co N transakcji dla szybszego odczytu\n",
    "- **Schema Enforcement**: Automatyczna walidacja typów danych przy zapisie\n",
    "- **Optimistic Concurrency**: Wiele czytających, jeden piszący w tym samym czasie\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Reliable ETL pipelines z gwarancjami ACID\n",
    "- Incremental data processing z możliwością rollback\n",
    "- Audit trails - pełna historia zmian w danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdcbfbe5-acbb-4d95-93a3-51ff36ed4099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.1: Utworzenie tabeli Delta z CSV\n",
    "\n",
    "**Cel:** Demonstracja utworzenia pierwszej tabeli Delta Lake z danych CSV\n",
    "\n",
    "**Podejście:**\n",
    "1. Wczytaj dane z CSV do DataFrame\n",
    "2. Zapisz DataFrame jako tabelę Delta w Unity Catalog\n",
    "3. Sprawdź metadane tabeli Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6feecea3-335b-41bc-b4f6-a48be566330f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 1.1 - Utworzenie tabeli Delta z CSV\n",
    "\n",
    "# Wczytaj dane klientów z CSV\n",
    "customers_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "print(f\"Wczytano {customers_df.count()} rekordów\")\n",
    "customers_df.printSchema()\n",
    "\n",
    "# Zapisz jako tabelę Delta\n",
    "(\n",
    "    customers_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(CUSTOMERS_DELTA)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Utworzono tabelę Delta: {CUSTOMERS_DELTA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fea3e727-886f-4adc-abc1-ead56a7cddd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Powyższy kod wykonuje trzy kluczowe operacje:\n",
    "1. **Wczytanie danych**: Spark DataFrame API wczytuje CSV z automatycznym wykrywaniem schematu\n",
    "2. **Zapis jako Delta**: `.format(\"delta\")` zapisuje dane w formacie Delta Lake (Parquet + Delta Log)\n",
    "3. **Rejestracja w Unity Catalog**: `.saveAsTable()` rejestruje tabelę w katalogu UC, co umożliwia governance\n",
    "\n",
    "Opcja `overwriteSchema=true` pozwala na zmianę schematu przy ponownym zapisie (użyteczne podczas development)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1fc036-9d27-4a0a-b9e5-7964d6014b33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.2: Inspektowanie Delta Log\n",
    "\n",
    "**Cel:** Zrozumienie struktury Delta Log i metadanych tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "357aea4c-83a0-4bee-b8f1-e090f9e9da9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 1.2 - DESCRIBE DETAIL\n",
    "\n",
    "# Wyświetl szczegółowe informacje o tabeli Delta\n",
    "detail_df = spark.sql(f\"DESCRIBE DETAIL {CUSTOMERS_DELTA}\")\n",
    "\n",
    "print(\"=== DESCRIBE DETAIL ===\")\n",
    "display(detail_df)\n",
    "\n",
    "# Kluczowe metadane\n",
    "row = detail_df.collect()[0]\n",
    "print(f\"\\n=== Kluczowe metadane ===\")\n",
    "print(f\"Format: {row['format']}\")\n",
    "print(f\"Lokalizacja: {row['location']}\")\n",
    "print(f\"Liczba plików: {row['numFiles']}\")\n",
    "print(f\"Rozmiar (bajty): {row['sizeInBytes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faaf9515-ec81-4696-ac6e-7287f7d84768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.3: Historia transakcji (DESCRIBE HISTORY)\n",
    "\n",
    "**Cel:** Przegląd pełnej historii operacji na tabeli Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "237524ba-bcb5-4b08-ae75-c28d0f92e084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 1.3 - DESCRIBE HISTORY\n",
    "\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY {CUSTOMERS_DELTA}\")\n",
    "\n",
    "print(\"=== DESCRIBE HISTORY ===\")\n",
    "print(\"Każdy wiersz reprezentuje jedną transakcję Delta\")\n",
    "display(\n",
    "    history_df.select(\n",
    "        \"version\",\n",
    "        \"timestamp\",\n",
    "        \"operation\",\n",
    "        \"operationParameters\",\n",
    "        \"userName\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01db04c9-6803-4ad8-908a-8b5e9a446be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 2: CRUD Operations\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Delta Lake wspiera pełne operacje CRUD (Create, Read, Update, Delete) na tabelach. W przeciwieństwie do tradycyjnych data lake'ów (immutable files), Delta Lake umożliwia modyfikację i usuwanie rekordów przy zachowaniu gwarancji ACID.\n",
    "\n",
    "**Kluczowe operacje:**\n",
    "- **INSERT**: Dodawanie nowych rekordów (append mode)\n",
    "- **UPDATE**: Modyfikacja istniejących rekordów na podstawie warunku\n",
    "- **DELETE**: Usuwanie rekordów spełniających warunek\n",
    "- **MERGE**: Upsert - INSERT nowych + UPDATE istniejących w jednej transakcji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630b30b4-06de-4af9-b04f-72f45e17fa6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.1: INSERT - Dodawanie nowych rekordów\n",
    "\n",
    "**Cel:** Demonstracja append mode - dodanie nowych klientów do tabeli\n",
    "\n",
    "**Podejście:**\n",
    "1. Utworzenie DataFrame z nowymi danymi\n",
    "2. Append do istniejącej tabeli Delta\n",
    "3. Weryfikacja wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0648ae5-cefd-4e16-8d12-1a96d63fc20b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763584841573}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 2.1 - INSERT (append)\n",
    "\n",
    "# Policz rekordy przed\n",
    "count_before = spark.table(CUSTOMERS_DELTA).count()\n",
    "print(f\"Liczba klientów przed INSERT: {count_before}\")\n",
    "\n",
    "# Utwórz nowe dane do dodania\n",
    "new_customers_data = [\n",
    "    (\"CUST_9001\", \"Jan\", \"Kowalski\", \"jan.kowalski@example.com\", \"Poland\", \"2025-01-15\"),\n",
    "    (\"CUST_9002\", \"Anna\", \"Nowak\", \"anna.nowak@example.com\", \"Poland\", \"2025-01-16\"),\n",
    "    (\"CUST_9003\", \"Piotr\", \"Wiśniewski\", \"piotr.wisniewski@example.com\", \"Poland\", \"2025-01-17\")\n",
    "]\n",
    "\n",
    "new_customers_df = spark.createDataFrame(\n",
    "    new_customers_data,\n",
    "    [\"customer_id\", \"first_name\",\"last_name\",\"email\", \"country\", \"registration_date\"]\n",
    ")\n",
    "from pyspark.sql.functions import col; new_customers_df = new_customers_df.select(\"customer_id\",\"first_name\",\"last_name\",\"email\", \"country\", col(\"registration_date\").cast(\"date\"))\n",
    "\n",
    "display(new_customers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1a419f-45bd-4d80-95ff-cb936e31f635",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.table(CUSTOMERS_DELTA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cedf631b-669c-4ede-97d6-c7e664b423a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Append nowych klientów\n",
    "new_customers_df.write.mode(\"append\").saveAsTable(CUSTOMERS_DELTA)\n",
    "\n",
    "# Policz rekordy po\n",
    "count_after = spark.table(CUSTOMERS_DELTA).count()\n",
    "print(f\"Liczba klientów po INSERT: {count_after}\")\n",
    "print(f\"Dodano: {count_after - count_before} nowych rekordów\")\n",
    "\n",
    "# Sprawdź nowe rekordy\n",
    "print(\"\\n=== Nowo dodane rekordy ===\")\n",
    "display(\n",
    "    spark.table(CUSTOMERS_DELTA)\n",
    "    .filter(F.col(\"customer_id\").isin([\"CUST_9001\", \"CUST_9002\", \"CUST_9003\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dff40ba-8d00-4e3a-aa50-59f4d421e83f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Operacja INSERT (append mode) dodaje nowe rekordy bez modyfikacji istniejących. Delta Lake:\n",
    "- Tworzy nowe pliki Parquet z nowymi danymi\n",
    "- Dodaje transakcję do Delta Log\n",
    "- Zachowuje atomicity - albo wszystkie rekordy są dodane, albo żaden\n",
    "\n",
    "Mode `append` jest najbezpieczniejszy - nie modyfikuje istniejących danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dfa5e9e-bf49-42b5-905c-412b0abd2ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.2: UPDATE - Modyfikacja rekordów\n",
    "\n",
    "**Cel:** Aktualizacja email dla określonych klientów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd1e46b7-904e-4c16-a86b-7c399c53845c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wyświetl rekordy przed UPDATE\n",
    "print(\"=== Przed UPDATE ===\")\n",
    "display(\n",
    "    spark.table(CUSTOMERS_DELTA)\n",
    "    .filter(F.col(\"customer_id\") == \"CUST_9001\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d323dd9-51ee-4d02-b974-bc952c664afb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 2.2 - UPDATE\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Wykonaj UPDATE używając Delta Table API\n",
    "deltaTable = DeltaTable.forName(spark, CUSTOMERS_DELTA)\n",
    "\n",
    "deltaTable.update(\n",
    "    condition = \"customer_id = 'CUST_9001'\",\n",
    "    set = { \"email\": \"'jan.kowalski.NEW@example.com'\" }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63f8aae2-c8b1-44dd-8d4d-601366b1bd6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n✓ UPDATE wykonany\")\n",
    "\n",
    "# Wyświetl rekordy po UPDATE\n",
    "print(\"\\n=== Po UPDATE ===\")\n",
    "display(\n",
    "    spark.table(CUSTOMERS_DELTA)\n",
    "    .filter(F.col(\"customer_id\") == \"CUST_9001\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae732ff3-5d2f-4cb4-bafe-af9d4181feb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "UPDATE w Delta Lake:\n",
    "- Nie modyfikuje istniejących plików Parquet (immutable)\n",
    "- Tworzy nowe pliki z zaktualizowanymi rekordami\n",
    "- Oznacza stare pliki jako usunięte w Delta Log\n",
    "- Copy-on-write semantics zapewnia izolację czytających transakcji\n",
    "\n",
    "Delta Table API (`DeltaTable.forName()`) jest rekomendowanym sposobem wykonywania UPDATE/DELETE/MERGE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbb0fb54-e481-4d1e-b1ad-5a1be4876020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.3: DELETE - Usuwanie rekordów\n",
    "\n",
    "**Cel:** Usunięcie określonych klientów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da736e9-5c2b-45ef-a1b2-3cf733cf0a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Policz rekordy przed DELETE\n",
    "count_before = spark.table(CUSTOMERS_DELTA).count()\n",
    "print(f\"Liczba klientów przed DELETE: {count_before}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ebe8c30-25fd-4e72-acf2-05710deb0e5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 2.3 - DELETE\n",
    "\n",
    "# Wykonaj DELETE\n",
    "deltaTable = DeltaTable.forName(spark, CUSTOMERS_DELTA)\n",
    "\n",
    "(\n",
    "    deltaTable.delete(\n",
    "        condition = \"customer_id IN ('CUST_9001', 'CUST_9002')\"\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deab507c-aa10-40eb-b0a2-5b42534adc92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n✓ DELETE wykonany\")\n",
    "\n",
    "# Policz rekordy po DELETE\n",
    "count_after = spark.table(CUSTOMERS_DELTA).count()\n",
    "print(f\"Liczba klientów po DELETE: {count_after}\")\n",
    "print(f\"Usunięto: {count_before - count_after} rekordów\")\n",
    "\n",
    "# Sprawdź, że rekordy zostały usunięte\n",
    "deleted_check = (\n",
    "    spark.table(CUSTOMERS_DELTA)\n",
    "    .filter(F.col(\"customer_id\").isin(['CUST_9001', 'CUST_9002']))\n",
    "    .count()\n",
    ")\n",
    "print(f\"\\nLiczba rekordów z ID 9002, 9003: {deleted_check} (powinno być 0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42a01f92-57e9-46cc-bc2f-402243f6d4c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 3: MERGE INTO - Upsert Logic\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "MERGE INTO to najbardziej potężna operacja Delta Lake, łącząca INSERT i UPDATE w jednej transakcji. Umożliwia implementację upsert logic: \"if record exists - update, else - insert\". Jest kluczowa dla Slowly Changing Dimensions (SCD) i incremental ETL.\n",
    "\n",
    "**Kluczowe koncepty:**\n",
    "- **Source**: DataFrame z nowymi/zmienionymi danymi\n",
    "- **Target**: Tabela Delta do aktualizacji\n",
    "- **Merge Key**: Kolumny do identyfikacji dopasowanych rekordów\n",
    "- **WHEN MATCHED**: Co zrobić z rekordami, które istnieją w obu\n",
    "- **WHEN NOT MATCHED**: Co zrobić z nowymi rekordami (tylko w source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c36e3aa7-590c-46b2-86f1-257ff3e93454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.1: Podstawowy MERGE (Upsert)\n",
    "\n",
    "**Cel:** Załaduj nowe zamówienia i zaktualizuj istniejące w jednej operacji\n",
    "\n",
    "**Podejście:**\n",
    "1. Utworzenie tabeli orders_delta z danych JSON\n",
    "2. Przygotowanie nowych/zmienionych zamówień\n",
    "3. MERGE na kluczu order_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcbdd520-4ba5-4bd9-9cd7-e770c06a72db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 3.1 - MERGE INTO (podstawowy upsert)\n",
    "\n",
    "# Krok 1: Utworzenie tabeli orders_delta\n",
    "orders_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "(\n",
    "    orders_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(ORDERS_DELTA)\n",
    ")\n",
    "\n",
    "print(f\"✓ Utworzono tabelę: {ORDERS_DELTA}\")\n",
    "count_initial = spark.table(ORDERS_DELTA).count()\n",
    "print(f\"Liczba zamówień (initial): {count_initial}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8969b012-5eec-4ada-88aa-1abc9539a2f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Krok 2: Przygotowanie danych do MERGE\n",
    "# Symulacja nowych i zmienionych zamówień\n",
    "\n",
    "updates_data = [\n",
    "    # Istniejące zamówienie - zmiana total_amount (UPDATE)\n",
    "    (\"ORD00000001\", \"CUST005909\", \"PROD000164\", \"STORE017\", \"2024-12-31T23:56:00\", \n",
    "     1, 206.74, 0, 250.00, \"Cash\"),\n",
    "    \n",
    "    # Nowe zamówienie (INSERT)\n",
    "    (\"ORD99999999\", \"CUST009999\", \"PROD001234\", \"STORE050\", \"2025-01-20T14:30:00\", \n",
    "     2, 149.99, 10, 269.98, \"Credit Card\"),\n",
    "    \n",
    "    # Kolejne nowe zamówienie (INSERT)\n",
    "    (\"ORD99999998\", \"CUST009998\", \"PROD000567\", \"STORE025\", \"2025-01-21T10:15:00\", \n",
    "     3, 66.50, 5, 189.53, \"Debit Card\")\n",
    "]\n",
    "\n",
    "updates_df = spark.createDataFrame(\n",
    "    updates_data,\n",
    "    [\"order_id\", \"customer_id\", \"product_id\", \"store_id\", \"order_datetime\",\n",
    "     \"quantity\", \"unit_price\", \"discount_percent\", \"total_amount\", \"payment_method\"]\n",
    ")\n",
    "\n",
    "print(\"=== Dane do MERGE ===\")\n",
    "print(\"Uwaga: Używamy order_datetime (jak w orders_batch.json)\")\n",
    "display(updates_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d455718-cbcb-4a66-adf5-b254a725401b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Krok 3: Wykonanie MERGE\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "deltaTable = DeltaTable.forName(spark, ORDERS_DELTA)\n",
    "\n",
    "(\n",
    "    deltaTable.alias(\"target\")\n",
    "    .merge(\n",
    "        updates_df.alias(\"source\"),\n",
    "        \"target.order_id = source.order_id\"  # Merge key\n",
    "    )\n",
    "    .whenMatchedUpdate(set = {\n",
    "        \"total_amount\": \"source.total_amount\",\n",
    "        \"payment_method\": \"source.payment_method\"\n",
    "    })\n",
    "    .whenNotMatchedInsert(values = {\n",
    "        \"order_id\": \"source.order_id\",\n",
    "        \"customer_id\": \"source.customer_id\",\n",
    "        \"product_id\": \"source.product_id\",\n",
    "        \"store_id\": \"source.store_id\",\n",
    "        \"order_datetime\": \"source.order_datetime\",\n",
    "        \"quantity\": \"source.quantity\",\n",
    "        \"unit_price\": \"source.unit_price\",\n",
    "        \"discount_percent\": \"source.discount_percent\",\n",
    "        \"total_amount\": \"source.total_amount\",\n",
    "        \"payment_method\": \"source.payment_method\"\n",
    "    })\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "print(\"\\n✓ MERGE wykonany\")\n",
    "\n",
    "count_after = spark.table(ORDERS_DELTA).count()\n",
    "print(f\"Liczba zamówień (po MERGE): {count_after}\")\n",
    "print(f\"Dodano nowych: {count_after - count_initial}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e61ee25a-914a-4f85-bf0d-a8b83cafbeb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weryfikacja wyników MERGE\n",
    "\n",
    "print(\"=== Zamówienie zaktualizowane (order_id = ORD00000001) ===\")\n",
    "display(\n",
    "    spark.table(ORDERS_DELTA)\n",
    "    .filter(F.col(\"order_id\") == \"ORD00000001\")\n",
    ")\n",
    "\n",
    "print(\"\\n=== Nowe zamówienia (order_id LIKE 'ORD99999%') ===\")\n",
    "display(\n",
    "    spark.table(ORDERS_DELTA)\n",
    "    .filter(F.col(\"order_id\").like(\"ORD99999%\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81c0d4bf-5f09-4fe1-959a-ae51f3bae808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "MERGE INTO w Delta Lake:\n",
    "1. **Merge Key**: `order_id` identyfikuje dopasowane rekordy (klucz biznesowy)\n",
    "2. **WHEN MATCHED**: Aktualizuje `total_amount` i `payment_method` dla istniejących zamówień\n",
    "3. **WHEN NOT MATCHED**: Wstawia nowe zamówienia z wszystkimi kolumnami ze schematu orders\n",
    "4. **Atomicity**: Cała operacja (UPDATE + INSERT) jest jedną transakcją ACID\n",
    "\n",
    "To najbardziej efektywny sposób implementacji incremental data loading w Delta Lake.\n",
    "\n",
    "**Zalety MERGE vs DELETE + INSERT:**\n",
    "- Jedna transakcja zamiast dwóch (szybsze, bezpieczniejsze)\n",
    "- Brak data loss risk przy failure\n",
    "- Lepsza performance dla dużych tabel\n",
    "- Automatyczne partition pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5894543-bc14-4fd3-a42b-65eb92fcfbde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 4: Time Travel\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Time Travel to unikalna funkcja Delta Lake umożliwiająca odczyt danych z dowolnego punktu w historii tabeli. Każda wersja tabeli jest identyfikowana przez:\n",
    "- **Version number**: Liczba całkowita (0, 1, 2, ...)\n",
    "- **Timestamp**: Data i czas transakcji\n",
    "\n",
    "**Kluczowe zastosowania:**\n",
    "- Audyt zmian w danych\n",
    "- Rollback do poprzedniego stanu po błędzie\n",
    "- Reprodukowalność analiz (odczyt danych \"as of\" określonej daty)\n",
    "- Porównanie zmian między wersjami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6705a1f3-7ac1-46a7-9e07-265f283b0a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.1: Odczyt historii wersji\n",
    "\n",
    "**Cel:** Przegląd wszystkich wersji tabeli i ich metadanych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d269429e-82f5-4f64-a5c2-e126cc5c0a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 4.1 - DESCRIBE HISTORY dla Time Travel\n",
    "\n",
    "# Wyświetl pełną historię tabeli customers_delta\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY {CUSTOMERS_DELTA}\")\n",
    "\n",
    "print(\"=== Historia transakcji (DESCRIBE HISTORY) ===\")\n",
    "print(\"Każda wersja reprezentuje transakcję ACID\")\n",
    "\n",
    "display(\n",
    "    history_df.select(\n",
    "        \"version\",\n",
    "        \"timestamp\",\n",
    "        \"operation\",\n",
    "        \"operationParameters\",\n",
    "        \"userName\",\n",
    "        \"operationMetrics\"\n",
    "    )\n",
    "    .orderBy(\"version\", ascending=False)\n",
    ")\n",
    "\n",
    "# Zapisz najnowszą i poprzednią wersję do późniejszego użycia\n",
    "latest_version = history_df.agg({\"version\": \"max\"}).collect()[0][0]\n",
    "print(f\"\\nNajnowsza wersja: {latest_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbfe3519-7c70-430d-bd74-01be18fdcfed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.2: Odczyt danych z określonej wersji\n",
    "\n",
    "**Cel:** Użycie Time Travel do odczytania danych z poprzedniej wersji tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e18892a-27f3-42db-895d-6f4427cedf9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 4.2 - Odczyt danych @ VERSION AS OF\n",
    "\n",
    "# Sprawdź liczbę rekordów w najnowszej wersji\n",
    "current_count = spark.table(CUSTOMERS_DELTA).count()\n",
    "print(f\"Liczba klientów (najnowsza wersja): {current_count}\")\n",
    "\n",
    "# Odczytaj dane z wersji 0 (initial load)\n",
    "version_0_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(CUSTOMERS_DELTA)\n",
    "version_0_count = version_0_df.count()\n",
    "\n",
    "print(f\"Liczba klientów (wersja 0): {version_0_count}\")\n",
    "print(f\"Różnica: {current_count - version_0_count} rekordów\")\n",
    "\n",
    "print(\"\\n=== Dane z wersji 0 (pierwsze 5 rekordów) ===\")\n",
    "display(version_0_df.limit(5))\n",
    "\n",
    "# Alternatywnie: odczyt przez SQL\n",
    "print(\"\\n=== Odczyt przez SQL (wersja 0) ===\")\n",
    "df_sql = spark.sql(f\"SELECT * FROM {CUSTOMERS_DELTA} VERSION AS OF 0 LIMIT 5\")\n",
    "display(df_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77092fce-2222-4512-82f8-df03d75c7213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.3: Rollback tabeli (RESTORE)\n",
    "\n",
    "**Cel:** Przywrócenie tabeli do stanu z poprzedniej wersji\n",
    "\n",
    "**Uwaga:** RESTORE wykonuje nową transakcję, nie usuwa historii!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e93575a-37dd-41fb-a09c-0d8d7692bc98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 4.3 - RESTORE TABLE\n",
    "\n",
    "# Sprawdź aktualną liczbę rekordów\n",
    "before_restore = spark.table(CUSTOMERS_DELTA).count()\n",
    "print(f\"Liczba klientów przed RESTORE: {before_restore}\")\n",
    "\n",
    "# Przywróć tabelę do wersji 0 (initial load)\n",
    "spark.sql(f\"RESTORE TABLE {CUSTOMERS_DELTA} TO VERSION AS OF 0\")\n",
    "\n",
    "print(\"\\n✓ RESTORE wykonany\")\n",
    "\n",
    "# Sprawdź liczbę rekordów po RESTORE\n",
    "after_restore = spark.table(CUSTOMERS_DELTA).count()\n",
    "print(f\"Liczba klientów po RESTORE: {after_restore}\")\n",
    "\n",
    "# Sprawdź historię - RESTORE dodaje nową transakcję!\n",
    "history_after_restore = spark.sql(f\"DESCRIBE HISTORY {CUSTOMERS_DELTA}\")\n",
    "print(\"\\n=== Historia po RESTORE ===\")\n",
    "display(\n",
    "    history_after_restore\n",
    "    .select(\"version\", \"timestamp\", \"operation\", \"operationParameters\")\n",
    "    .orderBy(\"version\", ascending=False)\n",
    "    .limit(3)\n",
    ")\n",
    "\n",
    "print(\"\\nUwaga: RESTORE utworzył nową wersję - historia NIE została utracona!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f03199-241b-4b7c-981f-296be0c61333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 5: Schema Evolution\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Schema Evolution to mechanizm automatycznej adaptacji schematu tabeli Delta do zmian w danych. Delta Lake wspiera:\n",
    "- **Additive schema changes**: Dodawanie nowych kolumn\n",
    "- **Schema enforcement**: Blokowanie niekompatybilnych zmian typu\n",
    "- **Schema merge**: Automatyczne dodawanie kolumn przy zapisie\n",
    "\n",
    "**Kluczowe opcje:**\n",
    "- `mergeSchema=true`: Automatyczne dodawanie nowych kolumn z source DataFrame\n",
    "- `overwriteSchema=true`: Całkowite zastąpienie schematu (uwaga: destructive!)\n",
    "\n",
    "**Kiedy używać:**\n",
    "- Dodawanie nowych atrybutów do danych bez przebudowy pipeline'u\n",
    "- Evolving data models w zgodzie z business requirements\n",
    "- Integracja nowych źródeł danych z dodatkowymi polami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1268b47a-4410-4224-9e84-40307aaff5a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 5.1: Automatyczne dodawanie nowych kolumn\n",
    "\n",
    "**Cel:** Demonstracja additive schema evolution z opcją `mergeSchema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa6cb4c4-762d-460e-81a9-57b70791adbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 5.1 - Schema Evolution z mergeSchema\n",
    "\n",
    "# KROK 1: Sprawdź aktualny schemat tabeli\n",
    "print(\"=== Aktualny schemat tabeli ===\")\n",
    "spark.table(CUSTOMERS_DELTA).printSchema()\n",
    "\n",
    "# Policz kolumny\n",
    "current_columns = len(spark.table(CUSTOMERS_DELTA).columns)\n",
    "print(f\"\\nAktualna liczba kolumn: {current_columns}\")\n",
    "\n",
    "# Utwórz DataFrame z nowymi kolumnami\n",
    "extended_customers = spark.createDataFrame([\n",
    "extended_customers = spark.createDataFrame([\n",
    "    (\"CUST_9999\", \"Ewa\", \"Wiśniewska\", \"ewa.wisn@example.com\", \"+48 600 123 456\",\n",
    "     \"Warsaw\", \"mazowieckie\", \"Poland\", \"2025-01-25\", \"Premium\", \"Premium\", \"PL\")\n",
    "], [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\", \"city\", \"state\", \n",
    "    \"country\", \"registration_date\", \"customer_segment\", \"loyalty_tier\", \"preferred_language\"])\n",
    "\n",
    "print(\"\\n=== Nowy DataFrame z dodatkowymi kolumnami ===\")\n",
    "print(\"Nowe kolumny: loyalty_tier, preferred_language\")\n",
    "extended_customers.printSchema()\n",
    "\n",
    "# Zapis bez mergeSchema - spowoduje błąd!\n",
    "try:\n",
    "    (\n",
    "        extended_customers\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(CUSTOMERS_DELTA)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Błąd (oczekiwany): {str(e)[:200]}...\")\n",
    "    print(\"\\nDelta Lake blokuje niezgodne schematy!\")\n",
    "\n",
    "# Zapis z mergeSchema - automatyczne dodanie kolumn\n",
    "(\n",
    "    extended_customers\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(CUSTOMERS_DELTA)\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Zapis z mergeSchema=true wykonany\")\n",
    "\n",
    "# Sprawdź nowy schemat\n",
    "print(\"\\n=== Schemat po schema evolution ===\")\n",
    "spark.table(CUSTOMERS_DELTA).printSchema()\n",
    "\n",
    "# Sprawdź nowy rekord\n",
    "print(\"\\n=== Nowy rekord z dodatkowymi kolumnami (loyalty_tier, preferred_language) ===\")\n",
    "display(\n",
    "    spark.table(CUSTOMERS_DELTA)\n",
    "    .filter(F.col(\"customer_id\") == \"CUST_9999\")\n",
    "    .select(\"customer_id\", \"first_name\", \"last_name\", \"email\", \"customer_segment\", \n",
    "            \"loyalty_tier\", \"preferred_language\")\n",
    ")\n",
    "\n",
    "# Sprawdź stare rekordy - mają NULL w nowych kolumnach\n",
    "print(\"\\n=== Stare rekordy (mają NULL w loyalty_tier, preferred_language) ===\")\n",
    "display(\n",
    "    spark.table(CUSTOMERS_DELTA)\n",
    "    .filter(F.col(\"customer_id\").like(\"CUST000%\"))\n",
    "    .select(\"customer_id\", \"first_name\", \"last_name\", \"customer_segment\", \n",
    "            \"loyalty_tier\", \"preferred_language\")\n",
    "    .limit(5)\n",
    ")\n",
    "\n",
    "print(\"\\n⚠️  Uwaga: Stare rekordy mają NULL w nowych kolumnach (loyalty_tier, preferred_language)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KROK 2: Utwórz DataFrame z nowymi kolumnami (loyalty_tier, preferred_language)\n",
    "# Używamy istniejącego schematu (first_name, last_name) + nowe kolumny\n",
    "extended_customers = spark.createDataFrame([\n",
    "    (\"CUST_9999\", \"Ewa\", \"Wiśniewska\", \"ewa.wisn@example.com\", \"+48 600 123 456\",\n",
    "     \"Warsaw\", \"mazowieckie\", \"Poland\", \"2025-01-25\", \"Premium\", \"Premium\", \"PL\")\n",
    "], [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\", \"city\", \"state\", \n",
    "    \"country\", \"registration_date\", \"customer_segment\", \"loyalty_tier\", \"preferred_language\"])\n",
    "\n",
    "print(\"=== Nowy DataFrame z dodatkowymi kolumnami ===\")\n",
    "print(\"Nowe kolumny: loyalty_tier, preferred_language\")\n",
    "extended_customers.printSchema()\n",
    "\n",
    "# Policz kolumny w nowym DataFrame\n",
    "new_columns = len(extended_customers.columns)\n",
    "print(f\"\\nLiczba kolumn w nowym DataFrame: {new_columns}\")\n",
    "print(f\"Różnica: +{new_columns - current_columns} nowe kolumny\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KROK 3: Zapis bez mergeSchema - spowoduje błąd!\n",
    "print(\"=== Próba zapisu bez mergeSchema ===\")\n",
    "try:\n",
    "    (\n",
    "        extended_customers\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(CUSTOMERS_DELTA)\n",
    "    )\n",
    "    print(\"✓ Zapis się powiódł (nieoczekiwane!)\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Błąd (oczekiwany): {str(e)[:200]}...\")\n",
    "    print(\"\\nDelta Lake blokuje niezgodne schematy!\")\n",
    "    print(\"Powód: Tabela ma {0} kolumn, nowy DataFrame ma {1} kolumn\".format(\n",
    "        current_columns, new_columns\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KROK 4: Zapis z mergeSchema - automatyczne dodanie kolumn\n",
    "print(\"=== Zapis z mergeSchema=true ===\")\n",
    "\n",
    "(\n",
    "    extended_customers\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(CUSTOMERS_DELTA)\n",
    ")\n",
    "\n",
    "print(\"✓ Zapis z mergeSchema=true wykonany pomyślnie!\")\n",
    "print(\"Delta Lake automatycznie dodał nowe kolumny do schematu tabeli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KROK 5: Sprawdź nowy schemat po schema evolution\n",
    "print(\"=== Schemat po schema evolution ===\")\n",
    "spark.table(CUSTOMERS_DELTA).printSchema()\n",
    "\n",
    "# Policz kolumny po evolution\n",
    "columns_after = len(spark.table(CUSTOMERS_DELTA).columns)\n",
    "print(f\"\\nLiczba kolumn PO evolution: {columns_after}\")\n",
    "print(f\"Dodano kolumn: {columns_after - current_columns}\")\n",
    "print(f\"Nowe kolumny: loyalty_tier, preferred_language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KROK 6: Weryfikacja - nowy rekord z dodatkowymi kolumnami\n",
    "print(\"=== Nowy rekord z dodatkowymi kolumnami (loyalty_tier, preferred_language) ===\")\n",
    "display(\n",
    "    spark.table(CUSTOMERS_DELTA)\n",
    "    .filter(F.col(\"customer_id\") == \"CUST_9999\")\n",
    "    .select(\"customer_id\", \"first_name\", \"last_name\", \"email\", \"customer_segment\", \n",
    "            \"loyalty_tier\", \"preferred_language\")\n",
    ")\n",
    "\n",
    "# Sprawdź stare rekordy - mają NULL w nowych kolumnach\n",
    "print(\"\\n=== Stare rekordy (mają NULL w loyalty_tier, preferred_language) ===\")\n",
    "display(\n",
    "    spark.table(CUSTOMERS_DELTA)\n",
    "    .filter(F.col(\"customer_id\").like(\"CUST000%\"))\n",
    "    .select(\"customer_id\", \"first_name\", \"last_name\", \"customer_segment\", \n",
    "            \"loyalty_tier\", \"preferred_language\")\n",
    "    .limit(5)\n",
    ")\n",
    "\n",
    "print(\"\\n⚠️  Uwaga: Stare rekordy mają NULL w nowych kolumnach (loyalty_tier, preferred_language)!\")\n",
    "print(\"To jest normalne zachowanie przy additive schema evolution.\")\n",
    "print(\"Możesz później wypełnić NULL-e używając UPDATE lub fillna()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec2dae63-fba0-4701-8cd6-8c0e976a98ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 6: Optymalizacja tabeli Delta\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Delta Lake akumuluje wiele małych plików przy częstych zapisach. Optymalizacja jest kluczowa dla performance:\n",
    "\n",
    "**OPTIMIZE (Compaction):**\n",
    "- Łączy małe pliki Parquet w większe (target: 1 GB)\n",
    "- Zmniejsza overhead odczytu (mniej plików = mniej I/O operations)\n",
    "- Poprawia query performance\n",
    "\n",
    "**ZORDER BY:**\n",
    "- Co-locality algorytm dla multi-dimensional clustering\n",
    "- Organizuje dane wg często filtrowanych kolumn\n",
    "- Zmniejsza data skipping overhead\n",
    "\n",
    "**VACUUM:**\n",
    "- Usuwa stare pliki Parquet nieużywane przez aktywne wersje\n",
    "- Zwalnia storage space\n",
    "- Domyślnie: retention 7 dni (chroni Time Travel)\n",
    "\n",
    "**Uwaga:** Po VACUUM nie można odczytać starszych wersji poza retention period!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7eea720a-ca53-4143-a7d0-c08f01234edb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 6.1: OPTIMIZE i ZORDER BY\n",
    "\n",
    "**Cel:** Optymalizacja tabeli orders_delta dla query performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18fa9b7f-023b-4a2b-908a-4f330743570f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 6.1 - OPTIMIZE i ZORDER BY\n",
    "\n",
    "# Sprawdź stan tabeli przed OPTIMIZE\n",
    "detail_before = spark.sql(f\"DESCRIBE DETAIL {ORDERS_DELTA}\").collect()[0]\n",
    "print(\"=== Stan tabeli PRZED OPTIMIZE ===\")\n",
    "print(f\"Liczba plików: {detail_before['numFiles']}\")\n",
    "print(f\"Rozmiar (bytes): {detail_before['sizeInBytes']}\")\n",
    "\n",
    "# Wykonaj OPTIMIZE\n",
    "spark.sql(f\"OPTIMIZE {ORDERS_DELTA}\")\n",
    "print(\"\\n✓ OPTIMIZE wykonany\")\n",
    "\n",
    "# Sprawdź stan po OPTIMIZE\n",
    "detail_after = spark.sql(f\"DESCRIBE DETAIL {ORDERS_DELTA}\").collect()[0]\n",
    "print(\"\\n=== Stan tabeli PO OPTIMIZE ===\")\n",
    "print(f\"Liczba plików: {detail_after['numFiles']}\")\n",
    "print(f\"Rozmiar (bytes): {detail_after['sizeInBytes']}\")\n",
    "\n",
    "# OPTIMIZE z ZORDER BY (dla często filtrowanych kolumn)\n",
    "spark.sql(f\"OPTIMIZE {ORDERS_DELTA} ZORDER BY (customer_id, order_datetime)\")\n",
    "print(\"\\n✓ OPTIMIZE ZORDER BY wykonany\")\n",
    "\n",
    "print(\"\\nUwaga: ZORDER BY sortuje dane wg customer_id i order_datetime dla data skipping!\")\n",
    "print(\"Zapytania z WHERE customer_id = 'X' AND order_datetime >= 'Y' będą znacznie szybsze!\")\n",
    "\n",
    "# Sprawdź historię optymalizacji\n",
    "optimize_history = spark.sql(f\"DESCRIBE HISTORY {ORDERS_DELTA}\")\n",
    "display(\n",
    "    optimize_history\n",
    "    .filter(F.col(\"operation\") == \"OPTIMIZE\")\n",
    "    .select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac711ff8-e3b4-43ea-b933-4ac8fcac12ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 6.2: VACUUM - czyszczenie starych plików\n",
    "\n",
    "**Cel:** Usunięcie nieużywanych plików Parquet dla zwolnienia storage\n",
    "\n",
    "**Ostrzeżenie:** VACUUM usuwa stare pliki - Time Travel będzie ograniczony do retention period!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2170b503-29f9-49ff-864b-5ed4e45b7379",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 6.2 - VACUUM\n",
    "\n",
    "# Sprawdź aktualną liczbę plików\n",
    "detail_before_vacuum = spark.sql(f\"DESCRIBE DETAIL {CUSTOMERS_DELTA}\").collect()[0]\n",
    "print(\"=== Przed VACUUM ===\")\n",
    "print(f\"Liczba plików: {detail_before_vacuum['numFiles']}\")\n",
    "\n",
    "# DRY RUN - pokaż co zostanie usunięte (bez faktycznego usuwania)\n",
    "print(\"\\n=== VACUUM DRY RUN (retention 0 hours dla demo) ===\")\n",
    "# Uwaga: W produkcji nigdy nie używaj retention 0 hours!\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "dry_run_result = spark.sql(f\"VACUUM {CUSTOMERS_DELTA} RETAIN 0 HOURS DRY RUN\")\n",
    "print(\"Pliki do usunięcia:\")\n",
    "display(dry_run_result)\n",
    "\n",
    "# Wykonaj VACUUM (dla demo: 0 hours, w prod: minimum 7 dni)\n",
    "spark.sql(f\"VACUUM {CUSTOMERS_DELTA} RETAIN 0 HOURS\")\n",
    "print(\"\\n✓ VACUUM wykonany\")\n",
    "\n",
    "# Przywróć domyślne ustawienie\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"true\")\n",
    "\n",
    "# Sprawdź efekt\n",
    "detail_after_vacuum = spark.sql(f\"DESCRIBE DETAIL {CUSTOMERS_DELTA}\").collect()[0]\n",
    "print(\"\\n=== Po VACUUM ===\")\n",
    "print(f\"Liczba plików: {detail_after_vacuum['numFiles']}\")\n",
    "print(f\"Rozmiar (bytes): {detail_after_vacuum['sizeInBytes']}\")\n",
    "\n",
    "print(\"\\n⚠️  Uwaga: Starsze wersje spoza retention period nie są już dostępne dla Time Travel!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "857c8ead-92f2-4e39-ad87-7d11cbac8e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "**Organizacja tabel Delta:**\n",
    "1. **Naming convention**: Używaj `{layer}.{domain}_{entity}` (np. `bronze.sales_orders`)\n",
    "2. **Partitioning**: Partycjonuj tylko duże tabele (>1TB) po często filtrowanych kolumnach (np. date)\n",
    "3. **Table properties**: Ustaw `delta.autoOptimize.optimizeWrite = true` dla częstych małych zapisów\n",
    "\n",
    "**Optymalizacja performance:**\n",
    "1. **OPTIMIZE**: Uruchamiaj regularnie (np. co noc) dla tabel z częstymi zapisami\n",
    "2. **ZORDER BY**: Używaj dla 2-4 najczęściej filtrowanych kolumn (więcej = diminishing returns)\n",
    "3. **Data skipping**: Wykorzystuj statistics w Delta Log - filtruj po kolumnach z ZORDER\n",
    "\n",
    "**CRUD operations:**\n",
    "1. **MERGE**: Preferuj zamiast DELETE + INSERT dla upsert logic (atomicity!)\n",
    "2. **Merge keys**: Zawsze używaj indeksowanych kolumn (np. primary keys)\n",
    "3. **Predicates**: Dodawaj dodatkowe predykaty w MERGE dla partition pruning\n",
    "\n",
    "**Time Travel i maintenance:**\n",
    "1. **VACUUM retention**: Minimum 7 dni (domyślnie) - chroni Time Travel i concurrent readers\n",
    "2. **DESCRIBE HISTORY**: Monitoruj operacje i rozmiar Delta Log\n",
    "3. **Checkpoint**: Tworzony automatycznie co 10 transakcji - nie wymaga interwencji\n",
    "\n",
    "**Schema evolution:**\n",
    "1. **mergeSchema**: Używaj ostrożnie - może spowodować NULL-e w starych rekordach\n",
    "2. **overwriteSchema**: Tylko dla development - destrukcyjna operacja!\n",
    "3. **NOT NULL constraints**: Definiuj przed pierwszym zapisem (trudno dodać później)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acbc440c-18b9-46a4-91a9-3cea4632b5ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Problem 1: \"Schema mismatch\" przy zapisie**\n",
    "```\n",
    "AnalysisException: A schema mismatch detected when writing to the Delta table\n",
    "```\n",
    "**Rozwiązanie:**\n",
    "- Użyj `option(\"mergeSchema\", \"true\")` dla additive changes\n",
    "- Użyj `option(\"overwriteSchema\", \"true\")` dla pełnej zmiany schematu (uwaga: destructive!)\n",
    "\n",
    "**Problem 2: \"ConcurrentAppendException\"**\n",
    "```\n",
    "ConcurrentAppendException: Files were added to the table by a concurrent update\n",
    "```\n",
    "**Rozwiązanie:**\n",
    "- Delta Lake używa optimistic concurrency - retry operacji\n",
    "- Dla częstych konfliktów: zastosuj partition pruning lub MERGE z predykatami\n",
    "\n",
    "**Problem 3: Time Travel nie działa dla starszych wersji**\n",
    "```\n",
    "VersionNotFoundException: Cannot find version X\n",
    "```\n",
    "**Rozwiązanie:**\n",
    "- Sprawdź czy VACUUM nie usunął starych plików\n",
    "- Zwiększ retention period: `VACUUM table RETAIN 30 DAYS`\n",
    "\n",
    "**Problem 4: Słaba performance query po wielu UPDATE/DELETE**\n",
    "**Rozwiązanie:**\n",
    "- Uruchom `OPTIMIZE` dla compaction małych plików\n",
    "- Użyj `OPTIMIZE ZORDER BY` dla często filtrowanych kolumn\n",
    "\n",
    "**Problem 5: Duże zużycie storage pomimo VACUUM**\n",
    "**Rozwiązanie:**\n",
    "- Sprawdź `DESCRIBE DETAIL` - czy Delta Log nie rośnie?\n",
    "- Checkpoint tworzony automatycznie co 10 transakcji\n",
    "- Rozważ manual checkpoint: `OPTIMIZE table` tworzy checkpoint jako side effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb9150e1-2b35-483e-997a-fe9cddcb0d1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "**W tym notebooku nauczyliśmy się:**\n",
    "\n",
    "✅ **Delta Lake Core Features:**\n",
    "- Transakcyjna warstwa ACID dla data lakehouse\n",
    "- Delta Log jako dziennik metadanych wszystkich operacji\n",
    "- Schema enforcement i automatic schema evolution\n",
    "\n",
    "✅ **CRUD Operations:**\n",
    "- INSERT (append mode) - dodawanie nowych rekordów\n",
    "- UPDATE - modyfikacja istniejących danych z Copy-on-Write semantics\n",
    "- DELETE - usuwanie rekordów spełniających warunek\n",
    "- MERGE INTO - upsert logic w jednej transakcji ACID\n",
    "\n",
    "✅ **Time Travel:**\n",
    "- Odczyt danych z dowolnej wersji: `VERSION AS OF`, `TIMESTAMP AS OF`\n",
    "- RESTORE TABLE - rollback do poprzedniego stanu (tworzy nową wersję!)\n",
    "- DESCRIBE HISTORY - audyt wszystkich operacji na tabeli\n",
    "\n",
    "✅ **Schema Evolution:**\n",
    "- Additive changes z `mergeSchema=true`\n",
    "- Automatyczna adaptacja do nowych kolumn w danych źródłowych\n",
    "- Schema enforcement chroni przed niekompatybilnymi zmianami\n",
    "\n",
    "✅ **Optymalizacja:**\n",
    "- OPTIMIZE - compaction małych plików dla performance\n",
    "- ZORDER BY - multi-dimensional clustering dla data skipping\n",
    "- VACUUM - usuwanie nieużywanych plików (uwaga: ogranicza Time Travel!)\n",
    "\n",
    "**Kluczowe wnioski:**\n",
    "1. Delta Lake rozwiązuje fundamentalne problemy tradycyjnych data lake'ów (brak transakcji, trudności z UPDATE/DELETE)\n",
    "2. MERGE INTO jest kluczową operacją dla incremental ETL i Slowly Changing Dimensions\n",
    "3. Time Travel umożliwia audyt, rollback i reprodukowalność analiz\n",
    "4. Regularna optymalizacja (OPTIMIZE, VACUUM) jest niezbędna dla production workloads\n",
    "5. Schema evolution pozwala na evolving data models bez przebudowy pipeline'ów\n",
    "\n",
    "**Następne kroki:**\n",
    "- Delta Live Tables - deklaratywne pipeline'y z automatycznym dependency management\n",
    "- Change Data Feed - incremental processing z CDC patterns\n",
    "- Delta Sharing - bezpieczne udostępnianie danych bez kopiowania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f178eef-dfae-474b-9d20-80e01c80305a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Opcjonalnie: usuń utworzone tabele Demo po zakończeniu ćwiczeń:\n",
    "\n",
    "**Uwaga:** Wykonaj cleanup tylko jeśli nie potrzebujesz już tych tabel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67d05eb6-fcc0-4320-b2e4-e2cb471f0b43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup - usuń tabele demo\n",
    "\n",
    "# Odkomentuj poniższe linie aby usunąć tabele:\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CUSTOMERS_DELTA}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {ORDERS_DELTA}\")\n",
    "# print(\"✓ Tabele usunięte\")\n",
    "\n",
    "print(\"Cleanup wyłączony (odkomentuj kod aby usunąć tabele)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "01_delta_lake_operations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
