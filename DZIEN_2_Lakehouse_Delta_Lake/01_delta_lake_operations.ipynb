{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a373df9",
   "metadata": {},
   "source": [
    "# Delta Lake Operations - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Zrozumienie fundamentów Delta Lake i praktyczne zastosowanie operacji CRUD, Time Travel, optymalizacji i Change Data Feed\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Delta Lake core features: ACID, Delta Log, Schema enforcement\n",
    "- Schema evolution (additive, automatic)\n",
    "- Time Travel i Copy-on-write\n",
    "- CRUD operations: CREATE TABLE, INSERT, UPDATE, DELETE, MERGE INTO\n",
    "- Optymalizacja: OPTIMIZE, ZORDER BY, VACUUM\n",
    "- Change Data Feed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7fab7c",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 2 - Lakehouse & Delta Lake\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z 2-4 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b9b43d",
   "metadata": {},
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Wprowadzenie do Delta Lake jako transakcyjnej warstwy storage nad Data Lake\n",
    "\n",
    "**Podstawowe pojęcia:**\n",
    "- **Delta Lake**: Open-source storage layer zapewniający ACID transactions dla Apache Spark\n",
    "- **Delta Log**: Transakcyjny log przechowujący metadane o wszystkich zmianach w tabeli\n",
    "- **Schema Enforcement**: Automatyczna walidacja zgodności schematów przy zapisie\n",
    "- **Time Travel**: Możliwość dostępu do poprzednich wersji danych\n",
    "\n",
    "**Dlaczego to ważne?**\n",
    "Delta Lake rozwiązuje fundamentalne problemy Data Lake: brak transakcji, schema drift, trudności z aktualizacjami i quality assurance. Zapewnia niezawodność Data Warehouse z elastycznością Data Lake."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246c0982",
   "metadata": {},
   "source": [
    "## Izolacja per użytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcef10ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278f8af6",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95619fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Wyświetl kontekst użytkownika\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (CATALOG, BRONZE_SCHEMA, SILVER_SCHEMA, GOLD_SCHEMA)\n",
    "    ], ['catalog', 'bronze_schema', 'silver_schema', 'gold_schema'])\n",
    ")\n",
    "\n",
    "# Ustaw katalog i schemat jako domyślne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31dea81",
   "metadata": {},
   "source": [
    "## Sekcja 1: Delta Lake Core Features\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Delta Lake to warstwa transakcyjna nad Parquet, która zapewnia ACID properties (Atomicity, Consistency, Isolation, Durability). Każda operacja na tabeli Delta jest rejestrowana w Delta Log - JSON pliku zawierającym metadane o zmianach.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **ACID Transactions**: Wszystkie operacje są atomowe i spójne\n",
    "- **Delta Log**: `_delta_log/` folder z JSON plikami opisującymi każdą transakcję\n",
    "- **Schema Enforcement**: Automatyczna walidacja zgodności schematów\n",
    "- **Unified Batch and Streaming**: Jedna tabela obsługuje zarówno batch jak i streaming\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Transakcyjne aktualizacje w Data Lake\n",
    "- Zapewnienie jakości danych poprzez schema validation\n",
    "- Jednolity dostęp do danych dla batch i streaming workloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcadceb9",
   "metadata": {},
   "source": [
    "### Przykład 1.1: Utworzenie pierwszej tabeli Delta\n",
    "\n",
    "**Cel:** Demonstracja tworzenia tabeli Delta i podstawowych właściwości\n",
    "\n",
    "**Podejście:**\n",
    "1. Wczytanie danych z Unity Catalog Volume\n",
    "2. Utworzenie managed table w formacie Delta\n",
    "3. Eksploracja Delta Log i metadanych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ffd89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wczytaj dane klientów z Unity Catalog Volume\n",
    "customers_df = (spark.read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(f\"{DATASET_BASE_PATH}/customers/customers.csv\")\n",
    ")\n",
    "\n",
    "# Utwórz managed Delta table\n",
    "customers_df.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "\n",
    "display(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be2ba69",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Utworzono managed Delta table w Unity Catalog. Format Delta automatycznie:\n",
    "- Stworzył `_delta_log/` folder z metadanymi transakcji\n",
    "- Zarejestrował schemat tabeli w Unity Catalog\n",
    "- Zastosował kompresję Parquet z dodatkowymi Delta features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a67da9b",
   "metadata": {},
   "source": [
    "### Przykład 1.2: Schema Enforcement w akcji\n",
    "\n",
    "**Cel:** Demonstracja automatycznej walidacji schematów przy wstawianiu danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d17904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź aktualny schemat tabeli\n",
    "spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9647afc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Próba wstawienia danych z niepoprawnym schematem\n",
    "invalid_data = spark.createDataFrame([\n",
    "    (999, \"Test Customer\", 25.5, \"invalid_email\", \"2023-01-01\")  # age jako float zamiast int\n",
    "], [\"customer_id\", \"name\", \"age\", \"email\", \"registration_date\"])\n",
    "\n",
    "try:\n",
    "    invalid_data.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "except Exception as e:\n",
    "    display(\n",
    "        spark.createDataFrame([\n",
    "            (\"Schema enforcement w działaniu\", str(e)[:100] + \"...\")\n",
    "        ], [\"message\", \"error\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731631ff",
   "metadata": {},
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Schema enforcement automatycznie odrzucił dane z niepoprawnym typem. Delta Lake porównuje schemat nowych danych ze schematem tabeli i blokuje niezgodne wstawienia, zapewniając consistency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bc3fcd",
   "metadata": {},
   "source": [
    "## Sekcja 2: Schema Evolution\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Schema Evolution pozwala na kontrolowane dodawanie nowych kolumn do istniejących tabel Delta bez przerywania działania aplikacji. Delta Lake wspiera additive schema changes automatycznie."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b97fb11",
   "metadata": {},
   "source": [
    "### Przykład 2.1: Automatyczne dodawanie kolumn\n",
    "\n",
    "**Cel:** Demonstracja automatycznej ewolucji schematu przy dodawaniu nowych kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215dabdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dane z dodatkową kolumną\n",
    "extended_customers = spark.createDataFrame([\n",
    "    (1001, \"New Customer\", 30, \"new@example.com\", \"2023-12-01\", \"Premium\"),\n",
    "    (1002, \"Another Customer\", 25, \"another@example.com\", \"2023-12-02\", \"Standard\")\n",
    "], [\"customer_id\", \"name\", \"age\", \"email\", \"registration_date\", \"customer_tier\"])\n",
    "\n",
    "# Włącz automatic schema evolution\n",
    "extended_customers.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b618e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź nowy schemat\n",
    "spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eef7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikuj dane - nowa kolumna ma NULL dla starych rekordów\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .select(\"customer_id\", \"name\", \"customer_tier\")\n",
    "    .orderBy(\"customer_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9287bda6",
   "metadata": {},
   "source": [
    "## Sekcja 3: Time Travel i Copy-on-Write\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Time Travel to kluczowa funkcjonalność Delta Lake umożliwiająca dostęp do poprzednich wersji danych. Bazuje na Copy-on-Write mechanizmie - każda zmiana tworzy nową wersję plików, a stare wersje pozostają dostępne."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24614da6",
   "metadata": {},
   "source": [
    "### Przykład 3.1: Eksploracja historii tabeli\n",
    "\n",
    "**Cel:** Użycie DESCRIBE HISTORY do analizy wszystkich operacji na tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78369254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pokaż historię wszystkich operacji na tabeli\n",
    "display(\n",
    "    spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc3076d",
   "metadata": {},
   "source": [
    "### Przykład 3.2: Time Travel queries\n",
    "\n",
    "**Cel:** Dostęp do poprzednich wersji danych używając VERSION AS OF i TIMESTAMP AS OF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b7bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dostęp do danych z wersji 0 (przed schema evolution)\n",
    "version_0_data = spark.sql(f\"\"\"\n",
    "    SELECT customer_id, name, age, email \n",
    "    FROM {CATALOG}.{BRONZE_SCHEMA}.customers_delta VERSION AS OF 0\n",
    "    ORDER BY customer_id\n",
    "\"\"\")\n",
    "\n",
    "display(version_0_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e025209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Porównaj liczbę rekordów między wersjami\n",
    "current_count = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").count()\n",
    "version_0_count = spark.sql(f\"SELECT * FROM {CATALOG}.{BRONZE_SCHEMA}.customers_delta VERSION AS OF 0\").count()\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Current version\", current_count),\n",
    "        (\"Version 0\", version_0_count)\n",
    "    ], [\"version\", \"record_count\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c220416b",
   "metadata": {},
   "source": [
    "## Sekcja 4: CRUD Operations\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Delta Lake wspiera pełen zakres operacji CRUD (Create, Read, Update, Delete), co czyni go idealnym dla transakcyjnych workloadów w Data Lake. Wszystkie operacje są atomowe i ACID-compliant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17433d1",
   "metadata": {},
   "source": [
    "### Przykład 4.1: INSERT operation\n",
    "\n",
    "**Cel:** Dodawanie nowych rekordów do istniejącej tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf6eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSERT nowych klientów\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    VALUES \n",
    "        (2001, 'Insert Customer 1', 28, 'insert1@example.com', '2023-12-10', 'Gold'),\n",
    "        (2002, 'Insert Customer 2', 35, 'insert2@example.com', '2023-12-11', 'Silver')\n",
    "\"\"\")\n",
    "\n",
    "# Weryfikuj wstawienie\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .filter(F.col(\"customer_id\") >= 2000)\n",
    "    .orderBy(\"customer_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90275d6d",
   "metadata": {},
   "source": [
    "### Przykład 4.2: UPDATE operation\n",
    "\n",
    "**Cel:** Aktualizacja istniejących rekordów w tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85312ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UPDATE customer tier dla specific customers\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    SET customer_tier = 'Platinum'\n",
    "    WHERE customer_id IN (1001, 2001)\n",
    "\"\"\")\n",
    "\n",
    "# Weryfikuj aktualizację\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .filter(F.col(\"customer_tier\") == \"Platinum\")\n",
    "    .select(\"customer_id\", \"name\", \"customer_tier\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5fd1d6",
   "metadata": {},
   "source": [
    "### Przykład 4.3: DELETE operation\n",
    "\n",
    "**Cel:** Usuwanie rekordów z tabeli Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce35b32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE specific customer\n",
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    WHERE customer_id = 2002\n",
    "\"\"\")\n",
    "\n",
    "# Weryfikuj usunięcie\n",
    "deleted_check = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\") \\\n",
    "    .filter(F.col(\"customer_id\") == 2002) \\\n",
    "    .count()\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Records with customer_id 2002\", deleted_check)\n",
    "    ], [\"description\", \"count\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47731f5",
   "metadata": {},
   "source": [
    "## Sekcja 5: MERGE INTO Operations\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "MERGE INTO to potężna operacja umożliwiająca upsert (update + insert) w jednej transakcji. Szczególnie przydatna przy przetwarzaniu zmian z systemów transakcyjnych (CDC - Change Data Capture)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f21549",
   "metadata": {},
   "source": [
    "### Przykład 5.1: Podstawowy MERGE INTO\n",
    "\n",
    "**Cel:** Demonstracja upsert operation - update istniejących i insert nowych rekordów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5782b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Przygotuj dane do merge (mix updates i nowych rekordów)\n",
    "merge_data = spark.createDataFrame([\n",
    "    (1001, \"Updated Customer Name\", 31, \"updated@example.com\", \"2023-12-01\", \"Diamond\"),  # Update\n",
    "    (3001, \"Brand New Customer\", 29, \"brand.new@example.com\", \"2023-12-15\", \"Bronze\"),   # Insert\n",
    "    (3002, \"Another New Customer\", 33, \"another.new@example.com\", \"2023-12-16\", \"Silver\") # Insert\n",
    "], [\"customer_id\", \"name\", \"age\", \"email\", \"registration_date\", \"customer_tier\"])\n",
    "\n",
    "# Utwórz temporary view dla merge operation\n",
    "merge_data.createOrReplaceTempView(\"customer_updates\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e747b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MERGE INTO operation\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta AS target\n",
    "    USING customer_updates AS source\n",
    "    ON target.customer_id = source.customer_id\n",
    "    \n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            name = source.name,\n",
    "            age = source.age,\n",
    "            email = source.email,\n",
    "            customer_tier = source.customer_tier\n",
    "    \n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (customer_id, name, age, email, registration_date, customer_tier)\n",
    "        VALUES (source.customer_id, source.name, source.age, source.email, source.registration_date, source.customer_tier)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423f9320",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikuj wyniki MERGE\n",
    "display(\n",
    "    spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "    .filter(F.col(\"customer_id\").isin([1001, 3001, 3002]))\n",
    "    .orderBy(\"customer_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999adf45",
   "metadata": {},
   "source": [
    "## Sekcja 6: Metadane i Analytics\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Delta Lake oferuje bogate metadane o tabelach i operacjach. DESCRIBE DETAIL dostarcza informacji o strukturze plików, partitioning, i właściwościach tabeli."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6133735b",
   "metadata": {},
   "source": [
    "### Przykład 6.1: DESCRIBE DETAIL\n",
    "\n",
    "**Cel:** Analiza metadanych tabeli Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c239525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Szczegółowe informacje o tabeli\n",
    "display(\n",
    "    spark.sql(f\"DESCRIBE DETAIL {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e5b2e7",
   "metadata": {},
   "source": [
    "### Przykład 6.2: Analiza historii operacji\n",
    "\n",
    "**Cel:** Głębsza analiza historii i metryk operacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d63af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Historia z dodatkowymi metrykami\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "\n",
    "display(\n",
    "    history_df.select(\n",
    "        \"version\", \n",
    "        \"timestamp\", \n",
    "        \"operation\", \n",
    "        \"operationMetrics.numTargetRowsInserted\",\n",
    "        \"operationMetrics.numTargetRowsUpdated\",\n",
    "        \"operationMetrics.numTargetRowsDeleted\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af958e",
   "metadata": {},
   "source": [
    "## Sekcja 7: Optymalizacja\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Delta Lake oferuje zaawansowane techniki optymalizacji: OPTIMIZE łączy małe pliki, ZORDER organizuje dane dla szybszych queries, VACUUM usuwa stare pliki."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d714b5fd",
   "metadata": {},
   "source": [
    "### Przykład 7.1: OPTIMIZE operation\n",
    "\n",
    "**Cel:** Kompaktowanie małych plików dla lepszej wydajności"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b204d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZE tabeli\n",
    "optimize_result = spark.sql(f\"\"\"\n",
    "    OPTIMIZE {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "\"\"\")\n",
    "\n",
    "display(optimize_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7d5894",
   "metadata": {},
   "source": [
    "### Przykład 7.2: ZORDER BY\n",
    "\n",
    "**Cel:** Organizacja danych według często używanych kolumn w filtrach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f6d777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZORDER BY customer_tier (kolumna często używana w filtrach)\n",
    "zorder_result = spark.sql(f\"\"\"\n",
    "    OPTIMIZE {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    ZORDER BY (customer_tier)\n",
    "\"\"\")\n",
    "\n",
    "display(zorder_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c2827b",
   "metadata": {},
   "source": [
    "### Przykład 7.3: VACUUM operation\n",
    "\n",
    "**Cel:** Usunięcie starych plików (starszych niż retention period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18c18b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VACUUM - usuń pliki starsze niż 0 godzin (tylko dla demo!)\n",
    "# W produkcji: domyślnie 7 dni, minimum 0 godzin z flagą\n",
    "spark.sql(\"SET spark.databricks.delta.retentionDurationCheck.enabled = false\")\n",
    "\n",
    "vacuum_result = spark.sql(f\"\"\"\n",
    "    VACUUM {CATALOG}.{BRONZE_SCHEMA}.customers_delta RETAIN 0 HOURS\n",
    "\"\"\")\n",
    "\n",
    "display(vacuum_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c00819",
   "metadata": {},
   "source": [
    "## Sekcja 8: Change Data Feed\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Change Data Feed (CDF) to feature Delta Lake umożliwiający tracking wszystkich zmian w tabeli. Każda operacja INSERT, UPDATE, DELETE jest rejestrowana z dodatkowymi metadanymi."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b5fe89",
   "metadata": {},
   "source": [
    "### Przykład 8.1: Włączenie Change Data Feed\n",
    "\n",
    "**Cel:** Aktywacja CDF dla istniejącej tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c964c993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Włącz Change Data Feed\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {CATALOG}.{BRONZE_SCHEMA}.customers_delta \n",
    "    SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c67aa",
   "metadata": {},
   "source": [
    "### Przykład 8.2: Generowanie zmian dla CDF\n",
    "\n",
    "**Cel:** Wykonanie operacji które będą śledzzone przez CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf14dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wykonaj więcej zmian po włączeniu CDF\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    VALUES (4001, 'CDF Test Customer', 27, 'cdf@example.com', '2023-12-20', 'Bronze')\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    UPDATE {CATALOG}.{BRONZE_SCHEMA}.customers_delta\n",
    "    SET customer_tier = 'Gold'\n",
    "    WHERE customer_id = 4001\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13549b59",
   "metadata": {},
   "source": [
    "### Przykład 8.3: Odczyt Change Data Feed\n",
    "\n",
    "**Cel:** Analiza wszystkich zmian zarejestrowanych przez CDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d485a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch read change data feed od określonej wersji\n",
    "changes_batch = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", 5) \\\n",
    "    .table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "\n",
    "display(\n",
    "    changes_batch.select(\n",
    "        \"customer_id\", \"name\", \"customer_tier\", \n",
    "        \"_change_type\", \"_commit_version\", \"_commit_timestamp\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67769b73",
   "metadata": {},
   "source": [
    "## Porównanie PySpark vs SQL\n",
    "\n",
    "**DataFrame API (PySpark):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28db220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark approach - MERGE simulation\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forName(spark, f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "\n",
    "new_data = spark.createDataFrame([\n",
    "    (5001, \"PySpark Customer\", 32, \"pyspark@example.com\", \"2023-12-25\", \"Silver\")\n",
    "], [\"customer_id\", \"name\", \"age\", \"email\", \"registration_date\", \"customer_tier\"])\n",
    "\n",
    "delta_table.alias(\"target\") \\\n",
    "    .merge(\n",
    "        new_data.alias(\"source\"),\n",
    "        \"target.customer_id = source.customer_id\"\n",
    "    ) \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d42518",
   "metadata": {},
   "source": [
    "**SQL Equivalent:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adfd093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SQL approach\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {CATALOG}.{BRONZE_SCHEMA}.customers_delta AS target\n",
    "    USING (SELECT 5002 as customer_id, 'SQL Customer' as name, 30 as age, \n",
    "                  'sql@example.com' as email, '2023-12-26' as registration_date, \n",
    "                  'Gold' as customer_tier) AS source\n",
    "    ON target.customer_id = source.customer_id\n",
    "    WHEN MATCHED THEN UPDATE SET *\n",
    "    WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f14c89f",
   "metadata": {},
   "source": [
    "**Porównanie:**\n",
    "- **Wydajność**: Identyczna - obydwa używają Catalyst optimizer\n",
    "- **Kiedy używać PySpark**: Programatic ETL, complex business logic, integration z ML pipelines\n",
    "- **Kiedy używać SQL**: Ad-hoc analysis, reporting, BI tools integration, łatwiejsze dla analityków"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54858520",
   "metadata": {},
   "source": [
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist - Co powinieneś uzyskać:\n",
    "- [ ] Tabela Delta utworzona z automatycznym schema enforcement\n",
    "- [ ] Schema evolution - dodana kolumna customer_tier\n",
    "- [ ] Time Travel queries działają dla poprzednich wersji\n",
    "- [ ] CRUD operations (INSERT, UPDATE, DELETE) wykonane poprawnie\n",
    "- [ ] MERGE INTO zaimplementowane z upsert logic\n",
    "- [ ] Optymalizacja OPTIMIZE i ZORDER zastosowana\n",
    "- [ ] Change Data Feed włączony i rejestruje zmiany\n",
    "\n",
    "### Komendy weryfikacyjne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3e28d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja wyników\n",
    "final_count = spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").count()\n",
    "final_schema_cols = len(spark.table(f\"{CATALOG}.{BRONZE_SCHEMA}.customers_delta\").columns)\n",
    "history_count = spark.sql(f\"DESCRIBE HISTORY {CATALOG}.{BRONZE_SCHEMA}.customers_delta\").count()\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Total records\", final_count),\n",
    "        (\"Schema columns\", final_schema_cols),\n",
    "        (\"History versions\", history_count)\n",
    "    ], [\"metric\", \"value\"])\n",
    ")\n",
    "\n",
    "# Sprawdź CDF properties\n",
    "table_properties = spark.sql(f\"SHOW TBLPROPERTIES {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "cdf_enabled = table_properties.filter(F.col(\"key\") == \"delta.enableChangeDataFeed\").count() > 0\n",
    "\n",
    "display(\n",
    "    spark.createDataFrame([\n",
    "        (\"Change Data Feed enabled\", cdf_enabled)\n",
    "    ], [\"property\", \"status\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c4541",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Problem 1: Schema enforcement błąd\n",
    "**Objawy:**\n",
    "- AnalysisException przy INSERT/MERGE z incompatible schema\n",
    "- \"Cannot write incompatible datatype\" message\n",
    "\n",
    "**Rozwiązanie:**\n",
    "```python\n",
    "# Użyj mergeSchema option dla schema evolution\n",
    "df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\")\n",
    "```\n",
    "\n",
    "### Problem 2: Time Travel - version not found\n",
    "**Objawy:** \n",
    "File not found dla określonej wersji po VACUUM\n",
    "\n",
    "**Rozwiązanie:** \n",
    "Sprawdź retention period i dostępne wersje przez DESCRIBE HISTORY\n",
    "\n",
    "### Problem 3: VACUUM usuwa zbyt dużo plików\n",
    "**Objawy:** Time Travel queries failują po VACUUM\n",
    "\n",
    "**Rozwiązanie:** \n",
    "Ustaw odpowiedni retention period (domyślnie 7 dni minimum)\n",
    "\n",
    "### Debugging tips:\n",
    "- Użyj `DESCRIBE HISTORY` aby zrozumieć operacje na tabeli\n",
    "- Sprawdź `DESCRIBE DETAIL` dla metadanych o plikach\n",
    "- Weryfikuj table properties przez `SHOW TBLPROPERTIES`\n",
    "- Monitoruj `_delta_log/` folder dla troubleshooting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff65565",
   "metadata": {},
   "source": [
    "## Best Practices\n",
    "\n",
    "### Wydajność:\n",
    "- Używaj ZORDER BY dla kolumn często występujących w WHERE clauses\n",
    "- Regularnie uruchamiaj OPTIMIZE dla kompaktowania small files\n",
    "- Partitioning tylko dla bardzo dużych tabel (TB+) ze skewed data\n",
    "\n",
    "### Jakość kodu:\n",
    "- Zawsze używaj explicit schema zamiast inferSchema w production\n",
    "- Implementuj schema evolution strategy dla backward compatibility\n",
    "- Używaj MERGE INTO zamiast separate DELETE + INSERT operations\n",
    "\n",
    "### Data Quality:\n",
    "- Włącz Change Data Feed dla audit trails i compliance\n",
    "- Regularne backup przez Time Travel snapshots\n",
    "- Implement data validation rules w Delta constraints\n",
    "\n",
    "### Governance:\n",
    "- Ustaw odpowiednie retention periods dla compliance requirements\n",
    "- Używaj Unity Catalog permissions dla row/column level security\n",
    "- Dokumentuj schema changes i business logic w table comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e27a544d",
   "metadata": {},
   "source": [
    "## Podsumowanie\n",
    "\n",
    "### Co zostało osiągnięte:\n",
    "- Demonstracja Delta Lake ACID properties i schema enforcement\n",
    "- Hands-on Schema Evolution z automatic column addition\n",
    "- Time Travel queries dla historical data access\n",
    "- Kompletne CRUD operations (CREATE, READ, UPDATE, DELETE)\n",
    "- Advanced MERGE INTO dla upsert scenarios\n",
    "- Performance optimization z OPTIMIZE, ZORDER, VACUUM\n",
    "- Change Data Feed dla comprehensive audit trails\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "1. **Delta Lake = Data Lake + ACID**: Łączy elastyczność Data Lake z niezawodnością transakcyjną\n",
    "2. **Schema Evolution bezpiecznie**: Additive changes są automatyczne, breaking changes wymagają planowania\n",
    "3. **Time Travel + Copy-on-Write**: Każda wersja jest preserved, umożliwiając rollback i audit\n",
    "\n",
    "### Quick Reference - Najważniejsze komendy:\n",
    "\n",
    "| Operacja | PySpark | SQL |\n",
    "|----------|---------|-----|\n",
    "| Create Delta Table | `df.write.format(\"delta\").saveAsTable()` | `CREATE TABLE USING DELTA` |\n",
    "| Time Travel | `spark.read.format(\"delta\").option(\"versionAsOf\", 1)` | `SELECT * FROM table VERSION AS OF 1` |\n",
    "| MERGE | `DeltaTable.forName().merge().execute()` | `MERGE INTO target USING source` |\n",
    "| Optimize | N/A | `OPTIMIZE table ZORDER BY col` |\n",
    "| History | N/A | `DESCRIBE HISTORY table` |\n",
    "\n",
    "### Następne kroki:\n",
    "- **Kolejny notebook**: 02_medallion_architecture.ipynb\n",
    "- **Warsztat praktyczny**: 01_delta_medallion_workshop.ipynb\n",
    "- **Materiały dodatkowe**: Delta Lake documentation, best practices guides"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf38219",
   "metadata": {},
   "source": [
    "## Czyszczenie zasobów\n",
    "\n",
    "Posprzątaj zasoby utworzone podczas notebooka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabadeeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcjonalne czyszczenie zasobów testowych\n",
    "# UWAGA: Uruchom tylko jeśli chcesz usunąć wszystkie utworzone dane\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{BRONZE_SCHEMA}.customers_delta\")\n",
    "# spark.sql(\"DROP VIEW IF EXISTS customer_updates\")\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "# display(spark.createDataFrame([(\"Zasoby zostały wyczyszczone\", \"✓\")], [\"status\", \"result\"]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
