{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b8a314b-58a6-4d37-b92c-772039a6c6a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Delta Lake Operations - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Opanowanie podstawowych i zaawansowanych operacji Delta Lake, w tym CRUD, MERGE, Time Travel i optymalizacji.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Delta Lake core features: ACID, Delta Log, Schema enforcement\n",
    "- Schema evolution (additive, automatic)\n",
    "- Time Travel i Copy-on-write\n",
    "- CRUD operations: CREATE TABLE, INSERT, UPDATE, DELETE\n",
    "- MERGE INTO - logika zmian na kluczach\n",
    "- DESCRIBE DETAIL, DESCRIBE HISTORY\n",
    "- Optymalizacja: OPTIMIZE, ZORDER BY, VACUUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d92e4e24-a75b-495e-9f13-63add1e0d834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 2 - Lakehouse & Delta Lake\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f0a0485-0e9d-4fbc-b4a4-2188c110a705",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wstęp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie fundamentów Delta Lake jako formatu tabel transakcyjnych dla data lakehouse.\n",
    "\n",
    "**Podstawowe pojęcia:**\n",
    "- **Delta Lake**: Format tabel oparty na Parquet z warstwą transakcyjną (ACID)\n",
    "- **Delta Log**: Dziennik transakcji przechowujący metadane wszystkich operacji\n",
    "- **ACID**: Atomicity, Consistency, Isolation, Durability - gwarancje transakcyjne\n",
    "- **Time Travel**: Możliwość odczytania danych z dowolnego punktu w historii\n",
    "- **Schema Evolution**: Automatyczne lub kontrolowane zmiany w schemacie tabeli\n",
    "\n",
    "**Dlaczego to ważne?**\n",
    "Delta Lake rozwiązuje fundamentalne problemy tradycyjnych data lake'ów: brak transakcji, trudności z aktualizacją danych, brak schema enforcement. Pozwala na reliable data pipelines z gwarancjami ACID i możliwością audytu zmian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5625466e-48b2-40d4-90d4-f64ec704109b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Izolacja per użytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b7d87ba-e270-4254-b6c5-963cf5706de3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f251543a-7c90-47e3-9cbf-643b5581b5d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbbf73e0-af1c-414b-ae25-36f75034a142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Konfiguracja zmiennych\n",
    "CATALOG = \"your_catalog\"\n",
    "BRONZE_SCHEMA = \"your_bronze_schema\"\n",
    "SILVER_SCHEMA = \"your_silver_schema\"\n",
    "GOLD_SCHEMA = \"your_gold_schema\"\n",
    "DATASET_BASE_PATH = \"your_dataset_base_path\"\n",
    "raw_user = \"your_user\"\n",
    "\n",
    "# Wyświetl kontekst użytkownika\n",
    "print(\"=== Kontekst użytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"Użytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog i schemat jako domyślne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Ścieżki do danych źródłowych\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "\n",
    "# Nazwy tabel Delta, które będziemy tworzyć\n",
    "CUSTOMERS_DELTA = f\"{BRONZE_SCHEMA}.customers_delta\"\n",
    "ORDERS_DELTA = f\"{BRONZE_SCHEMA}.orders_delta\"\n",
    "\n",
    "print(f\"\\n=== Tabele Delta ===\")\n",
    "print(f\"Customers: {CUSTOMERS_DELTA}\")\n",
    "print(f\"Orders: {ORDERS_DELTA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import bibliotek\n",
    "\n",
    "Importujemy podstawowe biblioteki PySpark do pracy z Delta Lake:\n",
    "- **functions as F**: Funkcje transformacji danych\n",
    "- **types**: Definicje typów danych (StructType, StringType)  \n",
    "- **Window**: Funkcje okienne do analiz\n",
    "- **datetime**: Operacje na datach i czasie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kontekst użytkownika i konfiguracja środowiska:**\n",
    "\n",
    "### Konfiguracja środowiska\n",
    "\n",
    "Używamy zmiennych z skryptu `00_setup` i ustawiamy kontekst sesji Spark:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definicja ścieżek\n",
    "\n",
    "Ustawiamy ścieżki do plików źródłowych i nazwy tabel Delta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "\n",
    "CUSTOMERS_DELTA = f\"{BRONZE_SCHEMA}.customers_delta\"\n",
    "ORDERS_DELTA = f\"{BRONZE_SCHEMA}.orders_delta\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weryfikacja konfiguracji\n",
    "\n",
    "Sprawdzamy poprawność ustawionych zmiennych środowiskowych:\n",
    "\n",
    "**Wyświetlane zmienne:**\n",
    "- **catalog**: Izolowany katalog per użytkownik\n",
    "- **bronze_schema**: Warstwa surowych danych  \n",
    "- **silver_schema**: Warstwa oczyszczonych danych\n",
    "- **gold_schema**: Warstwa zagregowanych danych\n",
    "- **user**: Identyfikator użytkownika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wyświetl konfigurację\n",
    "display(spark.sql(f\"SELECT '{CATALOG}' as catalog, '{BRONZE_SCHEMA}' as bronze_schema, '{SILVER_SCHEMA}' as silver_schema, '{GOLD_SCHEMA}' as gold_schema, '{raw_user}' as user\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14da3d4a-ecab-4c74-81d6-3bd560de98ba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 1: Delta Lake Core Features\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Delta Lake dodaje warstwę transakcyjną na top of Parquet files. Każda operacja zapisu (INSERT, UPDATE, DELETE, MERGE) jest zapisywana w Delta Log jako transakcja. Delta Log to seria JSON files w folderze `_delta_log/` zawierająca metadane wszystkich zmian.\n",
    "\n",
    "**Kluczowe pojęcia:**\n",
    "- **Transaction Log**: Pojedynczy wpis w Delta Log reprezentujący jedną transakcję\n",
    "- **Checkpoint**: Snapshots stanu tabeli co N transakcji dla szybszego odczytu\n",
    "- **Schema Enforcement**: Automatyczna walidacja typów danych przy zapisie\n",
    "- **Optimistic Concurrency**: Wiele czytających, jeden piszący w tym samym czasie\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Reliable ETL pipelines z gwarancjami ACID\n",
    "- Incremental data processing z możliwością rollback\n",
    "- Audit trails - pełna historia zmian w danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fdcbfbe5-acbb-4d95-93a3-51ff36ed4099",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.1: Utworzenie tabeli Delta z CSV\n",
    "\n",
    "**Cel:** Demonstracja utworzenia pierwszej tabeli Delta Lake z danych CSV\n",
    "\n",
    "**Podejście:**\n",
    "1. Wczytaj dane z CSV do DataFrame\n",
    "2. Zapisz DataFrame jako tabelę Delta w Unity Catalog\n",
    "3. Sprawdź metadane tabeli Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6feecea3-335b-41bc-b4f6-a48be566330f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 1.1 - Utworzenie tabeli Delta z CSV\n",
    "\n",
    "# Wczytaj dane klientów z CSV\n",
    "customers_df = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .load(CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "print(f\"Wczytano {customers_df.count()} rekordów\")\n",
    "customers_df.printSchema()\n",
    "\n",
    "# Zapisz jako tabelę Delta\n",
    "(\n",
    "    customers_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(CUSTOMERS_DELTA)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Utworzono tabelę Delta: {CUSTOMERS_DELTA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wczytane dane z CSV\n",
    "\n",
    "**Operacja:** Spark Reader wczytuje plik CSV z opcjami:\n",
    "- `header=true`: Pierwszy wiersz to nazwy kolumn\n",
    "- `inferSchema=true`: Automatyczne wykrywanie typów danych  \n",
    "\n",
    "**Schema:** Sprawdzamy strukturę i typy danych automatycznie wykryte przez Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapisz jako tabelę Delta\n",
    "(\n",
    "    customers_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(CUSTOMERS_DELTA)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zapis jako tabela Delta\n",
    "\n",
    "**Operacja:** DataFrame Writer zapisuje dane jako tabelę Delta w Unity Catalog:\n",
    "\n",
    "**Kluczowe opcje:**\n",
    "- `format(\"delta\")`: Format Delta Lake (Parquet + Delta Log)\n",
    "- `mode(\"overwrite\")`: Nadpisanie istniejącej tabeli  \n",
    "- `overwriteSchema=true`: Pozwala na zmianę schematu\n",
    "- `saveAsTable()`: Rejestracja w Unity Catalog dla governance\n",
    "\n",
    "**Rezultat:** Utworzona tabela Delta z pełnymi funkcjonalnościami ACID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    customers_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(CUSTOMERS_DELTA)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fea3e727-886f-4adc-abc1-ead56a7cddd2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Powyższy kod wykonuje trzy kluczowe operacje:\n",
    "1. **Wczytanie danych**: Spark DataFrame API wczytuje CSV z automatycznym wykrywaniem schematu\n",
    "2. **Zapis jako Delta**: `.format(\"delta\")` zapisuje dane w formacie Delta Lake (Parquet + Delta Log)\n",
    "3. **Rejestracja w Unity Catalog**: `.saveAsTable()` rejestruje tabelę w katalogu UC, co umożliwia governance\n",
    "\n",
    "Opcja `overwriteSchema=true` pozwala na zmianę schematu przy ponownym zapisie (użyteczne podczas development)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d1fc036-9d27-4a0a-b9e5-7964d6014b33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.2: Inspektowanie Delta Log\n",
    "\n",
    "**Cel:** Zrozumienie struktury Delta Log i metadanych tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "357aea4c-83a0-4bee-b8f1-e090f9e9da9a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 1.2 - DESCRIBE DETAIL\n",
    "\n",
    "# Wyświetl szczegółowe informacje o tabeli Delta\n",
    "detail_df = spark.sql(f\"DESCRIBE DETAIL {CUSTOMERS_DELTA}\")\n",
    "display(detail_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kluczowe metadane w DESCRIBE DETAIL:**\n",
    "- `format`: Format tabeli (delta)\n",
    "- `location`: Lokalizacja plików w storage\n",
    "- `numFiles`: Liczba plików Parquet\n",
    "- `sizeInBytes`: Rozmiar danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faaf9515-ec81-4696-ac6e-7287f7d84768",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 1.3: Historia transakcji (DESCRIBE HISTORY)\n",
    "\n",
    "**Cel:** Przegląd pełnej historii operacji na tabeli Delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "237524ba-bcb5-4b08-ae75-c28d0f92e084",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "history_df = spark.sql(f\"DESCRIBE HISTORY {CUSTOMERS_DELTA}\")\n",
    "\n",
    "display(\n",
    "    history_df.select(\n",
    "        \"version\",\n",
    "        \"timestamp\",\n",
    "        \"operation\",\n",
    "        \"operationParameters\",\n",
    "        \"userName\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01db04c9-6803-4ad8-908a-8b5e9a446be2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 2: CRUD Operations\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Delta Lake wspiera pełne operacje CRUD (Create, Read, Update, Delete) na tabelach. W przeciwieństwie do tradycyjnych data lake'ów (immutable files), Delta Lake umożliwia modyfikację i usuwanie rekordów przy zachowaniu gwarancji ACID.\n",
    "\n",
    "**Kluczowe operacje:**\n",
    "- **INSERT**: Dodawanie nowych rekordów (append mode)\n",
    "- **UPDATE**: Modyfikacja istniejących rekordów na podstawie warunku\n",
    "- **DELETE**: Usuwanie rekordów spełniających warunek\n",
    "- **MERGE**: Upsert - INSERT nowych + UPDATE istniejących w jednej transakcji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "630b30b4-06de-4af9-b04f-72f45e17fa6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.1: INSERT - Dodawanie nowych rekordów\n",
    "\n",
    "**Cel:** Demonstracja append mode - dodanie nowych klientów do tabeli\n",
    "\n",
    "**Podejście:**\n",
    "1. Utworzenie DataFrame z nowymi danymi\n",
    "2. Append do istniejącej tabeli Delta\n",
    "3. Weryfikacja wyników"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b0648ae5-cefd-4e16-8d12-1a96d63fc20b",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1763584841573}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 2.1 - INSERT (append)\n",
    "\n",
    "# Policz rekordy przed INSERT\n",
    "count_before = spark.table(CUSTOMERS_DELTA).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liczba rekordów przed INSERT\n",
    "\n",
    "Sprawdzamy aktualną liczbę klientów w tabeli Delta przed dodaniem nowych rekordów.\n",
    "Ta informacja posłuży do weryfikacji skuteczności operacji INSERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utwórz nowe dane do dodania:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_customers_data = [\n",
    "    (\"CUST_9001\", \"Jan\", \"Kowalski\", \"jan.kowalski@example.com\", \"Poland\", \"2025-01-15\"),\n",
    "    (\"CUST_9002\", \"Anna\", \"Nowak\", \"anna.nowak@example.com\", \"Poland\", \"2025-01-16\"),\n",
    "    (\"CUST_9003\", \"Piotr\", \"Wiśniewski\", \"piotr.wisniewski@example.com\", \"Poland\", \"2025-01-17\")\n",
    "]\n",
    "\n",
    "new_customers_df = spark.createDataFrame(\n",
    "    new_customers_data,\n",
    "    [\"customer_id\", \"first_name\",\"last_name\",\"email\", \"country\", \"registration_date\"]\n",
    ")\n",
    "\n",
    "new_customers_df = new_customers_df.select(\"customer_id\",\"first_name\",\"last_name\",\"email\", \"country\", \"registration_date\")\n",
    "display(new_customers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wykonaj append nowych klientów:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cedf631b-669c-4ede-97d6-c7e664b423a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_customers_df.write.mode(\"append\").saveAsTable(CUSTOMERS_DELTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_after = spark.table(CUSTOMERS_DELTA).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weryfikacja INSERT\n",
    "\n",
    "**Operacja wykonana:** Append nowych klientów do tabeli Delta\n",
    "\n",
    "**Sprawdzenie rezultatu:** Porównanie liczby rekordów przed i po operacji INSERT:\n",
    "- Przed INSERT: wartość z `count_before`\n",
    "- Po INSERT: wartość z `count_after`  \n",
    "- Różnica: liczba dodanych rekordów\n",
    "\n",
    "**Oczekiwany rezultat:** Zwiększenie liczby rekordów o 3 (dodano 3 nowych klientów)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weryfikacja - sprawdź nowo dodane rekordy:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.table(CUSTOMERS_DELTA)\n",
    "    .filter(F.col(\"customer_id\").isin([\"CUST_9001\", \"CUST_9002\", \"CUST_9003\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5dff40ba-8d00-4e3a-aa50-59f4d421e83f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "Operacja INSERT (append mode) dodaje nowe rekordy bez modyfikacji istniejących. Delta Lake:\n",
    "- Tworzy nowe pliki Parquet z nowymi danymi\n",
    "- Dodaje transakcję do Delta Log\n",
    "- Zachowuje atomicity - albo wszystkie rekordy są dodane, albo żaden\n",
    "\n",
    "Mode `append` jest najbezpieczniejszy - nie modyfikuje istniejących danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dfa5e9e-bf49-42b5-905c-412b0abd2ebb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.2: UPDATE - Modyfikacja rekordów\n",
    "\n",
    "**Cel:** Aktualizacja email dla określonych klientów\n",
    "\n",
    "**Sprawdź rekordy przed UPDATE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd1e46b7-904e-4c16-a86b-7c399c53845c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.table(CUSTOMERS_DELTA)\n",
    "    .filter(F.col(\"customer_id\") == \"CUST_9001\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wykonaj UPDATE używając Delta Table API:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utwórz referencję do tabeli Delta\n",
    "deltaTable = DeltaTable.forName(spark, CUSTOMERS_DELTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wykonaj UPDATE\n",
    "deltaTable.update(\n",
    "    condition = \"customer_id = 'CUST_9001'\",\n",
    "    set = { \"email\": \"'jan.kowalski.NEW@example.com'\" }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykonanie UPDATE\n",
    "\n",
    "**Delta Table API:** Używamy `DeltaTable.forName()` i `update()`:\n",
    "\n",
    "**Parametry UPDATE:**\n",
    "- `condition`: Warunek WHERE - które rekordy aktualizować\n",
    "- `set`: Mapowanie kolumn do nowych wartości\n",
    "\n",
    "**Operacja:** Aktualizacja email dla klienta 'CUST_9001'\n",
    "\n",
    "**Mechanizm:** Copy-on-write - nowe pliki Parquet + wpis w Delta Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weryfikacja UPDATE\n",
    "\n",
    "**Sprawdzenie rezultatu:** Po wykonaniu UPDATE sprawdzamy czy email został poprawnie zaktualizowany.\n",
    "\n",
    "**Oczekiwany rezultat:** Rekord z `customer_id = 'CUST_9001'` powinien mieć nowy email: `jan.kowalski.NEW@example.com`\n",
    "\n",
    "**Wyświetlenie:** Filtrujemy tabelę po customer_id i sprawdzamy zmianę"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "63f8aae2-c8b1-44dd-8d4d-601366b1bd6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Wyświetl rekordy po UPDATE\n",
    "display(\n",
    "    spark.table(CUSTOMERS_DELTA)\n",
    "    .filter(F.col(\"customer_id\") == \"CUST_9001\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae732ff3-5d2f-4cb4-bafe-af9d4181feb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "UPDATE w Delta Lake:\n",
    "- Nie modyfikuje istniejących plików Parquet (immutable)\n",
    "- Tworzy nowe pliki z zaktualizowanymi rekordami\n",
    "- Oznacza stare pliki jako usunięte w Delta Log\n",
    "- Copy-on-write semantics zapewnia izolację czytających transakcji\n",
    "\n",
    "Delta Table API (`DeltaTable.forName()`) jest rekomendowanym sposobem wykonywania UPDATE/DELETE/MERGE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbb0fb54-e481-4d1e-b1ad-5a1be4876020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 2.3: DELETE - Usuwanie rekordów\n",
    "\n",
    "**Cel:** Usunięcie określonych klientów z tabeli Delta\n",
    "\n",
    "**Krok 1:** Liczenie rekordów przed operacją DELETE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4da736e9-5c2b-45ef-a1b2-3cf733cf0a35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "count_before = spark.table(CUSTOMERS_DELTA).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Liczba rekordów przed DELETE\n",
    "\n",
    "Przechowujemy aktualną liczbę klientów do późniejszego porównania po operacji DELETE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wykonaj DELETE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ebe8c30-25fd-4e72-acf2-05710deb0e5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 2.3 - DELETE\n",
    "\n",
    "# Utwórz referencję do tabeli Delta\n",
    "deltaTable = DeltaTable.forName(spark, CUSTOMERS_DELTA)\n",
    "\n",
    "# Wykonaj DELETE\n",
    "deltaTable.delete(\n",
    "    condition = \"customer_id IN ('CUST_9001', 'CUST_9002')\"\n",
    ")\n",
    "\n",
    "print(\"✓ DELETE wykonany\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wykonaj DELETE\n",
    "deltaTable.delete(\n",
    "    condition = \"customer_id IN ('CUST_9001', 'CUST_9002')\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wykonanie DELETE\n",
    "\n",
    "**Delta Table API:** Używamy `delete()` z warunkiem:\n",
    "\n",
    "**Parametry DELETE:**\n",
    "- `condition`: Warunek WHERE - które rekordy usunąć\n",
    "\n",
    "**Operacja:** Usunięcie klientów z ID 'CUST_9001' i 'CUST_9002'\n",
    "\n",
    "**Mechanizm:** Oznaczenie plików jako usunięte w Delta Log (nie fizyczne usunięcie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weryfikacja DELETE\n",
    "\n",
    "**Sprawdzenie rezultatu:** Porównanie liczby rekordów przed i po operacji DELETE\n",
    "\n",
    "**Oczekiwany rezultat:** \n",
    "- Zmniejszenie liczby rekordów o 2 (usunięte CUST_9001 i CUST_9002)\n",
    "- Brak rekordów z usuniętymi customer_id (powinno być 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "deab507c-aa10-40eb-b0a2-5b42534adc92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(\"\\n✓ DELETE wykonany\")\n",
    "\n",
    "# Policz rekordy po DELETE\n",
    "count_after = spark.table(CUSTOMERS_DELTA).count()\n",
    "print(f\"Liczba klientów po DELETE: {count_after}\")\n",
    "print(f\"Usunięto: {count_before - count_after} rekordów\")\n",
    "\n",
    "deleted_check = (\n",
    "    spark.table(CUSTOMERS_DELTA)\n",
    "    .filter(F.col(\"customer_id\").isin(['CUST_9001', 'CUST_9002']))\n",
    "    .count()\n",
    ")\n",
    "print(f\"Liczba rekordów z ID 9001, 9002: {deleted_check} (powinno być 0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawdź czy rekordy zostały usunięte\n",
    "deleted_check = (\n",
    "    spark.table(CUSTOMERS_DELTA)\n",
    "    .filter(F.col(\"customer_id\").isin(['CUST_9001', 'CUST_9002']))\n",
    "    .count()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42a01f92-57e9-46cc-bc2f-402243f6d4c9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 3: MERGE INTO - Upsert Logic\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "MERGE INTO to najbardziej potężna operacja Delta Lake, łącząca INSERT i UPDATE w jednej transakcji. Umożliwia implementację upsert logic: \"if record exists - update, else - insert\". Jest kluczowa dla Slowly Changing Dimensions (SCD) i incremental ETL.\n",
    "\n",
    "**Kluczowe koncepty:**\n",
    "- **Source**: DataFrame z nowymi/zmienionymi danymi\n",
    "- **Target**: Tabela Delta do aktualizacji\n",
    "- **Merge Key**: Kolumny do identyfikacji dopasowanych rekordów\n",
    "- **WHEN MATCHED**: Co zrobić z rekordami, które istnieją w obu\n",
    "- **WHEN NOT MATCHED**: Co zrobić z nowymi rekordami (tylko w source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c36e3aa7-590c-46b2-86f1-257ff3e93454",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 3.1: Podstawowy MERGE (Upsert)\n",
    "\n",
    "**Cel:** Załaduj nowe zamówienia i zaktualizuj istniejące w jednej operacji\n",
    "\n",
    "**Podejście:**\n",
    "1. Utworzenie tabeli orders_delta z danych JSON\n",
    "2. Przygotowanie nowych/zmienionych zamówień\n",
    "3. MERGE na kluczu order_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bcbdd520-4ba5-4bd9-9cd7-e770c06a72db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 3.1 - MERGE INTO (podstawowy upsert)\n",
    "\n",
    "# Krok 1: Utworzenie tabeli orders_delta\n",
    "# Wczytaj dane zamówień z JSON\n",
    "orders_df = (\n",
    "    spark.read\n",
    "    .format(\"json\")\n",
    "    .option(\"multiLine\", \"true\")\n",
    "    .load(ORDERS_JSON)\n",
    ")\n",
    "\n",
    "(\n",
    "    orders_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(ORDERS_DELTA)\n",
    ")\n",
    "\n",
    "print(f\"✓ Utworzono tabelę: {ORDERS_DELTA}\")\n",
    "print(f\"Liczba zamówień (initial): {spark.table(ORDERS_DELTA).count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zapisz jako tabelę orders_delta\n",
    "(\n",
    "    orders_df\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(ORDERS_DELTA)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utworzenie tabeli orders_delta\n",
    "\n",
    "**Operacja:** Wczytanie danych zamówień z pliku JSON i utworzenie tabeli Delta\n",
    "\n",
    "**Opcje JSON:**\n",
    "- `multiLine=true`: Obsługa JSON rozłożonego na wiele linii\n",
    "\n",
    "**Rezultat:** Tabela orders_delta gotowa do operacji MERGE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Przygotowanie danych do MERGE (nowe i zmienione zamówienia):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8969b012-5eec-4ada-88aa-1abc9539a2f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Krok 2: Przygotowanie danych do MERGE\n",
    "# Symulacja nowych i zmienionych zamówień\n",
    "\n",
    "updates_data = [\n",
    "    # Istniejące zamówienie - zmiana total_amount (UPDATE)\n",
    "    (\"ORD00000001\", \"CUST005909\", \"PROD000164\", \"STORE017\", \"2024-12-31T23:56:00\", \n",
    "     1, 206.74, 0, 250.00, \"Cash\"),\n",
    "    \n",
    "    # Nowe zamówienie (INSERT)\n",
    "    (\"ORD99999999\", \"CUST009999\", \"PROD001234\", \"STORE050\", \"2025-01-20T14:30:00\", \n",
    "     2, 149.99, 10, 269.98, \"Credit Card\"),\n",
    "    \n",
    "    # Kolejne nowe zamówienie (INSERT)\n",
    "    (\"ORD99999998\", \"CUST009998\", \"PROD000567\", \"STORE025\", \"2025-01-21T10:15:00\", \n",
    "     3, 66.50, 5, 189.53, \"Debit Card\")\n",
    "]\n",
    "\n",
    "updates_df = spark.createDataFrame(\n",
    "    updates_data,\n",
    "    [\"order_id\", \"customer_id\", \"product_id\", \"store_id\", \"order_datetime\",\n",
    "     \"quantity\", \"unit_price\", \"discount_percent\", \"total_amount\", \"payment_method\"]\n",
    ")\n",
    "\n",
    "display(updates_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wykonaj MERGE INTO:**\n",
    "\n",
    "Merge key: `order_id`\n",
    "- **WHEN MATCHED**: Aktualizuj `total_amount` i `payment_method`\n",
    "- **WHEN NOT MATCHED**: Wstaw nowe zamówienie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d455718-cbcb-4a66-adf5-b254a725401b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Krok 3: Wykonanie MERGE\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "count_initial = spark.table(ORDERS_DELTA).count()\n",
    "\n",
    "deltaTable = DeltaTable.forName(spark, ORDERS_DELTA)\n",
    "\n",
    "(\n",
    "    deltaTable.alias(\"target\")\n",
    "    .merge(\n",
    "        updates_df.alias(\"source\"),\n",
    "        \"target.order_id = source.order_id\"\n",
    "    )\n",
    "    .whenMatchedUpdate(set = {\n",
    "        \"total_amount\": \"source.total_amount\",\n",
    "        \"payment_method\": \"source.payment_method\"\n",
    "    })\n",
    "    .whenNotMatchedInsert(values = {\n",
    "        \"order_id\": \"source.order_id\",\n",
    "        \"customer_id\": \"source.customer_id\",\n",
    "        \"product_id\": \"source.product_id\",\n",
    "        \"store_id\": \"source.store_id\",\n",
    "        \"order_datetime\": \"source.order_datetime\",\n",
    "        \"quantity\": \"source.quantity\",\n",
    "        \"unit_price\": \"source.unit_price\",\n",
    "        \"discount_percent\": \"source.discount_percent\",\n",
    "        \"total_amount\": \"source.total_amount\",\n",
    "        \"payment_method\": \"source.payment_method\"\n",
    "    })\n",
    "    .execute()\n",
    ")\n",
    "\n",
    "count_after = spark.table(ORDERS_DELTA).count()\n",
    "print(f\"✓ MERGE wykonany\")\n",
    "print(f\"Liczba zamówień (po MERGE): {count_after}\")\n",
    "print(f\"Dodano nowych: {count_after - count_initial}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forName(spark, ORDERS_DELTA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wykonaj MERGE\n",
    "(\n",
    "    deltaTable.alias(\"target\")\n",
    "    .merge(\n",
    "        updates_df.alias(\"source\"),\n",
    "        \"target.order_id = source.order_id\"\n",
    "    )\n",
    "    .whenMatchedUpdate(set = {\n",
    "        \"total_amount\": \"source.total_amount\",\n",
    "        \"payment_method\": \"source.payment_method\"\n",
    "    })\n",
    "    .whenNotMatchedInsert(values = {\n",
    "        \"order_id\": \"source.order_id\",\n",
    "        \"customer_id\": \"source.customer_id\",\n",
    "        \"product_id\": \"source.product_id\",\n",
    "        \"store_id\": \"source.store_id\",\n",
    "        \"order_datetime\": \"source.order_datetime\",\n",
    "        \"quantity\": \"source.quantity\",\n",
    "        \"unit_price\": \"source.unit_price\",\n",
    "        \"discount_percent\": \"source.discount_percent\",\n",
    "        \"total_amount\": \"source.total_amount\",\n",
    "        \"payment_method\": \"source.payment_method\"\n",
    "    })\n",
    "    .execute()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_after = spark.table(ORDERS_DELTA).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rezultat MERGE\n",
    "\n",
    "**Operacja wykonana:** MERGE INTO z upsert logic\n",
    "\n",
    "**Sprawdzenie wyników:**\n",
    "- Liczba zamówień przed MERGE: wartość z `count_initial`\n",
    "- Liczba zamówień po MERGE: wartość z `count_after`  \n",
    "- Dodano nowych: różnica między `count_after` a `count_initial`\n",
    "\n",
    "**Oczekiwany rezultat:** Zwiększenie o 2 zamówienia (nowe ORD99999999 i ORD99999998)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weryfikacja - zaktualizowane zamówienie\n",
    "\n",
    "**Sprawdzenie:** Rekord z `order_id = 'ORD00000001'` powinien mieć zaktualizowane:\n",
    "- `total_amount`: 250.00 (zmienione z oryginalnej wartości)\n",
    "- `payment_method`: \"Cash\" (zmienione z oryginalnej wartości)\n",
    "\n",
    "**WHEN MATCHED:** Operacja UPDATE została wykonana na istniejącym zamówieniu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e61ee25a-914a-4f85-bf0d-a8b83cafbeb2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Weryfikacja wyników MERGE\n",
    "\n",
    "# Weryfikacja - zaktualizowane zamówienie\n",
    "display(\n",
    "    spark.table(ORDERS_DELTA)\n",
    "    .filter(F.col(\"order_id\") == \"ORD00000001\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weryfikacja - nowe zamówienia\n",
    "\n",
    "**Sprawdzenie:** Nowe zamówienia z ID rozpoczynającymi się od 'ORD99999'\n",
    "\n",
    "**WHEN NOT MATCHED:** Operacja INSERT została wykonana dla nowych zamówień:\n",
    "- ORD99999999: Credit Card payment, total_amount: 269.98\n",
    "- ORD99999998: Debit Card payment, total_amount: 189.53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.table(ORDERS_DELTA)\n",
    "    .filter(F.col(\"order_id\").like(\"ORD99999%\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81c0d4bf-5f09-4fe1-959a-ae51f3bae808",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Wyjaśnienie:**\n",
    "\n",
    "MERGE INTO w Delta Lake:\n",
    "1. **Merge Key**: `order_id` identyfikuje dopasowane rekordy (klucz biznesowy)\n",
    "2. **WHEN MATCHED**: Aktualizuje `total_amount` i `payment_method` dla istniejących zamówień\n",
    "3. **WHEN NOT MATCHED**: Wstawia nowe zamówienia z wszystkimi kolumnami ze schematu orders\n",
    "4. **Atomicity**: Cała operacja (UPDATE + INSERT) jest jedną transakcją ACID\n",
    "\n",
    "To najbardziej efektywny sposób implementacji incremental data loading w Delta Lake.\n",
    "\n",
    "**Zalety MERGE vs DELETE + INSERT:**\n",
    "- Jedna transakcja zamiast dwóch (szybsze, bezpieczniejsze)\n",
    "- Brak data loss risk przy failure\n",
    "- Lepsza performance dla dużych tabel\n",
    "- Automatyczne partition pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5894543-bc14-4fd3-a42b-65eb92fcfbde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 4: Time Travel\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Time Travel to unikalna funkcja Delta Lake umożliwiająca odczyt danych z dowolnego punktu w historii tabeli. Każda wersja tabeli jest identyfikowana przez:\n",
    "- **Version number**: Liczba całkowita (0, 1, 2, ...)\n",
    "- **Timestamp**: Data i czas transakcji\n",
    "\n",
    "**Kluczowe zastosowania:**\n",
    "- Audyt zmian w danych\n",
    "- Rollback do poprzedniego stanu po błędzie\n",
    "- Reprodukowalność analiz (odczyt danych \"as of\" określonej daty)\n",
    "- Porównanie zmian między wersjami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6705a1f3-7ac1-46a7-9e07-265f283b0a58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.1: Odczyt historii wersji\n",
    "\n",
    "**Cel:** Przegląd wszystkich wersji tabeli i ich metadanych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d269429e-82f5-4f64-a5c2-e126cc5c0a6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "history_df = spark.sql(f\"DESCRIBE HISTORY {CUSTOMERS_DELTA}\")\n",
    "\n",
    "display(\n",
    "    history_df.select(\n",
    "        \"version\",\n",
    "        \"timestamp\",\n",
    "        \"operation\",\n",
    "        \"operationParameters\",\n",
    "        \"userName\",\n",
    "        \"operationMetrics\"\n",
    "    )\n",
    "    .orderBy(\"version\", ascending=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_version = history_df.agg({\"version\": \"max\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Najnowsza wersja tabeli\n",
    "\n",
    "**Operacja:** Znalezienie najwyższego numeru wersji w historii tabeli\n",
    "\n",
    "**Informacja:** Każda operacja (CREATE, INSERT, UPDATE, DELETE, MERGE) tworzy nową wersję w Delta Log.\n",
    "Numer wersji zaczyna się od 0 i inkrementuje o 1 przy każdej transakcji.\n",
    "\n",
    "**Wykorzystanie:** Zmienna `latest_version` zawiera aktualną wersję tabeli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbfe3519-7c70-430d-bd74-01be18fdcfed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.2: Odczyt danych z określonej wersji\n",
    "\n",
    "**Cel:** Użycie Time Travel do odczytania danych z poprzedniej wersji tabeli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e18892a-27f3-42db-895d-6f4427cedf9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 4.2 - Odczyt danych @ VERSION AS OF\n",
    "\n",
    "# Sprawdź liczbę rekordów w najnowszej wersji\n",
    "current_count = spark.table(CUSTOMERS_DELTA).count()\n",
    "\n",
    "print(f\"Liczba klientów (najnowsza wersja): {current_count}\")\n",
    "\n",
    "# Odczytaj dane z wersji 0\n",
    "version_0_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(CUSTOMERS_DELTA)\n",
    "version_0_count = version_0_df.count()\n",
    "\n",
    "print(f\"Liczba klientów (wersja 0): {version_0_count}\")\n",
    "print(f\"Różnica: {current_count - version_0_count} rekordów\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Odczytaj dane z wersji 0 (pierwotny stan)\n",
    "version_0_df = spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(CUSTOMERS_DELTA)\n",
    "version_0_count = version_0_df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Travel - odczyt poprzedniej wersji\n",
    "\n",
    "**VERSION AS OF syntax:** Odczyt danych z określonej wersji tabeli\n",
    "\n",
    "**Porównanie:**\n",
    "- **Aktualna wersja:** `spark.table(CUSTOMERS_DELTA).count()`\n",
    "- **Wersja 0:** `versionAsOf=0` - pierwotny stan po CREATE TABLE\n",
    "\n",
    "**Zastosowanie:** Analiza zmian między wersjami, rollback do poprzedniego stanu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Porównanie wersji\n",
    "\n",
    "**Analiza zmian:** Różnica między liczbą rekordów w różnych wersjach tabeli\n",
    "\n",
    "**Interpretacja wyników:**\n",
    "- **Dodatnia różnica:** Rekordy zostały dodane (INSERT, MERGE INSERT)\n",
    "- **Ujemna różnica:** Rekordy zostały usunięte (DELETE, MERGE DELETE)\n",
    "- **Zero:** Brak zmian w liczbie rekordów (UPDATE, MERGE UPDATE)\n",
    "\n",
    "**Zastosowanie biznesowe:** Monitoring wzrostu danych, audyt operacji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Odczytaj dane z wersji 0:**\n",
    "\n",
    "Version 0 to pierwsza wersja tabeli (initial load). Porównamy ją z aktualną wersją."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wyświetl dane z wersji 0 (pierwsze 5 rekordów):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(version_0_df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Alternatywnie - odczyt przez SQL:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sql = spark.sql(f\"SELECT * FROM {CUSTOMERS_DELTA} VERSION AS OF 0 LIMIT 5\")\n",
    "display(df_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77092fce-2222-4512-82f8-df03d75c7213",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 4.3: Rollback tabeli (RESTORE)\n",
    "\n",
    "**Cel:** Przywrócenie tabeli do stanu z poprzedniej wersji\n",
    "\n",
    "**Uwaga:** RESTORE wykonuje nową transakcję, nie usuwa historii!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e93575a-37dd-41fb-a09c-0d8d7692bc98",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 4.3 - RESTORE TABLE\n",
    "\n",
    "# Sprawdź aktualną liczbę rekordów\n",
    "before_restore = spark.table(CUSTOMERS_DELTA).count()\n",
    "print(f\"Liczba klientów przed RESTORE: {before_restore}\")\n",
    "\n",
    "# Przywróć tabelę do wersji 0 (initial load)\n",
    "spark.sql(f\"RESTORE TABLE {CUSTOMERS_DELTA} TO VERSION AS OF 0\")\n",
    "\n",
    "print(\"\\n✓ RESTORE wykonany\")\n",
    "\n",
    "# Sprawdź liczbę rekordów po RESTORE\n",
    "after_restore = spark.table(CUSTOMERS_DELTA).count()\n",
    "print(f\"Liczba klientów po RESTORE: {after_restore}\")\n",
    "\n",
    "# Sprawdź historię - RESTORE dodaje nową transakcję!\n",
    "history_after_restore = spark.sql(f\"DESCRIBE HISTORY {CUSTOMERS_DELTA}\")\n",
    "print(\"\\n=== Historia po RESTORE ===\")\n",
    "display(\n",
    "    history_after_restore\n",
    "    .select(\"version\", \"timestamp\", \"operation\", \"operationParameters\")\n",
    "    .orderBy(\"version\", ascending=False)\n",
    "    .limit(3)\n",
    ")\n",
    "\n",
    "print(\"\\nUwaga: RESTORE utworzył nową wersję - historia NIE została utracona!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wykonaj RESTORE do wersji 0:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"RESTORE TABLE {CUSTOMERS_DELTA} TO VERSION AS OF 0\")\n",
    "print(\"✓ RESTORE wykonany\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weryfikacja - sprawdź liczbę rekordów po RESTORE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_restore = spark.table(CUSTOMERS_DELTA).count()\n",
    "print(f\"Liczba klientów po RESTORE: {after_restore}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sprawdź historię - RESTORE dodaje nową transakcję:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_after_restore = spark.sql(f\"DESCRIBE HISTORY {CUSTOMERS_DELTA}\")\n",
    "display(\n",
    "    history_after_restore\n",
    "    .select(\"version\", \"timestamp\", \"operation\", \"operationParameters\")\n",
    "    .orderBy(\"version\", ascending=False)\n",
    "    .limit(3)\n",
    ")\n",
    "\n",
    "print(\"Uwaga: RESTORE utworzył nową wersję - historia NIE została utracona!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5f03199-241b-4b7c-981f-296be0c61333",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 5: Schema Evolution\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Schema Evolution to mechanizm automatycznej adaptacji schematu tabeli Delta do zmian w danych. Delta Lake wspiera:\n",
    "- **Additive schema changes**: Dodawanie nowych kolumn\n",
    "- **Schema enforcement**: Blokowanie niekompatybilnych zmian typu\n",
    "- **Schema merge**: Automatyczne dodawanie kolumn przy zapisie\n",
    "\n",
    "**Kluczowe opcje:**\n",
    "- `mergeSchema=true`: Automatyczne dodawanie nowych kolumn z source DataFrame\n",
    "- `overwriteSchema=true`: Całkowite zastąpienie schematu (uwaga: destructive!)\n",
    "\n",
    "**Kiedy używać:**\n",
    "- Dodawanie nowych atrybutów do danych bez przebudowy pipeline'u\n",
    "- Evolving data models w zgodzie z business requirements\n",
    "- Integracja nowych źródeł danych z dodatkowymi polami"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1268b47a-4410-4224-9e84-40307aaff5a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 5.1: Automatyczne dodawanie nowych kolumn\n",
    "\n",
    "**Cel:** Demonstracja additive schema evolution z opcją `mergeSchema`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41fb4255-2be9-4df6-aff4-81b0438c34a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 5.1 - Schema Evolution z mergeSchema\n",
    "\n",
    "# KROK 1: Sprawdź aktualny schemat tabeli\n",
    "spark.table(CUSTOMERS_DELTA).printSchema()\n",
    "\n",
    "current_columns = len(spark.table(CUSTOMERS_DELTA).columns)\n",
    "print(f\"Aktualna liczba kolumn: {current_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utwórz DataFrame z nowymi kolumnami:**\n",
    "\n",
    "Nowe kolumny: `loyalty_tier`, `preferred_language`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa6cb4c4-762d-460e-81a9-57b70791adbb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "extended_customers = spark.createDataFrame([\n",
    "    (\"CUST_9999\", \"Ewa\", \"Wiśniewska\", \"ewa.wisn@example.com\", \"+48 600 123 456\",\n",
    "     \"Warsaw\", \"mazowieckie\", \"Poland\", \"2025-01-25\", \"Premium\", \"Premium\", \"PL\")\n",
    "], [\"customer_id\", \"first_name\", \"last_name\", \"email\", \"phone\", \"city\", \"state\", \n",
    "    \"country\", \"registration_date\", \"customer_segment\", \"loyalty_tier\", \"preferred_language\"])\n",
    "\n",
    "print(\"Nowe kolumny: loyalty_tier, preferred_language\")\n",
    "extended_customers.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Próba zapisu bez mergeSchema (spowoduje błąd):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c83d877f-eae2-4a75-9df6-2f644fb16aa9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    (\n",
    "        extended_customers\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .saveAsTable(CUSTOMERS_DELTA)\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(f\"❌ Błąd (oczekiwany): {str(e)[:200]}...\")\n",
    "    print(\"Delta Lake blokuje niezgodne schematy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zapis z mergeSchema=true (automatyczne dodanie kolumn):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73c821ac-f7b4-45c2-bb31-4329fd335dd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Zapis z mergeSchema - automatyczne dodanie kolumn (registration_date (DATE))\n",
    "(\n",
    "    extended_customers\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .saveAsTable(CUSTOMERS_DELTA)\n",
    ")\n",
    "\n",
    "print(\"✓ Zapis z mergeSchema=true wykonany\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schemat po schema evolution:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc1e8094-2895-427f-8182-db33bd325f9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.table(CUSTOMERS_DELTA).printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nowy rekord z dodatkowymi kolumnami:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af30e9dd-6fce-4f0b-a356-a126982ec923",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.table(CUSTOMERS_DELTA)\n",
    "    .filter(F.col(\"customer_id\") == \"CUST_9999\")\n",
    "    .select(\"customer_id\", \"first_name\", \"last_name\", \"email\", \"customer_segment\", \n",
    "            \"loyalty_tier\", \"preferred_language\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stare rekordy (mają NULL w nowych kolumnach):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    spark.table(CUSTOMERS_DELTA)\n",
    "    .filter(F.col(\"customer_id\").like(\"CUST000%\"))\n",
    "    .select(\"customer_id\", \"first_name\", \"last_name\", \"customer_segment\", \n",
    "            \"loyalty_tier\", \"preferred_language\")\n",
    "    .limit(5)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ec2dae63-fba0-4701-8cd6-8c0e976a98ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 6: Optymalizacja tabeli Delta\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Delta Lake akumuluje wiele małych plików przy częstych zapisach. Optymalizacja jest kluczowa dla performance:\n",
    "\n",
    "**OPTIMIZE (Compaction):**\n",
    "- Łączy małe pliki Parquet w większe (target: 1 GB)\n",
    "- Zmniejsza overhead odczytu (mniej plików = mniej I/O operations)\n",
    "- Poprawia query performance\n",
    "\n",
    "**ZORDER BY:**\n",
    "- Co-locality algorytm dla multi-dimensional clustering\n",
    "- Organizuje dane wg często filtrowanych kolumn\n",
    "- Zmniejsza data skipping overhead\n",
    "\n",
    "**VACUUM:**\n",
    "- Usuwa stare pliki Parquet nieużywane przez aktywne wersje\n",
    "- Zwalnia storage space\n",
    "- Domyślnie: retention 7 dni (chroni Time Travel)\n",
    "\n",
    "**Uwaga:** Po VACUUM nie można odczytać starszych wersji poza retention period!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ae08d57-2b4d-4c09-a458-67097a0d4fd3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Optymalizacja tabel Delta – gdzie szukać szczegółów?\n",
    "\n",
    "Delta Lake oferuje mechanizmy optymalizacji wydajności, m.in.:\n",
    "\n",
    "- **OPTIMIZE** – łączenie małych plików w większe,\n",
    "- **ZORDER BY** – lepszy data skipping dla często filtrowanych kolumn,\n",
    "- **VACUUM** – usuwanie starych plików i utrzymywanie historii.\n",
    "\n",
    "Szczegółowe przykłady i best practices znajdziesz w notebooku:\n",
    "**`05_optimization_best_practices`** (Dzień 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b50a6fbe-b40f-4526-8ed1-500059a4eb7a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 7: Change Data Feed (CDF)\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "**Change Data Feed (CDF)** to feature Delta Lake umożliwiający śledzenie wszystkich zmian (inserts, updates, deletes) w tabeli Delta. CDF rejestruje:\n",
    "- **Typ zmiany**: insert, update_preimage, update_postimage, delete\n",
    "- **Zmienione dane**: before/after values\n",
    "- **Metadata**: version, timestamp operacji\n",
    "\n",
    "**Kluczowe koncepty:**\n",
    "\n",
    "**1. Change Types:**\n",
    "```\n",
    "- insert: Nowy rekord dodany\n",
    "- update_preimage: Stan rekordu PRZED update\n",
    "- update_postimage: Stan rekordu PO update\n",
    "- delete: Rekord usunięty\n",
    "```\n",
    "\n",
    "**2. CDF vs Time Travel:**\n",
    "| Feature | Time Travel | Change Data Feed |\n",
    "|---------|-------------|------------------|\n",
    "| **Use Case** | Snapshot całej tabeli | Tylko zmiany (delta) |\n",
    "| **Performance** | Odczyt całej tabeli | Odczyt tylko zmian |\n",
    "| **Format** | Full table | Change rows only |\n",
    "| **Best For** | Rollback, audit | Incremental ETL, CDC |\n",
    "\n",
    "**3. Kiedy używać CDF:**\n",
    "- ✅ **Incremental ETL**: Process tylko nowe/zmienione rekordy\n",
    "- ✅ **CDC (Change Data Capture)**: Replikacja zmian do downstream systems\n",
    "- ✅ **Audit trail**: Pełna historia zmian dla compliance\n",
    "- ✅ **Materialized views**: Incrementally refresh aggregates\n",
    "- ✅ **Event streaming**: Publish changes to Kafka/EventHub\n",
    "\n",
    "**4. CDF Architecture:**\n",
    "```\n",
    "Delta Table → CDF Enabled → Change Log\n",
    "                                ↓\n",
    "              Read via table_changes() function\n",
    "                                ↓\n",
    "           INSERT/UPDATE/DELETE events\n",
    "```\n",
    "\n",
    "**Włączenie CDF:**\n",
    "```sql\n",
    "-- Method 1: Przy tworzeniu tabeli\n",
    "CREATE TABLE table_name (...) \n",
    "TBLPROPERTIES (delta.enableChangeDataFeed = true);\n",
    "\n",
    "-- Method 2: Dla istniejącej tabeli\n",
    "ALTER TABLE table_name \n",
    "SET TBLPROPERTIES (delta.enableChangeDataFeed = true);\n",
    "```\n",
    "\n",
    "**Odczyt CDF:**\n",
    "```python\n",
    "# PySpark API\n",
    "changes_df = spark.read.format(\"delta\") \\\n",
    "    .option(\"readChangeFeed\", \"true\") \\\n",
    "    .option(\"startingVersion\", 0) \\\n",
    "    .table(\"table_name\")\n",
    "\n",
    "# SQL API\n",
    "SELECT * FROM table_changes('table_name', 0)  -- Od wersji 0\n",
    "SELECT * FROM table_changes('table_name', 0, 10)  -- Od wersji 0 do 10\n",
    "```\n",
    "\n",
    "**Change Schema:**\n",
    "```\n",
    "Standard columns + CDF metadata:\n",
    "- _change_type: insert | update_preimage | update_postimage | delete\n",
    "- _commit_version: Version number\n",
    "- _commit_timestamp: Timestamp operacji\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "1. **Enable CDF early**: Najlepiej przy CREATE TABLE (nie można odzyskać historii sprzed włączenia)\n",
    "2. **Incremental reads**: Używaj `startingVersion`/`endingVersion` dla efficiency\n",
    "3. **Storage overhead**: CDF dodaje ~30% storage (change data stored separately)\n",
    "4. **Retention**: CDF podlega tej samej retention policy co VACUUM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "857c8ead-92f2-4e39-ad87-7d11cbac8e17",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "**Organizacja tabel Delta:**\n",
    "1. **Naming convention**: Używaj `{layer}.{domain}_{entity}` (np. `bronze.sales_orders`)\n",
    "2. **Partitioning**: Partycjonuj tylko duże tabele (>1TB) po często filtrowanych kolumnach (np. date)\n",
    "3. **Table properties**: Ustaw `delta.autoOptimize.optimizeWrite = true` dla częstych małych zapisów\n",
    "\n",
    "**Optymalizacja performance:**\n",
    "1. **OPTIMIZE**: Uruchamiaj regularnie (np. co noc) dla tabel z częstymi zapisami\n",
    "2. **ZORDER BY**: Używaj dla 2-4 najczęściej filtrowanych kolumn (więcej = diminishing returns)\n",
    "3. **Data skipping**: Wykorzystuj statistics w Delta Log - filtruj po kolumnach z ZORDER\n",
    "\n",
    "**CRUD operations:**\n",
    "1. **MERGE**: Preferuj zamiast DELETE + INSERT dla upsert logic (atomicity!)\n",
    "2. **Merge keys**: Zawsze używaj indeksowanych kolumn (np. primary keys)\n",
    "3. **Predicates**: Dodawaj dodatkowe predykaty w MERGE dla partition pruning\n",
    "\n",
    "**Time Travel i maintenance:**\n",
    "1. **VACUUM retention**: Minimum 7 dni (domyślnie) - chroni Time Travel i concurrent readers\n",
    "2. **DESCRIBE HISTORY**: Monitoruj operacje i rozmiar Delta Log\n",
    "3. **Checkpoint**: Tworzony automatycznie co 10 transakcji - nie wymaga interwencji\n",
    "\n",
    "**Schema evolution:**\n",
    "1. **mergeSchema**: Używaj ostrożnie - może spowodować NULL-e w starych rekordach\n",
    "2. **overwriteSchema**: Tylko dla development - destrukcyjna operacja!\n",
    "3. **NOT NULL constraints**: Definiuj przed pierwszym zapisem (trudno dodać później)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "acbc440c-18b9-46a4-91a9-3cea4632b5ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Problem 1: \"Schema mismatch\" przy zapisie**\n",
    "```\n",
    "AnalysisException: A schema mismatch detected when writing to the Delta table\n",
    "```\n",
    "**Rozwiązanie:**\n",
    "- Użyj `option(\"mergeSchema\", \"true\")` dla additive changes\n",
    "- Użyj `option(\"overwriteSchema\", \"true\")` dla pełnej zmiany schematu (uwaga: destructive!)\n",
    "\n",
    "**Problem 2: \"ConcurrentAppendException\"**\n",
    "```\n",
    "ConcurrentAppendException: Files were added to the table by a concurrent update\n",
    "```\n",
    "**Rozwiązanie:**\n",
    "- Delta Lake używa optimistic concurrency - retry operacji\n",
    "- Dla częstych konfliktów: zastosuj partition pruning lub MERGE z predykatami\n",
    "\n",
    "**Problem 3: Time Travel nie działa dla starszych wersji**\n",
    "```\n",
    "VersionNotFoundException: Cannot find version X\n",
    "```\n",
    "**Rozwiązanie:**\n",
    "- Sprawdź czy VACUUM nie usunął starych plików\n",
    "- Zwiększ retention period: `VACUUM table RETAIN 30 DAYS`\n",
    "\n",
    "**Problem 4: Słaba performance query po wielu UPDATE/DELETE**\n",
    "**Rozwiązanie:**\n",
    "- Uruchom `OPTIMIZE` dla compaction małych plików\n",
    "- Użyj `OPTIMIZE ZORDER BY` dla często filtrowanych kolumn\n",
    "\n",
    "**Problem 5: Duże zużycie storage pomimo VACUUM**\n",
    "**Rozwiązanie:**\n",
    "- Sprawdź `DESCRIBE DETAIL` - czy Delta Log nie rośnie?\n",
    "- Checkpoint tworzony automatycznie co 10 transakcji\n",
    "- Rozważ manual checkpoint: `OPTIMIZE table` tworzy checkpoint jako side effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb9150e1-2b35-483e-997a-fe9cddcb0d1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "**W tym notebooku nauczyliśmy się:**\n",
    "\n",
    "✅ **Delta Lake Core Features:**\n",
    "- Transakcyjna warstwa ACID dla data lakehouse\n",
    "- Delta Log jako dziennik metadanych wszystkich operacji\n",
    "- Schema enforcement i automatic schema evolution\n",
    "\n",
    "✅ **CRUD Operations:**\n",
    "- INSERT (append mode) - dodawanie nowych rekordów\n",
    "- UPDATE - modyfikacja istniejących danych z Copy-on-Write semantics\n",
    "- DELETE - usuwanie rekordów spełniających warunek\n",
    "- MERGE INTO - upsert logic w jednej transakcji ACID\n",
    "\n",
    "✅ **Time Travel:**\n",
    "- Odczyt danych z dowolnej wersji: `VERSION AS OF`, `TIMESTAMP AS OF`\n",
    "- RESTORE TABLE - rollback do poprzedniego stanu (tworzy nową wersję!)\n",
    "- DESCRIBE HISTORY - audyt wszystkich operacji na tabeli\n",
    "\n",
    "✅ **Schema Evolution:**\n",
    "- Additive changes z `mergeSchema=true`\n",
    "- Automatyczna adaptacja do nowych kolumn w danych źródłowych\n",
    "- Schema enforcement chroni przed niekompatybilnymi zmianami\n",
    "\n",
    "✅ **Optymalizacja:**\n",
    "- OPTIMIZE - compaction małych plików dla performance\n",
    "- ZORDER BY - multi-dimensional clustering dla data skipping\n",
    "- VACUUM - usuwanie nieużywanych plików (uwaga: ogranicza Time Travel!)\n",
    "\n",
    "**Kluczowe wnioski:**\n",
    "1. Delta Lake rozwiązuje fundamentalne problemy tradycyjnych data lake'ów (brak transakcji, trudności z UPDATE/DELETE)\n",
    "2. MERGE INTO jest kluczową operacją dla incremental ETL i Slowly Changing Dimensions\n",
    "3. Time Travel umożliwia audyt, rollback i reprodukowalność analiz\n",
    "4. Regularna optymalizacja (OPTIMIZE, VACUUM) jest niezbędna dla production workloads\n",
    "5. Schema evolution pozwala na evolving data models bez przebudowy pipeline'ów\n",
    "\n",
    "**Następne kroki:**\n",
    "- Delta Live Tables - deklaratywne pipeline'y z automatycznym dependency management\n",
    "- Change Data Feed - incremental processing z CDC patterns\n",
    "- Delta Sharing - bezpieczne udostępnianie danych bez kopiowania"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9f178eef-dfae-474b-9d20-80e01c80305a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Opcjonalnie: usuń utworzone tabele Demo po zakończeniu ćwiczeń:\n",
    "\n",
    "**Uwaga:** Wykonaj cleanup tylko jeśli nie potrzebujesz już tych tabel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67d05eb6-fcc0-4320-b2e4-e2cb471f0b43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup - usuń tabele demo\n",
    "\n",
    "# Odkomentuj poniższe linie aby usunąć tabele:\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CUSTOMERS_DELTA}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {ORDERS_DELTA}\")\n",
    "# print(\"✓ Tabele usunięte\")\n",
    "\n",
    "print(\"Cleanup wyłączony (odkomentuj kod aby usunąć tabele)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cleanup wyłączony (odkomentuj kod aby usunąć tabele)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Status cleanup:**\n",
    "\n",
    "Cleanup jest wyłączony - tabele pozostają zachowane do dalszych eksperymentów. Odkomentuj kod powyżej aby usunąć tabele demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04d6514-ede8-4a7b-8998-b8058f05a951",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 7.1: Włączenie Change Data Feed\n",
    "\n",
    "**Cel:** Włączenie CDF dla tabeli orders_delta i wykonanie operacji CRUD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d901adfe-638f-4783-9080-5350dd054431",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 7.1 - Włączenie Change Data Feed\n",
    "\n",
    "# Krok 1: Sprawdź czy CDF jest włączony\n",
    "print(\"=== Sprawdzenie stanu CDF ===\")\n",
    "table_props = spark.sql(f\"SHOW TBLPROPERTIES {ORDERS_DELTA}\").collect()\n",
    "cdf_enabled = any(row['key'] == 'delta.enableChangeDataFeed' and row['value'] == 'true' for row in table_props)\n",
    "\n",
    "print(f\"CDF włączony: {cdf_enabled}\")\n",
    "\n",
    "# Krok 2: Włącz CDF\n",
    "if not cdf_enabled:\n",
    "    spark.sql(f\"\"\"\n",
    "        ALTER TABLE {ORDERS_DELTA}\n",
    "        SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "    \"\"\")\n",
    "    print(\"✓ CDF włączony dla tabeli orders_delta\")\n",
    "else:\n",
    "    print(\"✓ CDF już był włączony\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not cdf_enabled:\n",
    "    spark.sql(f\"\"\"\n",
    "        ALTER TABLE {ORDERS_DELTA}\n",
    "        SET TBLPROPERTIES (delta.enableChangeDataFeed = true)\n",
    "    \"\"\")\n",
    "    print(\"✓ CDF włączony dla tabeli orders_delta\")\n",
    "else:\n",
    "    print(\"✓ CDF już był włączony\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Włącz CDF dla tabeli orders_delta:**\n",
    "\n",
    "Change Data Feed musi być włączony przed wykonaniem operacji CRUD, które chcemy śledzić."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Start version dla CDF tracking: {start_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start version obliczony:**\n",
    "\n",
    "Ta wersja będzie punktem startowym dla śledzenia zmian CDF. Wszystkie kolejne operacje CRUD będą zarejestrowane w Change Data Feed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_orders.write.mode(\"append\").saveAsTable(ORDERS_DELTA)\n",
    "print(\"✓ INSERT: 2 nowe zamówienia dodane\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wykonaj INSERT (append nowych zamówień):**\n",
    "\n",
    "Te zamówienia zostaną dodane jako nowe rekordy i zarejestrowane w CDF jako events typu `insert`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✓ UPDATE: Zmieniono payment_method i total_amount dla ORD_CDF_001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**UPDATE wykonany:**\n",
    "\n",
    "CDF zarejestruje dwa events: `update_preimage` (stan przed) i `update_postimage` (stan po zmianie)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"✓ DELETE: Usunięto zamówienie ORD_CDF_002\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DELETE wykonany:**\n",
    "\n",
    "CDF zarejestruje event typu `delete` z ostatnimi wartościami usuniętego rekordu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Start version: {start_version}\")\n",
    "print(f\"End version: {end_version}\")\n",
    "print(f\"Liczba transakcji tracked: {end_version - start_version + 1}\")\n",
    "print(\"\\nOperacje wykonane:\")\n",
    "print(\"  1. INSERT (2 records)\")\n",
    "print(\"  2. UPDATE (1 record)\")\n",
    "print(\"  3. DELETE (1 record)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Podsumowanie tracked operacji:**\n",
    "\n",
    "Wszystkie wykonane operacje CRUD zostały zarejestrowane w Change Data Feed między wersjami start a end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CDF włączony: {cdf_enabled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Status CDF dla tabeli:**\n",
    "\n",
    "CDF jest obecnie wyłączony - musimy go włączyć za pomocą ALTER TABLE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zapisz start version dla CDF tracking:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = spark.sql(f\"DESCRIBE HISTORY {ORDERS_DELTA}\")\n",
    "start_version = history.agg({\"version\": \"max\"}).collect()[0][0] + 1\n",
    "\n",
    "print(f\"Start version dla CDF tracking: {start_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Operacja 1: INSERT (nowe zamówienia):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_orders = spark.createDataFrame([\n",
    "    (\"ORD_CDF_001\", \"CUST001234\", \"PROD000123\", \"STORE010\", \"2025-01-22T10:00:00\", \n",
    "     5, 99.99, 10, 449.96, \"Credit Card\"),\n",
    "    (\"ORD_CDF_002\", \"CUST001235\", \"PROD000456\", \"STORE020\", \"2025-01-22T11:30:00\", \n",
    "     2, 149.50, 5, 284.05, \"Debit Card\")\n",
    "], [\"order_id\", \"customer_id\", \"product_id\", \"store_id\", \"order_datetime\",\n",
    "    \"quantity\", \"unit_price\", \"discount_percent\", \"total_amount\", \"payment_method\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Operacja 2: UPDATE (zmiana payment_method):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    UPDATE {ORDERS_DELTA}\n",
    "    SET payment_method = 'Cash', total_amount = 400.00\n",
    "    WHERE order_id = 'ORD_CDF_001'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Operacja 3: DELETE (usunięcie zamówienia):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "    DELETE FROM {ORDERS_DELTA}\n",
    "    WHERE order_id = 'ORD_CDF_002'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Podsumowanie wykonanych operacji:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_version = spark.sql(f\"DESCRIBE HISTORY {ORDERS_DELTA}\").agg({\"version\": \"max\"}).collect()[0][0]\n",
    "\n",
    "operations = [\n",
    "    {\"type\": \"INSERT\", \"count\": 2},\n",
    "    {\"type\": \"UPDATE\", \"count\": 1},\n",
    "    {\"type\": \"DELETE\", \"count\": 1},\n",
    "]\n",
    "\n",
    "print(f\"Start version: {start_version}\")\n",
    "print(f\"End version: {end_version}\")\n",
    "print(f\"Liczba transakcji tracked: {end_version - start_version + 1}\")\n",
    "print(\"\\nOperacje wykonane:\")\n",
    "for operation in operations:\n",
    "    print(f\"  {operation['type']} ({operation['count']} records)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4815574e-f493-4bb8-9ea0-d130a0b2fd76",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 7.2: Odczyt Change Data Feed\n",
    "\n",
    "**Cel:** Odczyt i analiza zmian z CDF - wszystkie typy change events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a71ec82-7bcf-4c1e-9096-375120ac878c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 7.2 - Odczyt Change Data Feed\n",
    "\n",
    "# Metoda 1: PySpark API (readChangeFeed)\n",
    "print(\"=== Metoda 1: PySpark API ===\")\n",
    "\n",
    "# Odczytaj zmiany od start_version używając PySpark API\n",
    "changes_df = (spark.read\n",
    "    .format(\"delta\")\n",
    "    .option(\"readChangeFeed\", \"true\")\n",
    "    .option(\"startingVersion\", start_version)\n",
    "    .table(ORDERS_DELTA)\n",
    ")\n",
    "\n",
    "print(f\"Liczba change events: {changes_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Liczba change events: {changes_df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Liczba zarejestrowanych change events:**\n",
    "\n",
    "CDF rejestruje każdą operację CRUD jako separate events. UPDATE generuje 2 events (preimage + postimage)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schema CDF (zauważ dodatkowe kolumny):**\n",
    "\n",
    "Kolumny CDF:\n",
    "- `_change_type`: insert | update_preimage | update_postimage | delete\n",
    "- `_commit_version`: Version number\n",
    "- `_commit_timestamp`: Timestamp operacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(\n",
    "    changes_df\n",
    "    .select(\n",
    "        \"order_id\", \"customer_id\", \"total_amount\", \"payment_method\",\n",
    "        \"_change_type\", \"_commit_version\", \"_commit_timestamp\"\n",
    "    )\n",
    "    .orderBy(\"_commit_version\", \"_change_type\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Podsumowanie zmian po typach:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "change_summary = changes_df.groupBy(\"_change_type\").count().orderBy(\"_change_type\")\n",
    "display(change_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59dc57b4-d929-4e43-a070-34d272f540db",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przykład 7.3: Use Case - Incremental ETL z CDF\n",
    "\n",
    "**Cel:** Demonstracja praktycznego użycia CDF dla incremental processing\n",
    "\n",
    "**Scenario:** \n",
    "- Source table (orders_delta) zmienia się często\n",
    "- Target table (orders_summary) agreguje zamówienia\n",
    "- Chcemy aktualizować summary tylko dla zmienionych rekordów (nie full refresh!)\n",
    "\n",
    "**Podejście:**\n",
    "1. Read CDF od ostatniej processed version\n",
    "2. Process tylko changed records\n",
    "3. Update target table incrementally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2838f606-53b2-41b2-9e4d-240e156af0b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przykład 7.3 - Incremental ETL z CDF\n",
    "\n",
    "print(\"=== Use Case: Incremental ETL z CDF ===\\n\")\n",
    "\n",
    "# Krok 1: Przygotuj target table (orders_summary)\n",
    "ORDERS_SUMMARY = f\"{GOLD_SCHEMA}.orders_summary_cdf\"\n",
    "\n",
    "# Initial load - full aggregation (tylko raz)\n",
    "print(\"Krok 1: Initial load (full aggregation)\")\n",
    "initial_summary = (spark.table(ORDERS_DELTA)\n",
    "    .groupBy(\"customer_id\", \"payment_method\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"order_count\"),\n",
    "        F.sum(\"total_amount\").alias(\"total_spent\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_order_value\")\n",
    "    )\n",
    ")\n",
    "\n",
    "initial_summary.write.format(\"delta\").mode(\"overwrite\").saveAsTable(ORDERS_SUMMARY)\n",
    "print(f\"✓ Utworzono {ORDERS_SUMMARY}\")\n",
    "print(f\"  Records: {spark.table(ORDERS_SUMMARY).count()}\")\n",
    "\n",
    "# Zapisz last processed version\n",
    "last_processed_version = spark.sql(f\"DESCRIBE HISTORY {ORDERS_DELTA}\") \\\n",
    "    .agg({\"version\": \"max\"}).collect()[0][0]\n",
    "print(f\"  Last processed version: {last_processed_version}\\n\")\n",
    "\n",
    "# Krok 2: Symulacja nowych zmian w source table\n",
    "print(\"Krok 2: Symulacja nowych zmian w orders_delta\")\n",
    "\n",
    "# Dodaj nowe zamówienia\n",
    "new_orders_2 = spark.createDataFrame([\n",
    "    (\"ORD_CDF_003\", \"CUST001234\", \"PROD000789\", \"STORE030\", \"2025-01-23T09:00:00\", \n",
    "     3, 66.66, 0, 199.98, \"Credit Card\"),\n",
    "    (\"ORD_CDF_004\", \"CUST001236\", \"PROD000111\", \"STORE040\", \"2025-01-23T10:00:00\", \n",
    "     1, 299.99, 15, 254.99, \"Cash\")\n",
    "], [\"order_id\", \"customer_id\", \"product_id\", \"store_id\", \"order_datetime\",\n",
    "    \"quantity\", \"unit_price\", \"discount_percent\", \"total_amount\", \"payment_method\"])\n",
    "\n",
    "new_orders_2.write.mode(\"append\").saveAsTable(ORDERS_DELTA)\n",
    "print(\"✓ Dodano 2 nowe zamówienia\\n\")\n",
    "\n",
    "# Krok 3: Incremental processing z CDF\n",
    "print(\"Krok 3: Incremental processing (tylko zmiany!)\")\n",
    "\n",
    "current_version = spark.sql(f\"DESCRIBE HISTORY {ORDERS_DELTA}\") \\\n",
    "    .agg({\"version\": \"max\"}).collect()[0][0]\n",
    "\n",
    "# Read tylko changes od last processed version\n",
    "changes_incremental = (spark.read\n",
    "    .format(\"delta\")\n",
    "    .option(\"readChangeFeed\", \"true\")\n",
    "    .option(\"startingVersion\", last_processed_version + 1)\n",
    "    .table(ORDERS_DELTA)\n",
    ")\n",
    "\n",
    "print(f\"CDF changes (versions {last_processed_version + 1} to {current_version}):\")\n",
    "print(f\"  Change events: {changes_incremental.count()}\")\n",
    "\n",
    "# Filter tylko inserts (dla simplicity - w production obsłużyć update/delete)\n",
    "inserts_only = changes_incremental.filter(F.col(\"_change_type\") == \"insert\")\n",
    "\n",
    "if inserts_only.count() > 0:\n",
    "    # Compute incremental aggregates\n",
    "    incremental_agg = (inserts_only\n",
    "        .groupBy(\"customer_id\", \"payment_method\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"order_count\"),\n",
    "            F.sum(\"total_amount\").alias(\"total_spent\"),\n",
    "            F.avg(\"total_amount\").alias(\"avg_order_value\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"  Incremental aggregates: {incremental_agg.count()} rows\")\n",
    "    display(incremental_agg)\n",
    "    \n",
    "    # MERGE incremental aggregates into summary table\n",
    "    from delta.tables import DeltaTable\n",
    "    \n",
    "    summary_table = DeltaTable.forName(spark, ORDERS_SUMMARY)\n",
    "    \n",
    "    (summary_table.alias(\"target\")\n",
    "        .merge(\n",
    "            incremental_agg.alias(\"source\"),\n",
    "            \"target.customer_id = source.customer_id AND target.payment_method = source.payment_method\"\n",
    "        )\n",
    "        .whenMatchedUpdate(set = {\n",
    "            \"order_count\": \"target.order_count + source.order_count\",\n",
    "            \"total_spent\": \"target.total_spent + source.total_spent\",\n",
    "            \"avg_order_value\": \"(target.total_spent + source.total_spent) / (target.order_count + source.order_count)\"\n",
    "        })\n",
    "        .whenNotMatchedInsert(values = {\n",
    "            \"customer_id\": \"source.customer_id\",\n",
    "            \"payment_method\": \"source.payment_method\",\n",
    "            \"order_count\": \"source.order_count\",\n",
    "            \"total_spent\": \"source.total_spent\",\n",
    "            \"avg_order_value\": \"source.avg_order_value\"\n",
    "        })\n",
    "        .execute()\n",
    "    )\n",
    "    \n",
    "    print(\"\\n✓ Incremental MERGE wykonany!\")\n",
    "else:\n",
    "    print(\"  Brak nowych insertów do przetworzenia\")\n",
    "\n",
    "# Krok 4: Weryfikacja\n",
    "print(\"\\n=== Podsumowanie ===\")\n",
    "print(f\"✅ Incremental ETL zakończony pomyślnie!\")\n",
    "print(f\"   Przetworzone wersje: {last_processed_version + 1} → {current_version}\")\n",
    "print(f\"   Tylko {inserts_only.count()} change events (nie full table scan!)\")\n",
    "print(f\"\\n💡 Korzyści CDF:\")\n",
    "print(f\"   • Performance: Odczyt tylko zmian (nie full table)\")\n",
    "print(f\"   • Efficiency: Incremental processing\")\n",
    "print(f\"   • Scalability: Działa dla billion-row tables\")\n",
    "\n",
    "print(\"\\n=== Orders Summary (po incremental update) ===\")\n",
    "display(spark.table(ORDERS_SUMMARY).orderBy(F.desc(\"total_spent\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(spark.table(ORDERS_SUMMARY).orderBy(F.desc(\"total_spent\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Orders Summary (po incremental update):**\n",
    "\n",
    "Wynik incremental ETL - tabela podsumowań została zaktualizowana tylko na podstawie zmian z CDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Podsumowanie ===\")\n",
    "print(f\"✅ Incremental ETL zakończony pomyślnie!\")\n",
    "print(f\"Przetworzone wersje: {last_processed_version + 1} → {current_version}\")\n",
    "print(f\"Tylko {inserts_count} change events (nie full table scan!)\")\n",
    "print(\"\\n💡 Korzyści CDF:\")\n",
    "print(\"• Performance: Odczyt tylko zmian (nie full table)\")\n",
    "print(\"• Efficiency: Incremental processing\")\n",
    "print(\"• Scalability: Działa dla billion-row tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Podsumowanie incremental ETL:**\n",
    "\n",
    "Dzięki CDF przetworzyliśmy tylko zmienione rekordy zamiast całej tabeli - znacznie lepsze performance dla dużych datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if inserts_count > 0:\n",
    "    from delta.tables import DeltaTable\n",
    "    \n",
    "    summary_table = DeltaTable.forName(spark, ORDERS_SUMMARY)\n",
    "    \n",
    "    (summary_table.alias(\"target\")\n",
    "        .merge(\n",
    "            incremental_agg.alias(\"source\"),\n",
    "            \"target.customer_id = source.customer_id AND target.payment_method = source.payment_method\"\n",
    "        )\n",
    "        .whenMatchedUpdate(set = {\n",
    "            \"order_count\": \"target.order_count + source.order_count\",\n",
    "            \"total_spent\": \"target.total_spent + source.total_spent\",\n",
    "            \"avg_order_value\": \"(target.total_spent + source.total_spent) / (target.order_count + source.order_count)\"\n",
    "        })\n",
    "        .whenNotMatchedInsert(values = {\n",
    "            \"customer_id\": \"source.customer_id\",\n",
    "            \"payment_method\": \"source.payment_method\",\n",
    "            \"order_count\": \"source.order_count\",\n",
    "            \"total_spent\": \"source.total_spent\",\n",
    "            \"avg_order_value\": \"source.avg_order_value\"\n",
    "        })\n",
    "        .execute()\n",
    "    )\n",
    "    \n",
    "    print(\"✓ Incremental MERGE wykonany!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MERGE incremental aggregates do summary table:**\n",
    "\n",
    "Używamy MERGE aby zaktualizować istniejące aggregates lub wstawić nowe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if inserts_count > 0:\n",
    "    incremental_agg = (inserts_only\n",
    "        .groupBy(\"customer_id\", \"payment_method\")\n",
    "        .agg(\n",
    "            F.count(\"*\").alias(\"order_count\"),\n",
    "            F.sum(\"total_amount\").alias(\"total_spent\"),\n",
    "            F.avg(\"total_amount\").alias(\"avg_order_value\")\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"Incremental aggregates: {incremental_agg.count()} rows\")\n",
    "    display(incremental_agg)\n",
    "else:\n",
    "    print(\"Brak nowych insertów do przetworzenia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Oblicz incremental aggregates:**\n",
    "\n",
    "Agregujemy tylko nowe rekordy (z CDF insert events) zamiast przetwarzać całą tabelę."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inserts_only = changes_incremental.filter(F.col(\"_change_type\") == \"insert\")\n",
    "inserts_count = inserts_only.count()\n",
    "\n",
    "print(f\"Insert events to process: {inserts_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filtruj tylko INSERT events (dla simplicity):**\n",
    "\n",
    "W production należałoby obsłużyć wszystkie typy zmian (update, delete), ale dla demonstracji skupiamy się na insert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "changes_incremental = (spark.read\n",
    "    .format(\"delta\")\n",
    "    .option(\"readChangeFeed\", \"true\")\n",
    "    .option(\"startingVersion\", last_processed_version + 1)\n",
    "    .table(ORDERS_DELTA)\n",
    ")\n",
    "\n",
    "print(f\"CDF changes (versions {last_processed_version + 1} to {current_version}):\")\n",
    "print(f\"Change events: {changes_incremental.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Odczytaj tylko changes od last processed version:**\n",
    "\n",
    "CDF pozwala na efektywny odczyt tylko zmienionych rekordów między wersjami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_version = spark.sql(f\"DESCRIBE HISTORY {ORDERS_DELTA}\") \\\n",
    "    .agg({\"version\": \"max\"}).collect()[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Incremental processing z CDF:**\n",
    "\n",
    "Zamiast przetwarzać całą tabelę, używamy CDF aby odczytać tylko zmiany od last processed version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_orders_2.write.mode(\"append\").saveAsTable(ORDERS_DELTA)\n",
    "print(\"✓ Dodano 2 nowe zamówienia\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zapisz nowe zamówienia:**\n",
    "\n",
    "Te zamówienia utworzą nowe events w CDF, które będziemy przetwarzać incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_orders_2 = spark.createDataFrame([\n",
    "    (\"ORD_CDF_003\", \"CUST001234\", \"PROD000789\", \"STORE030\", \"2025-01-23T09:00:00\", \n",
    "     3, 66.66, 0, 199.98, \"Credit Card\"),\n",
    "    (\"ORD_CDF_004\", \"CUST001236\", \"PROD000111\", \"STORE040\", \"2025-01-23T10:00:00\", \n",
    "     1, 299.99, 15, 254.99, \"Cash\")\n",
    "], [\"order_id\", \"customer_id\", \"product_id\", \"store_id\", \"order_datetime\",\n",
    "    \"quantity\", \"unit_price\", \"discount_percent\", \"total_amount\", \"payment_method\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Symulacja nowych zmian w source table:**\n",
    "\n",
    "Dodajemy kolejne zamówienia aby zademonstrować incremental processing z CDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_processed_version = spark.sql(f\"DESCRIBE HISTORY {ORDERS_DELTA}\") \\\n",
    "    .agg({\"version\": \"max\"}).collect()[0][0]\n",
    "    \n",
    "print(f\"Last processed version: {last_processed_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zapisz last processed version:**\n",
    "\n",
    "Zapamiętujemy do której wersji przetworzyliśmy dane - będzie to baseline dla incremental processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_summary.write.format(\"delta\").mode(\"overwrite\").saveAsTable(ORDERS_SUMMARY)\n",
    "\n",
    "records_count = spark.table(ORDERS_SUMMARY).count()\n",
    "print(f\"✓ Utworzono {ORDERS_SUMMARY}\")\n",
    "print(f\"Records: {records_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Zapisz summary table:**\n",
    "\n",
    "Zapisujemy initial aggregation jako Delta table w Gold layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_summary = (spark.table(ORDERS_DELTA)\n",
    "    .groupBy(\"customer_id\", \"payment_method\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"order_count\"),\n",
    "        F.sum(\"total_amount\").alias(\"total_spent\"),\n",
    "        F.avg(\"total_amount\").alias(\"avg_order_value\")\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial load - full aggregation:**\n",
    "\n",
    "Tworzymy tabelę podsumowań z pełnej agregacji wszystkich obecnych zamówień."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "01_delta_lake_operations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
