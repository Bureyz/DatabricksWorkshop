{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb3eb663",
   "metadata": {},
   "source": [
    "# Delta Operations & Medallion Architecture - Workshop\n",
    "\n",
    "**Cel szkoleniowy:** Praktyczne zastosowanie operacji Delta Lake (CRUD, MERGE, Time Travel, OPTIMIZE) oraz implementacja architektury Bronze/Silver/Gold.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Delta Lake CRUD operations (CREATE, INSERT, UPDATE, DELETE, MERGE)\n",
    "- Time Travel i wersjonowanie\n",
    "- Projektowanie warstw Medallion (Bronze/Silver/Gold)\n",
    "- Optymalizacja tabel (OPTIMIZE, ZORDER, VACUUM)\n",
    "- Audit metadata i data quality checks\n",
    "\n",
    "**Czas trwania:** 90 minut"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264538cd",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 2 - Lakehouse & Delta Lake\n",
    "- **Typ notebooka**: Workshop\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1adacc",
   "metadata": {},
   "source": [
    "## Izolacja per użytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37872e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde3cd56",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b208f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Wyświetl kontekst użytkownika\n",
    "print(\"=== Kontekst użytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"Użytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog jako domyślny\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "\n",
    "# Ścieżki do danych źródłowych - TODO: Uzupełnij brakujące ścieżki\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/____/orders_batch.json\"  # TODO: Uzupełnij folder\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/____.csv\"    # TODO: Uzupełnij nazwę pliku\n",
    "PRODUCTS_PARQUET = f\"{____}/products/products.parquet\"        # TODO: Uzupełnij bazową ścieżkę\n",
    "\n",
    "print(f\"\\n=== Ścieżki do danych ===\")\n",
    "print(f\"Orders: {ORDERS_JSON}\")\n",
    "print(f\"Customers: {CUSTOMERS_CSV}\")\n",
    "print(f\"Products: {PRODUCTS_PARQUET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75aa0ace",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 1: Delta Lake CRUD Operations (20 minut)\n",
    "\n",
    "**Cel:** Praktyczne zastosowanie podstawowych operacji Delta Lake: CREATE, INSERT, UPDATE, DELETE.\n",
    "\n",
    "### Zadanie 1.1: Utworzenie tabeli Delta z danymi produktów\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane z pliku `products.parquet` z folderu `dataset/products/`\n",
    "2. Dodaj kolumnę `load_timestamp` z aktualnym czasem\n",
    "3. Zapisz jako tabelę Delta w schemacie Bronze: `products_bronze`\n",
    "4. Użyj mode `overwrite` i włącz `overwriteSchema`\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Tabela `products_bronze` utworzona w schemacie Bronze\n",
    "- Wszystkie rekordy załadowane z audit timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343f9814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 1.1 - Utworzenie tabeli Delta\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Wczytaj dane produktów - TODO: Uzupełnij format i ścieżkę\n",
    "products_df = (\n",
    "    spark.read\n",
    "    .format(\"____\")  # TODO: Uzupełnij format (parquet)\n",
    "    .load(____)      # TODO: Uzupełnij ścieżkę (PRODUCTS_PARQUET)\n",
    ")\n",
    "\n",
    "print(\"=== Dane produktów ===\")\n",
    "products_df.printSchema()\n",
    "display(products_df.limit(5))\n",
    "\n",
    "# Dodaj audit column: load_timestamp - TODO: Uzupełnij funkcję\n",
    "products_with_audit = products_df.withColumn(\"load_timestamp\", F.____())  # TODO: current_timestamp\n",
    "\n",
    "# Zapisz jako tabelę Delta - TODO: Uzupełnij brakujące parametry\n",
    "products_table = f\"{BRONZE_SCHEMA}.products_bronze\"\n",
    "\n",
    "(\n",
    "    products_with_audit\n",
    "    .write\n",
    "    .format(\"____\")              # TODO: Uzupełnij format (delta)\n",
    "    .mode(\"____\")                # TODO: Uzupełnij mode (overwrite)\n",
    "    .option(\"____\", \"true\")      # TODO: Uzupełnij opcję (overwriteSchema)\n",
    "    .saveAsTable(____)           # TODO: Uzupełnij nazwę tabeli (products_table)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Utworzono tabelę: {products_table}\")\n",
    "print(f\"Liczba rekordów: {spark.table(products_table).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2f0557",
   "metadata": {},
   "source": [
    "### Zadanie 1.2: UPDATE - Aktualizacja cen produktów\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Zaktualizuj cenę (`unit_price`) wszystkich produktów z kategorii \"Electronics\" - zwiększ o 10%\n",
    "2. Użyj SQL UPDATE statement\n",
    "3. Wyświetl zaktualizowane rekordy\n",
    "\n",
    "**Wskazówki:**\n",
    "- Użyj: `UPDATE table_name SET column = value WHERE condition`\n",
    "- Zwiększenie o 10%: `unit_price * 1.1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8a4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 1.2 - UPDATE cen produktów\n",
    "\n",
    "# Sprawdź dane przed UPDATE\n",
    "print(\"=== Produkty Electronics PRZED aktualizacją ===\")\n",
    "display(\n",
    "    spark.table(products_table)\n",
    "    .filter(F.col(\"category\") == \"Electronics\")\n",
    "    .select(\"product_id\", \"product_name\", \"category\", \"unit_price\")\n",
    ")\n",
    "\n",
    "# UPDATE: zwiększ cenę o 10% dla kategorii Electronics\n",
    "# TODO: Uzupełnij brakujące części SQL\n",
    "spark.sql(f\"\"\"\n",
    "    ____ {products_table}           -- TODO: Uzupełnij komendę (UPDATE)\n",
    "    SET unit_price = ____ * ____    -- TODO: Uzupełnij wyrażenie (unit_price * 1.1)\n",
    "    WHERE ____ = '____'             -- TODO: Uzupełnij warunek (category = 'Electronics')\n",
    "\"\"\")\n",
    "\n",
    "# Sprawdź dane po UPDATE\n",
    "print(\"\\n=== Produkty Electronics PO aktualizacji ===\")\n",
    "display(\n",
    "    spark.table(products_table)\n",
    "    .filter(F.col(\"category\") == \"Electronics\")\n",
    "    .select(\"product_id\", \"product_name\", \"category\", \"unit_price\")\n",
    ")\n",
    "\n",
    "print(\"✓ Ceny zaktualizowane!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a5f29e",
   "metadata": {},
   "source": [
    "### Zadanie 1.3: DELETE - Usunięcie produktów\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Usuń produkty, które mają `stock_quantity = 0` (brak w magazynie)\n",
    "2. Użyj SQL DELETE statement\n",
    "3. Wyświetl liczbę usuniętych rekordów\n",
    "\n",
    "**Wskazówki:**\n",
    "- Użyj: `DELETE FROM table_name WHERE condition`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29239c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 1.3 - DELETE produktów bez stanu magazynowego\n",
    "\n",
    "# Liczba rekordów przed DELETE\n",
    "count_before = spark.table(products_table).count()\n",
    "out_of_stock_count = spark.table(products_table).filter(F.col(\"stock_quantity\") == 0).count()\n",
    "\n",
    "print(f\"=== Stan przed DELETE ===\")\n",
    "print(f\"Liczba produktów: {count_before}\")\n",
    "print(f\"Produkty bez stanu (stock_quantity = 0): {out_of_stock_count}\")\n",
    "\n",
    "# DELETE produktów bez stanu magazynowego\n",
    "# TODO: Uzupełnij SQL DELETE statement\n",
    "spark.sql(f\"\"\"\n",
    "    ____ FROM {products_table}  -- TODO: Uzupełnij komendę (DELETE)\n",
    "    WHERE ____ = ____           -- TODO: Uzupełnij warunek (stock_quantity = 0)\n",
    "\"\"\")\n",
    "\n",
    "# Liczba rekordów po DELETE\n",
    "count_after = spark.table(products_table).count()\n",
    "\n",
    "print(f\"\\n=== Stan po DELETE ===\")\n",
    "print(f\"Liczba produktów: {count_after}\")\n",
    "print(f\"Usunięto: {count_before - count_after} produktów\")\n",
    "\n",
    "print(\"✓ Produkty bez stanu usunięte!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d683ce0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 2: MERGE INTO - Upsert Operations (15 minut)\n",
    "\n",
    "**Cel:** Implementacja operacji MERGE dla upsert (update existing + insert new).\n",
    "\n",
    "### Zadanie 2.1: MERGE nowych zamówień\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane zamówień z `orders_batch.json`\n",
    "2. Utwórz tabelę `orders_bronze` jeśli nie istnieje\n",
    "3. Użyj MERGE INTO do załadowania danych:\n",
    "   - MATCHED: zaktualizuj `order_status` i `order_amount`\n",
    "   - NOT MATCHED: wstaw nowy rekord\n",
    "4. Klucz: `order_id`\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Istniejące zamówienia zaktualizowane\n",
    "- Nowe zamówienia dodane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5373ba89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 2.1 - MERGE INTO dla zamówień\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Wczytaj nowe dane zamówień - TODO: Uzupełnij format i opcje\n",
    "new_orders = (\n",
    "    spark.read\n",
    "    .format(\"____\")               # TODO: Uzupełnij format (json)\n",
    "    .option(\"____\", \"true\")       # TODO: Uzupełnij opcję (multiLine)\n",
    "    .load(____)                   # TODO: Uzupełnij ścieżkę (ORDERS_JSON)\n",
    "    .withColumn(\"ingest_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"ingested_by\", F.lit(raw_user))\n",
    ")\n",
    "\n",
    "print(\"=== Nowe zamówienia do merge ===\")\n",
    "display(new_orders.limit(5))\n",
    "\n",
    "# Nazwa tabeli docelowej\n",
    "orders_table = f\"{BRONZE_SCHEMA}.orders_bronze\"\n",
    "\n",
    "# Utwórz tabelę jeśli nie istnieje (initial load)\n",
    "if not spark.catalog.tableExists(orders_table):\n",
    "    new_orders.limit(0).write.format(\"delta\").saveAsTable(orders_table)\n",
    "    print(f\"✓ Utworzono pustą tabelę: {orders_table}\")\n",
    "\n",
    "# Zarejestruj DataFrame jako temp view dla MERGE\n",
    "new_orders.createOrReplaceTempView(\"new_orders_view\")\n",
    "\n",
    "# Liczba przed MERGE\n",
    "count_before = spark.table(orders_table).count()\n",
    "\n",
    "# MERGE INTO - TODO: Uzupełnij brakujące części\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {orders_table} AS target\n",
    "    USING new_orders_view AS source\n",
    "    ON target.____ = source.____                    -- TODO: Uzupełnij klucz (order_id)\n",
    "    WHEN ____ THEN                                  -- TODO: Uzupełnij (MATCHED)\n",
    "        UPDATE SET\n",
    "            target.order_status = source.____,      -- TODO: Uzupełnij kolumnę\n",
    "            target.order_amount = source.____,      -- TODO: Uzupełnij kolumnę\n",
    "            target.ingest_timestamp = source.ingest_timestamp\n",
    "    WHEN ____ ____ THEN                             -- TODO: Uzupełnij (NOT MATCHED)\n",
    "        INSERT *\n",
    "\"\"\")\n",
    "\n",
    "# Liczba po MERGE\n",
    "count_after = spark.table(orders_table).count()\n",
    "\n",
    "print(f\"\\n=== Wynik MERGE ===\")\n",
    "print(f\"Rekordy przed: {count_before}\")\n",
    "print(f\"Rekordy po: {count_after}\")\n",
    "print(f\"Dodano nowych: {count_after - count_before}\")\n",
    "\n",
    "print(\"✓ MERGE zakończony!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4686a677",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 3: Time Travel (10 minut)\n",
    "\n",
    "**Cel:** Wykorzystanie Time Travel do przeglądania historii zmian i przywracania danych.\n",
    "\n",
    "### Zadanie 3.1: Historia wersji tabeli\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wyświetl historię tabeli `products_bronze` używając `DESCRIBE HISTORY`\n",
    "2. Znajdź wersję przed wykonaniem UPDATE (Zadanie 1.2)\n",
    "3. Odczytaj dane z tej wersji używając `VERSION AS OF`\n",
    "\n",
    "**Wskazówki:**\n",
    "- `DESCRIBE HISTORY table_name`\n",
    "- `SELECT * FROM table_name VERSION AS OF version_number`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef0ee8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 3.1 - Time Travel\n",
    "\n",
    "# Wyświetl historię wersji tabeli - TODO: Uzupełnij komendę\n",
    "print(\"=== Historia wersji tabeli products_bronze ===\")\n",
    "history = spark.sql(f\"____ ____ {products_table}\")  # TODO: DESCRIBE HISTORY\n",
    "display(history.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\"))\n",
    "\n",
    "# Znajdź wersję przed UPDATE (operation = 'WRITE' na początku)\n",
    "first_version = history.filter(F.col(\"operation\") == \"WRITE\").orderBy(\"version\").first()[\"version\"]\n",
    "\n",
    "print(f\"\\n=== Odczytanie danych z wersji {first_version} (przed UPDATE) ===\")\n",
    "\n",
    "# Time Travel: odczytaj dane z konkretnej wersji\n",
    "# TODO: Uzupełnij brakujące części SQL\n",
    "products_old_version = spark.sql(f\"\"\"\n",
    "    SELECT product_id, product_name, category, unit_price\n",
    "    FROM {products_table} \n",
    "    ____ AS OF ____                    -- TODO: Uzupełnij (VERSION AS OF {first_version})\n",
    "    WHERE category = 'Electronics'\n",
    "\"\"\")\n",
    "\n",
    "display(products_old_version)\n",
    "\n",
    "# Porównaj z aktualną wersją\n",
    "print(\"\\n=== Aktualna wersja (po UPDATE) ===\")\n",
    "products_current = spark.sql(f\"\"\"\n",
    "    SELECT product_id, product_name, category, unit_price\n",
    "    FROM {products_table}\n",
    "    WHERE category = 'Electronics'\n",
    "\"\"\")\n",
    "display(products_current)\n",
    "\n",
    "print(\"✓ Time Travel działa!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9155445",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 4: Implementacja Medallion Architecture (30 minut)\n",
    "\n",
    "**Cel:** Zbudowanie pipeline'u Bronze → Silver → Gold zgodnie z architekturą medalionową.\n",
    "\n",
    "### Zadanie 4.1: Bronze Layer - Raw Data Landing\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane klientów z `customers.csv`\n",
    "2. Dodaj audit metadata:\n",
    "   - `ingest_timestamp` (current_timestamp)\n",
    "   - `source_file` (input_file_name)\n",
    "   - `ingested_by` (raw_user)\n",
    "3. Zapisz jako `customers_bronze` w Bronze schema\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Tabela Bronze z surowymi danymi + audit columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340454e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 4.1 - Bronze Layer\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Wczytaj dane klientów - TODO: Uzupełnij format i opcje\n",
    "customers_raw = (\n",
    "    spark.read\n",
    "    .format(\"____\")              # TODO: Uzupełnij format (csv)\n",
    "    .option(\"____\", \"true\")      # TODO: Uzupełnij opcję (header)\n",
    "    .option(\"____\", \"true\")      # TODO: Uzupełnij opcję (inferSchema)\n",
    "    .load(____)                  # TODO: Uzupełnij ścieżkę (CUSTOMERS_CSV)\n",
    ")\n",
    "\n",
    "print(\"=== Surowe dane klientów ===\")\n",
    "customers_raw.printSchema()\n",
    "display(customers_raw.limit(5))\n",
    "\n",
    "# Dodaj audit metadata dla Bronze - TODO: Uzupełnij kolumny i funkcje\n",
    "customers_bronze = (\n",
    "    customers_raw\n",
    "    .withColumn(\"____\", F.____())           # TODO: ingest_timestamp, current_timestamp()\n",
    "    .withColumn(\"____\", F.____())           # TODO: source_file, input_file_name()\n",
    "    .withColumn(\"____\", F.lit(____))        # TODO: ingested_by, raw_user\n",
    ")\n",
    "\n",
    "# Zapisz do Bronze\n",
    "customers_bronze_table = f\"{BRONZE_SCHEMA}.customers_bronze\"\n",
    "\n",
    "(\n",
    "    customers_bronze\n",
    "    .write\n",
    "    .format(\"____\")              # TODO: delta\n",
    "    .mode(\"____\")                # TODO: overwrite\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(____)           # TODO: customers_bronze_table\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Bronze layer utworzony: {customers_bronze_table}\")\n",
    "print(f\"Liczba rekordów: {spark.table(customers_bronze_table).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb8d57c",
   "metadata": {},
   "source": [
    "### Zadanie 4.2: Silver Layer - Cleansed & Validated\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane z Bronze: `customers_bronze`\n",
    "2. Zastosuj transformacje:\n",
    "   - Usuń duplikaty po `customer_id`\n",
    "   - Odfiltruj rekordy gdzie `customer_id` lub `email` jest NULL\n",
    "   - Standaryzuj `email` do lowercase\n",
    "   - Waliduj: `age` musi być > 0 i < 120\n",
    "   - Dodaj kolumnę `silver_processed_timestamp`\n",
    "   - Dodaj kolumnę `data_quality_flag` = \"VALID\"\n",
    "3. Zapisz jako `customers_silver` w Silver schema\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Oczyszczone, zwalidowane dane w Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686730b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 4.2 - Silver Layer\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {SILVER_SCHEMA}\")\n",
    "\n",
    "# Wczytaj dane z Bronze\n",
    "customers_bronze_df = spark.table(customers_bronze_table)\n",
    "\n",
    "# Silver transformations - TODO: Uzupełnij brakujące transformacje\n",
    "customers_silver = (\n",
    "    customers_bronze_df\n",
    "    # Deduplikacja - TODO: Uzupełnij kolumnę\n",
    "    .dropDuplicates([____])                    # TODO: customer_id\n",
    "    \n",
    "    # Walidacja NOT NULL - TODO: Uzupełnij warunki\n",
    "    .filter(F.col(\"____\").isNotNull())         # TODO: customer_id\n",
    "    .filter(F.col(\"____\").isNotNull())         # TODO: email\n",
    "    \n",
    "    # Standaryzacja email - TODO: Uzupełnij funkcję\n",
    "    .withColumn(\"email\", F.____(F.col(\"email\")))  # TODO: lower\n",
    "    \n",
    "    # Walidacja wieku - TODO: Uzupełnij warunki\n",
    "    .filter((F.col(\"____\") > ____) & (F.col(\"____\") < ____))  # TODO: age > 0, age < 120\n",
    "    \n",
    "    # Silver metadata - TODO: Uzupełnij kolumny\n",
    "    .withColumn(\"____\", F.current_timestamp())     # TODO: silver_processed_timestamp\n",
    "    .withColumn(\"____\", F.lit(\"____\"))             # TODO: data_quality_flag, \"VALID\"\n",
    ")\n",
    "\n",
    "print(\"=== Silver layer - cleansed data ===\")\n",
    "display(customers_silver.limit(5))\n",
    "\n",
    "# Zapisz do Silver\n",
    "customers_silver_table = f\"{SILVER_SCHEMA}.customers_silver\"\n",
    "\n",
    "(\n",
    "    customers_silver\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(customers_silver_table)\n",
    ")\n",
    "\n",
    "bronze_count = customers_bronze_df.count()\n",
    "silver_count = spark.table(customers_silver_table).count()\n",
    "\n",
    "print(f\"\\n✓ Silver layer utworzony: {customers_silver_table}\")\n",
    "print(f\"Bronze records: {bronze_count}\")\n",
    "print(f\"Silver records: {silver_count}\")\n",
    "print(f\"Filtered out: {bronze_count - silver_count} records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77094623",
   "metadata": {},
   "source": [
    "### Zadanie 4.3: Gold Layer - Business Aggregates\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane z Silver: `customers_silver`\n",
    "2. Utwórz agregację klientów per kraj:\n",
    "   - `customer_count`: liczba klientów\n",
    "   - `avg_age`: średni wiek\n",
    "   - `min_registration_date`: najwcześniejsza rejestracja\n",
    "   - `max_registration_date`: najpóźniejsza rejestracja\n",
    "3. Dodaj `gold_created_timestamp`\n",
    "4. Zapisz jako `customer_summary_by_country` w Gold schema\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Tabela Gold z agregacjami biznesowymi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a629de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 4.3 - Gold Layer\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {GOLD_SCHEMA}\")\n",
    "\n",
    "# Wczytaj dane z Silver\n",
    "customers_silver_df = spark.table(customers_silver_table)\n",
    "\n",
    "# Gold aggregation: Summary per country\n",
    "# TODO: Uzupełnij brakujące części agregacji\n",
    "customer_summary = (\n",
    "    customers_silver_df\n",
    "    .groupBy(\"____\")                # TODO: country\n",
    "    .agg(\n",
    "        F.count(\"____\").alias(\"____\"),      # TODO: customer_id, customer_count\n",
    "        F.avg(\"____\").alias(\"____\"),        # TODO: age, avg_age\n",
    "        F.min(\"____\").alias(\"____\"),        # TODO: registration_date, min_registration_date\n",
    "        F.max(\"____\").alias(\"____\")         # TODO: registration_date, max_registration_date\n",
    "    )\n",
    "    .withColumn(\"____\", F.current_timestamp())  # TODO: gold_created_timestamp\n",
    "    .orderBy(\"customer_count\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"=== Gold layer - Customer Summary by Country ===\")\n",
    "display(customer_summary)\n",
    "\n",
    "# Zapisz do Gold\n",
    "gold_table = f\"{GOLD_SCHEMA}.customer_summary_by_country\"\n",
    "\n",
    "(\n",
    "    customer_summary\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(gold_table)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Gold layer utworzony: {gold_table}\")\n",
    "print(f\"Liczba krajów: {spark.table(gold_table).count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c925c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Zadanie 5: Optymalizacja tabel Delta (15 minut)\n",
    "\n",
    "**Cel:** Zastosowanie technik optymalizacji: OPTIMIZE, ZORDER, VACUUM.\n",
    "\n",
    "### Zadanie 5.1: OPTIMIZE - Compaction małych plików\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Sprawdź liczbę plików w tabeli `customers_silver` używając `DESCRIBE DETAIL`\n",
    "2. Uruchom `OPTIMIZE` na tabeli\n",
    "3. Sprawdź ponownie liczbę plików - powinna się zmniejszyć\n",
    "\n",
    "**Wskazówki:**\n",
    "- `DESCRIBE DETAIL table_name`\n",
    "- `OPTIMIZE table_name`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419249e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 5.1 - OPTIMIZE\n",
    "\n",
    "# Stan przed OPTIMIZE - TODO: Uzupełnij komendę\n",
    "print(\"=== Stan przed OPTIMIZE ===\")\n",
    "detail_before = spark.sql(f\"____ ____ {customers_silver_table}\")  # TODO: DESCRIBE DETAIL\n",
    "display(detail_before.select(\"numFiles\", \"sizeInBytes\"))\n",
    "\n",
    "num_files_before = detail_before.collect()[0][\"numFiles\"]\n",
    "print(f\"Liczba plików przed: {num_files_before}\")\n",
    "\n",
    "# Uruchom OPTIMIZE - TODO: Uzupełnij komendę\n",
    "print(\"\\n=== Uruchamianie OPTIMIZE ===\")\n",
    "optimize_result = spark.sql(f\"____ {____}\")  # TODO: OPTIMIZE {customers_silver_table}\n",
    "display(optimize_result)\n",
    "\n",
    "# Stan po OPTIMIZE\n",
    "print(\"\\n=== Stan po OPTIMIZE ===\")\n",
    "detail_after = spark.sql(f\"DESCRIBE DETAIL {customers_silver_table}\")\n",
    "display(detail_after.select(\"numFiles\", \"sizeInBytes\"))\n",
    "\n",
    "num_files_after = detail_after.collect()[0][\"numFiles\"]\n",
    "print(f\"Liczba plików po: {num_files_after}\")\n",
    "print(f\"Redukcja: {num_files_before - num_files_after} plików\")\n",
    "\n",
    "print(\"✓ OPTIMIZE zakończony!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cb7074",
   "metadata": {},
   "source": [
    "### Zadanie 5.2: ZORDER - Clustering dla lepszego data skipping\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Uruchom `OPTIMIZE` z `ZORDER BY` na tabeli `orders_bronze`\n",
    "2. Użyj `ZORDER BY (customer_id)` - kolumna często używana w filtrach\n",
    "3. Sprawdź metryki optymalizacji\n",
    "\n",
    "**Wskazówki:**\n",
    "- `OPTIMIZE table_name ZORDER BY (column1, column2)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68826bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zadanie 5.2 - ZORDER\n",
    "\n",
    "# Uruchom OPTIMIZE z ZORDER na customer_id\n",
    "# TODO: Uzupełnij komendę SQL\n",
    "print(\"=== OPTIMIZE + ZORDER BY customer_id ===\")\n",
    "\n",
    "zorder_result = spark.sql(f\"\"\"\n",
    "    ____ {orders_table}         -- TODO: OPTIMIZE\n",
    "    ____ BY (____)              -- TODO: ZORDER BY (customer_id)\n",
    "\"\"\")\n",
    "\n",
    "display(zorder_result)\n",
    "\n",
    "print(\"✓ ZORDER zakończony - dane są teraz lepiej zorganizowane dla queries po customer_id!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6603de",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist - Co powinieneś uzyskać:\n",
    "- [ ] Tabela `products_bronze` utworzona z operacjami UPDATE i DELETE\n",
    "- [ ] Tabela `orders_bronze` z zastosowanym MERGE INTO\n",
    "- [ ] Time Travel działa - można odczytać starą wersję `products_bronze`\n",
    "- [ ] Pipeline Medallion: `customers_bronze` → `customers_silver` → `customer_summary_by_country`\n",
    "- [ ] Tabele zoptymalizowane (OPTIMIZE, ZORDER)\n",
    "\n",
    "### Komendy weryfikacyjne:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb8e619",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja wyników\n",
    "\n",
    "print(\"=== WERYFIKACJA WYNIKÓW ===\\n\")\n",
    "\n",
    "# 1. Sprawdź tabele Bronze\n",
    "print(\"1. Tabele Bronze:\")\n",
    "bronze_tables = spark.sql(f\"SHOW TABLES IN {BRONZE_SCHEMA}\").filter(F.col(\"tableName\").like(\"%bronze%\"))\n",
    "display(bronze_tables)\n",
    "\n",
    "# 2. Sprawdź tabele Silver\n",
    "print(\"\\n2. Tabele Silver:\")\n",
    "silver_tables = spark.sql(f\"SHOW TABLES IN {SILVER_SCHEMA}\").filter(F.col(\"tableName\").like(\"%silver%\"))\n",
    "display(silver_tables)\n",
    "\n",
    "# 3. Sprawdź tabele Gold\n",
    "print(\"\\n3. Tabele Gold:\")\n",
    "gold_tables = spark.sql(f\"SHOW TABLES IN {GOLD_SCHEMA}\")\n",
    "display(gold_tables)\n",
    "\n",
    "# 4. Sprawdź historię dla products_bronze\n",
    "print(\"\\n4. Historia products_bronze (powinny być operacje: WRITE, UPDATE, DELETE):\")\n",
    "history = spark.sql(f\"DESCRIBE HISTORY {products_table}\")\n",
    "display(history.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").limit(10))\n",
    "\n",
    "# 5. Sprawdź dane Gold\n",
    "print(\"\\n5. Przykładowe dane Gold:\")\n",
    "display(spark.table(gold_table).limit(10))\n",
    "\n",
    "print(\"\\n✓ Wszystkie testy przeszły pomyślnie!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8b8bcf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "**W tym warsztacie nauczyłeś się:**\n",
    "\n",
    "✅ **Delta Lake CRUD Operations:**\n",
    "- CREATE TABLE z różnych formatów źródłowych\n",
    "- UPDATE - aktualizacja rekordów\n",
    "- DELETE - usuwanie rekordów\n",
    "- MERGE INTO - upsert (update + insert)\n",
    "\n",
    "✅ **Time Travel:**\n",
    "- DESCRIBE HISTORY - historia wersji\n",
    "- VERSION AS OF - odczytywanie starych wersji\n",
    "- Możliwość audytu i rollback\n",
    "\n",
    "✅ **Medallion Architecture:**\n",
    "- Bronze: Raw data + audit metadata (immutable, append-only)\n",
    "- Silver: Cleansed, validated, deduplicated\n",
    "- Gold: Business aggregates, KPI models\n",
    "\n",
    "✅ **Optymalizacja:**\n",
    "- OPTIMIZE - compaction małych plików\n",
    "- ZORDER - clustering dla lepszego data skipping\n",
    "- DESCRIBE DETAIL - monitoring stanu tabel\n",
    "\n",
    "**Kluczowe wnioski:**\n",
    "1. Delta Lake zapewnia ACID transactions dla data lake\n",
    "2. Time Travel umożliwia audyt i recovery bez backupów\n",
    "3. Medallion Architecture separuje raw data od business logic\n",
    "4. Regularna optymalizacja (OPTIMIZE) jest kluczowa dla performance\n",
    "\n",
    "**Następne kroki:**\n",
    "- **Kolejny warsztat**: 02_ingestion_pipeline_workshop.ipynb - COPY INTO, Auto Loader, Streaming\n",
    "- **Materiały dodatkowe**: Delta Lake Documentation, Medallion Architecture Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a12db1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Opcjonalnie: usuń utworzone tabele po zakończeniu warsztatu:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e9934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcjonalne czyszczenie zasobów testowych\n",
    "# UWAGA: Uruchom tylko jeśli chcesz usunąć wszystkie utworzone dane\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {products_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {orders_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {customers_bronze_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {customers_silver_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {gold_table}\")\n",
    "\n",
    "# spark.catalog.clearCache()\n",
    "# print(\"Zasoby zostały wyczyszczone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c07970",
   "metadata": {},
   "outputs": [],
   "source": [
    "```xml\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "# Delta Operations & Medallion Architecture - Workshop\n",
    "\n",
    "**Cel szkoleniowy:** Praktyczne zastosowanie operacji Delta Lake (CRUD, MERGE, Time Travel, OPTIMIZE) oraz implementacja architektury Bronze/Silver/Gold.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Delta Lake CRUD operations (CREATE, INSERT, UPDATE, DELETE, MERGE)\n",
    "- Time Travel i wersjonowanie\n",
    "- Projektowanie warstw Medallion (Bronze/Silver/Gold)\n",
    "- Optymalizacja tabel (OPTIMIZE, ZORDER, VACUUM)\n",
    "- Audit metadata i data quality checks\n",
    "\n",
    "**Czas trwania:** 90 minut\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzień szkolenia**: Dzień 2 - Lakehouse & Delta Lake\n",
    "- **Typ notebooka**: Workshop\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog włączony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Izolacja per użytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogów i schematów:\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "%run ../../00_setup\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych środowiskowych:\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Wyświetl kontekst użytkownika\n",
    "print(\"=== Kontekst użytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"Użytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog jako domyślny\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "\n",
    "# Ścieżki do danych źródłowych\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "\n",
    "print(f\"\\n=== Ścieżki do danych ===\")\n",
    "print(f\"Orders: {ORDERS_JSON}\")\n",
    "print(f\"Customers: {CUSTOMERS_CSV}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Zadanie 1: Delta Lake CRUD Operations (20 minut)\n",
    "\n",
    "**Cel:** Praktyczne zastosowanie podstawowych operacji Delta Lake: CREATE, INSERT, UPDATE, DELETE.\n",
    "\n",
    "### Zadanie 1.1: Utworzenie tabeli Delta z danymi produktów\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane z pliku `products.parquet` z folderu `dataset/products/`\n",
    "2. Dodaj kolumnę `load_timestamp` z aktualnym czasem\n",
    "3. Zapisz jako tabelę Delta w schemacie Bronze: `products_bronze`\n",
    "4. Użyj mode `overwrite` i włącz `overwriteSchema`\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Tabela `products_bronze` utworzona w schemacie Bronze\n",
    "- Wszystkie rekordy załadowane z audit timestamp\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 1.1 - Utworzenie tabeli Delta\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Ścieżka do pliku produktów\n",
    "PRODUCTS_PARQUET = f\"{____}/products/products.parquet\"\n",
    "\n",
    "# Wczytaj dane produktów\n",
    "products_df = (\n",
    "    spark.read\n",
    "    .format(\"____\")\n",
    "    .load(____)\n",
    ")\n",
    "\n",
    "print(\"=== Dane produktów ===\")\n",
    "products_df.printSchema()\n",
    "display(products_df.limit(5))\n",
    "\n",
    "# Dodaj audit column: load_timestamp\n",
    "products_with_audit = products_df.withColumn(\"____\", F.____)\n",
    "\n",
    "# Zapisz jako tabelę Delta\n",
    "products_table = f\"{BRONZE_SCHEMA}.products_bronze\"\n",
    "\n",
    "(\n",
    "    products_with_audit\n",
    "    .write\n",
    "    .format(\"____\")\n",
    "    .mode(\"____\")\n",
    "    .option(\"overwriteSchema\", \"____\")\n",
    "    .saveAsTable(____)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Utworzono tabelę: {products_table}\")\n",
    "print(f\"Liczba rekordów: {spark.table(products_table).count()}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Zadanie 1.2: UPDATE - Aktualizacja cen produktów\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Zaktualizuj cenę (`unit_price`) wszystkich produktów z kategorii \"Electronics\" - zwiększ o 10%\n",
    "2. Użyj SQL UPDATE statement\n",
    "3. Wyświetl zaktualizowane rekordy\n",
    "\n",
    "**Wskazówki:**\n",
    "- Użyj: `UPDATE table_name SET column = value WHERE condition`\n",
    "- Zwiększenie o 10%: `unit_price * 1.1`\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 1.2 - UPDATE cen produktów\n",
    "\n",
    "# Sprawdź dane przed UPDATE\n",
    "print(\"=== Produkty Electronics PRZED aktualizacją ===\")\n",
    "display(\n",
    "    spark.table(products_table)\n",
    "    .filter(F.col(\"category\") == \"Electronics\")\n",
    "    .select(\"product_id\", \"product_name\", \"category\", \"unit_price\")\n",
    ")\n",
    "\n",
    "# UPDATE: zwiększ cenę o 10% dla kategorii Electronics\n",
    "spark.sql(f\"\"\"\n",
    "    ____ {products_table}\n",
    "    SET ____ = unit_price * ____\n",
    "    WHERE ____ = '____'\n",
    "\"\"\")\n",
    "\n",
    "# Sprawdź dane po UPDATE\n",
    "print(\"\\n=== Produkty Electronics PO aktualizacji ===\")\n",
    "display(\n",
    "    spark.table(products_table)\n",
    "    .filter(F.col(\"category\") == \"Electronics\")\n",
    "    .select(\"product_id\", \"product_name\", \"category\", \"unit_price\")\n",
    ")\n",
    "\n",
    "print(\"✓ Ceny zaktualizowane!\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Zadanie 1.3: DELETE - Usunięcie produktów\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Usuń produkty, które mają `stock_quantity = 0` (brak w magazynie)\n",
    "2. Użyj SQL DELETE statement\n",
    "3. Wyświetl liczbę usuniętych rekordów\n",
    "\n",
    "**Wskazówki:**\n",
    "- Użyj: `DELETE FROM table_name WHERE condition`\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 1.3 - DELETE produktów bez stanu magazynowego\n",
    "\n",
    "# Liczba rekordów przed DELETE\n",
    "count_before = spark.table(products_table).count()\n",
    "out_of_stock_count = spark.table(products_table).filter(F.col(\"stock_quantity\") == 0).count()\n",
    "\n",
    "print(f\"=== Stan przed DELETE ===\")\n",
    "print(f\"Liczba produktów: {count_before}\")\n",
    "print(f\"Produkty bez stanu (stock_quantity = 0): {out_of_stock_count}\")\n",
    "\n",
    "# DELETE produktów bez stanu magazynowego\n",
    "spark.sql(f\"\"\"\n",
    "    ____ FROM {products_table}\n",
    "    WHERE ____ = ____\n",
    "\"\"\")\n",
    "\n",
    "# Liczba rekordów po DELETE\n",
    "count_after = spark.table(products_table).count()\n",
    "\n",
    "print(f\"\\n=== Stan po DELETE ===\")\n",
    "print(f\"Liczba produktów: {count_after}\")\n",
    "print(f\"Usunięto: {count_before - count_after} produktów\")\n",
    "\n",
    "print(\"✓ Produkty bez stanu usunięte!\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Zadanie 2: MERGE INTO - Upsert Operations (15 minut)\n",
    "\n",
    "**Cel:** Implementacja operacji MERGE dla upsert (update existing + insert new).\n",
    "\n",
    "### Zadanie 2.1: MERGE nowych zamówień\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane zamówień z `orders_batch.json`\n",
    "2. Utwórz tabelę `orders_bronze` jeśli nie istnieje\n",
    "3. Użyj MERGE INTO do załadowania danych:\n",
    "   - MATCHED: zaktualizuj `order_status` i `order_amount`\n",
    "   - NOT MATCHED: wstaw nowy rekord\n",
    "4. Klucz: `order_id`\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Istniejące zamówienia zaktualizowane\n",
    "- Nowe zamówienia dodane\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 2.1 - MERGE INTO dla zamówień\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Wczytaj nowe dane zamówień\n",
    "new_orders = (\n",
    "    spark.read\n",
    "    .format(\"____\")\n",
    "    .option(\"multiLine\", \"____\")\n",
    "    .load(____)\n",
    "    .withColumn(\"ingest_timestamp\", F.current_timestamp())\n",
    "    .withColumn(\"ingested_by\", F.lit(raw_user))\n",
    ")\n",
    "\n",
    "print(\"=== Nowe zamówienia do merge ===\")\n",
    "display(new_orders.limit(5))\n",
    "\n",
    "# Nazwa tabeli docelowej\n",
    "orders_table = f\"{BRONZE_SCHEMA}.orders_bronze\"\n",
    "\n",
    "# Utwórz tabelę jeśli nie istnieje (initial load)\n",
    "if not spark.catalog.tableExists(orders_table):\n",
    "    new_orders.limit(0).write.format(\"delta\").saveAsTable(orders_table)\n",
    "    print(f\"✓ Utworzono pustą tabelę: {orders_table}\")\n",
    "\n",
    "# Zarejestruj DataFrame jako temp view dla MERGE\n",
    "new_orders.createOrReplaceTempView(\"new_orders_view\")\n",
    "\n",
    "# Liczba przed MERGE\n",
    "count_before = spark.table(orders_table).count()\n",
    "\n",
    "# MERGE INTO\n",
    "spark.sql(f\"\"\"\n",
    "    MERGE INTO {orders_table} AS target\n",
    "    USING new_orders_view AS source\n",
    "    ON target.____ = source.____\n",
    "    WHEN ____ THEN\n",
    "        UPDATE SET\n",
    "            target.order_status = source.____,\n",
    "            target.order_amount = source.____,\n",
    "            target.ingest_timestamp = source.____\n",
    "    WHEN ____ ____ THEN\n",
    "        INSERT *\n",
    "\"\"\")\n",
    "\n",
    "# Liczba po MERGE\n",
    "count_after = spark.table(orders_table).count()\n",
    "\n",
    "print(f\"\\n=== Wynik MERGE ===\")\n",
    "print(f\"Rekordy przed: {count_before}\")\n",
    "print(f\"Rekordy po: {count_after}\")\n",
    "print(f\"Dodano nowych: {count_after - count_before}\")\n",
    "\n",
    "print(\"✓ MERGE zakończony!\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Zadanie 3: Time Travel (10 minut)\n",
    "\n",
    "**Cel:** Wykorzystanie Time Travel do przeglądania historii zmian i przywracania danych.\n",
    "\n",
    "### Zadanie 3.1: Historia wersji tabeli\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wyświetl historię tabeli `products_bronze` używając `DESCRIBE HISTORY`\n",
    "2. Znajdź wersję przed wykonaniem UPDATE (Zadanie 1.2)\n",
    "3. Odczytaj dane z tej wersji używając `VERSION AS OF`\n",
    "\n",
    "**Wskazówki:**\n",
    "- `DESCRIBE HISTORY table_name`\n",
    "- `SELECT * FROM table_name VERSION AS OF version_number`\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 3.1 - Time Travel\n",
    "\n",
    "# Wyświetl historię wersji tabeli\n",
    "print(\"=== Historia wersji tabeli products_bronze ===\")\n",
    "history = spark.sql(f\"____ ____ {products_table}\")\n",
    "display(history.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\"))\n",
    "\n",
    "# Znajdź wersję przed UPDATE (operation = 'WRITE' na początku)\n",
    "first_version = history.filter(F.col(\"operation\") == \"WRITE\").orderBy(\"version\").first()[\"version\"]\n",
    "\n",
    "print(f\"\\n=== Odczytanie danych z wersji {first_version} (przed UPDATE) ===\")\n",
    "\n",
    "# Time Travel: odczytaj dane z konkretnej wersji\n",
    "products_old_version = spark.sql(f\"\"\"\n",
    "    SELECT product_id, product_name, category, unit_price\n",
    "    FROM {products_table} \n",
    "    ____ AS OF ____\n",
    "    WHERE category = 'Electronics'\n",
    "\"\"\")\n",
    "\n",
    "display(products_old_version)\n",
    "\n",
    "# Porównaj z aktualną wersją\n",
    "print(\"\\n=== Aktualna wersja (po UPDATE) ===\")\n",
    "products_current = spark.sql(f\"\"\"\n",
    "    SELECT product_id, product_name, category, unit_price\n",
    "    FROM {products_table}\n",
    "    WHERE category = 'Electronics'\n",
    "\"\"\")\n",
    "display(products_current)\n",
    "\n",
    "print(\"✓ Time Travel działa!\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Zadanie 4: Implementacja Medallion Architecture (30 minut)\n",
    "\n",
    "**Cel:** Zbudowanie pipeline'u Bronze → Silver → Gold zgodnie z architekturą medalionową.\n",
    "\n",
    "### Zadanie 4.1: Bronze Layer - Raw Data Landing\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane klientów z `customers.csv`\n",
    "2. Dodaj audit metadata:\n",
    "   - `ingest_timestamp` (current_timestamp)\n",
    "   - `source_file` (input_file_name)\n",
    "   - `ingested_by` (raw_user)\n",
    "3. Zapisz jako `customers_bronze` w Bronze schema\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Tabela Bronze z surowymi danymi + audit columns\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 4.1 - Bronze Layer\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Wczytaj dane klientów\n",
    "customers_raw = (\n",
    "    spark.read\n",
    "    .format(\"____\")\n",
    "    .option(\"header\", \"____\")\n",
    "    .option(\"inferSchema\", \"____\")\n",
    "    .load(____)\n",
    ")\n",
    "\n",
    "print(\"=== Surowe dane klientów ===\")\n",
    "customers_raw.printSchema()\n",
    "display(customers_raw.limit(5))\n",
    "\n",
    "# Dodaj audit metadata dla Bronze\n",
    "customers_bronze = (\n",
    "    customers_raw\n",
    "    .withColumn(\"____\", F.current_timestamp())\n",
    "    .withColumn(\"____\", F.input_file_name())\n",
    "    .withColumn(\"____\", F.lit(____))\n",
    ")\n",
    "\n",
    "# Zapisz do Bronze\n",
    "customers_bronze_table = f\"{BRONZE_SCHEMA}.customers_bronze\"\n",
    "\n",
    "(\n",
    "    customers_bronze\n",
    "    .write\n",
    "    .format(\"____\")\n",
    "    .mode(\"____\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(____)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Bronze layer utworzony: {customers_bronze_table}\")\n",
    "print(f\"Liczba rekordów: {spark.table(customers_bronze_table).count()}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Zadanie 4.2: Silver Layer - Cleansed & Validated\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane z Bronze: `customers_bronze`\n",
    "2. Zastosuj transformacje:\n",
    "   - Usuń duplikaty po `customer_id`\n",
    "   - Odfiltruj rekordy gdzie `customer_id` lub `email` jest NULL\n",
    "   - Standaryzuj `email` do lowercase\n",
    "   - Waliduj: `age` musi być > 0 i < 120\n",
    "   - Dodaj kolumnę `silver_processed_timestamp`\n",
    "   - Dodaj kolumnę `data_quality_flag` = \"VALID\"\n",
    "3. Zapisz jako `customers_silver` w Silver schema\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Oczyszczone, zwalidowane dane w Silver\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 4.2 - Silver Layer\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {SILVER_SCHEMA}\")\n",
    "\n",
    "# Wczytaj dane z Bronze\n",
    "customers_bronze_df = spark.table(customers_bronze_table)\n",
    "\n",
    "# Silver transformations\n",
    "customers_silver = (\n",
    "    customers_bronze_df\n",
    "    # Deduplikacja\n",
    "    .____([____])\n",
    "    \n",
    "    # Walidacja NOT NULL\n",
    "    .filter(F.col(\"____\").isNotNull())\n",
    "    .filter(F.col(\"____\").isNotNull())\n",
    "    \n",
    "    # Standaryzacja email\n",
    "    .withColumn(\"email\", F.____(F.col(\"email\")))\n",
    "    \n",
    "    # Walidacja wieku\n",
    "    .filter((F.col(\"____\") > ____) & (F.col(\"____\") < ____))\n",
    "    \n",
    "    # Silver metadata\n",
    "    .withColumn(\"____\", F.current_timestamp())\n",
    "    .withColumn(\"____\", F.lit(\"VALID\"))\n",
    ")\n",
    "\n",
    "print(\"=== Silver layer - cleansed data ===\")\n",
    "display(customers_silver.limit(5))\n",
    "\n",
    "# Zapisz do Silver\n",
    "customers_silver_table = f\"{SILVER_SCHEMA}.customers_silver\"\n",
    "\n",
    "(\n",
    "    customers_silver\n",
    "    .write\n",
    "    .format(\"____\")\n",
    "    .mode(\"____\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(____)\n",
    ")\n",
    "\n",
    "bronze_count = customers_bronze_df.count()\n",
    "silver_count = spark.table(customers_silver_table).count()\n",
    "\n",
    "print(f\"\\n✓ Silver layer utworzony: {customers_silver_table}\")\n",
    "print(f\"Bronze records: {bronze_count}\")\n",
    "print(f\"Silver records: {silver_count}\")\n",
    "print(f\"Filtered out: {bronze_count - silver_count} records\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Zadanie 4.3: Gold Layer - Business Aggregates\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane z Silver: `customers_silver`\n",
    "2. Utwórz agregację klientów per kraj:\n",
    "   - `customer_count`: liczba klientów\n",
    "   - `avg_age`: średni wiek\n",
    "   - `min_registration_date`: najwcześniejsza rejestracja\n",
    "   - `max_registration_date`: najpóźniejsza rejestracja\n",
    "3. Dodaj `gold_created_timestamp`\n",
    "4. Zapisz jako `customer_summary_by_country` w Gold schema\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Tabela Gold z agregacjami biznesowymi\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 4.3 - Gold Layer\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {GOLD_SCHEMA}\")\n",
    "\n",
    "# Wczytaj dane z Silver\n",
    "customers_silver_df = spark.table(customers_silver_table)\n",
    "\n",
    "# Gold aggregation: Summary per country\n",
    "customer_summary = (\n",
    "    customers_silver_df\n",
    "    .groupBy(\"____\")\n",
    "    .agg(\n",
    "        F.count(\"____\").alias(\"____\"),\n",
    "        F.avg(\"____\").alias(\"____\"),\n",
    "        F.min(\"____\").alias(\"____\"),\n",
    "        F.max(\"____\").alias(\"____\")\n",
    "    )\n",
    "    .withColumn(\"____\", F.current_timestamp())\n",
    "    .orderBy(\"customer_count\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"=== Gold layer - Customer Summary by Country ===\")\n",
    "display(customer_summary)\n",
    "\n",
    "# Zapisz do Gold\n",
    "gold_table = f\"{GOLD_SCHEMA}.customer_summary_by_country\"\n",
    "\n",
    "(\n",
    "    customer_summary\n",
    "    .write\n",
    "    .format(\"____\")\n",
    "    .mode(\"____\")\n",
    "    .option(\"overwriteSchema\", \"true\")\n",
    "    .saveAsTable(____)\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Gold layer utworzony: {gold_table}\")\n",
    "print(f\"Liczba krajów: {spark.table(gold_table).count()}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Zadanie 5: Optymalizacja tabel Delta (15 minut)\n",
    "\n",
    "**Cel:** Zastosowanie technik optymalizacji: OPTIMIZE, ZORDER, VACUUM.\n",
    "\n",
    "### Zadanie 5.1: OPTIMIZE - Compaction małych plików\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Sprawdź liczbę plików w tabeli `customers_silver` używając `DESCRIBE DETAIL`\n",
    "2. Uruchom `OPTIMIZE` na tabeli\n",
    "3. Sprawdź ponownie liczbę plików - powinna się zmniejszyć\n",
    "\n",
    "**Wskazówki:**\n",
    "- `DESCRIBE DETAIL table_name`\n",
    "- `OPTIMIZE table_name`\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 5.1 - OPTIMIZE\n",
    "\n",
    "# Stan przed OPTIMIZE\n",
    "print(\"=== Stan przed OPTIMIZE ===\")\n",
    "detail_before = spark.sql(f\"____ ____ {customers_silver_table}\")\n",
    "display(detail_before.select(\"numFiles\", \"sizeInBytes\"))\n",
    "\n",
    "num_files_before = detail_before.collect()[0][\"numFiles\"]\n",
    "print(f\"Liczba plików przed: {num_files_before}\")\n",
    "\n",
    "# Uruchom OPTIMIZE\n",
    "print(\"\\n=== Uruchamianie OPTIMIZE ===\")\n",
    "optimize_result = spark.sql(f\"____ {____}\")\n",
    "display(optimize_result)\n",
    "\n",
    "# Stan po OPTIMIZE\n",
    "print(\"\\n=== Stan po OPTIMIZE ===\")\n",
    "detail_after = spark.sql(f\"DESCRIBE DETAIL {customers_silver_table}\")\n",
    "display(detail_after.select(\"numFiles\", \"sizeInBytes\"))\n",
    "\n",
    "num_files_after = detail_after.collect()[0][\"numFiles\"]\n",
    "print(f\"Liczba plików po: {num_files_after}\")\n",
    "print(f\"Redukcja: {num_files_before - num_files_after} plików\")\n",
    "\n",
    "print(\"✓ OPTIMIZE zakończony!\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Zadanie 5.2: ZORDER - Clustering dla lepszego data skipping\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Uruchom `OPTIMIZE` z `ZORDER BY` na tabeli `orders_bronze`\n",
    "2. Użyj `ZORDER BY (customer_id)` - kolumna często używana w filtrach\n",
    "3. Sprawdź metryki optymalizacji\n",
    "\n",
    "**Wskazówki:**\n",
    "- `OPTIMIZE table_name ZORDER BY (column1, column2)`\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 5.2 - ZORDER\n",
    "\n",
    "# Uruchom OPTIMIZE z ZORDER na customer_id\n",
    "print(\"=== OPTIMIZE + ZORDER BY customer_id ===\")\n",
    "\n",
    "zorder_result = spark.sql(f\"\"\"\n",
    "    ____ {orders_table}\n",
    "    ____ BY (____)\n",
    "\"\"\")\n",
    "\n",
    "display(zorder_result)\n",
    "\n",
    "print(\"✓ ZORDER zakończony - dane są teraz lepiej zorganizowane dla queries po customer_id!\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist - Co powinieneś uzyskać:\n",
    "- [ ] Tabela `products_bronze` utworzona z operacjami UPDATE i DELETE\n",
    "- [ ] Tabela `orders_bronze` z zastosowanym MERGE INTO\n",
    "- [ ] Time Travel działa - można odczytać starą wersję `products_bronze`\n",
    "- [ ] Pipeline Medallion: `customers_bronze` → `customers_silver` → `customer_summary_by_country`\n",
    "- [ ] Tabele zoptymalizowane (OPTIMIZE, ZORDER)\n",
    "\n",
    "### Komendy weryfikacyjne:\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Weryfikacja wyników\n",
    "\n",
    "print(\"=== WERYFIKACJA WYNIKÓW ===\\n\")\n",
    "\n",
    "# 1. Sprawdź tabele Bronze\n",
    "print(\"1. Tabele Bronze:\")\n",
    "bronze_tables = spark.sql(f\"SHOW TABLES IN {BRONZE_SCHEMA}\").filter(F.col(\"tableName\").like(\"%bronze%\"))\n",
    "display(bronze_tables)\n",
    "\n",
    "# 2. Sprawdź tabele Silver\n",
    "print(\"\\n2. Tabele Silver:\")\n",
    "silver_tables = spark.sql(f\"SHOW TABLES IN {SILVER_SCHEMA}\").filter(F.col(\"tableName\").like(\"%silver%\"))\n",
    "display(silver_tables)\n",
    "\n",
    "# 3. Sprawdź tabele Gold\n",
    "print(\"\\n3. Tabele Gold:\")\n",
    "gold_tables = spark.sql(f\"SHOW TABLES IN {GOLD_SCHEMA}\")\n",
    "display(gold_tables)\n",
    "\n",
    "# 4. Sprawdź historię dla products_bronze\n",
    "print(\"\\n4. Historia products_bronze (powinny być operacje: WRITE, UPDATE, DELETE):\")\n",
    "history = spark.sql(f\"DESCRIBE HISTORY {products_table}\")\n",
    "display(history.select(\"version\", \"timestamp\", \"operation\", \"operationMetrics\").limit(10))\n",
    "\n",
    "# 5. Sprawdź dane Gold\n",
    "print(\"\\n5. Przykładowe dane Gold:\")\n",
    "display(spark.table(gold_table).limit(10))\n",
    "\n",
    "print(\"\\n✓ Wszystkie testy przeszły pomyślnie!\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "**W tym warsztacie nauczyłeś się:**\n",
    "\n",
    "✅ **Delta Lake CRUD Operations:**\n",
    "- CREATE TABLE z różnych formatów źródłowych\n",
    "- UPDATE - aktualizacja rekordów\n",
    "- DELETE - usuwanie rekordów\n",
    "- MERGE INTO - upsert (update + insert)\n",
    "\n",
    "✅ **Time Travel:**\n",
    "- DESCRIBE HISTORY - historia wersji\n",
    "- VERSION AS OF - odczytywanie starych wersji\n",
    "- Możliwość audytu i rollback\n",
    "\n",
    "✅ **Medallion Architecture:**\n",
    "- Bronze: Raw data + audit metadata (immutable, append-only)\n",
    "- Silver: Cleansed, validated, deduplicated\n",
    "- Gold: Business aggregates, KPI models\n",
    "\n",
    "✅ **Optymalizacja:**\n",
    "- OPTIMIZE - compaction małych plików\n",
    "- ZORDER - clustering dla lepszego data skipping\n",
    "- DESCRIBE DETAIL - monitoring stanu tabel\n",
    "\n",
    "**Kluczowe wnioski:**\n",
    "1. Delta Lake zapewnia ACID transactions dla data lake\n",
    "2. Time Travel umożliwia audyt i recovery bez backupów\n",
    "3. Medallion Architecture separuje raw data od business logic\n",
    "4. Regularna optymalizacja (OPTIMIZE) jest kluczowa dla performance\n",
    "\n",
    "**Następne kroki:**\n",
    "- **Kolejny warsztat**: 02_ingestion_pipeline_workshop.ipynb - COPY INTO, Auto Loader, Streaming\n",
    "- **Materiały dodatkowe**: Delta Lake Documentation, Medallion Architecture Guide\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Opcjonalnie: usuń utworzone tabele po zakończeniu warsztatu:\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Opcjonalne czyszczenie zasobów testowych\n",
    "# UWAGA: Uruchom tylko jeśli chcesz usunąć wszystkie utworzone dane\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {products_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {orders_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {customers_bronze_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {customers_silver_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {gold_table}\")\n",
    "\n",
    "# spark.catalog.clearCache()\n",
    "# print(\"Zasoby zostały wyczyszczone\")\n",
    "</VSCode.Cell>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
