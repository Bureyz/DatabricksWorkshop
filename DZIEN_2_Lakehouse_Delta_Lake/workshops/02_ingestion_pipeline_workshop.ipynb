{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0aaf6c",
   "metadata": {},
   "source": [
    "# Warsztat 2: Ingestion Pipeline - COPY INTO & Auto Loader\n",
    "\n",
    "**Cel warsztatu:**\n",
    "- Implementacja batch ingestion u≈ºywajƒÖc COPY INTO\n",
    "- Konfiguracja Auto Loader dla streaming ingestion\n",
    "- Obs≈Çuga r√≥≈ºnych format√≥w plik√≥w (CSV, JSON, Parquet)\n",
    "- Monitorowanie i zarzƒÖdzanie pipeline'ami ingestion\n",
    "\n",
    "**Czas:** 90 minut\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420c2ef",
   "metadata": {},
   "source": [
    "## üìö Inicjalizacja ≈õrodowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9347b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0635808",
   "metadata": {},
   "source": [
    "## üéØ Czƒô≈õƒá 1: COPY INTO - Batch Ingestion\n",
    "\n",
    "### Zadanie 1.1: Ingestion plik√≥w CSV\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Przygotuj tabelƒô docelowƒÖ `bronze_customers_batch`\n",
    "2. U≈ºyj `COPY INTO` do za≈Çadowania danych z `customers.csv`\n",
    "3. Zweryfikuj liczbƒô za≈Çadowanych rekord√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utw√≥rz tabelƒô docelowƒÖ\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_customers_batch (\n",
    "        customer_id INT,\n",
    "        name STRING,\n",
    "        email STRING,\n",
    "        city STRING,\n",
    "        country STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    LOCATION '{____}/customers_batch'  -- Uzupe≈Çnij BRONZE_PATH\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b63d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COPY INTO z pliku CSV\n",
    "spark.sql(f\"\"\"\n",
    "    ____ INTO {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            name,\n",
    "            email,\n",
    "            city,\n",
    "            country,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{____}/____'  -- Uzupe≈Çnij SOURCE_DATA_PATH i nazwƒô pliku\n",
    "    )\n",
    "    FILEFORMAT = ____  -- Uzupe≈Çnij format (CSV)\n",
    "    FORMAT_OPTIONS (\n",
    "        'header' = '____',  -- Czy plik ma nag≈Ç√≥wek?\n",
    "        'inferSchema' = '____'  -- Czy inferowaƒá schemat?\n",
    "    )\n",
    "    COPY_OPTIONS (\n",
    "        'mergeSchema' = '____'  -- Czy mergowaƒá schemat?\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as total_records \n",
    "    FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cffcff",
   "metadata": {},
   "source": [
    "### Zadanie 1.2: Ingestion plik√≥w JSON\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Przygotuj tabelƒô `bronze_orders_batch`\n",
    "2. U≈ºyj `COPY INTO` do za≈Çadowania danych z `orders_batch.json`\n",
    "3. Obs≈Çu≈º zagnie≈ºd≈ºonƒÖ strukturƒô JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e72509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utw√≥rz tabelƒô dla zam√≥wie≈Ñ\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_orders_batch (\n",
    "        order_id INT,\n",
    "        customer_id INT,\n",
    "        order_date DATE,\n",
    "        total_amount DOUBLE,\n",
    "        status STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING ____  -- Uzupe≈Çnij format (DELTA)\n",
    "    LOCATION '{____}/orders_batch'  -- Uzupe≈Çnij ≈õcie≈ºkƒô\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COPY INTO z pliku JSON\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{SCHEMA}.bronze_orders_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            order_id,\n",
    "            customer_id,\n",
    "            TO_DATE(order_date) as order_date,\n",
    "            total_amount,\n",
    "            status,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{____}/____'  -- Uzupe≈Çnij ≈õcie≈ºkƒô do JSON\n",
    "    )\n",
    "    FILEFORMAT = ____  -- Uzupe≈Çnij format\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM {CATALOG}.{SCHEMA}.bronze_orders_batch LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44cfc5",
   "metadata": {},
   "source": [
    "### Zadanie 1.3: Ingestion plik√≥w Parquet\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Przygotuj tabelƒô `bronze_products_batch`\n",
    "2. U≈ºyj `COPY INTO` do za≈Çadowania danych z `products.parquet`\n",
    "3. Dodaj kolumnƒô z metadanymi pliku ≈∫r√≥d≈Çowego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5561ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utw√≥rz tabelƒô dla produkt√≥w\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_products_batch (\n",
    "        product_id INT,\n",
    "        product_name STRING,\n",
    "        category STRING,\n",
    "        price DOUBLE,\n",
    "        stock_quantity INT,\n",
    "        _source_file STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    LOCATION '{____}/products_batch'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316176df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COPY INTO z pliku Parquet\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{SCHEMA}.bronze_products_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            product_name,\n",
    "            category,\n",
    "            price,\n",
    "            stock_quantity,\n",
    "            ____ as _source_file,  -- U≈ºyj _metadata.file_path\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{____}/____'  -- Uzupe≈Çnij ≈õcie≈ºkƒô do Parquet\n",
    "    )\n",
    "    FILEFORMAT = ____\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b754ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT product_id, product_name, category, _source_file \n",
    "    FROM {CATALOG}.{SCHEMA}.bronze_products_batch \n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf241e",
   "metadata": {},
   "source": [
    "### Zadanie 1.4: Idempotentno≈õƒá - Ponowne uruchomienie COPY INTO\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Uruchom ponownie `COPY INTO` dla tej samej tabeli\n",
    "2. Zweryfikuj ≈ºe dane nie zosta≈Çy zduplikowane\n",
    "3. Sprawd≈∫ historiƒô operacji `COPY INTO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbb03d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawd≈∫ liczbƒô rekord√≥w przed ponownym COPY INTO\n",
    "before_count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as count \n",
    "    FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").collect()[0][\"count\"]\n",
    "\n",
    "print(f\"Liczba rekord√≥w przed ponownym COPY INTO: {before_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c1cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ponowne wykonanie COPY INTO\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            name,\n",
    "            email,\n",
    "            city,\n",
    "            country,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{SOURCE_DATA_PATH}/customers.csv'\n",
    "    )\n",
    "    FILEFORMAT = CSV\n",
    "    FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'true')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8805ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja idempotentno≈õci\n",
    "after_count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as count \n",
    "    FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").collect()[0][\"count\"]\n",
    "\n",
    "print(f\"Liczba rekord√≥w po ponownym COPY INTO: {after_count}\")\n",
    "print(f\"Czy dane zosta≈Çy zduplikowane? {before_count != after_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9637c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üîÑ Czƒô≈õƒá 2: Auto Loader - Streaming Ingestion\n",
    "\n",
    "### Zadanie 2.1: Konfiguracja Auto Loader dla CSV\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Przygotuj lokalizacjƒô checkpointu\n",
    "2. U≈ºyj `.format(\"cloudFiles\")` do utworzenia streaming read\n",
    "3. Skonfiguruj schema inference i evolution\n",
    "4. Zapisz stream do tabeli `bronze_customers_stream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cad73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Streaming read z Auto Loader\n",
    "customers_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"____\")  # Uzupe≈Çnij format (cloudFiles)\n",
    "    .option(\"cloudFiles.format\", \"____\")  # Format plik√≥w ≈∫r√≥d≈Çowych (csv)\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/____\")  # Checkpoint dla schematu\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(f\"{SOURCE_DATA_PATH}/customers.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba519b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dodaj kolumny metadata\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "customers_enriched = (\n",
    "    customers_stream\n",
    "    .withColumn(\"_ingestion_timestamp\", ____)  # Dodaj timestamp\n",
    "    .withColumn(\"_source_file\", ____)  # Dodaj nazwƒô pliku ≈∫r√≥d≈Çowego\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zapisz stream do tabeli Delta\n",
    "query_customers = (\n",
    "    customers_enriched.writeStream\n",
    "    .format(\"____\")  # Uzupe≈Çnij format\n",
    "    .outputMode(\"____\")  # Tryb zapisu (append)\n",
    "    .option(\"checkpointLocation\", f\"{____}/customers_stream\")  # Checkpoint\n",
    "    .option(\"mergeSchema\", \"____\")  # Schema evolution\n",
    "    .table(f\"{CATALOG}.{SCHEMA}.bronze_customers_stream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6114af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja streamu\n",
    "import time\n",
    "time.sleep(10)  # Poczekaj na przetworzenie\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as total_records \n",
    "    FROM {CATALOG}.{SCHEMA}.bronze_customers_stream\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130ce52",
   "metadata": {},
   "source": [
    "### Zadanie 2.2: Auto Loader dla JSON z Schema Hints\n",
    "\n",
    "**Instrukcje:**\n",
    "1. U≈ºyj Auto Loader do odczytu `orders_batch.json`\n",
    "2. Dodaj schema hints dla kolumn z konkretnymi typami\n",
    "3. Skonfiguruj rescue data column dla niepoprawnych rekord√≥w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc65dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Auto Loader z schema hints\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"____\")  # Format JSON\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/orders_schema\")\n",
    "    .option(\"cloudFiles.schemaHints\", \"____\")  # Podpowied≈∫: \"order_date DATE, total_amount DOUBLE\"\n",
    "    .option(\"cloudFiles.rescuedDataColumn\", \"____\")  # Kolumna dla rescue data\n",
    "    .load(f\"{____}/orders_batch.json\")  # Uzupe≈Çnij ≈õcie≈ºkƒô\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c54b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zapisz stream\n",
    "query_orders = (\n",
    "    orders_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"____\")  # Tryb append\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/____\")  # Checkpoint\n",
    "    .table(f\"{CATALOG}.{SCHEMA}.bronze_orders_stream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc4725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja\n",
    "time.sleep(10)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM {CATALOG}.{SCHEMA}.bronze_orders_stream LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645db40",
   "metadata": {},
   "source": [
    "### Zadanie 2.3: Monitoring streaming queries\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wy≈õwietl aktywne streaming queries\n",
    "2. Sprawd≈∫ status i ostatni progress ka≈ºdego query\n",
    "3. Pobierz metryki: liczba przetworzonych rekord√≥w, batch duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a1321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wy≈õwietl aktywne streamy\n",
    "active_streams = spark.streams.____  # Uzupe≈Çnij metodƒô (active)\n",
    "\n",
    "print(f\"Liczba aktywnych stream√≥w: {len(active_streams)}\")\n",
    "for stream in active_streams:\n",
    "    print(f\"\\nStream ID: {stream.id}\")\n",
    "    print(f\"Name: {stream.name}\")\n",
    "    print(f\"Status: {stream.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6974ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sprawd≈∫ ostatni progress\n",
    "if len(active_streams) > 0:\n",
    "    last_progress = active_streams[0].____  # Uzupe≈Çnij metodƒô (lastProgress)\n",
    "    \n",
    "    if last_progress:\n",
    "        print(f\"Batch ID: {last_progress['batchId']}\")\n",
    "        print(f\"Przetworzone rekordy: {last_progress['numInputRows']}\")\n",
    "        print(f\"Czas przetwarzania: {last_progress['batchDuration']} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55841f5d",
   "metadata": {},
   "source": [
    "### Zadanie 2.4: Zatrzymanie streaming queries\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Zatrzymaj wszystkie aktywne streaming queries\n",
    "2. Zweryfikuj ≈ºe wszystkie streamy sƒÖ zatrzymane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae308ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zatrzymaj wszystkie streamy\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"Zatrzymujƒô stream: {stream.name}\")\n",
    "    stream.____()  # Uzupe≈Çnij metodƒô (stop)\n",
    "\n",
    "print(\"\\nWszystkie streamy zatrzymane!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c772d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja\n",
    "print(f\"Liczba aktywnych stream√≥w: {len(spark.streams.active)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9819090",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Czƒô≈õƒá 3: Por√≥wnanie COPY INTO vs Auto Loader\n",
    "\n",
    "### Zadanie 3.1: Analiza wydajno≈õci\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Por√≥wnaj liczbƒô rekord√≥w za≈Çadowanych przez COPY INTO vs Auto Loader\n",
    "2. Sprawd≈∫ historiƒô operacji dla obu metod\n",
    "3. Zidentyfikuj przypadki u≈ºycia dla ka≈ºdej metody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbad7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por√≥wnanie liczby rekord√≥w\n",
    "copy_into_count = spark.sql(f\"\"\"\n",
    "    SELECT 'COPY INTO' as method, COUNT(*) as records \n",
    "    FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\")\n",
    "\n",
    "auto_loader_count = spark.sql(f\"\"\"\n",
    "    SELECT 'Auto Loader' as method, COUNT(*) as records \n",
    "    FROM {CATALOG}.{SCHEMA}.bronze_customers_stream\n",
    "\"\"\")\n",
    "\n",
    "copy_into_count.union(auto_loader_count).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6833f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Historia operacji COPY INTO\n",
    "spark.sql(f\"\"\"\n",
    "    ____ HISTORY {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").select(\"version\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd43d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Historia operacji Auto Loader\n",
    "spark.sql(f\"\"\"\n",
    "    DESCRIBE HISTORY {CATALOG}.{SCHEMA}.bronze_customers_stream\n",
    "\"\"\").select(\"version\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956008d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Podsumowanie warsztatu\n",
    "\n",
    "**Zrealizowane cele:**\n",
    "- ‚úÖ Implementacja batch ingestion z COPY INTO\n",
    "- ‚úÖ Konfiguracja Auto Loader dla streaming ingestion\n",
    "- ‚úÖ Obs≈Çuga r√≥≈ºnych format√≥w (CSV, JSON, Parquet)\n",
    "- ‚úÖ Monitoring i zarzƒÖdzanie pipeline'ami\n",
    "\n",
    "**Kiedy u≈ºyƒá COPY INTO:**\n",
    "- Batch processing z okre≈õlonym harmonogramem\n",
    "- Znana i stabilna struktura danych\n",
    "- Potrzeba kontroli nad procesem ≈Çadowania\n",
    "- Idempotentno≈õƒá out-of-the-box\n",
    "\n",
    "**Kiedy u≈ºyƒá Auto Loader:**\n",
    "- Near real-time processing\n",
    "- Schema evolution i automatyczna inferencja\n",
    "- CiƒÖg≈Çe monitorowanie nowych plik√≥w\n",
    "- Skalowalno≈õƒá i efektywno≈õƒá kosztowa\n",
    "\n",
    "---\n",
    "\n",
    "## üßπ Cleanup (opcjonalnie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335b1e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usu≈Ñ utworzone tabele (opcjonalnie)\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.bronze_customers_batch\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.bronze_orders_batch\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.bronze_products_batch\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.bronze_customers_stream\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.bronze_orders_stream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f15d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "```xml\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "# Ingestion Pipeline - Workshop\n",
    "\n",
    "**Cel szkoleniowy:** Praktyczne zastosowanie r√≥≈ºnych metod ≈Çadowania danych: COPY INTO (batch), Auto Loader (incremental), i Structured Streaming.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- COPY INTO - batch file ingestion\n",
    "- Auto Loader (CloudFiles) - incremental file ingestion\n",
    "- Schema inference i schema evolution\n",
    "- Structured Streaming - real-time processing\n",
    "- Checkpointing i exactly-once semantics\n",
    "- MERGE na streamingu\n",
    "\n",
    "**Czas trwania:** 90 minut\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzie≈Ñ szkolenia**: Dzie≈Ñ 2 - Lakehouse & Delta Lake\n",
    "- **Typ notebooka**: Workshop\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog w≈ÇƒÖczony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Izolacja per u≈ºytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalog√≥w i schemat√≥w:\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "%run ../../00_setup\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych ≈õrodowiskowych:\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Wy≈õwietl kontekst u≈ºytkownika\n",
    "print(\"=== Kontekst u≈ºytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"U≈ºytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog i schemat jako domy≈õlne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# ≈öcie≈ºki do danych\n",
    "BASE_DATA_PATH = f\"{DATASET_BASE_PATH}\"\n",
    "ORDERS_PATH = f\"{BASE_DATA_PATH}/orders\"\n",
    "CUSTOMERS_PATH = f\"{BASE_DATA_PATH}/customers\"\n",
    "\n",
    "# ≈öcie≈ºka do checkpoint√≥w (per user)\n",
    "CHECKPOINT_BASE = f\"/tmp/{raw_user}/checkpoints\"\n",
    "\n",
    "print(f\"\\n=== ≈öcie≈ºki ===\")\n",
    "print(f\"Data path: {BASE_DATA_PATH}\")\n",
    "print(f\"Checkpoint path: {CHECKPOINT_BASE}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Zadanie 1: COPY INTO - Batch File Ingestion (20 minut)\n",
    "\n",
    "**Cel:** U≈ºycie COPY INTO do efektywnego ≈Çadowania plik√≥w batch'owych z automatycznƒÖ deduplikacjƒÖ.\n",
    "\n",
    "### Zadanie 1.1: COPY INTO dla plik√≥w JSON\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Utw√≥rz tabelƒô docelowƒÖ `orders_copy_into` je≈õli nie istnieje\n",
    "2. U≈ºyj `COPY INTO` do za≈Çadowania plik√≥w JSON z folderu `orders/`\n",
    "3. Parametry:\n",
    "   - FORMAT = JSON\n",
    "   - PATTERN = '*.json'\n",
    "   - Dodaj audit columns: `_metadata.file_path`, `_metadata.file_modification_time`\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Dane za≈Çadowane z automatycznƒÖ deduplikacjƒÖ (COPY INTO ≈õledzi za≈Çadowane pliki)\n",
    "\n",
    "**Wskaz√≥wki:**\n",
    "- COPY INTO automatycznie pomija ju≈º za≈Çadowane pliki (idempotentno≈õƒá)\n",
    "- U≈ºywaj `_metadata` dla audit trail\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 1.1 - COPY INTO dla JSON\n",
    "\n",
    "# Nazwa tabeli docelowej\n",
    "copy_into_table = f\"{BRONZE_SCHEMA}.orders_copy_into\"\n",
    "\n",
    "# Utw√≥rz pustƒÖ tabelƒô je≈õli nie istnieje\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {copy_into_table} (\n",
    "        order_id BIGINT,\n",
    "        customer_id BIGINT,\n",
    "        order_date STRING,\n",
    "        order_amount DOUBLE,\n",
    "        order_status STRING,\n",
    "        source_file STRING,\n",
    "        load_timestamp TIMESTAMP\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úì Tabela utworzona: {copy_into_table}\")\n",
    "\n",
    "# Sprawd≈∫ stan przed COPY INTO\n",
    "count_before = spark.table(copy_into_table).count()\n",
    "print(f\"Liczba rekord√≥w przed COPY INTO: {count_before}\")\n",
    "\n",
    "# COPY INTO - za≈Çaduj pliki JSON\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {copy_into_table}\n",
    "    FROM (\n",
    "        SELECT \n",
    "            order_id,\n",
    "            customer_id,\n",
    "            order_date,\n",
    "            order_amount,\n",
    "            order_status,\n",
    "            _metadata.____ AS source_file,\n",
    "            current_timestamp() AS load_timestamp\n",
    "        FROM '{____}'\n",
    "    )\n",
    "    FILEFORMAT = ____\n",
    "    PATTERN = '____'\n",
    "    FORMAT_OPTIONS ('multiLine' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "# Sprawd≈∫ stan po COPY INTO\n",
    "count_after = spark.table(copy_into_table).count()\n",
    "print(f\"\\n=== Wynik COPY INTO ===\")\n",
    "print(f\"Liczba rekord√≥w po: {count_after}\")\n",
    "print(f\"Nowo za≈Çadowane: {count_after - count_before}\")\n",
    "\n",
    "# Wy≈õwietl dane\n",
    "display(spark.table(copy_into_table).limit(5))\n",
    "\n",
    "print(\"‚úì COPY INTO zako≈Ñczony!\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Zadanie 1.2: COPY INTO - Idempotentno≈õƒá (ponowne uruchomienie)\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Uruchom ponownie COPY INTO z poprzedniego zadania\n",
    "2. Sprawd≈∫, czy liczba rekord√≥w siƒô nie zmieni≈Ça (idempotentno≈õƒá)\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Liczba rekord√≥w taka sama - COPY INTO pomija ju≈º za≈Çadowane pliki\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 1.2 - Test idempotentno≈õci COPY INTO\n",
    "\n",
    "# Liczba przed ponownym COPY INTO\n",
    "count_before_rerun = spark.table(copy_into_table).count()\n",
    "print(f\"Liczba rekord√≥w przed ponownym uruchomieniem: {count_before_rerun}\")\n",
    "\n",
    "# Uruchom COPY INTO ponownie (skopiuj kod z Zadania 1.1)\n",
    "spark.sql(f\"\"\"\n",
    "    ____ ____ {copy_into_table}\n",
    "    FROM (\n",
    "        SELECT \n",
    "            order_id,\n",
    "            customer_id,\n",
    "            order_date,\n",
    "            order_amount,\n",
    "            order_status,\n",
    "            _metadata.file_path AS source_file,\n",
    "            current_timestamp() AS load_timestamp\n",
    "        FROM '{ORDERS_PATH}'\n",
    "    )\n",
    "    FILEFORMAT = JSON\n",
    "    PATTERN = '*.json'\n",
    "    FORMAT_OPTIONS ('multiLine' = 'true')\n",
    "\"\"\")\n",
    "\n",
    "# Liczba po ponownym COPY INTO\n",
    "count_after_rerun = spark.table(copy_into_table).count()\n",
    "print(f\"\\n=== Test idempotentno≈õci ===\")\n",
    "print(f\"Liczba rekord√≥w po ponownym uruchomieniu: {count_after_rerun}\")\n",
    "print(f\"R√≥≈ºnica: {count_after_rerun - count_before_rerun}\")\n",
    "\n",
    "if count_after_rerun == count_before_rerun:\n",
    "    print(\"‚úì COPY INTO jest idempotentny - pliki nie zosta≈Çy za≈Çadowane ponownie!\")\n",
    "else:\n",
    "    print(\"‚ö† Uwaga: wykryto duplikaty - sprawd≈∫ konfiguracjƒô\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Zadanie 2: Auto Loader (CloudFiles) - Incremental Ingestion (25 minut)\n",
    "\n",
    "**Cel:** U≈ºycie Auto Loader do automatycznego ≈Çadowania nowych plik√≥w z schema inference i evolution.\n",
    "\n",
    "### Zadanie 2.1: Auto Loader - podstawowa konfiguracja\n",
    "\n",
    "**Instrukcje:**\n",
    "1. U≈ºyj `spark.readStream.format(\"cloudFiles\")` do wczytywania CSV z `customers/`\n",
    "2. W≈ÇƒÖcz schema inference: `.option(\"cloudFiles.schemaLocation\", checkpoint_path)`\n",
    "3. Dodaj audit column: `_metadata.file_path`\n",
    "4. Zapisz do tabeli `customers_autoloader` u≈ºywajƒÖc `writeStream`\n",
    "5. Trigger: `availableNow=True` (process all available, then stop)\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Auto Loader automatycznie wykrywa nowe pliki\n",
    "- Schema inference dzia≈Ça automatycznie\n",
    "\n",
    "**Wskaz√≥wki:**\n",
    "- Auto Loader wymaga checkpoint location dla tracking files\n",
    "- Schema location przechowuje inferred schema\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 2.1 - Auto Loader basics\n",
    "\n",
    "# ≈öcie≈ºki\n",
    "autoloader_table = f\"{BRONZE_SCHEMA}.customers_autoloader\"\n",
    "checkpoint_path = f\"{CHECKPOINT_BASE}/customers_autoloader\"\n",
    "schema_path = f\"{CHECKPOINT_BASE}/customers_schema\"\n",
    "\n",
    "print(f\"=== Konfiguracja Auto Loader ===\")\n",
    "print(f\"Tabela: {autoloader_table}\")\n",
    "print(f\"Checkpoint: {checkpoint_path}\")\n",
    "print(f\"Schema location: {schema_path}\")\n",
    "\n",
    "# Auto Loader - read stream\n",
    "customers_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"____\")  # cloudFiles\n",
    "    .option(\"cloudFiles.format\", \"____\")  # csv\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"cloudFiles.schemaLocation\", ____)  # schema_path\n",
    "    .load(____)  # CUSTOMERS_PATH\n",
    "    .withColumn(\"source_file\", F.col(\"_metadata.____\"))\n",
    "    .withColumn(\"ingest_timestamp\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "print(\"\\n=== Inferred Schema ===\")\n",
    "customers_stream.printSchema()\n",
    "\n",
    "# Write stream do tabeli Delta\n",
    "query = (\n",
    "    customers_stream\n",
    "    .writeStream\n",
    "    .format(\"____\")  # delta\n",
    "    .option(\"checkpointLocation\", ____)  # checkpoint_path\n",
    "    .trigger(____=True)  # availableNow\n",
    "    .table(____)  # autoloader_table\n",
    ")\n",
    "\n",
    "# Czekaj na zako≈Ñczenie\n",
    "query.awaitTermination()\n",
    "\n",
    "# Sprawd≈∫ wyniki\n",
    "count = spark.table(autoloader_table).count()\n",
    "print(f\"\\n‚úì Auto Loader zako≈Ñczony!\")\n",
    "print(f\"Za≈Çadowano rekord√≥w: {count}\")\n",
    "\n",
    "display(spark.table(autoloader_table).limit(5))\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Zadanie 2.2: Auto Loader - Schema Evolution\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Symuluj dodanie nowej kolumny do ≈∫r√≥d≈Çowych danych\n",
    "2. W≈ÇƒÖcz schema evolution: `.option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")`\n",
    "3. Uruchom Auto Loader ponownie\n",
    "4. Sprawd≈∫ czy nowa kolumna zosta≈Ça dodana automatycznie\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Schema evolution automatycznie dodaje nowƒÖ kolumnƒô\n",
    "\n",
    "**Wskaz√≥wki:**\n",
    "- Schema evolution modes: `none`, `addNewColumns`, `rescue`\n",
    "- Rescue mode zapisuje niepasujƒÖce dane do `_rescued_data` column\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 2.2 - Schema Evolution\n",
    "\n",
    "# Symulacja: dodaj nowƒÖ kolumnƒô do danych (w praktyce nowy plik w cloud storage)\n",
    "# Dla demonstracji u≈ºyjemy istniejƒÖcych danych + nowƒÖ kolumnƒô\n",
    "\n",
    "print(\"=== Test Schema Evolution ===\")\n",
    "print(\"Symulacja: nowy plik CSV z dodatkowƒÖ kolumnƒÖ 'loyalty_tier'\")\n",
    "\n",
    "# Utw√≥rz testowy DataFrame z nowƒÖ kolumnƒÖ\n",
    "test_data = spark.table(autoloader_table).limit(10).withColumn(\"loyalty_tier\", F.lit(\"Gold\"))\n",
    "\n",
    "# Zapisz jako tymczasowy CSV (symulacja nowego pliku)\n",
    "temp_csv_path = f\"/tmp/{raw_user}/test_customers_new_column\"\n",
    "test_data.write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(temp_csv_path)\n",
    "\n",
    "print(f\"‚úì Utworzono testowy plik z nowƒÖ kolumnƒÖ: {temp_csv_path}\")\n",
    "\n",
    "# Nowa ≈õcie≈ºka checkpoint dla evolution test\n",
    "evolution_checkpoint = f\"{CHECKPOINT_BASE}/customers_evolution\"\n",
    "evolution_schema = f\"{CHECKPOINT_BASE}/customers_evolution_schema\"\n",
    "\n",
    "# Auto Loader z schema evolution\n",
    "customers_evolved_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"cloudFiles.schemaLocation\", evolution_schema)\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"____\")  # addNewColumns\n",
    "    .load(temp_csv_path)\n",
    "    .withColumn(\"source_file\", F.col(\"_metadata.file_path\"))\n",
    "    .withColumn(\"ingest_timestamp\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "print(\"\\n=== Evolved Schema (z nowƒÖ kolumnƒÖ) ===\")\n",
    "customers_evolved_stream.printSchema()\n",
    "\n",
    "# Write stream\n",
    "evolution_table = f\"{BRONZE_SCHEMA}.customers_evolution_test\"\n",
    "\n",
    "query_evolution = (\n",
    "    customers_evolved_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", evolution_checkpoint)\n",
    "    .option(\"mergeSchema\", \"____\")  # true - w≈ÇƒÖcz schema merge\n",
    "    .trigger(availableNow=True)\n",
    "    .table(evolution_table)\n",
    ")\n",
    "\n",
    "query_evolution.awaitTermination()\n",
    "\n",
    "print(f\"\\n‚úì Schema Evolution zako≈Ñczony!\")\n",
    "print(\"Sprawd≈∫ schema - powinna byƒá nowa kolumna 'loyalty_tier':\")\n",
    "\n",
    "spark.table(evolution_table).printSchema()\n",
    "display(spark.table(evolution_table).select(\"customer_id\", \"email\", \"loyalty_tier\").limit(5))\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Zadanie 3: Structured Streaming - Real-time Processing (25 minut)\n",
    "\n",
    "**Cel:** Implementacja streaming pipeline z continuous processing i MERGE INTO.\n",
    "\n",
    "### Zadanie 3.1: Streaming read + aggregation\n",
    "\n",
    "**Instrukcje:**\n",
    "1. U≈ºyj `readStream` do wczytywania z Delta table `orders_copy_into`\n",
    "2. Agreguj dane w czasie rzeczywistym:\n",
    "   - Grupuj po `order_status`\n",
    "   - Policz liczbƒô zam√≥wie≈Ñ per status\n",
    "   - Zsumuj `order_amount` per status\n",
    "3. Zapisz do tabeli `order_status_summary` jako streaming table\n",
    "4. Trigger: `processingTime='10 seconds'`\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Streaming aggregation aktualizuje summary w czasie rzeczywistym\n",
    "\n",
    "**Wskaz√≥wki:**\n",
    "- Streaming DataFrame u≈ºywa incremental processing\n",
    "- OutputMode dla agregacji: `complete` (pe≈Çna tabela) lub `update` (tylko zmiany)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 3.1 - Streaming aggregation\n",
    "\n",
    "# Tabela ≈∫r√≥d≈Çowa (ju≈º za≈Çadowana w Zadaniu 1)\n",
    "source_table = copy_into_table\n",
    "\n",
    "# ≈öcie≈ºka checkpoint\n",
    "streaming_checkpoint = f\"{CHECKPOINT_BASE}/streaming_aggregation\"\n",
    "\n",
    "# Read stream z Delta table\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"____\")  # delta\n",
    "    .table(____)  # source_table\n",
    ")\n",
    "\n",
    "# Agregacja: liczba zam√≥wie≈Ñ i suma per status\n",
    "order_summary_stream = (\n",
    "    orders_stream\n",
    "    .groupBy(\"____\")  # order_status\n",
    "    .agg(\n",
    "        F.count(\"____\").alias(\"____\"),  # order_id, order_count\n",
    "        F.sum(\"____\").alias(\"____\"),    # order_amount, total_amount\n",
    "        F.avg(\"____\").alias(\"____\")     # order_amount, avg_amount\n",
    "    )\n",
    "    .withColumn(\"updated_at\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "print(\"=== Streaming Aggregation Schema ===\")\n",
    "order_summary_stream.printSchema()\n",
    "\n",
    "# Write stream do tabeli summary\n",
    "summary_table = f\"{BRONZE_SCHEMA}.order_status_summary\"\n",
    "\n",
    "query_summary = (\n",
    "    order_summary_stream\n",
    "    .writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"____\")  # complete - pe≈Çna tabela przy ka≈ºdym update\n",
    "    .option(\"checkpointLocation\", streaming_checkpoint)\n",
    "    .trigger(processingTime=\"____ seconds\")  # 10\n",
    "    .table(summary_table)\n",
    ")\n",
    "\n",
    "# Uruchom przez 30 sekund, potem stop\n",
    "import time\n",
    "time.sleep(30)\n",
    "query_summary.stop()\n",
    "\n",
    "print(f\"\\n‚úì Streaming zako≈Ñczony!\")\n",
    "print(f\"Wyniki agregacji:\")\n",
    "\n",
    "display(spark.table(summary_table))\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Zadanie 3.2: Streaming MERGE (Upsert)\n",
    "\n",
    "**Cel:** U≈ºycie `foreachBatch` do wykonania MERGE w streaming pipeline.\n",
    "\n",
    "**Instrukcje:**\n",
    "1. U≈ºyj streaming source z `customers_autoloader`\n",
    "2. Zaimplementuj funkcjƒô `upsert_to_delta` kt√≥ra wykonuje MERGE INTO\n",
    "3. U≈ºyj `.foreachBatch(upsert_to_delta)` w writeStream\n",
    "4. MERGE logic:\n",
    "   - MATCHED: update email, age\n",
    "   - NOT MATCHED: insert new customer\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Streaming pipeline wykonuje upsert (MERGE) dla ka≈ºdego micro-batch\n",
    "\n",
    "**Wskaz√≥wki:**\n",
    "- `foreachBatch` pozwala na custom logic per batch\n",
    "- U≈ºyj `microBatchOutputDF` w funkcji upsert\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 3.2 - Streaming MERGE\n",
    "\n",
    "# Tabela docelowa\n",
    "target_merge_table = f\"{BRONZE_SCHEMA}.customers_merged\"\n",
    "\n",
    "# Utw√≥rz tabelƒô docelowƒÖ je≈õli nie istnieje\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {target_merge_table} (\n",
    "        customer_id BIGINT,\n",
    "        first_name STRING,\n",
    "        last_name STRING,\n",
    "        email STRING,\n",
    "        age INT,\n",
    "        country STRING,\n",
    "        registration_date STRING,\n",
    "        last_updated TIMESTAMP\n",
    "    ) USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úì Tabela docelowa: {target_merge_table}\")\n",
    "\n",
    "# Funkcja upsert - wykonywana dla ka≈ºdego micro-batch\n",
    "def upsert_to_delta(microBatchDF, batchId):\n",
    "    \"\"\"\n",
    "    Upsert function dla streaming MERGE\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Processing batch {batchId} ===\")\n",
    "    print(f\"Batch size: {microBatchDF.count()} records\")\n",
    "    \n",
    "    # Dodaj timestamp\n",
    "    microBatchDF = microBatchDF.withColumn(\"last_updated\", F.current_timestamp())\n",
    "    \n",
    "    # Zarejestruj jako temp view\n",
    "    microBatchDF.createOrReplaceTempView(\"updates\")\n",
    "    \n",
    "    # MERGE INTO\n",
    "    spark.sql(f\"\"\"\n",
    "        MERGE INTO {target_merge_table} AS target\n",
    "        USING updates AS source\n",
    "        ON target.____ = source.____\n",
    "        WHEN ____ THEN\n",
    "            UPDATE SET\n",
    "                target.email = source.____,\n",
    "                target.age = source.____,\n",
    "                target.last_updated = source.____\n",
    "        WHEN ____ ____ THEN\n",
    "            INSERT *\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"‚úì Batch {batchId} merged!\")\n",
    "\n",
    "# Read stream z Auto Loader table\n",
    "customers_stream_merge = (\n",
    "    spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .table(autoloader_table)\n",
    ")\n",
    "\n",
    "# Checkpoint\n",
    "merge_checkpoint = f\"{CHECKPOINT_BASE}/streaming_merge\"\n",
    "\n",
    "# Write stream z foreachBatch\n",
    "query_merge = (\n",
    "    customers_stream_merge\n",
    "    .writeStream\n",
    "    .foreachBatch(____)  # upsert_to_delta\n",
    "    .option(\"checkpointLocation\", ____)  # merge_checkpoint\n",
    "    .trigger(____=True)  # availableNow\n",
    "    .start()\n",
    ")\n",
    "\n",
    "query_merge.awaitTermination()\n",
    "\n",
    "print(f\"\\n‚úì Streaming MERGE zako≈Ñczony!\")\n",
    "print(f\"Liczba rekord√≥w w tabeli docelowej: {spark.table(target_merge_table).count()}\")\n",
    "\n",
    "display(spark.table(target_merge_table).limit(5))\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Zadanie 4: Por√≥wnanie metod ingestion (10 minut)\n",
    "\n",
    "**Cel:** Zrozumienie r√≥≈ºnic miƒôdzy COPY INTO, Auto Loader i Streaming.\n",
    "\n",
    "### Zadanie 4.1: Analiza por√≥wnawcza\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wype≈Çnij tabelƒô por√≥wnawczƒÖ poni≈ºej\n",
    "2. Dla ka≈ºdej metody okre≈õl:\n",
    "   - Use case (kiedy u≈ºywaƒá)\n",
    "   - Idempotency (czy automatyczna)\n",
    "   - Schema evolution (czy wspierana)\n",
    "   - Performance (batch vs streaming)\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Kompletna tabela por√≥wnawcza\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 4.1 - Analiza por√≥wnawcza\n",
    "\n",
    "# Wype≈Çnij tabelƒô por√≥wnawczƒÖ\n",
    "\n",
    "comparison_data = [\n",
    "    {\n",
    "        \"Method\": \"COPY INTO\",\n",
    "        \"Use Case\": \"____\",  # np. \"Batch loads, daily/hourly ingestion\"\n",
    "        \"Idempotency\": \"____\",  # \"Automatic\" / \"Manual\"\n",
    "        \"Schema Evolution\": \"____\",  # \"Manual\" / \"Automatic\"\n",
    "        \"Performance\": \"____\",  # \"Fast for batch\" / \"Slower\"\n",
    "        \"Complexity\": \"____\"  # \"Low\" / \"Medium\" / \"High\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"Auto Loader\",\n",
    "        \"Use Case\": \"____\",  # np. \"Incremental file loads, landing zones\"\n",
    "        \"Idempotency\": \"____\",\n",
    "        \"Schema Evolution\": \"____\",\n",
    "        \"Performance\": \"____\",\n",
    "        \"Complexity\": \"____\"\n",
    "    },\n",
    "    {\n",
    "        \"Method\": \"Structured Streaming\",\n",
    "        \"Use Case\": \"____\",  # np. \"Real-time, continuous processing\"\n",
    "        \"Idempotency\": \"____\",\n",
    "        \"Schema Evolution\": \"____\",\n",
    "        \"Performance\": \"____\",\n",
    "        \"Complexity\": \"____\"\n",
    "    }\n",
    "]\n",
    "\n",
    "comparison_df = spark.createDataFrame(comparison_data)\n",
    "\n",
    "print(\"=== Por√≥wnanie metod ingestion ===\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Twoja analiza:\n",
    "print(\"\"\"\n",
    "=== Kiedy u≈ºywaƒá kt√≥rej metody? ===\n",
    "\n",
    "COPY INTO:\n",
    "- ____\n",
    "\n",
    "Auto Loader:\n",
    "- ____\n",
    "\n",
    "Structured Streaming:\n",
    "- ____\n",
    "\"\"\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist - Co powiniene≈õ uzyskaƒá:\n",
    "- [ ] COPY INTO: Tabela `orders_copy_into` za≈Çadowana z idempotento≈õciƒÖ\n",
    "- [ ] Auto Loader: Tabela `customers_autoloader` z automatycznƒÖ schema inference\n",
    "- [ ] Schema Evolution: Tabela `customers_evolution_test` z nowƒÖ kolumnƒÖ\n",
    "- [ ] Streaming Aggregation: Tabela `order_status_summary` z real-time agregacjami\n",
    "- [ ] Streaming MERGE: Tabela `customers_merged` z upsert logic\n",
    "- [ ] Analiza por√≥wnawcza wype≈Çniona\n",
    "\n",
    "### Komendy weryfikacyjne:\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Weryfikacja wynik√≥w\n",
    "\n",
    "print(\"=== WERYFIKACJA WYNIK√ìW ===\\n\")\n",
    "\n",
    "# 1. COPY INTO\n",
    "print(\"1. COPY INTO:\")\n",
    "copy_count = spark.table(copy_into_table).count()\n",
    "print(f\"   Liczba rekord√≥w: {copy_count}\")\n",
    "assert copy_count > 0, \"B≈ÇƒÖd: Tabela pusta\"\n",
    "\n",
    "# 2. Auto Loader\n",
    "print(\"\\n2. Auto Loader:\")\n",
    "autoloader_count = spark.table(autoloader_table).count()\n",
    "print(f\"   Liczba rekord√≥w: {autoloader_count}\")\n",
    "assert autoloader_count > 0, \"B≈ÇƒÖd: Tabela pusta\"\n",
    "\n",
    "# 3. Schema Evolution\n",
    "print(\"\\n3. Schema Evolution:\")\n",
    "evolution_columns = spark.table(evolution_table).columns\n",
    "print(f\"   Kolumny: {evolution_columns}\")\n",
    "assert \"loyalty_tier\" in evolution_columns, \"B≈ÇƒÖd: Brak nowej kolumny loyalty_tier\"\n",
    "print(\"   ‚úì Nowa kolumna loyalty_tier wykryta!\")\n",
    "\n",
    "# 4. Streaming Aggregation\n",
    "print(\"\\n4. Streaming Aggregation:\")\n",
    "summary_count = spark.table(summary_table).count()\n",
    "print(f\"   Liczba status√≥w: {summary_count}\")\n",
    "display(spark.table(summary_table))\n",
    "\n",
    "# 5. Streaming MERGE\n",
    "print(\"\\n5. Streaming MERGE:\")\n",
    "merge_count = spark.table(target_merge_table).count()\n",
    "print(f\"   Liczba rekord√≥w: {merge_count}\")\n",
    "\n",
    "# 6. Checkpoints\n",
    "print(\"\\n6. Checkpoints (powinny istnieƒá):\")\n",
    "checkpoints = [\n",
    "    checkpoint_path,\n",
    "    streaming_checkpoint,\n",
    "    merge_checkpoint\n",
    "]\n",
    "for cp in checkpoints:\n",
    "    exists = \"‚úì Exists\" if spark._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "        spark._jsc.hadoopConfiguration()\n",
    "    ).exists(spark._jvm.org.apache.hadoop.fs.Path(cp)) else \"‚úó Missing\"\n",
    "    print(f\"   {cp}: {exists}\")\n",
    "\n",
    "print(\"\\n‚úì Wszystkie testy przesz≈Çy pomy≈õlnie!\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Problem 1: Auto Loader - Schema conflicts**\n",
    "**Objawy:** B≈ÇƒÖd: \"Schema mismatch\" podczas Auto Loader\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "```python\n",
    "# Usu≈Ñ checkpoint i schema location, zacznij od nowa\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "dbutils.fs.rm(schema_path, True)\n",
    "\n",
    "# Lub w≈ÇƒÖcz schema evolution:\n",
    ".option(\"cloudFiles.schemaEvolutionMode\", \"addNewColumns\")\n",
    "```\n",
    "\n",
    "**Problem 2: Streaming - Checkpoint conflicts**\n",
    "**Objawy:** B≈ÇƒÖd: \"Incompatible checkpoint\"\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "```python\n",
    "# Usu≈Ñ checkpoint dla nowego streama\n",
    "dbutils.fs.rm(checkpoint_path, True)\n",
    "\n",
    "# Lub u≈ºyj nowej ≈õcie≈ºki checkpoint\n",
    "checkpoint_path = f\"{CHECKPOINT_BASE}/new_stream_{timestamp}\"\n",
    "```\n",
    "\n",
    "**Problem 3: COPY INTO - No new files**\n",
    "**Objawy:** COPY INTO nie ≈Çaduje danych ponownie\n",
    "\n",
    "**RozwiƒÖzanie:**\n",
    "```python\n",
    "# COPY INTO ≈õledzi za≈Çadowane pliki (idempotentno≈õƒá)\n",
    "# To jest FEATURE, nie bug!\n",
    "# Je≈õli chcesz re-load: u≈ºyj FORCE = TRUE\n",
    "\n",
    "COPY INTO table_name\n",
    "FROM 'path'\n",
    "FILEFORMAT = JSON\n",
    "COPY_OPTIONS ('force' = 'true')  # Re-load all files\n",
    "```\n",
    "\n",
    "**Problem 4: Streaming - Memory issues**\n",
    "**RozwiƒÖzanie:**\n",
    "```python\n",
    "# Ogranicz shuffle partitions dla ma≈Çych stream√≥w\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "\n",
    "# U≈ºyj trigger availableNow zamiast continuous\n",
    ".trigger(availableNow=True)\n",
    "```\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "**W tym warsztacie nauczy≈Çe≈õ siƒô:**\n",
    "\n",
    "‚úÖ **COPY INTO:**\n",
    "- Batch file ingestion z automatycznƒÖ deduplikacjƒÖ\n",
    "- Idempotentno≈õƒá - pliki ≈Çadowane tylko raz\n",
    "- PATTERN matching dla selective loading\n",
    "- Audit trail z _metadata\n",
    "\n",
    "‚úÖ **Auto Loader (CloudFiles):**\n",
    "- Incremental file ingestion z automatycznym file tracking\n",
    "- Schema inference - automatyczne wykrywanie schema\n",
    "- Schema evolution - automatyczne dodawanie nowych kolumn\n",
    "- Production-ready dla landing zones\n",
    "\n",
    "‚úÖ **Structured Streaming:**\n",
    "- Real-time processing z micro-batches\n",
    "- Streaming aggregations (complete output mode)\n",
    "- foreachBatch dla custom logic (MERGE)\n",
    "- Checkpointing dla exactly-once semantics\n",
    "\n",
    "**Por√≥wnanie metod:**\n",
    "\n",
    "| Metoda | Use Case | Idempotency | Schema Evolution | Performance |\n",
    "|--------|----------|-------------|------------------|-------------|\n",
    "| COPY INTO | Daily/hourly batch | Automatic | Manual | Fast batch |\n",
    "| Auto Loader | Incremental files | Automatic | Automatic | Efficient incremental |\n",
    "| Streaming | Real-time | Manual (checkpoint) | Manual/Rescue | Low latency |\n",
    "\n",
    "**Kluczowe wnioski:**\n",
    "1. COPY INTO - best dla scheduled batch loads (daily, hourly)\n",
    "2. Auto Loader - best dla landing zones z unpredictable file arrival\n",
    "3. Structured Streaming - best dla low-latency, real-time processing\n",
    "4. Wszystkie metody wspierajƒÖ exactly-once semantics z checkpointing\n",
    "\n",
    "**Nastƒôpne kroki:**\n",
    "- **Kolejny warsztat**: 03_end_to_end_bronze_silver_gold_workshop.ipynb\n",
    "- **Materia≈Çy dodatkowe**: Delta Lake Streaming Guide, Auto Loader Best Practices\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Opcjonalnie: usu≈Ñ utworzone tabele i checkpointy:\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Opcjonalne czyszczenie zasob√≥w\n",
    "# UWAGA: Uruchom tylko je≈õli chcesz usunƒÖƒá wszystkie utworzone dane\n",
    "\n",
    "# Usu≈Ñ tabele\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {copy_into_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {autoloader_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {evolution_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {summary_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {target_merge_table}\")\n",
    "\n",
    "# Usu≈Ñ checkpointy\n",
    "# dbutils.fs.rm(CHECKPOINT_BASE, True)\n",
    "\n",
    "# Wyczy≈õƒá cache\n",
    "# spark.catalog.clearCache()\n",
    "\n",
    "# print(\"Zasoby zosta≈Çy wyczyszczone\")\n",
    "</VSCode.Cell>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
