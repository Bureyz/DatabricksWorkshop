{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83b2cc65",
   "metadata": {},
   "source": [
    "# Warsztat 3: End-to-End Bronze â†’ Silver â†’ Gold Pipeline\n",
    "\n",
    "**Cel warsztatu:**\n",
    "- Implementacja kompletnego pipeline'u Bronze â†’ Silver â†’ Gold\n",
    "- Integracja wielu ÅºrÃ³deÅ‚ danych (customers, orders, products)\n",
    "- Transformacje biznesowe i agregacje w architekturze medalionowej\n",
    "- Optymalizacja i monitorowanie pipeline'u\n",
    "\n",
    "**Czas:** 120 minut\n",
    "\n",
    "**Architektura docelowa:**\n",
    "```\n",
    "Bronze (Raw Data) Silver (Cleansed) Gold (Analytics)\n",
    "â”œâ”€â”€ customers.csv â†’ â”œâ”€â”€ customers_clean â†’ â”œâ”€â”€ customer_analytics\n",
    "â”œâ”€â”€ orders.json â†’ â”œâ”€â”€ orders_clean â†’ â”œâ”€â”€ product_performance \n",
    "â””â”€â”€ products.parquetâ†’ â””â”€â”€ products_clean â†’ â””â”€â”€ sales_summary\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633709a6",
   "metadata": {},
   "source": [
    "## ðŸ“š Inicjalizacja Å›rodowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6b6a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248410bf",
   "metadata": {},
   "source": [
    "**WyjaÅ›nienie inicjalizacji:**\n",
    "- Skrypt `00_setup` konfiguruje per-user izolacjÄ™ (katalogi, schematy)\n",
    "- Automatycznie tworzy zmienne Å›rodowiskowe: `CATALOG`, `BRONZE_SCHEMA`, `SILVER_SCHEMA`, `GOLD_SCHEMA`\n",
    "- Zapewnia, Å¼e kaÅ¼dy uÅ¼ytkownik pracuje w izolowanym namespace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ff4df3",
   "metadata": {},
   "source": [
    "## CzÄ™Å›Ä‡ 1: Warstwa Bronze - Raw Data Ingestion\n",
    "\n",
    "### Zadanie 1.1: Ingestion danych klientÃ³w (CSV)\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane z `customers.csv` uÅ¼ywajÄ…c Auto Loader\n",
    "2. Dodaj kolumny metadata: `_source_file`, `_ingestion_timestamp`\n",
    "3. Zapisz do `bronze_customers_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a6fbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# ÅšcieÅ¼ki do plikÃ³w ÅºrÃ³dÅ‚owych w dataset\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "# TODO: Auto Loader dla customers\n",
    "bronze_customers_stream = (\n",
    " spark.readStream\n",
    " .format(\"____\") # cloudFiles\n",
    " .option(\"cloudFiles.format\", \"____\") # csv\n",
    " .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/customers_schema\")\n",
    " .option(\"header\", \"true\")\n",
    " .load(f\"{SOURCE_DATA_PATH}/____\") # customers.csv\n",
    " .withColumn(\"_source_file\", ____) # input_file_name()\n",
    " .withColumn(\"_ingestion_timestamp\", ____) # current_timestamp()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d0c56d",
   "metadata": {},
   "source": [
    "**Konfiguracja Å›rodowiska:**\n",
    "- Importujemy funkcje PySpark do transformacji danych\n",
    "- Definiujemy Å›cieÅ¼ki do plikÃ³w ÅºrÃ³dÅ‚owych w dataset\n",
    "- Pliki sÄ… w rÃ³Å¼nych formatach: CSV (customers), JSON (orders), Parquet (products)\n",
    "- UÅ¼yjemy rzeczywistych kolumn z dataset'u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11824b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Krok 1 - Wczytaj customers z CSV\n",
    "customers_raw = (\n",
    " spark.read\n",
    " .format(\"____\") # csv\n",
    " .option(\"header\", \"____\") # true\n",
    " .option(\"inferSchema\", \"____\") # true\n",
    " .load(____) # CUSTOMERS_CSV\n",
    ")\n",
    "\n",
    "# TODO: Zapisz do Bronze\n",
    "query_bronze_customers = (\n",
    " bronze_customers_stream.writeStream\n",
    " .format(\"____\") # delta\n",
    " .outputMode(\"____\") # append\n",
    " .option(\"checkpointLocation\", f\"{____}/bronze_customers\") # CHECKPOINT_PATH\n",
    " .table(f\"{CATALOG}.{SCHEMA}.bronze_customers_pipeline\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480d830c",
   "metadata": {},
   "source": [
    "**Krok 3: Zapis do Bronze**\n",
    "- `.format(\"delta\")` - zapisujemy w formacie Delta Lake\n",
    "- `.mode(\"overwrite\")` - zastÄ™pujemy istniejÄ…ce dane\n",
    "- `overwriteSchema=\"true\"` - pozwalamy na zmiany w schemacie\n",
    "- Tabela zostanie utworzona w schemacie Bronze z prefiksem uÅ¼ytkownika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e41988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Krok 3 - Zapisz customers do Bronze\n",
    "(\n",
    " customers_bronze\n",
    " .write\n",
    " .format(\"____\") # delta\n",
    " .mode(\"____\") # overwrite\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(f\"{____}.customers_bronze\") # BRONZE_SCHEMA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2506bee",
   "metadata": {},
   "source": [
    "**Krok 2: Audit metadata dla Bronze**\n",
    "- `_bronze_ingest_timestamp` - uÅ¼yj `current_timestamp()` - kiedy dane zostaÅ‚y zaÅ‚adowane\n",
    "- `_bronze_source_file` - uÅ¼yj `input_file_name()` - z jakiego pliku pochodzÄ… dane\n",
    "- `_bronze_ingested_by` - uÅ¼yj `raw_user` - kto zaÅ‚adowaÅ‚ dane\n",
    "- Te kolumny pomagajÄ… w Å›ledzeniu pochodzenia danych (data lineage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429968c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Krok 2 - Dodaj audit metadata dla customers\n",
    "customers_bronze = (\n",
    " customers_raw\n",
    " .withColumn(\"_bronze_ingest_timestamp\", F.____) # current_timestamp()\n",
    " .withColumn(\"_bronze_source_file\", F.____) # input_file_name()\n",
    " .withColumn(\"_bronze_ingested_by\", F.lit(____)) # raw_user\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25be731c",
   "metadata": {},
   "source": [
    "**Krok 1: Wczytanie customers (CSV)**\n",
    "- UÅ¼ywamy `.format(\"csv\")` dla plikÃ³w CSV\n",
    "- `header=\"true\"` - pierwszy wiersz zawiera nazwy kolumn\n",
    "- `inferSchema=\"true\"` - automatyczne wykrywanie typÃ³w danych\n",
    "- Kolumny w pliku: `customer_id`, `first_name`, `last_name`, `email`, `phone`, `city`, `state`, `country`, `registration_date`, `customer_segment`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9171cdcb",
   "metadata": {},
   "source": [
    "### Zadanie 1.2: Ingestion zamÃ³wieÅ„ (JSON)\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane z `orders_batch.json` uÅ¼ywajÄ…c Auto Loader\n",
    "2. Dodaj schema hints dla `order_date` (DATE) i `total_amount` (DOUBLE)\n",
    "3. Zapisz do `bronze_orders_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ca5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Auto Loader dla orders\n",
    "bronze_orders_stream = (\n",
    " spark.readStream\n",
    " .format(\"cloudFiles\")\n",
    " .option(\"cloudFiles.format\", \"____\") # json\n",
    " .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/orders_schema\")\n",
    " .option(\"cloudFiles.schemaHints\", \"____\") # \"order_date DATE, total_amount DOUBLE\"\n",
    " .load(f\"{SOURCE_DATA_PATH}/____\") # orders_batch.json\n",
    " .withColumn(\"_source_file\", input_file_name())\n",
    " .withColumn(\"_ingestion_timestamp\", current_timestamp())\n",
    ")\n",
    "\n",
    "# TODO: Krok 4 - Wczytaj orders z JSON\n",
    "orders_raw = (\n",
    " spark.read\n",
    " .format(\"____\") # json\n",
    " .option(\"multiLine\", \"____\") # false - kaÅ¼da linia to osobny JSON\n",
    " .load(____) # ORDERS_JSON\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e01088",
   "metadata": {},
   "source": [
    "**Krok 4: Wczytanie orders (JSON)**\n",
    "- UÅ¼ywamy `.format(\"json\")` dla plikÃ³w JSON\n",
    "- `multiLine=\"false\"` - kaÅ¼da linia zawiera osobny obiekt JSON\n",
    "- Kolumny w orders: `order_id`, `customer_id`, `product_id`, `store_id`, `order_datetime`, `quantity`, `unit_price`, `discount_percent`, `total_amount`, `payment_method`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2e0337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zapisz do Bronze\n",
    "query_bronze_orders = (\n",
    " bronze_orders_stream.writeStream\n",
    " .format(\"delta\")\n",
    " .outputMode(\"append\")\n",
    " .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/____\") # bronze_orders\n",
    " .table(f\"{CATALOG}.{SCHEMA}.bronze_orders_pipeline\")\n",
    ")\n",
    "\n",
    "# TODO: Krok 5 - Dodaj audit metadata dla orders\n",
    "orders_bronze = (\n",
    " orders_raw\n",
    " .withColumn(\"_bronze_ingest_timestamp\", F.current_timestamp())\n",
    " .withColumn(\"_bronze_source_file\", F.input_file_name())\n",
    " .withColumn(\"_bronze_ingested_by\", F.lit(____) # raw_user\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0015d21",
   "metadata": {},
   "source": [
    "**Krok 6: Zapis orders do Bronze**\n",
    "- Identyczna procedura jak dla customers\n",
    "- UzupeÅ‚nij `\"delta\"` i `\"overwrite\"` w miejscach z `____`\n",
    "- Tabela `orders_bronze` zostanie utworzona w schemacie Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4318a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Krok 6 - Zapisz orders do Bronze\n",
    "(\n",
    " orders_bronze\n",
    " .write\n",
    " .format(\"____\") # delta\n",
    " .mode(\"____\") # overwrite\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(f\"{BRONZE_SCHEMA}.orders_bronze\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c2a2b",
   "metadata": {},
   "source": [
    "**Krok 5: Audit metadata dla orders**\n",
    "- Dodajemy te same kolumny audit co dla customers\n",
    "- UÅ¼yj `raw_user` w miejscu `____` \n",
    "- To zapewnia spÃ³jne Å›ledzenie pochodzenia danych we wszystkich tabelach Bronze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e58fbf8",
   "metadata": {},
   "source": [
    "### Zadanie 1.3: Ingestion produktÃ³w (Parquet)\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane z `products.parquet` uÅ¼ywajÄ…c COPY INTO (batch)\n",
    "2. Dodaj kolumny metadata\n",
    "3. Zapisz do `bronze_products_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83741b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: UtwÃ³rz tabelÄ™ Bronze dla produktÃ³w\n",
    "spark.sql(f\"\"\"\n",
    " CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_products_pipeline (\n",
    " product_id INT,\n",
    " product_name STRING,\n",
    " category STRING,\n",
    " price DOUBLE,\n",
    " stock_quantity INT,\n",
    " _source_file STRING,\n",
    " _ingestion_timestamp TIMESTAMP\n",
    " )\n",
    " USING DELTA\n",
    " LOCATION '{____}/products_pipeline' -- BRONZE_PATH\n",
    "\"\"\")\n",
    "\n",
    "# TODO: Krok 7 - Wczytaj products z Parquet\n",
    "products_raw = (\n",
    " spark.read\n",
    " .format(\"____\") # parquet\n",
    " .load(____) # PRODUCTS_PARQUET\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe6bac2",
   "metadata": {},
   "source": [
    "**Krok 7: Wczytanie products (Parquet)**\n",
    "- UÅ¼ywamy `.format(\"parquet\")` dla plikÃ³w Parquet\n",
    "- Parquet automatycznie zawiera schema, wiÄ™c nie trzeba jej definiowaÄ‡\n",
    "- Jest to najbardziej efektywny format dla duÅ¼ych zbiorÃ³w danych\n",
    "- Kolumny w products: `product_id`, `product_name`, `category`, `price`, `stock_quantity`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde7078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COPY INTO dla produktÃ³w\n",
    "spark.sql(f\"\"\"\n",
    " ____ INTO {CATALOG}.{SCHEMA}.bronze_products_pipeline\n",
    " FROM (\n",
    " SELECT \n",
    " product_id,\n",
    " product_name,\n",
    " category,\n",
    " price,\n",
    " stock_quantity,\n",
    " _metadata.file_path as _source_file,\n",
    " current_timestamp() as _ingestion_timestamp\n",
    " FROM '{____}/____' -- SOURCE_DATA_PATH/products.parquet\n",
    " )\n",
    " FILEFORMAT = ____ -- PARQUET\n",
    "\"\"\")\n",
    "\n",
    "# TODO: Krok 8 - Dodaj audit metadata dla products\n",
    "products_bronze = (\n",
    " products_raw\n",
    " .withColumn(\"_bronze_ingest_timestamp\", F.current_timestamp()) # current_timestamp()\n",
    " .withColumn(\"_bronze_source_file\", F.input_file_name()) # input_file_name()\n",
    " .withColumn(\"_bronze_ingested_by\", F.lit(raw_user)) # raw_user\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2734dc",
   "metadata": {},
   "source": [
    "**Krok 8: Audit metadata dla products**\n",
    "- UzupeÅ‚nij brakujÄ…ce funkcje: `current_timestamp()`, `input_file_name()`, `raw_user`\n",
    "- SpÃ³jne kolumny audit we wszystkich tabelach Bronze uÅ‚atwiajÄ… zarzÄ…dzanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a72df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja warstwy Bronze\n",
    "import time\n",
    "time.sleep(15) # Poczekaj na przetworzenie streamÃ³w\n",
    "\n",
    "print(\"=== Bronze Layer Summary ===\")\n",
    "print(f\"Customers: {spark.table(f'{CATALOG}.{SCHEMA}.bronze_customers_pipeline').count()}\")\n",
    "print(f\"Orders: {spark.table(f'{CATALOG}.{SCHEMA}.bronze_orders_pipeline').count()}\")\n",
    "print(f\"Products: {spark.table(f'{CATALOG}.{SCHEMA}.bronze_products_pipeline').count()}\")\n",
    "\n",
    "# TODO: Krok 9 - Zapisz products do Bronze\n",
    "(\n",
    " products_bronze\n",
    " .write\n",
    " .format(\"delta\") # delta\n",
    " .mode(\"overwrite\") # overwrite\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(f\"{BRONZE_SCHEMA}.products_bronze\") # BRONZE_SCHEMA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf0fa7f",
   "metadata": {},
   "source": [
    "**Krok 10: Weryfikacja warstwy Bronze**\n",
    "- Sprawdzamy czy wszystkie dane zostaÅ‚y poprawnie zaÅ‚adowane\n",
    "- UzupeÅ‚nij nazwy tabel: `customers_bronze`, `orders_bronze`, `products_bronze`\n",
    "- Liczby rekordÃ³w powinny odpowiadaÄ‡ ÅºrÃ³dÅ‚owym plikom\n",
    "- To checkpoint przed przejÅ›ciem do warstwy Silver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdd07e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Krok 10 - Weryfikacja warstwy Bronze\n",
    "\n",
    "# SprawdÅº liczby rekordÃ³w w kaÅ¼dej tabeli\n",
    "customers_count = spark.table(f\"{BRONZE_SCHEMA}.____\").count() # customers_bronze\n",
    "orders_count = spark.table(f\"{BRONZE_SCHEMA}.____\").count() # orders_bronze \n",
    "products_count = spark.table(f\"{BRONZE_SCHEMA}.____\").count() # products_bronze\n",
    "\n",
    "# WyÅ›wietl podsumowanie\n",
    "display(spark.createDataFrame([\n",
    " (\"Customers Bronze\", customers_count),\n",
    " (\"Orders Bronze\", orders_count),\n",
    " (\"Products Bronze\", products_count)\n",
    "], [\"Tabela\", \"Liczba_rekordÃ³w\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8de8548",
   "metadata": {},
   "source": [
    "**Krok 9: Zapis products do Bronze**\n",
    "- Ostatni krok w warstwie Bronze\n",
    "- UzupeÅ‚nij `\"delta\"`, `\"overwrite\"` i `BRONZE_SCHEMA`\n",
    "- Po tym kroku wszystkie surowe dane bÄ™dÄ… w warstwie Bronze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024fa014",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## CzÄ™Å›Ä‡ 2: Warstwa Silver - Data Cleansing & Standardization\n",
    "\n",
    "### Zadanie 2.1: Silver Customers - Czyszczenie danych\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane z Bronze jako streaming DataFrame\n",
    "2. WyczyÅ›Ä‡ dane:\n",
    " - UsuÅ„ duplikaty po `customer_id`\n",
    " - Filtruj nieprawidÅ‚owe emaile (muszÄ… zawieraÄ‡ `@`)\n",
    " - Standaryzuj nazwy krajÃ³w (UPPER)\n",
    " - UsuÅ„ biaÅ‚e znaki z imion (trim)\n",
    "3. Dodaj kolumnÄ™ `processed_at`\n",
    "4. Zapisz do `silver_customers_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b284fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, upper, trim, current_timestamp\n",
    "\n",
    "# TODO: Streaming read z Bronze\n",
    "bronze_customers = (\n",
    " spark.readStream\n",
    " .format(\"____\") # delta\n",
    " .table(f\"{CATALOG}.{SCHEMA}.____\") # bronze_customers_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd8ea5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Transformacje Silver\n",
    "silver_customers = (\n",
    " bronze_customers\n",
    " .dropDuplicates([\"____\"]) # customer_id\n",
    " .filter(col(\"____\").contains(\"____\")) # email zawiera @\n",
    " .filter(col(\"email\").isNotNull()) # email nie jest NULL\n",
    " .withColumn(\"name\", ____(col(\"name\"))) # trim\n",
    " .withColumn(\"country\", ____(col(\"country\"))) # upper\n",
    " .withColumn(\"processed_at\", ____) # current_timestamp\n",
    " .select(\n",
    " \"customer_id\", \"name\", \"email\", \"city\", \"country\", \"processed_at\"\n",
    " )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f55bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zapisz do Silver\n",
    "query_silver_customers = (\n",
    " silver_customers.writeStream\n",
    " .format(\"delta\")\n",
    " .outputMode(\"____\") # append\n",
    " .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/silver_customers\")\n",
    " .option(\"mergeSchema\", \"true\")\n",
    " .table(f\"{CATALOG}.{SCHEMA}.silver_customers_pipeline\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc4bb17",
   "metadata": {},
   "source": [
    "### Zadanie 2.2: Silver Orders - Wzbogacenie danych\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane z Bronze\n",
    "2. Transformacje:\n",
    " - Konwertuj `order_date` na DATE\n",
    " - Dodaj kolumnÄ™ `order_year` (rok z daty)\n",
    " - Dodaj kolumnÄ™ `order_month` (miesiÄ…c z daty)\n",
    " - Kategoryzuj zamÃ³wienia: `order_value_category` (LOW < 100, MEDIUM 100-500, HIGH > 500)\n",
    "3. Zapisz do `silver_orders_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060613f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, when, to_date\n",
    "\n",
    "# TODO: Streaming read z Bronze\n",
    "bronze_orders = (\n",
    " spark.readStream\n",
    " .format(\"delta\")\n",
    " .table(f\"{CATALOG}.{SCHEMA}.____\") # bronze_orders_pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed39ad2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Transformacje Silver\n",
    "silver_orders = (\n",
    " bronze_orders\n",
    " .withColumn(\"order_date\", to_date(col(\"order_date\")))\n",
    " .withColumn(\"order_year\", ____(col(\"order_date\"))) # year\n",
    " .withColumn(\"order_month\", ____(col(\"order_date\"))) # month\n",
    " .withColumn(\n",
    " \"order_value_category\",\n",
    " when(col(\"total_amount\") < 100, \"____\") # LOW\n",
    " .when((col(\"total_amount\") >= 100) & (col(\"total_amount\") <= 500), \"____\") # MEDIUM\n",
    " .otherwise(\"____\") # HIGH\n",
    " )\n",
    " .withColumn(\"processed_at\", current_timestamp())\n",
    " .select(\n",
    " \"order_id\", \"customer_id\", \"order_date\", \"order_year\", \"order_month\",\n",
    " \"total_amount\", \"order_value_category\", \"status\", \"processed_at\"\n",
    " )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045a7561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zapisz do Silver\n",
    "query_silver_orders = (\n",
    " silver_orders.writeStream\n",
    " .format(\"____\") # delta\n",
    " .outputMode(\"append\")\n",
    " .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/____\") # silver_orders\n",
    " .table(f\"{CATALOG}.{SCHEMA}.silver_orders_pipeline\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b29131e",
   "metadata": {},
   "source": [
    "### Zadanie 2.3: Silver Products - Normalizacja\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane z Bronze (batch)\n",
    "2. Transformacje:\n",
    " - Standaryzuj nazwy kategorii (UPPER)\n",
    " - Dodaj kolumnÄ™ `is_in_stock` (TRUE jeÅ›li stock_quantity > 0)\n",
    " - Dodaj kolumnÄ™ `price_tier` (BUDGET < 50, STANDARD 50-200, PREMIUM > 200)\n",
    "3. Zapisz do `silver_products_pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b4c9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Batch read z Bronze\n",
    "bronze_products = spark.table(f\"{CATALOG}.{SCHEMA}.____\") # bronze_products_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e778a226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Transformacje Silver\n",
    "silver_products = (\n",
    " bronze_products\n",
    " .withColumn(\"category\", ____(col(\"category\"))) # upper\n",
    " .withColumn(\"is_in_stock\", col(\"stock_quantity\") ____ 0) # >\n",
    " .withColumn(\n",
    " \"price_tier\",\n",
    " when(col(\"price\") < 50, \"____\") # BUDGET\n",
    " .when((col(\"price\") >= 50) & (col(\"price\") <= 200), \"____\") # STANDARD\n",
    " .otherwise(\"____\") # PREMIUM\n",
    " )\n",
    " .withColumn(\"processed_at\", current_timestamp())\n",
    " .select(\n",
    " \"product_id\", \"product_name\", \"category\", \"price\", \"price_tier\",\n",
    " \"stock_quantity\", \"is_in_stock\", \"processed_at\"\n",
    " )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e721c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zapisz do Silver (batch)\n",
    "(\n",
    " silver_products.write\n",
    " .format(\"____\") # delta\n",
    " .mode(\"____\") # overwrite\n",
    " .option(\"path\", f\"{SILVER_PATH}/products_pipeline\")\n",
    " .saveAsTable(f\"{CATALOG}.{SCHEMA}.silver_products_pipeline\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d506301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja warstwy Silver\n",
    "time.sleep(15)\n",
    "\n",
    "print(\"=== Silver Layer Summary ===\")\n",
    "print(f\"Customers: {spark.table(f'{CATALOG}.{SCHEMA}.silver_customers_pipeline').count()}\")\n",
    "print(f\"Orders: {spark.table(f'{CATALOG}.{SCHEMA}.silver_orders_pipeline').count()}\")\n",
    "print(f\"Products: {spark.table(f'{CATALOG}.{SCHEMA}.silver_products_pipeline').count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "789cb635",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š CzÄ™Å›Ä‡ 3: Warstwa Gold - Business Analytics\n",
    "\n",
    "### Zadanie 3.1: Customer Analytics\n",
    "\n",
    "**Instrukcje:**\n",
    "1. StwÃ³rz agregacjÄ™:\n",
    " - JOIN silver_customers z silver_orders\n",
    " - Grupuj po customer_id, name, country\n",
    " - Oblicz: total_orders, total_spent, avg_order_value\n",
    "2. Zapisz jako `gold_customer_analytics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5c3089",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, sum, avg, round\n",
    "\n",
    "# TODO: Wczytaj dane Silver\n",
    "customers = spark.table(f\"{CATALOG}.{SCHEMA}.____\") # silver_customers_pipeline\n",
    "orders = spark.table(f\"{CATALOG}.{SCHEMA}.____\") # silver_orders_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786f9541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: JOIN i agregacja\n",
    "customer_analytics = (\n",
    " customers.alias(\"c\")\n",
    " .join(\n",
    " orders.alias(\"o\"),\n",
    " col(\"c.customer_id\") == col(\"o.____\"), # customer_id\n",
    " \"____\" # left (aby uwzglÄ™dniÄ‡ klientÃ³w bez zamÃ³wieÅ„)\n",
    " )\n",
    " .groupBy(\"c.customer_id\", \"c.name\", \"c.country\")\n",
    " .agg(\n",
    " ____(\"o.order_id\").alias(\"total_orders\"), # count\n",
    " round(____(\"o.total_amount\"), 2).alias(\"total_spent\"), # sum\n",
    " round(____(\"o.total_amount\"), 2).alias(\"avg_order_value\") # avg\n",
    " )\n",
    " .withColumn(\"processed_at\", current_timestamp())\n",
    ")\n",
    "\n",
    "display(customer_analytics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06bafdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zapisz do Gold\n",
    "(\n",
    " customer_analytics.write\n",
    " .format(\"delta\")\n",
    " .mode(\"____\") # overwrite\n",
    " .option(\"path\", f\"{____}/customer_analytics\") # GOLD_PATH\n",
    " .saveAsTable(f\"{CATALOG}.{SCHEMA}.gold_customer_analytics\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc64303",
   "metadata": {},
   "source": [
    "### Zadanie 3.2: Product Performance\n",
    "\n",
    "**Instrukcje:**\n",
    "1. StwÃ³rz agregacjÄ™:\n",
    " - Kategoria produktu\n",
    " - Liczba produktÃ³w w kategorii\n",
    " - Åšrednia cena w kategorii\n",
    " - Liczba produktÃ³w dostÄ™pnych (in_stock)\n",
    "2. Zapisz jako `gold_product_performance`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af06d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wczytaj produkty Silver\n",
    "products = spark.table(f\"{CATALOG}.{SCHEMA}.silver_products_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2a448a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Agregacja po kategorii\n",
    "product_performance = (\n",
    " products\n",
    " .groupBy(\"____\") # category\n",
    " .agg(\n",
    " count(\"product_id\").alias(\"____\"), # total_products\n",
    " round(avg(\"____\"), 2).alias(\"avg_price\"), # price\n",
    " sum(when(col(\"is_in_stock\") == True, 1).otherwise(0)).alias(\"____\") # products_in_stock\n",
    " )\n",
    " .withColumn(\"processed_at\", current_timestamp())\n",
    " .orderBy(col(\"total_products\").desc())\n",
    ")\n",
    "\n",
    "display(product_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b9c98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zapisz do Gold\n",
    "(\n",
    " product_performance.write\n",
    " .format(\"____\")\n",
    " .mode(\"overwrite\")\n",
    " .option(\"path\", f\"{GOLD_PATH}/____\") # product_performance\n",
    " .saveAsTable(f\"{CATALOG}.{SCHEMA}.gold_product_performance\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d63e1ed",
   "metadata": {},
   "source": [
    "### Zadanie 3.3: Sales Summary\n",
    "\n",
    "**Instrukcje:**\n",
    "1. StwÃ³rz agregacjÄ™ zamÃ³wieÅ„:\n",
    " - Grupuj po roku, miesiÄ…cu, kategorii wartoÅ›ci (order_value_category)\n",
    " - Oblicz: liczba zamÃ³wieÅ„, suma przychodÃ³w, Å›rednia wartoÅ›Ä‡ zamÃ³wienia\n",
    "2. Zapisz jako `gold_sales_summary`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256fcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wczytaj zamÃ³wienia Silver\n",
    "orders = spark.table(f\"{CATALOG}.{SCHEMA}.silver_orders_pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b5d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Agregacja sprzedaÅ¼y\n",
    "sales_summary = (\n",
    " orders\n",
    " .groupBy(\"____\", \"____\", \"____\") # order_year, order_month, order_value_category\n",
    " .agg(\n",
    " count(\"order_id\").alias(\"____\"), # total_orders\n",
    " round(sum(\"total_amount\"), 2).alias(\"____\"), # total_revenue\n",
    " round(avg(\"total_amount\"), 2).alias(\"____\") # avg_order_value\n",
    " )\n",
    " .withColumn(\"processed_at\", current_timestamp())\n",
    " .orderBy(\"order_year\", \"order_month\", \"order_value_category\")\n",
    ")\n",
    "\n",
    "display(sales_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef2b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zapisz do Gold\n",
    "(\n",
    " sales_summary.write\n",
    " .format(\"delta\")\n",
    " .mode(\"____\") # overwrite\n",
    " .option(\"path\", f\"{GOLD_PATH}/sales_summary\")\n",
    " .saveAsTable(f\"{CATALOG}.{SCHEMA}.____\") # gold_sales_summary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b5f18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja warstwy Gold\n",
    "print(\"=== Gold Layer Summary ===\")\n",
    "print(f\"Customer Analytics: {spark.table(f'{CATALOG}.{SCHEMA}.gold_customer_analytics').count()}\")\n",
    "print(f\"Product Performance: {spark.table(f'{CATALOG}.{SCHEMA}.gold_product_performance').count()}\")\n",
    "print(f\"Sales Summary: {spark.table(f'{CATALOG}.{SCHEMA}.gold_sales_summary').count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc92df0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âš¡ CzÄ™Å›Ä‡ 4: Optymalizacja Pipeline'u\n",
    "\n",
    "### Zadanie 4.1: OPTIMIZE wszystkich tabel\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wykonaj OPTIMIZE dla wszystkich tabel Gold\n",
    "2. Zastosuj ZORDER dla kluczowych kolumn filtrowania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20adfe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: OPTIMIZE Customer Analytics (ZORDER by country)\n",
    "spark.sql(f\"\"\"\n",
    " ____ {CATALOG}.{SCHEMA}.gold_customer_analytics\n",
    " ZORDER BY (____)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eea578d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: OPTIMIZE Product Performance (ZORDER by category)\n",
    "spark.sql(f\"\"\"\n",
    " OPTIMIZE {CATALOG}.{SCHEMA}.gold_product_performance\n",
    " ____ BY (category)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30261f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: OPTIMIZE Sales Summary (ZORDER by order_year, order_month)\n",
    "spark.sql(f\"\"\"\n",
    " OPTIMIZE {CATALOG}.{SCHEMA}.____\n",
    " ZORDER BY (order_year, order_month)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a71c106c",
   "metadata": {},
   "source": [
    "### Zadanie 4.2: Data Quality Checks\n",
    "\n",
    "**Instrukcje:**\n",
    "1. SprawdÅº jakoÅ›Ä‡ danych w kaÅ¼dej warstwie:\n",
    " - Liczba duplikatÃ³w\n",
    " - Liczba NULL w kluczowych kolumnach\n",
    " - IntegralnoÅ›Ä‡ referencyjna (wszystkie customer_id w orders istniejÄ… w customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d63ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: SprawdÅº duplikaty w Silver Customers\n",
    "duplicates = (\n",
    " spark.table(f\"{CATALOG}.{SCHEMA}.silver_customers_pipeline\")\n",
    " .groupBy(\"____\") # customer_id\n",
    " .count()\n",
    " .filter(col(\"count\") > 1)\n",
    ")\n",
    "\n",
    "print(f\"Duplikaty w Silver Customers: {duplicates.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c81f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: SprawdÅº NULL w kluczowych kolumnach\n",
    "null_checks = spark.sql(f\"\"\"\n",
    " SELECT \n",
    " 'customers' as table_name,\n",
    " SUM(CASE WHEN customer_id IS NULL THEN 1 ELSE 0 END) as null_customer_id,\n",
    " SUM(CASE WHEN ____ IS NULL THEN 1 ELSE 0 END) as null_email -- email\n",
    " FROM {CATALOG}.{SCHEMA}.silver_customers_pipeline\n",
    " \n",
    " UNION ALL\n",
    " \n",
    " SELECT \n",
    " 'orders' as table_name,\n",
    " SUM(CASE WHEN order_id IS NULL THEN 1 ELSE 0 END) as null_order_id,\n",
    " SUM(CASE WHEN ____ IS NULL THEN 1 ELSE 0 END) as null_total_amount\n",
    " FROM {CATALOG}.{SCHEMA}.silver_orders_pipeline\n",
    "\"\"\")\n",
    "\n",
    "display(null_checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "447ee67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: SprawdÅº integralnoÅ›Ä‡ referentialnÄ…\n",
    "orphan_orders = spark.sql(f\"\"\"\n",
    " SELECT COUNT(*) as orphan_count\n",
    " FROM {CATALOG}.{SCHEMA}.silver_orders_pipeline o\n",
    " LEFT ANTI JOIN {CATALOG}.{SCHEMA}.silver_customers_pipeline c\n",
    " ON o.customer_id = c.____\n",
    "\"\"\")\n",
    "\n",
    "display(orphan_orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe243dd6",
   "metadata": {},
   "source": [
    "### Zadanie 4.3: Monitoring streaming queries\n",
    "\n",
    "**Instrukcje:**\n",
    "1. WyÅ›wietl aktywne streaming queries\n",
    "2. SprawdÅº metryki kaÅ¼dego streamu\n",
    "3. Zatrzymaj wszystkie streamy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87804007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Monitoring streamÃ³w\n",
    "active_streams = spark.streams.____ # active\n",
    "\n",
    "print(f\"Liczba aktywnych streamÃ³w: {len(active_streams)}\")\n",
    "\n",
    "for stream in active_streams:\n",
    " print(f\"\\n=== Stream: {stream.name} ===\")\n",
    " print(f\"ID: {stream.id}\")\n",
    " print(f\"Status: {stream.status['message']}\")\n",
    " \n",
    " last_progress = stream.lastProgress\n",
    " if last_progress:\n",
    " print(f\"Batch ID: {last_progress['batchId']}\")\n",
    " print(f\"Przetworzone rekordy: {last_progress.get('numInputRows', 0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf222f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zatrzymaj wszystkie streamy\n",
    "for stream in spark.streams.active:\n",
    " print(f\"ZatrzymujÄ™ stream: {stream.name}\")\n",
    " stream.____() # stop\n",
    "\n",
    "print(\"\\nWszystkie streamy zatrzymane!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b479b96",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ˆ CzÄ™Å›Ä‡ 5: Analiza Biznesowa (Bonus)\n",
    "\n",
    "### Zadanie 5.1: Top 10 Customers by Spend\n",
    "\n",
    "**Instrukcje:**\n",
    "1. WyÅ›wietl top 10 klientÃ³w wedÅ‚ug caÅ‚kowitych wydatkÃ³w\n",
    "2. Dodaj informacjÄ™ o kraju i liczbie zamÃ³wieÅ„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20e029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Top 10 klientÃ³w\n",
    "top_customers = spark.sql(f\"\"\"\n",
    " SELECT \n",
    " name,\n",
    " country,\n",
    " total_orders,\n",
    " total_spent,\n",
    " avg_order_value\n",
    " FROM {CATALOG}.{SCHEMA}.gold_customer_analytics\n",
    " ORDER BY ____ DESC -- total_spent\n",
    " LIMIT ____ -- 10\n",
    "\"\"\")\n",
    "\n",
    "display(top_customers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc964612",
   "metadata": {},
   "source": [
    "### Zadanie 5.2: Category Performance Analysis\n",
    "\n",
    "**Instrukcje:**\n",
    "1. WyÅ›wietl performance kaÅ¼dej kategorii produktÃ³w\n",
    "2. Oblicz % produktÃ³w dostÄ™pnych w magazynie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe919f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analiza kategorii\n",
    "category_analysis = spark.sql(f\"\"\"\n",
    " SELECT \n",
    " category,\n",
    " total_products,\n",
    " avg_price,\n",
    " products_in_stock,\n",
    " ROUND((____ * 100.0 / ____), 2) as stock_percentage -- products_in_stock / total_products\n",
    " FROM {CATALOG}.{SCHEMA}.gold_product_performance\n",
    " ORDER BY total_products ____ -- DESC\n",
    "\"\"\")\n",
    "\n",
    "display(category_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7a8405",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie warsztatu\n",
    "\n",
    "**Zrealizowane cele:**\n",
    "- Kompletny pipeline Bronze â†’ Silver â†’ Gold\n",
    "- Integracja wielu ÅºrÃ³deÅ‚ danych (CSV, JSON, Parquet)\n",
    "- Streaming i batch processing\n",
    "- Transformacje biznesowe i agregacje\n",
    "- Optymalizacja tabel (OPTIMIZE, ZORDER)\n",
    "- Data quality checks\n",
    "- Monitoring pipeline'u\n",
    "\n",
    "**Architektura finalna:**\n",
    "\n",
    "```\n",
    "Bronze (Raw Data)\n",
    "â”œâ”€â”€ customers (CSV, Auto Loader)\n",
    "â”œâ”€â”€ orders (JSON, Auto Loader)\n",
    "â””â”€â”€ products (Parquet, COPY INTO)\n",
    " â†“\n",
    "Silver (Cleansed & Enriched)\n",
    "â”œâ”€â”€ customers (deduplicated, validated)\n",
    "â”œâ”€â”€ orders (categorized, dated)\n",
    "â””â”€â”€ products (normalized, categorized)\n",
    " â†“\n",
    "Gold (Business Analytics)\n",
    "â”œâ”€â”€ customer_analytics\n",
    "â”œâ”€â”€ product_performance\n",
    "â””â”€â”€ sales_summary\n",
    "```\n",
    "\n",
    "**Best Practices zastosowane:**\n",
    "1. Schema evolution z Auto Loader\n",
    "2. Metadata tracking (_source_file, _ingestion_timestamp)\n",
    "3. Data quality validation\n",
    "4. Incremental processing\n",
    "5. IdempotentnoÅ›Ä‡ operacji\n",
    "6. Optymalizacja dla wydajnoÅ›ci\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¹ Cleanup (opcjonalnie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6f7217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UsuÅ„ wszystkie tabele pipeline'u (opcjonalnie)\n",
    "# tables_to_drop = [\n",
    "# \"bronze_customers_pipeline\", \"bronze_orders_pipeline\", \"bronze_products_pipeline\",\n",
    "# \"silver_customers_pipeline\", \"silver_orders_pipeline\", \"silver_products_pipeline\",\n",
    "# \"gold_customer_analytics\", \"gold_product_performance\", \"gold_sales_summary\"\n",
    "# ]\n",
    "\n",
    "# for table in tables_to_drop:\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {CATALOG}.{SCHEMA}.{table}\")\n",
    "# print(f\"Dropped table: {table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70dcc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "```xml\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "# End-to-End Bronze-Silver-Gold Pipeline - Workshop\n",
    "\n",
    "**Cel szkoleniowy:** Zbudowanie kompletnego, produkcyjnego pipeline'u danych od surowych plikÃ³w przez Bronze/Silver do Gold layer z optymalizacjÄ… i monitoringiem.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Raw â†’ Bronze: Ingest z audit metadata\n",
    "- Bronze â†’ Silver: Cleaning, validation, JSON flattening, deduplikacja\n",
    "- Silver â†’ Gold: Business aggregates, KPI modeling, denormalizacja\n",
    "- Performance optimization: partitioning, OPTIMIZE, ZORDER\n",
    "- Monitoring: data quality metrics, lineage tracking\n",
    "\n",
    "**Czas trwania:** 120 minut\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Kontekst i wymagania\n",
    "\n",
    "- **DzieÅ„ szkolenia**: DzieÅ„ 2 - Lakehouse & Delta Lake\n",
    "- **Typ notebooka**: Workshop\n",
    "- **Wymagania techniczne**:\n",
    " - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    " - Unity Catalog wÅ‚Ä…czony\n",
    " - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    " - Klaster: Standard z minimum 2 workers\n",
    "\n",
    "**Business Scenario:**\n",
    "Firma e-commerce potrzebuje pipeline do analizy zamÃ³wieÅ„:\n",
    "- Raw data: Orders (JSON), Customers (CSV), Products (Parquet)\n",
    "- Bronze: Landing zone z audit trail\n",
    "- Silver: Oczyszczone dane z joinami\n",
    "- Gold: Daily sales KPIs, customer segments, product performance\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Izolacja per uÅ¼ytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogÃ³w i schematÃ³w:\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "%run ../../00_setup\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych Å›rodowiskowych:\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# WyÅ›wietl kontekst uÅ¼ytkownika\n",
    "print(\"=== Kontekst uÅ¼ytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"UÅ¼ytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog jako domyÅ›lny\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "\n",
    "# ÅšcieÅ¼ki do danych ÅºrÃ³dÅ‚owych\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "print(f\"\\n=== ÅšcieÅ¼ki do danych ===\")\n",
    "print(f\"Orders: {ORDERS_JSON}\")\n",
    "print(f\"Customers: {CUSTOMERS_CSV}\")\n",
    "print(f\"Products: {PRODUCTS_PARQUET}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Zadanie 1: Bronze Layer - Raw Data Ingestion (20 minut)\n",
    "\n",
    "**Cel:** ZaÅ‚adowanie surowych danych z rÃ³Å¼nych formatÃ³w do Bronze layer z peÅ‚nym audit trail.\n",
    "\n",
    "### Zadanie 1.1: Bronze - Orders (JSON)\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj dane z `orders_batch.json` (format: multiline JSON)\n",
    "2. Dodaj audit columns:\n",
    " - `bronze_ingest_timestamp` (current_timestamp)\n",
    " - `bronze_source_file` (input_file_name)\n",
    " - `bronze_ingested_by` (raw_user)\n",
    " - `bronze_version` (1)\n",
    "3. Zapisz jako `orders_bronze` w Bronze schema\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Tabela Bronze z surowymi danymi JSON + audit metadata\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 1.1 - Bronze Orders\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Wczytaj surowe zamÃ³wienia JSON\n",
    "orders_raw = (\n",
    " spark.read\n",
    " .format(\"____\") # json\n",
    " .option(\"____\", \"true\") # multiLine\n",
    " .load(____) # ORDERS_JSON\n",
    ")\n",
    "\n",
    "print(\"=== Surowe dane orders ===\")\n",
    "orders_raw.printSchema()\n",
    "display(orders_raw.limit(3))\n",
    "\n",
    "# Dodaj audit metadata dla Bronze\n",
    "orders_bronze = (\n",
    " orders_raw\n",
    " .withColumn(\"____\", F.____) # bronze_ingest_timestamp, current_timestamp()\n",
    " .withColumn(\"____\", F.____) # bronze_source_file, input_file_name()\n",
    " .withColumn(\"____\", F.lit(____)) # bronze_ingested_by, raw_user\n",
    " .withColumn(\"____\", F.lit(____)) # bronze_version, 1\n",
    ")\n",
    "\n",
    "# Zapisz do Bronze\n",
    "orders_bronze_table = f\"{BRONZE_SCHEMA}.orders_bronze\"\n",
    "\n",
    "(\n",
    " orders_bronze\n",
    " .write\n",
    " .format(\"____\") # delta\n",
    " .mode(\"____\") # overwrite\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(____) # orders_bronze_table\n",
    ")\n",
    "\n",
    "print(f\"\\n Bronze Orders created: {orders_bronze_table}\")\n",
    "print(f\"Liczba rekordÃ³w: {spark.table(orders_bronze_table).count()}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Zadanie 1.2: Bronze - Customers (CSV) i Products (Parquet)\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj `customers.csv` (header=true, inferSchema=true)\n",
    "2. Wczytaj `products.parquet`\n",
    "3. Dla obu: dodaj te same audit columns co w Zadaniu 1.1\n",
    "4. Zapisz jako `customers_bronze` i `products_bronze`\n",
    "\n",
    "**WskazÃ³wki:**\n",
    "- UÅ¼yj tego samego wzorca audit columns\n",
    "- MoÅ¼esz stworzyÄ‡ funkcjÄ™ helper dla dodania audit metadata\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 1.2 - Bronze Customers & Products\n",
    "\n",
    "# Helper function dla audit metadata (opcjonalnie)\n",
    "def add_bronze_audit(df, version=1):\n",
    " \"\"\"Dodaj standardowe audit columns dla Bronze layer\"\"\"\n",
    " return (\n",
    " df\n",
    " .withColumn(\"bronze_ingest_timestamp\", F.____)\n",
    " .withColumn(\"bronze_source_file\", F.____)\n",
    " .withColumn(\"bronze_ingested_by\", F.lit(____))\n",
    " .withColumn(\"bronze_version\", F.lit(____))\n",
    " )\n",
    "\n",
    "# Customers - CSV\n",
    "customers_raw = (\n",
    " spark.read\n",
    " .format(\"____\")\n",
    " .option(\"header\", \"____\")\n",
    " .option(\"inferSchema\", \"____\")\n",
    " .load(____)\n",
    ")\n",
    "\n",
    "customers_bronze = add_bronze_audit(customers_raw)\n",
    "customers_bronze_table = f\"{BRONZE_SCHEMA}.customers_bronze\"\n",
    "\n",
    "customers_bronze.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(customers_bronze_table)\n",
    "\n",
    "print(f\" Bronze Customers: {customers_bronze_table}\")\n",
    "print(f\" Liczba rekordÃ³w: {spark.table(customers_bronze_table).count()}\")\n",
    "\n",
    "# Products - Parquet\n",
    "products_raw = spark.read.format(\"____\").load(____)\n",
    "\n",
    "products_bronze = add_bronze_audit(____)\n",
    "products_bronze_table = f\"{BRONZE_SCHEMA}.products_bronze\"\n",
    "\n",
    "products_bronze.write.format(\"____\").mode(\"____\").option(\"overwriteSchema\", \"true\").saveAsTable(____)\n",
    "\n",
    "print(f\"\\n Bronze Products: {products_bronze_table}\")\n",
    "print(f\" Liczba rekordÃ³w: {spark.table(products_bronze_table).count()}\")\n",
    "\n",
    "print(\"\\n Wszystkie Bronze tables utworzone!\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Zadanie 2: Silver Layer - Cleaning & Validation (30 minut)\n",
    "\n",
    "**Cel:** Transformacja Bronze â†’ Silver z cleaning, validation, JSON flattening i deduplikacjÄ….\n",
    "\n",
    "### Zadanie 2.1: Silver - Orders (cleaning + validation)\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj z Bronze: `orders_bronze`\n",
    "2. Cleaning & validation:\n",
    " - Deduplikacja po `order_id`\n",
    " - Filtr: `order_id` i `customer_id` NOT NULL\n",
    " - Filtr: `order_amount` > 0\n",
    " - Convert `order_date` z string na date type\n",
    " - Standaryzacja `order_status` (UPPER + TRIM)\n",
    "3. Dodaj Silver metadata:\n",
    " - `silver_processed_timestamp`\n",
    " - `data_quality_flag` = \"VALID\"\n",
    "4. Zapisz jako `orders_silver`\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Oczyszczone zamÃ³wienia w Silver\n",
    "- CzÄ™Å›Ä‡ rekordÃ³w odfiltrowana (invalid data)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 2.1 - Silver Orders\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {SILVER_SCHEMA}\")\n",
    "\n",
    "# Wczytaj Bronze\n",
    "orders_bronze_df = spark.table(orders_bronze_table)\n",
    "\n",
    "# Silver transformations\n",
    "orders_silver = (\n",
    " orders_bronze_df\n",
    " # Deduplikacja\n",
    " .dropDuplicates([____]) # order_id\n",
    " \n",
    " # Walidacja NOT NULL\n",
    " .filter(F.col(\"____\").isNotNull()) # order_id\n",
    " .filter(F.col(\"____\").isNotNull()) # customer_id\n",
    " \n",
    " # Walidacja biznesowa\n",
    " .filter(F.col(\"____\") ____ ____) # order_amount > 0\n",
    " \n",
    " # Type conversion\n",
    " .withColumn(\"order_date\", F.____(F.col(\"order_date\"))) # to_date\n",
    " \n",
    " # Standaryzacja\n",
    " .withColumn(\"order_status\", F.____(F.____(F.col(\"order_status\")))) # upper(trim())\n",
    " \n",
    " # Silver metadata\n",
    " .withColumn(\"____\", F.current_timestamp())\n",
    " .withColumn(\"____\", F.lit(\"VALID\"))\n",
    ")\n",
    "\n",
    "print(\"=== Silver Orders - cleaned data ===\")\n",
    "display(orders_silver.limit(5))\n",
    "\n",
    "# Zapisz do Silver\n",
    "orders_silver_table = f\"{SILVER_SCHEMA}.orders_silver\"\n",
    "\n",
    "orders_silver.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(orders_silver_table)\n",
    "\n",
    "bronze_count = orders_bronze_df.count()\n",
    "silver_count = spark.table(orders_silver_table).count()\n",
    "\n",
    "print(f\"\\n Silver Orders: {orders_silver_table}\")\n",
    "print(f\"Bronze â†’ Silver: {bronze_count} â†’ {silver_count} (filtered: {bronze_count - silver_count})\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Zadanie 2.2: Silver - Customers (advanced cleaning)\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj z Bronze: `customers_bronze`\n",
    "2. Cleaning:\n",
    " - Deduplikacja po `customer_id`\n",
    " - Filtr: `customer_id`, `email` NOT NULL\n",
    " - Standaryzacja: `email` â†’ lowercase, `country` â†’ uppercase\n",
    " - Walidacja: `age` miÄ™dzy 18 a 100\n",
    " - Convert `registration_date` na date type\n",
    "3. Quality flags:\n",
    " - Dodaj kolumnÄ™ `email_domain` (extract domain z email)\n",
    " - Dodaj kolumnÄ™ `customer_segment` (based on age: 18-30=\"Young\", 31-50=\"Middle\", 51+=\"Senior\")\n",
    "4. Zapisz jako `customers_silver`\n",
    "\n",
    "**WskazÃ³wki:**\n",
    "- Email domain: uÅ¼yj `split` i `getItem`\n",
    "- Customer segment: uÅ¼yj `when().otherwise()`\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 2.2 - Silver Customers\n",
    "\n",
    "# Wczytaj Bronze\n",
    "customers_bronze_df = spark.table(customers_bronze_table)\n",
    "\n",
    "# Silver transformations\n",
    "customers_silver = (\n",
    " customers_bronze_df\n",
    " # Deduplikacja\n",
    " .dropDuplicates([____])\n",
    " \n",
    " # Walidacja NOT NULL\n",
    " .filter(F.col(\"____\").isNotNull())\n",
    " .filter(F.col(\"____\").isNotNull())\n",
    " \n",
    " # Standaryzacja\n",
    " .withColumn(\"email\", F.____(F.col(\"email\"))) # lowercase\n",
    " .withColumn(\"country\", F.____(F.col(\"country\"))) # upper\n",
    " \n",
    " # Walidacja wieku\n",
    " .filter((F.col(\"____\") >= ____) & (F.col(\"____\") <= ____)) # age between 18 and 100\n",
    " \n",
    " # Type conversion\n",
    " .withColumn(\"registration_date\", F.____(F.col(\"registration_date\")))\n",
    " \n",
    " # Email domain extraction\n",
    " .withColumn(\"email_domain\", \n",
    " F.split(F.col(\"____\"), \"@\").getItem(____)) # email, 1\n",
    " \n",
    " # Customer segmentation\n",
    " .withColumn(\"customer_segment\",\n",
    " F.when(F.col(\"____\") <= ____, \"____\") # age <= 30, \"Young\"\n",
    " .when(F.col(\"____\") <= ____, \"____\") # age <= 50, \"Middle\"\n",
    " .otherwise(\"____\")) # \"Senior\"\n",
    " \n",
    " # Silver metadata\n",
    " .withColumn(\"silver_processed_timestamp\", F.current_timestamp())\n",
    " .withColumn(\"data_quality_flag\", F.lit(\"VALID\"))\n",
    ")\n",
    "\n",
    "print(\"=== Silver Customers - with segmentation ===\")\n",
    "display(customers_silver.select(\"customer_id\", \"email\", \"age\", \"customer_segment\", \"email_domain\").limit(5))\n",
    "\n",
    "# Zapisz do Silver\n",
    "customers_silver_table = f\"{SILVER_SCHEMA}.customers_silver\"\n",
    "\n",
    "customers_silver.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(customers_silver_table)\n",
    "\n",
    "print(f\"\\n Silver Customers: {customers_silver_table}\")\n",
    "print(f\"Liczba rekordÃ³w: {spark.table(customers_silver_table).count()}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Zadanie 2.3: Silver - Products (validation + enrichment)\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wczytaj z Bronze: `products_bronze`\n",
    "2. Cleaning:\n",
    " - Deduplikacja po `product_id`\n",
    " - Filtr: `product_id`, `product_name` NOT NULL\n",
    " - Walidacja: `unit_price` > 0, `stock_quantity` >= 0\n",
    " - Standaryzacja: `category` â†’ uppercase\n",
    "3. Enrichment:\n",
    " - Dodaj `stock_status`: \"In Stock\" (quantity > 0), \"Out of Stock\" (quantity = 0)\n",
    " - Dodaj `price_tier`: \"Budget\" (<50), \"Standard\" (50-200), \"Premium\" (>200)\n",
    "4. Zapisz jako `products_silver`\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 2.3 - Silver Products\n",
    "\n",
    "# Wczytaj Bronze\n",
    "products_bronze_df = spark.table(products_bronze_table)\n",
    "\n",
    "# Silver transformations\n",
    "products_silver = (\n",
    " products_bronze_df\n",
    " # Deduplikacja\n",
    " .____([____])\n",
    " \n",
    " # Walidacja NOT NULL\n",
    " .filter(F.col(\"____\").isNotNull())\n",
    " .filter(F.col(\"____\").isNotNull())\n",
    " \n",
    " # Walidacja wartoÅ›ci\n",
    " .filter(F.col(\"____\") > ____) # unit_price > 0\n",
    " .filter(F.col(\"____\") >= ____) # stock_quantity >= 0\n",
    " \n",
    " # Standaryzacja\n",
    " .withColumn(\"category\", F.____(F.col(\"category\")))\n",
    " \n",
    " # Stock status\n",
    " .withColumn(\"stock_status\",\n",
    " F.when(F.col(\"____\") ____ ____, \"____\") # stock_quantity > 0, \"In Stock\"\n",
    " .otherwise(\"____\")) # \"Out of Stock\"\n",
    " \n",
    " # Price tier\n",
    " .withColumn(\"price_tier\",\n",
    " F.when(F.col(\"____\") < ____, \"____\") # unit_price < 50, \"Budget\"\n",
    " .when(F.col(\"____\") <= ____, \"____\") # unit_price <= 200, \"Standard\"\n",
    " .otherwise(\"____\")) # \"Premium\"\n",
    " \n",
    " # Silver metadata\n",
    " .withColumn(\"silver_processed_timestamp\", F.current_timestamp())\n",
    " .withColumn(\"data_quality_flag\", F.lit(\"VALID\"))\n",
    ")\n",
    "\n",
    "print(\"=== Silver Products - enriched ===\")\n",
    "display(products_silver.select(\"product_id\", \"product_name\", \"unit_price\", \"stock_quantity\", \"stock_status\", \"price_tier\").limit(5))\n",
    "\n",
    "# Zapisz do Silver\n",
    "products_silver_table = f\"{SILVER_SCHEMA}.products_silver\"\n",
    "\n",
    "products_silver.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(products_silver_table)\n",
    "\n",
    "print(f\"\\n Silver Products: {products_silver_table}\")\n",
    "print(f\"Liczba rekordÃ³w: {spark.table(products_silver_table).count()}\")\n",
    "\n",
    "print(\"\\n Wszystkie Silver tables utworzone!\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Zadanie 3: Gold Layer - Business Aggregates & KPIs (35 minut)\n",
    "\n",
    "**Cel:** Utworzenie Gold layer z business-level agregacjami, KPI i raportami.\n",
    "\n",
    "### Zadanie 3.1: Gold - Daily Sales Summary\n",
    "\n",
    "**Instrukcje:**\n",
    "1. JOIN orders_silver + customers_silver + products_silver\n",
    "2. Agreguj per `order_date`:\n",
    " - `total_orders`: liczba zamÃ³wieÅ„\n",
    " - `total_revenue`: suma order_amount\n",
    " - `avg_order_value`: Å›rednia order_amount\n",
    " - `unique_customers`: distinct customer_id\n",
    " - `unique_products`: distinct product_id\n",
    "3. Dodaj `gold_created_timestamp`\n",
    "4. Partycjonuj po `order_date`\n",
    "5. Zapisz jako `daily_sales_summary`\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Gold table z daily KPIs\n",
    "- Partycjonowana dla efektywnych zapytaÅ„\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 3.1 - Gold Daily Sales Summary\n",
    "\n",
    "spark.sql(f\"USE SCHEMA {GOLD_SCHEMA}\")\n",
    "\n",
    "# Wczytaj Silver tables\n",
    "orders_df = spark.table(orders_silver_table)\n",
    "customers_df = spark.table(customers_silver_table)\n",
    "products_df = spark.table(products_silver_table)\n",
    "\n",
    "# Join orders + customers + products (dla kompletnego kontekstu)\n",
    "# Dla daily summary nie potrzebujemy wszystkich kolumn, ale dla pÃ³Åºniejszych analiz warto mieÄ‡ JOIN\n",
    "orders_enriched = (\n",
    " orders_df\n",
    " .join(customers_df, \"____\", \"____\") # customer_id, left\n",
    " .join(products_df, orders_df[\"____\"] == products_df[\"____\"], \"left\") # product_id (zakÅ‚adamy Å¼e jest w orders)\n",
    ")\n",
    "\n",
    "# UWAGA: Nasze dane orders nie majÄ… product_id - uproszczona wersja\n",
    "# Agregacja Daily Sales Summary (bez products dla uproszczenia)\n",
    "daily_sales = (\n",
    " orders_df\n",
    " .join(customers_df, \"customer_id\", \"left\")\n",
    " .groupBy(\"____\") # order_date\n",
    " .agg(\n",
    " F.count(\"____\").alias(\"____\"), # order_id, total_orders\n",
    " F.sum(\"____\").alias(\"____\"), # order_amount, total_revenue\n",
    " F.avg(\"____\").alias(\"____\"), # order_amount, avg_order_value\n",
    " F.min(\"____\").alias(\"____\"), # order_amount, min_order_value\n",
    " F.max(\"____\").alias(\"____\"), # order_amount, max_order_value\n",
    " F.countDistinct(\"____\").alias(\"____\"), # customer_id, unique_customers\n",
    " F.collect_set(\"____\").alias(\"____\") # order_status, order_statuses (array)\n",
    " )\n",
    " .withColumn(\"____\", F.current_timestamp())\n",
    " .orderBy(\"order_date\")\n",
    ")\n",
    "\n",
    "print(\"=== Gold Daily Sales Summary ===\")\n",
    "display(daily_sales)\n",
    "\n",
    "# Zapisz do Gold z partycjonowaniem\n",
    "daily_sales_table = f\"{GOLD_SCHEMA}.daily_sales_summary\"\n",
    "\n",
    "(\n",
    " daily_sales\n",
    " .write\n",
    " .format(\"delta\")\n",
    " .mode(\"overwrite\")\n",
    " .partitionBy(\"____\") # order_date\n",
    " .option(\"overwriteSchema\", \"true\")\n",
    " .saveAsTable(____)\n",
    ")\n",
    "\n",
    "print(f\"\\n Gold Daily Sales: {daily_sales_table}\")\n",
    "print(f\"Liczba dni: {spark.table(daily_sales_table).count()}\")\n",
    "\n",
    "# SprawdÅº partycje\n",
    "print(\"\\n=== Partycje ===\")\n",
    "display(spark.sql(f\"SHOW PARTITIONS {daily_sales_table}\"))\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Zadanie 3.2: Gold - Customer Lifetime Value (CLV)\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Agreguj per `customer_id`:\n",
    " - `total_orders`: liczba zamÃ³wieÅ„ klienta\n",
    " - `total_spent`: suma order_amount\n",
    " - `avg_order_value`: Å›rednia order_amount\n",
    " - `first_order_date`: min(order_date)\n",
    " - `last_order_date`: max(order_date)\n",
    " - `customer_tenure_days`: rÃ³Å¼nica dni miÄ™dzy first i last order\n",
    "2. JOIN z customers_silver dla kontekstu (country, age, segment)\n",
    "3. Dodaj `clv_tier`: \"High\" (>1000), \"Medium\" (500-1000), \"Low\" (<500)\n",
    "4. Zapisz jako `customer_lifetime_value`\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Gold table z customer-level KPIs\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 3.2 - Gold Customer Lifetime Value\n",
    "\n",
    "# Agregacja per customer\n",
    "customer_orders_agg = (\n",
    " orders_df\n",
    " .groupBy(\"____\") # customer_id\n",
    " .agg(\n",
    " F.count(\"____\").alias(\"____\"), # order_id, total_orders\n",
    " F.sum(\"____\").alias(\"____\"), # order_amount, total_spent\n",
    " F.avg(\"____\").alias(\"____\"), # order_amount, avg_order_value\n",
    " F.min(\"____\").alias(\"____\"), # order_date, first_order_date\n",
    " F.max(\"____\").alias(\"____\") # order_date, last_order_date\n",
    " )\n",
    " # Customer tenure\n",
    " .withColumn(\"customer_tenure_days\",\n",
    " F.datediff(F.col(\"____\"), F.col(\"____\"))) # last_order_date, first_order_date\n",
    ")\n",
    "\n",
    "# JOIN z customers dla kontekstu\n",
    "customer_clv = (\n",
    " customer_orders_agg\n",
    " .join(customers_df, \"____\", \"____\") # customer_id, left\n",
    " # CLV Tier\n",
    " .withColumn(\"clv_tier\",\n",
    " F.when(F.col(\"____\") ____ ____, \"____\") # total_spent > 1000, \"High\"\n",
    " .when(F.col(\"____\") ____ ____, \"____\") # total_spent >= 500, \"Medium\"\n",
    " .otherwise(\"____\")) # \"Low\"\n",
    " \n",
    " .withColumn(\"gold_created_timestamp\", F.current_timestamp())\n",
    " .select(\n",
    " \"customer_id\", \"first_name\", \"last_name\", \"email\", \"country\", \"customer_segment\",\n",
    " \"total_orders\", \"total_spent\", \"avg_order_value\",\n",
    " \"first_order_date\", \"last_order_date\", \"customer_tenure_days\",\n",
    " \"clv_tier\", \"gold_created_timestamp\"\n",
    " )\n",
    " .orderBy(\"total_spent\", ascending=False)\n",
    ")\n",
    "\n",
    "print(\"=== Gold Customer Lifetime Value ===\")\n",
    "display(customer_clv.limit(10))\n",
    "\n",
    "# Zapisz do Gold\n",
    "clv_table = f\"{GOLD_SCHEMA}.customer_lifetime_value\"\n",
    "\n",
    "customer_clv.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(clv_table)\n",
    "\n",
    "print(f\"\\n Gold CLV: {clv_table}\")\n",
    "print(f\"Liczba klientÃ³w: {spark.table(clv_table).count()}\")\n",
    "\n",
    "# CLV Distribution\n",
    "print(\"\\n=== CLV Tier Distribution ===\")\n",
    "display(spark.table(clv_table).groupBy(\"clv_tier\").count().orderBy(\"count\", ascending=False))\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Zadanie 3.3: Gold - Product Performance Summary\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Agreguj per `category` (z products_silver):\n",
    " - `product_count`: liczba produktÃ³w w kategorii\n",
    " - `avg_price`: Å›rednia unit_price\n",
    " - `total_stock_value`: sum(unit_price * stock_quantity)\n",
    " - `out_of_stock_count`: liczba produktÃ³w z stock_quantity = 0\n",
    "2. Dodaj `category_rank` (ranking kategorii po total_stock_value)\n",
    "3. Zapisz jako `product_performance_by_category`\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Gold table z product/category analytics\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 3.3 - Gold Product Performance\n",
    "\n",
    "# Agregacja per category\n",
    "product_performance = (\n",
    " products_df\n",
    " .groupBy(\"____\") # category\n",
    " .agg(\n",
    " F.count(\"____\").alias(\"____\"), # product_id, product_count\n",
    " F.avg(\"____\").alias(\"____\"), # unit_price, avg_price\n",
    " F.sum(F.col(\"____\") * F.col(\"____\")).alias(\"____\"), # unit_price * stock_quantity, total_stock_value\n",
    " F.sum(F.when(F.col(\"____\") == ____, 1).otherwise(0)).alias(\"____\") # stock_quantity == 0, out_of_stock_count\n",
    " )\n",
    " .withColumn(\"gold_created_timestamp\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "# Dodaj category rank (window function)\n",
    "window_spec = Window.orderBy(F.col(\"____\").desc()) # total_stock_value\n",
    "\n",
    "product_performance_ranked = (\n",
    " product_performance\n",
    " .withColumn(\"category_rank\", F.____().over(____)) # row_number(), window_spec\n",
    " .orderBy(\"category_rank\")\n",
    ")\n",
    "\n",
    "print(\"=== Gold Product Performance by Category ===\")\n",
    "display(product_performance_ranked)\n",
    "\n",
    "# Zapisz do Gold\n",
    "product_perf_table = f\"{GOLD_SCHEMA}.product_performance_by_category\"\n",
    "\n",
    "product_performance_ranked.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(product_perf_table)\n",
    "\n",
    "print(f\"\\n Gold Product Performance: {product_perf_table}\")\n",
    "print(f\"Liczba kategorii: {spark.table(product_perf_table).count()}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Zadanie 4: Optymalizacja Pipeline (20 minut)\n",
    "\n",
    "**Cel:** Zastosowanie technik optymalizacji dla production-ready pipeline.\n",
    "\n",
    "### Zadanie 4.1: OPTIMIZE + ZORDER na Silver tables\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Uruchom `OPTIMIZE` na wszystkich Silver tables\n",
    "2. Zastosuj `ZORDER BY`:\n",
    " - `orders_silver`: ZORDER BY (customer_id, order_date)\n",
    " - `customers_silver`: ZORDER BY (customer_id)\n",
    " - `products_silver`: ZORDER BY (product_id)\n",
    "3. SprawdÅº metryki optymalizacji\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Zoptymalizowane Silver tables dla szybkich queries\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 4.1 - OPTIMIZE Silver tables\n",
    "\n",
    "print(\"=== Optymalizacja Silver Tables ===\\n\")\n",
    "\n",
    "# Orders Silver\n",
    "print(\"1. Orders Silver:\")\n",
    "orders_optimize = spark.sql(f\"\"\"\n",
    " ____ {orders_silver_table}\n",
    " ____ BY (____, ____)\n",
    "\"\"\")\n",
    "display(orders_optimize)\n",
    "\n",
    "# Customers Silver\n",
    "print(\"\\n2. Customers Silver:\")\n",
    "customers_optimize = spark.sql(f\"\"\"\n",
    " OPTIMIZE {____}\n",
    " ZORDER BY (____)\n",
    "\"\"\")\n",
    "display(customers_optimize)\n",
    "\n",
    "# Products Silver\n",
    "print(\"\\n3. Products Silver:\")\n",
    "products_optimize = spark.sql(f\"\"\"\n",
    " ____ {____}\n",
    " ____ BY (____)\n",
    "\"\"\")\n",
    "display(products_optimize)\n",
    "\n",
    "print(\"\\n Wszystkie Silver tables zoptymalizowane!\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "### Zadanie 4.2: Performance comparison (przed/po optimization)\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wykonaj query na `orders_silver` z filtrem po `customer_id`\n",
    "2. SprawdÅº `EXPLAIN` dla zobaczenia data skipping\n",
    "3. PorÃ³wnaj query time (symulacja)\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- EXPLAIN pokazuje data skipping (pushed filters)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 4.2 - Performance comparison\n",
    "\n",
    "# Query z filtrem (post-ZORDER)\n",
    "test_query = f\"\"\"\n",
    " SELECT order_id, customer_id, order_date, order_amount\n",
    " FROM {orders_silver_table}\n",
    " WHERE customer_id = 105\n",
    " ORDER BY order_date DESC\n",
    "\"\"\"\n",
    "\n",
    "print(\"=== Query Plan (po ZORDER) ===\")\n",
    "spark.sql(f\"EXPLAIN ____ {test_query}\").show(truncate=False)\n",
    "\n",
    "# Wykonaj query\n",
    "print(\"\\n=== Query Results ===\")\n",
    "result = spark.sql(test_query)\n",
    "display(result)\n",
    "\n",
    "print(f\"\\nLiczba rekordÃ³w: {result.count()}\")\n",
    "\n",
    "# DESCRIBE DETAIL - sprawdÅº numFiles\n",
    "print(\"\\n=== Table Details (po OPTIMIZE) ===\")\n",
    "detail = spark.sql(f\"DESCRIBE DETAIL {orders_silver_table}\")\n",
    "display(detail.select(\"numFiles\", \"sizeInBytes\"))\n",
    "\n",
    "print(\"\\n ZORDER poprawiÅ‚ data skipping - mniej plikÃ³w czytanych dla selective queries!\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Zadanie 5: Monitoring & Data Quality (15 minut)\n",
    "\n",
    "**Cel:** Implementacja monitoringu pipeline i data quality metrics.\n",
    "\n",
    "### Zadanie 5.1: Data Quality Metrics\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Dla kaÅ¼dej warstwy (Bronze/Silver/Gold) oblicz:\n",
    " - Liczba rekordÃ³w\n",
    " - Liczba plikÃ³w\n",
    " - Rozmiar w MB\n",
    " - Data freshness (max timestamp)\n",
    "2. UtwÃ³rz summary DataFrame\n",
    "3. Zapisz jako `pipeline_monitoring_summary` w Gold\n",
    "\n",
    "**Oczekiwany rezultat:**\n",
    "- Monitoring dashboard data dla pipeline\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# TODO: Zadanie 5.1 - Data Quality Metrics\n",
    "\n",
    "def get_table_metrics(table_name, layer):\n",
    " \"\"\"Pobierz metryki dla tabeli\"\"\"\n",
    " detail = spark.sql(f\"DESCRIBE DETAIL {table_name}\").collect()[0]\n",
    " \n",
    " df = spark.table(table_name)\n",
    " count = df.count()\n",
    " \n",
    " # ZnajdÅº timestamp column (rÃ³Å¼ne nazwy per layer)\n",
    " timestamp_col = None\n",
    " if \"bronze_ingest_timestamp\" in df.columns:\n",
    " timestamp_col = \"bronze_ingest_timestamp\"\n",
    " elif \"silver_processed_timestamp\" in df.columns:\n",
    " timestamp_col = \"silver_processed_timestamp\"\n",
    " elif \"gold_created_timestamp\" in df.columns:\n",
    " timestamp_col = \"gold_created_timestamp\"\n",
    " \n",
    " max_ts = df.agg(F.max(timestamp_col)).collect()[0][0] if timestamp_col else None\n",
    " \n",
    " return {\n",
    " \"layer\": layer,\n",
    " \"table_name\": table_name.split(\".\")[-1],\n",
    " \"record_count\": count,\n",
    " \"num_files\": detail[\"numFiles\"],\n",
    " \"size_mb\": round(detail[\"sizeInBytes\"] / (1024*1024), 2),\n",
    " \"data_freshness\": max_ts,\n",
    " \"monitored_at\": datetime.now()\n",
    " }\n",
    "\n",
    "# Zbierz metryki dla wszystkich tabel\n",
    "metrics = []\n",
    "\n",
    "# Bronze\n",
    "metrics.append(get_table_metrics(____, \"____\")) # orders_bronze_table, \"Bronze\"\n",
    "metrics.append(get_table_metrics(____, \"____\"))\n",
    "metrics.append(get_table_metrics(____, \"____\"))\n",
    "\n",
    "# Silver\n",
    "metrics.append(get_table_metrics(____, \"____\"))\n",
    "metrics.append(get_table_metrics(____, \"____\"))\n",
    "metrics.append(get_table_metrics(____, \"____\"))\n",
    "\n",
    "# Gold\n",
    "metrics.append(get_table_metrics(____, \"____\"))\n",
    "metrics.append(get_table_metrics(____, \"____\"))\n",
    "metrics.append(get_table_metrics(____, \"____\"))\n",
    "\n",
    "# UtwÃ³rz summary DataFrame\n",
    "monitoring_df = spark.createDataFrame(metrics)\n",
    "\n",
    "print(\"=== Pipeline Monitoring Summary ===\")\n",
    "display(monitoring_df.orderBy(\"layer\", \"table_name\"))\n",
    "\n",
    "# Zapisz do Gold\n",
    "monitoring_table = f\"{GOLD_SCHEMA}.pipeline_monitoring_summary\"\n",
    "\n",
    "monitoring_df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").saveAsTable(monitoring_table)\n",
    "\n",
    "print(f\"\\n Monitoring summary: {monitoring_table}\")\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Walidacja i weryfikacja\n",
    "\n",
    "### Checklist - Co powinieneÅ› uzyskaÄ‡:\n",
    "- [ ] Bronze: 3 tabele (orders, customers, products) z audit metadata\n",
    "- [ ] Silver: 3 tabele (orders, customers, products) - cleaned, validated, enriched\n",
    "- [ ] Gold: 3 tabele (daily_sales, customer_clv, product_performance)\n",
    "- [ ] Optymalizacja: OPTIMIZE + ZORDER na Silver tables\n",
    "- [ ] Monitoring: pipeline_monitoring_summary w Gold\n",
    "- [ ] Partycjonowanie: daily_sales_summary partitioned by order_date\n",
    "\n",
    "### Komendy weryfikacyjne:\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Weryfikacja koÅ„cowa pipeline\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"WERYFIKACJA END-TO-END PIPELINE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Bronze Layer\n",
    "print(\"\\n1. BRONZE LAYER:\")\n",
    "bronze_tables = [\"orders_bronze\", \"customers_bronze\", \"products_bronze\"]\n",
    "for table in bronze_tables:\n",
    " full_name = f\"{BRONZE_SCHEMA}.{table}\"\n",
    " count = spark.table(full_name).count()\n",
    " print(f\" {table}: {count} records\")\n",
    "\n",
    "# 2. Silver Layer\n",
    "print(\"\\n2. SILVER LAYER:\")\n",
    "silver_tables = [\"orders_silver\", \"customers_silver\", \"products_silver\"]\n",
    "for table in silver_tables:\n",
    " full_name = f\"{SILVER_SCHEMA}.{table}\"\n",
    " count = spark.table(full_name).count()\n",
    " print(f\" {table}: {count} records\")\n",
    "\n",
    "# 3. Gold Layer\n",
    "print(\"\\n3. GOLD LAYER:\")\n",
    "gold_tables = [\"daily_sales_summary\", \"customer_lifetime_value\", \"product_performance_by_category\"]\n",
    "for table in gold_tables:\n",
    " full_name = f\"{GOLD_SCHEMA}.{table}\"\n",
    " count = spark.table(full_name).count()\n",
    " print(f\" {table}: {count} records\")\n",
    "\n",
    "# 4. SprawdÅº partycjonowanie\n",
    "print(\"\\n4. PARTITIONING:\")\n",
    "partitions = spark.sql(f\"SHOW PARTITIONS {daily_sales_table}\")\n",
    "print(f\" daily_sales_summary partitions: {partitions.count()}\")\n",
    "\n",
    "# 5. SprawdÅº optymalizacjÄ™\n",
    "print(\"\\n5. OPTIMIZATION:\")\n",
    "for table in [orders_silver_table, customers_silver_table, products_silver_table]:\n",
    " detail = spark.sql(f\"DESCRIBE DETAIL {table}\").collect()[0]\n",
    " print(f\" {table.split('.')[-1]}: {detail['numFiles']} files\")\n",
    "\n",
    "# 6. Data Quality Check\n",
    "print(\"\\n6. DATA QUALITY:\")\n",
    "print(\" Silver filtering effectiveness:\")\n",
    "bronze_orders = spark.table(orders_bronze_table).count()\n",
    "silver_orders = spark.table(orders_silver_table).count()\n",
    "filtered_pct = ((bronze_orders - silver_orders) / bronze_orders * 100) if bronze_orders > 0 else 0\n",
    "print(f\" Orders: {filtered_pct:.1f}% filtered out (quality issues)\")\n",
    "\n",
    "# 7. Monitoring\n",
    "print(\"\\n7. MONITORING:\")\n",
    "if spark.catalog.tableExists(monitoring_table):\n",
    " print(f\" Monitoring summary exists: {monitoring_table}\")\n",
    " display(spark.table(monitoring_table))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\" WSZYSTKIE TESTY PRZESZÅY POMYÅšLNIE!\")\n",
    "print(\"=\" * 80)\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "**W tym warsztacie zbudowaÅ‚eÅ›:**\n",
    "\n",
    " **Kompletny pipeline Bronze â†’ Silver â†’ Gold:**\n",
    "- **Bronze**: 3 ÅºrÃ³dÅ‚a danych (JSON, CSV, Parquet) + audit trail\n",
    "- **Silver**: Cleaning, validation, enrichment, segmentation\n",
    "- **Gold**: Business KPIs - daily sales, customer CLV, product performance\n",
    "\n",
    " **Zaawansowane transformacje:**\n",
    "- Type conversion i validation (dates, numerics)\n",
    "- Standaryzacja (case, trim, format)\n",
    "- Enrichment (segmentation, tiers, derived columns)\n",
    "- Window functions (ranking)\n",
    "\n",
    " **Production best practices:**\n",
    "- Partycjonowanie dla performance (date partitions)\n",
    "- OPTIMIZE + ZORDER dla query optimization\n",
    "- Monitoring i data quality metrics\n",
    "- Lineage tracking (audit metadata)\n",
    "\n",
    "**Kluczowe metryki pipeline:**\n",
    "\n",
    "| Layer | Tables | Records | Purpose |\n",
    "|-------|--------|---------|---------|\n",
    "| Bronze | 3 | ~X,XXX | Raw data landing + audit |\n",
    "| Silver | 3 | ~X,XXX | Cleaned, validated, enriched |\n",
    "| Gold | 3 | ~XXX | Business KPIs, aggregates |\n",
    "\n",
    "**Performance improvements:**\n",
    "- OPTIMIZE: Reduced small files by XX%\n",
    "- ZORDER: Improved selective queries by X-XXx\n",
    "- Partitioning: Partition pruning for date queries\n",
    "\n",
    "**NastÄ™pne kroki:**\n",
    "- **Delta Live Tables**: Automatyczne utrzymanie tego pipeline\n",
    "- **Databricks Jobs**: Orchestration i scheduling\n",
    "- **Unity Catalog**: Governance i permissions per layer\n",
    "- **BI Integration**: Connect Gold tables to Power BI/Tableau\n",
    "\n",
    "**Gratulacje! Masz produkcyjny, zoptymalizowany pipeline Medallion Architecture!** \n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"markdown\">\n",
    "---\n",
    "\n",
    "## Cleanup\n",
    "\n",
    "Opcjonalnie: usuÅ„ utworzone tabele po zakoÅ„czeniu warsztatu:\n",
    "</VSCode.Cell>\n",
    "<VSCode.Cell language=\"python\">\n",
    "# Opcjonalne czyszczenie zasobÃ³w\n",
    "# UWAGA: Uruchom tylko jeÅ›li chcesz usunÄ…Ä‡ wszystkie utworzone dane\n",
    "\n",
    "# Bronze\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {orders_bronze_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {customers_bronze_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {products_bronze_table}\")\n",
    "\n",
    "# Silver\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {orders_silver_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {customers_silver_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {products_silver_table}\")\n",
    "\n",
    "# Gold\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {daily_sales_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {clv_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {product_perf_table}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {monitoring_table}\")\n",
    "\n",
    "# spark.catalog.clearCache()\n",
    "# print(\"Zasoby zostaÅ‚y wyczyszczone\")\n",
    "</VSCode.Cell>\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}