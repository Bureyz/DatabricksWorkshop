{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0aaf6c",
   "metadata": {},
   "source": [
    "# Warsztat 2: Ingestion Pipeline - COPY INTO & Auto Loader\n",
    "\n",
    "**Cel warsztatu:**\n",
    "- Implementacja batch ingestion uÅ¼ywajÄ…c COPY INTO\n",
    "- Konfiguracja Auto Loader dla streaming ingestion\n",
    "- ObsÅ‚uga rÃ³Å¼nych formatÃ³w plikÃ³w (CSV, JSON, Parquet)\n",
    "- Monitorowanie i zarzÄ…dzanie pipeline'ami ingestion\n",
    "\n",
    "**Czas:** 90 minut\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420c2ef",
   "metadata": {},
   "source": [
    "## ðŸ“š Inicjalizacja Å›rodowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9347b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0635808",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ CzÄ™Å›Ä‡ 1: COPY INTO - Batch Ingestion\n",
    "\n",
    "### Zadanie 1.1: Ingestion plikÃ³w CSV\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Przygotuj tabelÄ™ docelowÄ… `bronze_customers_batch`\n",
    "2. UÅ¼yj `COPY INTO` do zaÅ‚adowania danych z `customers.csv`\n",
    "3. Zweryfikuj liczbÄ™ zaÅ‚adowanych rekordÃ³w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: UtwÃ³rz tabelÄ™ docelowÄ…\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_customers_batch (\n",
    "        customer_id INT,\n",
    "        name STRING,\n",
    "        email STRING,\n",
    "        city STRING,\n",
    "        country STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    LOCATION '{____}/customers_batch'  -- UzupeÅ‚nij BRONZE_PATH\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b63d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COPY INTO z pliku CSV\n",
    "spark.sql(f\"\"\"\n",
    "    ____ INTO {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            name,\n",
    "            email,\n",
    "            city,\n",
    "            country,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{____}/____'  -- UzupeÅ‚nij SOURCE_DATA_PATH i nazwÄ™ pliku\n",
    "    )\n",
    "    FILEFORMAT = ____  -- UzupeÅ‚nij format (CSV)\n",
    "    FORMAT_OPTIONS (\n",
    "        'header' = '____',  -- Czy plik ma nagÅ‚Ã³wek?\n",
    "        'inferSchema' = '____'  -- Czy inferowaÄ‡ schemat?\n",
    "    )\n",
    "    COPY_OPTIONS (\n",
    "        'mergeSchema' = '____'  -- Czy mergowaÄ‡ schemat?\n",
    "    )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as total_records \n",
    "    FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cffcff",
   "metadata": {},
   "source": [
    "### Zadanie 1.2: Ingestion plikÃ³w JSON\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Przygotuj tabelÄ™ `bronze_orders_batch`\n",
    "2. UÅ¼yj `COPY INTO` do zaÅ‚adowania danych z `orders_batch.json`\n",
    "3. ObsÅ‚uÅ¼ zagnieÅ¼dÅ¼onÄ… strukturÄ™ JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e72509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: UtwÃ³rz tabelÄ™ dla zamÃ³wieÅ„\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_orders_batch (\n",
    "        order_id INT,\n",
    "        customer_id INT,\n",
    "        order_date DATE,\n",
    "        total_amount DOUBLE,\n",
    "        status STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING ____  -- UzupeÅ‚nij format (DELTA)\n",
    "    LOCATION '{____}/orders_batch'  -- UzupeÅ‚nij Å›cieÅ¼kÄ™\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COPY INTO z pliku JSON\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{SCHEMA}.bronze_orders_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            order_id,\n",
    "            customer_id,\n",
    "            TO_DATE(order_date) as order_date,\n",
    "            total_amount,\n",
    "            status,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{____}/____'  -- UzupeÅ‚nij Å›cieÅ¼kÄ™ do JSON\n",
    "    )\n",
    "    FILEFORMAT = ____  -- UzupeÅ‚nij format\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM {CATALOG}.{SCHEMA}.bronze_orders_batch LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44cfc5",
   "metadata": {},
   "source": [
    "### Zadanie 1.3: Ingestion plikÃ³w Parquet\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Przygotuj tabelÄ™ `bronze_products_batch`\n",
    "2. UÅ¼yj `COPY INTO` do zaÅ‚adowania danych z `products.parquet`\n",
    "3. Dodaj kolumnÄ™ z metadanymi pliku ÅºrÃ³dÅ‚owego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5561ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: UtwÃ³rz tabelÄ™ dla produktÃ³w\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_products_batch (\n",
    "        product_id INT,\n",
    "        product_name STRING,\n",
    "        category STRING,\n",
    "        price DOUBLE,\n",
    "        stock_quantity INT,\n",
    "        _source_file STRING,\n",
    "        _ingestion_timestamp TIMESTAMP\n",
    "    )\n",
    "    USING DELTA\n",
    "    LOCATION '{____}/products_batch'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316176df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COPY INTO z pliku Parquet\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{SCHEMA}.bronze_products_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            product_id,\n",
    "            product_name,\n",
    "            category,\n",
    "            price,\n",
    "            stock_quantity,\n",
    "            ____ as _source_file,  -- UÅ¼yj _metadata.file_path\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{____}/____'  -- UzupeÅ‚nij Å›cieÅ¼kÄ™ do Parquet\n",
    "    )\n",
    "    FILEFORMAT = ____\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b754ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT product_id, product_name, category, _source_file \n",
    "    FROM {CATALOG}.{SCHEMA}.bronze_products_batch \n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf241e",
   "metadata": {},
   "source": [
    "### Zadanie 1.4: IdempotentnoÅ›Ä‡ - Ponowne uruchomienie COPY INTO\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Uruchom ponownie `COPY INTO` dla tej samej tabeli\n",
    "2. Zweryfikuj Å¼e dane nie zostaÅ‚y zduplikowane\n",
    "3. SprawdÅº historiÄ™ operacji `COPY INTO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbb03d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SprawdÅº liczbÄ™ rekordÃ³w przed ponownym COPY INTO\n",
    "before_count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as count \n",
    "    FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").collect()[0][\"count\"]\n",
    "\n",
    "print(f\"Liczba rekordÃ³w przed ponownym COPY INTO: {before_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c1cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ponowne wykonanie COPY INTO\n",
    "spark.sql(f\"\"\"\n",
    "    COPY INTO {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "    FROM (\n",
    "        SELECT \n",
    "            customer_id,\n",
    "            name,\n",
    "            email,\n",
    "            city,\n",
    "            country,\n",
    "            current_timestamp() as _ingestion_timestamp\n",
    "        FROM '{SOURCE_DATA_PATH}/customers.csv'\n",
    "    )\n",
    "    FILEFORMAT = CSV\n",
    "    FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'true')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8805ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja idempotentnoÅ›ci\n",
    "after_count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as count \n",
    "    FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").collect()[0][\"count\"]\n",
    "\n",
    "print(f\"Liczba rekordÃ³w po ponownym COPY INTO: {after_count}\")\n",
    "print(f\"Czy dane zostaÅ‚y zduplikowane? {before_count != after_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9637c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”„ CzÄ™Å›Ä‡ 2: Auto Loader - Streaming Ingestion\n",
    "\n",
    "### Zadanie 2.1: Konfiguracja Auto Loader dla CSV\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Przygotuj lokalizacjÄ™ checkpointu\n",
    "2. UÅ¼yj `.format(\"cloudFiles\")` do utworzenia streaming read\n",
    "3. Skonfiguruj schema inference i evolution\n",
    "4. Zapisz stream do tabeli `bronze_customers_stream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cad73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Streaming read z Auto Loader\n",
    "customers_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"____\")  # UzupeÅ‚nij format (cloudFiles)\n",
    "    .option(\"cloudFiles.format\", \"____\")  # Format plikÃ³w ÅºrÃ³dÅ‚owych (csv)\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/____\")  # Checkpoint dla schematu\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(f\"{SOURCE_DATA_PATH}/customers.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba519b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dodaj kolumny metadata\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "customers_enriched = (\n",
    "    customers_stream\n",
    "    .withColumn(\"_ingestion_timestamp\", ____)  # Dodaj timestamp\n",
    "    .withColumn(\"_source_file\", ____)  # Dodaj nazwÄ™ pliku ÅºrÃ³dÅ‚owego\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zapisz stream do tabeli Delta\n",
    "query_customers = (\n",
    "    customers_enriched.writeStream\n",
    "    .format(\"____\")  # UzupeÅ‚nij format\n",
    "    .outputMode(\"____\")  # Tryb zapisu (append)\n",
    "    .option(\"checkpointLocation\", f\"{____}/customers_stream\")  # Checkpoint\n",
    "    .option(\"mergeSchema\", \"____\")  # Schema evolution\n",
    "    .table(f\"{CATALOG}.{SCHEMA}.bronze_customers_stream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6114af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja streamu\n",
    "import time\n",
    "time.sleep(10)  # Poczekaj na przetworzenie\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as total_records \n",
    "    FROM {CATALOG}.{SCHEMA}.bronze_customers_stream\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130ce52",
   "metadata": {},
   "source": [
    "### Zadanie 2.2: Auto Loader dla JSON z Schema Hints\n",
    "\n",
    "**Instrukcje:**\n",
    "1. UÅ¼yj Auto Loader do odczytu `orders_batch.json`\n",
    "2. Dodaj schema hints dla kolumn z konkretnymi typami\n",
    "3. Skonfiguruj rescue data column dla niepoprawnych rekordÃ³w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc65dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Auto Loader z schema hints\n",
    "orders_stream = (\n",
    "    spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"____\")  # Format JSON\n",
    "    .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/orders_schema\")\n",
    "    .option(\"cloudFiles.schemaHints\", \"____\")  # PodpowiedÅº: \"order_date DATE, total_amount DOUBLE\"\n",
    "    .option(\"cloudFiles.rescuedDataColumn\", \"____\")  # Kolumna dla rescue data\n",
    "    .load(f\"{____}/orders_batch.json\")  # UzupeÅ‚nij Å›cieÅ¼kÄ™\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c54b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zapisz stream\n",
    "query_orders = (\n",
    "    orders_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"____\")  # Tryb append\n",
    "    .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/____\")  # Checkpoint\n",
    "    .table(f\"{CATALOG}.{SCHEMA}.bronze_orders_stream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc4725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja\n",
    "time.sleep(10)\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM {CATALOG}.{SCHEMA}.bronze_orders_stream LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645db40",
   "metadata": {},
   "source": [
    "### Zadanie 2.3: Monitoring streaming queries\n",
    "\n",
    "**Instrukcje:**\n",
    "1. WyÅ›wietl aktywne streaming queries\n",
    "2. SprawdÅº status i ostatni progress kaÅ¼dego query\n",
    "3. Pobierz metryki: liczba przetworzonych rekordÃ³w, batch duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a1321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: WyÅ›wietl aktywne streamy\n",
    "active_streams = spark.streams.____  # UzupeÅ‚nij metodÄ™ (active)\n",
    "\n",
    "print(f\"Liczba aktywnych streamÃ³w: {len(active_streams)}\")\n",
    "for stream in active_streams:\n",
    "    print(f\"\\nStream ID: {stream.id}\")\n",
    "    print(f\"Name: {stream.name}\")\n",
    "    print(f\"Status: {stream.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6974ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: SprawdÅº ostatni progress\n",
    "if len(active_streams) > 0:\n",
    "    last_progress = active_streams[0].____  # UzupeÅ‚nij metodÄ™ (lastProgress)\n",
    "    \n",
    "    if last_progress:\n",
    "        print(f\"Batch ID: {last_progress['batchId']}\")\n",
    "        print(f\"Przetworzone rekordy: {last_progress['numInputRows']}\")\n",
    "        print(f\"Czas przetwarzania: {last_progress['batchDuration']} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55841f5d",
   "metadata": {},
   "source": [
    "### Zadanie 2.4: Zatrzymanie streaming queries\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Zatrzymaj wszystkie aktywne streaming queries\n",
    "2. Zweryfikuj Å¼e wszystkie streamy sÄ… zatrzymane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae308ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zatrzymaj wszystkie streamy\n",
    "for stream in spark.streams.active:\n",
    "    print(f\"ZatrzymujÄ™ stream: {stream.name}\")\n",
    "    stream.____()  # UzupeÅ‚nij metodÄ™ (stop)\n",
    "\n",
    "print(\"\\nWszystkie streamy zatrzymane!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c772d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja\n",
    "print(f\"Liczba aktywnych streamÃ³w: {len(spark.streams.active)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9819090",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“Š CzÄ™Å›Ä‡ 3: PorÃ³wnanie COPY INTO vs Auto Loader\n",
    "\n",
    "### Zadanie 3.1: Analiza wydajnoÅ›ci\n",
    "\n",
    "**Instrukcje:**\n",
    "1. PorÃ³wnaj liczbÄ™ rekordÃ³w zaÅ‚adowanych przez COPY INTO vs Auto Loader\n",
    "2. SprawdÅº historiÄ™ operacji dla obu metod\n",
    "3. Zidentyfikuj przypadki uÅ¼ycia dla kaÅ¼dej metody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbad7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PorÃ³wnanie liczby rekordÃ³w\n",
    "copy_into_count = spark.sql(f\"\"\"\n",
    "    SELECT 'COPY INTO' as method, COUNT(*) as records \n",
    "    FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\")\n",
    "\n",
    "auto_loader_count = spark.sql(f\"\"\"\n",
    "    SELECT 'Auto Loader' as method, COUNT(*) as records \n",
    "    FROM {CATALOG}.{SCHEMA}.bronze_customers_stream\n",
    "\"\"\")\n",
    "\n",
    "copy_into_count.union(auto_loader_count).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6833f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Historia operacji COPY INTO\n",
    "spark.sql(f\"\"\"\n",
    "    ____ HISTORY {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").select(\"version\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd43d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Historia operacji Auto Loader\n",
    "spark.sql(f\"\"\"\n",
    "    DESCRIBE HISTORY {CATALOG}.{SCHEMA}.bronze_customers_stream\n",
    "\"\"\").select(\"version\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956008d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## âœ… Podsumowanie warsztatu\n",
    "\n",
    "**Zrealizowane cele:**\n",
    "- âœ… Implementacja batch ingestion z COPY INTO\n",
    "- âœ… Konfiguracja Auto Loader dla streaming ingestion\n",
    "- âœ… ObsÅ‚uga rÃ³Å¼nych formatÃ³w (CSV, JSON, Parquet)\n",
    "- âœ… Monitoring i zarzÄ…dzanie pipeline'ami\n",
    "\n",
    "**Kiedy uÅ¼yÄ‡ COPY INTO:**\n",
    "- Batch processing z okreÅ›lonym harmonogramem\n",
    "- Znana i stabilna struktura danych\n",
    "- Potrzeba kontroli nad procesem Å‚adowania\n",
    "- IdempotentnoÅ›Ä‡ out-of-the-box\n",
    "\n",
    "**Kiedy uÅ¼yÄ‡ Auto Loader:**\n",
    "- Near real-time processing\n",
    "- Schema evolution i automatyczna inferencja\n",
    "- CiÄ…gÅ‚e monitorowanie nowych plikÃ³w\n",
    "- SkalowalnoÅ›Ä‡ i efektywnoÅ›Ä‡ kosztowa\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§¹ Cleanup (opcjonalnie)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
