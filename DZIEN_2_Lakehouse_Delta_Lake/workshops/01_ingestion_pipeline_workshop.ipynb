{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b0aaf6c",
   "metadata": {},
   "source": [
    "# Warsztat 2: Ingestion Pipeline - COPY INTO & Auto Loader\n",
    "\n",
    "**Cel warsztatu:**\n",
    "- Implementacja batch ingestion u偶ywajc COPY INTO\n",
    "- Konfiguracja Auto Loader dla streaming ingestion\n",
    "- Obsuga r贸偶nych format贸w plik贸w (CSV, JSON, Parquet)\n",
    "- Monitorowanie i zarzdzanie pipeline'ami ingestion\n",
    "\n",
    "**Czas:** 90 minut\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4420c2ef",
   "metadata": {},
   "source": [
    "##  Inicjalizacja rodowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a9347b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0635808",
   "metadata": {},
   "source": [
    "## Cz 1: COPY INTO - Batch Ingestion\n",
    "\n",
    "### Zadanie 1.1: Ingestion plik贸w CSV\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Przygotuj tabel docelow `bronze_customers_batch`\n",
    "2. U偶yj `COPY INTO` do zaadowania danych z `customers.csv`\n",
    "3. Zweryfikuj liczb zaadowanych rekord贸w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c13add1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utw贸rz tabel docelow\n",
    "spark.sql(f\"\"\"\n",
    " CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_customers_batch (\n",
    " customer_id INT,\n",
    " name STRING,\n",
    " email STRING,\n",
    " city STRING,\n",
    " country STRING,\n",
    " _ingestion_timestamp TIMESTAMP\n",
    " )\n",
    " USING DELTA\n",
    " LOCATION '{____}/customers_batch' -- Uzupenij BRONZE_PATH\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b63d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COPY INTO z pliku CSV\n",
    "spark.sql(f\"\"\"\n",
    " ____ INTO {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    " FROM (\n",
    " SELECT \n",
    " customer_id,\n",
    " name,\n",
    " email,\n",
    " city,\n",
    " country,\n",
    " current_timestamp() as _ingestion_timestamp\n",
    " FROM '{____}/____' -- Uzupenij SOURCE_DATA_PATH i nazw pliku\n",
    " )\n",
    " FILEFORMAT = ____ -- Uzupenij format (CSV)\n",
    " FORMAT_OPTIONS (\n",
    " 'header' = '____', -- Czy plik ma nag贸wek?\n",
    " 'inferSchema' = '____' -- Czy inferowa schemat?\n",
    " )\n",
    " COPY_OPTIONS (\n",
    " 'mergeSchema' = '____' -- Czy mergowa schemat?\n",
    " )\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae99837b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja\n",
    "spark.sql(f\"\"\"\n",
    " SELECT COUNT(*) as total_records \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cffcff",
   "metadata": {},
   "source": [
    "### Zadanie 1.2: Ingestion plik贸w JSON\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Przygotuj tabel `bronze_orders_batch`\n",
    "2. U偶yj `COPY INTO` do zaadowania danych z `orders_batch.json`\n",
    "3. Obsu偶 zagnie偶d偶on struktur JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e72509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utw贸rz tabel dla zam贸wie\n",
    "spark.sql(f\"\"\"\n",
    " CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_orders_batch (\n",
    " order_id INT,\n",
    " customer_id INT,\n",
    " order_date DATE,\n",
    " total_amount DOUBLE,\n",
    " status STRING,\n",
    " _ingestion_timestamp TIMESTAMP\n",
    " )\n",
    " USING ____ -- Uzupenij format (DELTA)\n",
    " LOCATION '{____}/orders_batch' -- Uzupenij cie偶k\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4554a627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COPY INTO z pliku JSON\n",
    "spark.sql(f\"\"\"\n",
    " COPY INTO {CATALOG}.{SCHEMA}.bronze_orders_batch\n",
    " FROM (\n",
    " SELECT \n",
    " order_id,\n",
    " customer_id,\n",
    " TO_DATE(order_date) as order_date,\n",
    " total_amount,\n",
    " status,\n",
    " current_timestamp() as _ingestion_timestamp\n",
    " FROM '{____}/____' -- Uzupenij cie偶k do JSON\n",
    " )\n",
    " FILEFORMAT = ____ -- Uzupenij format\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a9d3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja\n",
    "spark.sql(f\"\"\"\n",
    " SELECT * FROM {CATALOG}.{SCHEMA}.bronze_orders_batch LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a44cfc5",
   "metadata": {},
   "source": [
    "### Zadanie 1.3: Ingestion plik贸w Parquet\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Przygotuj tabel `bronze_products_batch`\n",
    "2. U偶yj `COPY INTO` do zaadowania danych z `products.parquet`\n",
    "3. Dodaj kolumn z metadanymi pliku 藕r贸dowego"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e5561ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utw贸rz tabel dla produkt贸w\n",
    "spark.sql(f\"\"\"\n",
    " CREATE TABLE IF NOT EXISTS {CATALOG}.{SCHEMA}.bronze_products_batch (\n",
    " product_id INT,\n",
    " product_name STRING,\n",
    " category STRING,\n",
    " price DOUBLE,\n",
    " stock_quantity INT,\n",
    " _source_file STRING,\n",
    " _ingestion_timestamp TIMESTAMP\n",
    " )\n",
    " USING DELTA\n",
    " LOCATION '{____}/products_batch'\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316176df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: COPY INTO z pliku Parquet\n",
    "spark.sql(f\"\"\"\n",
    " COPY INTO {CATALOG}.{SCHEMA}.bronze_products_batch\n",
    " FROM (\n",
    " SELECT \n",
    " product_id,\n",
    " product_name,\n",
    " category,\n",
    " price,\n",
    " stock_quantity,\n",
    " ____ as _source_file, -- U偶yj _metadata.file_path\n",
    " current_timestamp() as _ingestion_timestamp\n",
    " FROM '{____}/____' -- Uzupenij cie偶k do Parquet\n",
    " )\n",
    " FILEFORMAT = ____\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b754ecd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja\n",
    "spark.sql(f\"\"\"\n",
    " SELECT product_id, product_name, category, _source_file \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_products_batch \n",
    " LIMIT 10\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf241e",
   "metadata": {},
   "source": [
    "### Zadanie 1.4: Idempotentno - Ponowne uruchomienie COPY INTO\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Uruchom ponownie `COPY INTO` dla tej samej tabeli\n",
    "2. Zweryfikuj 偶e dane nie zostay zduplikowane\n",
    "3. Sprawd藕 histori operacji `COPY INTO`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbb03d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sprawd藕 liczb rekord贸w przed ponownym COPY INTO\n",
    "before_count = spark.sql(f\"\"\"\n",
    " SELECT COUNT(*) as count \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").collect()[0][\"count\"]\n",
    "\n",
    "print(f\"Liczba rekord贸w przed ponownym COPY INTO: {before_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c1cfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Ponowne wykonanie COPY INTO\n",
    "spark.sql(f\"\"\"\n",
    " COPY INTO {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    " FROM (\n",
    " SELECT \n",
    " customer_id,\n",
    " name,\n",
    " email,\n",
    " city,\n",
    " country,\n",
    " current_timestamp() as _ingestion_timestamp\n",
    " FROM '{SOURCE_DATA_PATH}/customers.csv'\n",
    " )\n",
    " FILEFORMAT = CSV\n",
    " FORMAT_OPTIONS ('header' = 'true', 'inferSchema' = 'true')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8805ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja idempotentnoci\n",
    "after_count = spark.sql(f\"\"\"\n",
    " SELECT COUNT(*) as count \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").collect()[0][\"count\"]\n",
    "\n",
    "print(f\"Liczba rekord贸w po ponownym COPY INTO: {after_count}\")\n",
    "print(f\"Czy dane zostay zduplikowane? {before_count != after_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d9637c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Cz 2: Auto Loader - Streaming Ingestion\n",
    "\n",
    "### Zadanie 2.1: Konfiguracja Auto Loader dla CSV\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Przygotuj lokalizacj checkpointu\n",
    "2. U偶yj `.format(\"cloudFiles\")` do utworzenia streaming read\n",
    "3. Skonfiguruj schema inference i evolution\n",
    "4. Zapisz stream do tabeli `bronze_customers_stream`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cad73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Streaming read z Auto Loader\n",
    "customers_stream = (\n",
    " spark.readStream\n",
    " .format(\"____\") # Uzupenij format (cloudFiles)\n",
    " .option(\"cloudFiles.format\", \"____\") # Format plik贸w 藕r贸dowych (csv)\n",
    " .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/____\") # Checkpoint dla schematu\n",
    " .option(\"header\", \"true\")\n",
    " .load(f\"{SOURCE_DATA_PATH}/customers.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba519b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Dodaj kolumny metadata\n",
    "from pyspark.sql.functions import current_timestamp, input_file_name\n",
    "\n",
    "customers_enriched = (\n",
    " customers_stream\n",
    " .withColumn(\"_ingestion_timestamp\", ____) # Dodaj timestamp\n",
    " .withColumn(\"_source_file\", ____) # Dodaj nazw pliku 藕r贸dowego\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9129b686",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zapisz stream do tabeli Delta\n",
    "query_customers = (\n",
    " customers_enriched.writeStream\n",
    " .format(\"____\") # Uzupenij format\n",
    " .outputMode(\"____\") # Tryb zapisu (append)\n",
    " .option(\"checkpointLocation\", f\"{____}/customers_stream\") # Checkpoint\n",
    " .option(\"mergeSchema\", \"____\") # Schema evolution\n",
    " .table(f\"{CATALOG}.{SCHEMA}.bronze_customers_stream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6114af95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja streamu\n",
    "import time\n",
    "time.sleep(10) # Poczekaj na przetworzenie\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    " SELECT COUNT(*) as total_records \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_stream\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a130ce52",
   "metadata": {},
   "source": [
    "### Zadanie 2.2: Auto Loader dla JSON z Schema Hints\n",
    "\n",
    "**Instrukcje:**\n",
    "1. U偶yj Auto Loader do odczytu `orders_batch.json`\n",
    "2. Dodaj schema hints dla kolumn z konkretnymi typami\n",
    "3. Skonfiguruj rescue data column dla niepoprawnych rekord贸w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc65dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Auto Loader z schema hints\n",
    "orders_stream = (\n",
    " spark.readStream\n",
    " .format(\"cloudFiles\")\n",
    " .option(\"cloudFiles.format\", \"____\") # Format JSON\n",
    " .option(\"cloudFiles.schemaLocation\", f\"{CHECKPOINT_PATH}/orders_schema\")\n",
    " .option(\"cloudFiles.schemaHints\", \"____\") # Podpowied藕: \"order_date DATE, total_amount DOUBLE\"\n",
    " .option(\"cloudFiles.rescuedDataColumn\", \"____\") # Kolumna dla rescue data\n",
    " .load(f\"{____}/orders_batch.json\") # Uzupenij cie偶k\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af4c54b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zapisz stream\n",
    "query_orders = (\n",
    " orders_stream.writeStream\n",
    " .format(\"delta\")\n",
    " .outputMode(\"____\") # Tryb append\n",
    " .option(\"checkpointLocation\", f\"{CHECKPOINT_PATH}/____\") # Checkpoint\n",
    " .table(f\"{CATALOG}.{SCHEMA}.bronze_orders_stream\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc4725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja\n",
    "time.sleep(10)\n",
    "spark.sql(f\"\"\"\n",
    " SELECT * FROM {CATALOG}.{SCHEMA}.bronze_orders_stream LIMIT 10\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645db40",
   "metadata": {},
   "source": [
    "### Zadanie 2.3: Monitoring streaming queries\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Wywietl aktywne streaming queries\n",
    "2. Sprawd藕 status i ostatni progress ka偶dego query\n",
    "3. Pobierz metryki: liczba przetworzonych rekord贸w, batch duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25a1321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wywietl aktywne streamy\n",
    "active_streams = spark.streams.____ # Uzupenij metod (active)\n",
    "\n",
    "print(f\"Liczba aktywnych stream贸w: {len(active_streams)}\")\n",
    "for stream in active_streams:\n",
    " print(f\"\\nStream ID: {stream.id}\")\n",
    " print(f\"Name: {stream.name}\")\n",
    " print(f\"Status: {stream.status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6974ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Sprawd藕 ostatni progress\n",
    "if len(active_streams) > 0:\n",
    " last_progress = active_streams[0].____ # Uzupenij metod (lastProgress)\n",
    " \n",
    " if last_progress:\n",
    " print(f\"Batch ID: {last_progress['batchId']}\")\n",
    " print(f\"Przetworzone rekordy: {last_progress['numInputRows']}\")\n",
    " print(f\"Czas przetwarzania: {last_progress['batchDuration']} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55841f5d",
   "metadata": {},
   "source": [
    "### Zadanie 2.4: Zatrzymanie streaming queries\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Zatrzymaj wszystkie aktywne streaming queries\n",
    "2. Zweryfikuj 偶e wszystkie streamy s zatrzymane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae308ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Zatrzymaj wszystkie streamy\n",
    "for stream in spark.streams.active:\n",
    " print(f\"Zatrzymuj stream: {stream.name}\")\n",
    " stream.____() # Uzupenij metod (stop)\n",
    "\n",
    "print(\"\\nWszystkie streamy zatrzymane!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c772d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weryfikacja\n",
    "print(f\"Liczba aktywnych stream贸w: {len(spark.streams.active)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9819090",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  Cz 3: Por贸wnanie COPY INTO vs Auto Loader\n",
    "\n",
    "### Zadanie 3.1: Analiza wydajnoci\n",
    "\n",
    "**Instrukcje:**\n",
    "1. Por贸wnaj liczb rekord贸w zaadowanych przez COPY INTO vs Auto Loader\n",
    "2. Sprawd藕 histori operacji dla obu metod\n",
    "3. Zidentyfikuj przypadki u偶ycia dla ka偶dej metody"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cbad7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Por贸wnanie liczby rekord贸w\n",
    "copy_into_count = spark.sql(f\"\"\"\n",
    " SELECT 'COPY INTO' as method, COUNT(*) as records \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\")\n",
    "\n",
    "auto_loader_count = spark.sql(f\"\"\"\n",
    " SELECT 'Auto Loader' as method, COUNT(*) as records \n",
    " FROM {CATALOG}.{SCHEMA}.bronze_customers_stream\n",
    "\"\"\")\n",
    "\n",
    "copy_into_count.union(auto_loader_count).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6833f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Historia operacji COPY INTO\n",
    "spark.sql(f\"\"\"\n",
    " ____ HISTORY {CATALOG}.{SCHEMA}.bronze_customers_batch\n",
    "\"\"\").select(\"version\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83bd43d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Historia operacji Auto Loader\n",
    "spark.sql(f\"\"\"\n",
    " DESCRIBE HISTORY {CATALOG}.{SCHEMA}.bronze_customers_stream\n",
    "\"\"\").select(\"version\", \"operation\", \"operationMetrics\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1956008d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie warsztatu\n",
    "\n",
    "**Zrealizowane cele:**\n",
    "- Implementacja batch ingestion z COPY INTO\n",
    "- Konfiguracja Auto Loader dla streaming ingestion\n",
    "- Obsuga r贸偶nych format贸w (CSV, JSON, Parquet)\n",
    "- Monitoring i zarzdzanie pipeline'ami\n",
    "\n",
    "**Kiedy u偶y COPY INTO:**\n",
    "- Batch processing z okrelonym harmonogramem\n",
    "- Znana i stabilna struktura danych\n",
    "- Potrzeba kontroli nad procesem adowania\n",
    "- Idempotentno out-of-the-box\n",
    "\n",
    "**Kiedy u偶y Auto Loader:**\n",
    "- Near real-time processing\n",
    "- Schema evolution i automatyczna inferencja\n",
    "- Cige monitorowanie nowych plik贸w\n",
    "- Skalowalno i efektywno kosztowa\n",
    "\n",
    "---\n",
    "\n",
    "## Ч Cleanup (opcjonalnie)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}