{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ca99596-2bd8-4fcd-9d2a-b4ae51b48e94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Batch Data Ingestion - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Opanowanie technik idempotentnego ≈Çadowania danych batch do Delta Lake.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- COPY INTO (idempotent batch load)\n",
    "- R√≥≈ºne formaty plik√≥w (CSV, JSON, Parquet)\n",
    "- Schema management (inference vs enforcement)\n",
    "- Error handling (badRecordsPath)\n",
    "- CTAS (CREATE TABLE AS SELECT)\n",
    "- Incremental loading patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "510deaa9-5253-48d2-b8fc-35af59828ccf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzie≈Ñ szkolenia**: Dzie≈Ñ 2 - Delta Lake & Lakehouse\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog w≈ÇƒÖczony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58fc711e-377f-4369-a513-3ff5e77dbe2c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wstƒôp teoretyczny\n",
    "\n",
    "**COPY INTO** - najbardziej rekomendowana metoda dla batch ingestion:\n",
    "- **Idempotency**: Automatyczne ≈õledzenie przetworzonych plik√≥w\n",
    "- **File tracking**: Delta Lake zapisuje checksums - tylko nowe pliki sƒÖ ≈Çadowane\n",
    "- **Zastosowanie**: Incremental batch loads, data lake ingestion z S3/ADLS/GCS\n",
    "\n",
    "**CTAS (CREATE TABLE AS SELECT)**:\n",
    "- Tworzy tabelƒô z zapytania SELECT\n",
    "- **NIE** jest idempotentne\n",
    "- **Zastosowanie**: One-time loads, transformacje, agregacje\n",
    "\n",
    "**Dataset KION**:\n",
    "- **customers.csv**: customer_id, first_name, last_name, email, phone, city, state, country, registration_date, customer_segment\n",
    "- **orders_batch.json**: order_id, customer_id, product_id, store_id, order_datetime, quantity, unit_price, discount_percent, total_amount, payment_method\n",
    "- **products.parquet**: product_id, product_name, subcategory_code, brand, unit_cost, list_price, weight_kg, status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7ae8b30f-35dc-424a-8e88-a5114a531313",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Izolacja per u≈ºytkownik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d3bde39-850f-4171-a54b-b7b0c6e58d4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85d0f0e5-dd6b-454e-a94e-cb51fbbbee9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "880de8de-0642-461e-9d67-7f01157099aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "\n",
    "# ≈öcie≈ºki do danych\n",
    "CUSTOMERS_CSV = f\"{DATASET_BASE_PATH}/customers/customers.csv\"\n",
    "ORDERS_JSON = f\"{DATASET_BASE_PATH}/orders/orders_batch.json\"\n",
    "PRODUCTS_PARQUET = f\"{DATASET_BASE_PATH}/products/products.parquet\"\n",
    "\n",
    "# Wy≈õwietl kontekst u≈ºytkownika (zmienne z 00_setup)\n",
    "print(\"=== Kontekst u≈ºytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"U≈ºytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog i schemat jako domy≈õlne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "print(f\"\\n Domy≈õlny katalog: {CATALOG}\")\n",
    "print(f\" Domy≈õlny schemat: {BRONZE_SCHEMA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d272cdf9-3397-46ba-b23f-f6126965c1ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 1: COPY INTO - CSV (Customers)\n",
    "\n",
    "**Cel:** Idempotentne ≈Çadowanie danych klient√≥w z CSV.\n",
    "\n",
    "**Schema customers.csv:**\n",
    "- customer_id, first_name, last_name, email, phone\n",
    "- city, state, country\n",
    "- registration_date, customer_segment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "94e15d5b-8587-4c5f-8e6a-f81fab33cbe6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 1.1: COPY INTO z CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ccbb0c2-e568-40d2-a6eb-bcb71d8e897b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.1 - COPY INTO from CSV\n",
    "\n",
    "TABLE_CUSTOMERS = f\"{BRONZE_SCHEMA}.customers_batch\"\n",
    "\n",
    "# Krok 1: Utw√≥rz target table ze schematem zgodnym z customers.csv\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE_CUSTOMERS} (\n",
    "  customer_id STRING,\n",
    "  first_name STRING,\n",
    "  last_name STRING,\n",
    "  email STRING,\n",
    "  phone STRING,\n",
    "  city STRING,\n",
    "  state STRING,\n",
    "  country STRING,\n",
    "  registration_date DATE,\n",
    "  customer_segment STRING,\n",
    "  _ingestion_timestamp TIMESTAMP\n",
    ") USING DELTA\n",
    "COMMENT 'Customers data - Bronze layer'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úì Tabela {TABLE_CUSTOMERS} gotowa\")\n",
    "\n",
    "# Krok 2: COPY INTO z transformacjami\n",
    "result = spark.sql(f\"\"\"\n",
    "COPY INTO {TABLE_CUSTOMERS}\n",
    "FROM (\n",
    "  SELECT \n",
    "    customer_id,\n",
    "    first_name,\n",
    "    last_name,\n",
    "    email,\n",
    "    phone,\n",
    "    city,\n",
    "    state,\n",
    "    country,\n",
    "    TO_DATE(registration_date) as registration_date,\n",
    "    customer_segment,\n",
    "    current_timestamp() as _ingestion_timestamp\n",
    "  FROM '{CUSTOMERS_CSV}'\n",
    ")\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS (\n",
    "  'header' = 'true',\n",
    "  'delimiter' = ','\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "display(result)\n",
    "\n",
    "count = spark.table(TABLE_CUSTOMERS).count()\n",
    "print(f\"\\n‚úì Za≈Çadowano {count} klient√≥w\")\n",
    "\n",
    "# Sprawd≈∫ dane\n",
    "print(\"\\n=== Przyk≈Çadowe dane ===\")\n",
    "display(spark.table(TABLE_CUSTOMERS).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e685bea-f5e5-4bb2-8fc9-549b632269fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Idempotency Test:**\n",
    "\n",
    "Uruchom powy≈ºszƒÖ kom√≥rkƒô ponownie - zobaczysz ≈ºe COPY INTO nie za≈Çaduje duplikat√≥w! Delta Lake ≈õledzi przetworzone pliki w transaction log."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e15e3b58-2856-4790-aee8-2f0c33e3894e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 2: COPY INTO - JSON (Orders)\n",
    "\n",
    "**Cel:** ≈Åadowanie zam√≥wie≈Ñ z JSON z audit columns.\n",
    "\n",
    "**Schema orders_batch.json:**\n",
    "- order_id, customer_id, product_id, store_id\n",
    "- order_datetime, quantity, unit_price\n",
    "- discount_percent, total_amount, payment_method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "facdb15f-157f-4348-b680-131ab8639c03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 2.1: COPY INTO z JSON + audit columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "566b961a-0006-48de-a913-4a07d60e7f74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 2.1 - COPY INTO from JSON\n",
    "\n",
    "TABLE_ORDERS = f\"{BRONZE_SCHEMA}.orders_batch\"\n",
    "\n",
    "# Utw√≥rz tabelƒô ze schematem zgodnym z orders_batch.json\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE_ORDERS} (\n",
    "  order_id STRING,\n",
    "  customer_id STRING,\n",
    "  product_id STRING,\n",
    "  store_id STRING,\n",
    "  order_datetime TIMESTAMP,\n",
    "  quantity INT,\n",
    "  unit_price DECIMAL(10,2),\n",
    "  discount_percent INT,\n",
    "  total_amount DECIMAL(10,2),\n",
    "  payment_method STRING,\n",
    "  _ingestion_timestamp TIMESTAMP,\n",
    "  _source_file STRING\n",
    ") USING DELTA\n",
    "COMMENT 'Orders data - Bronze layer'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úì Tabela {TABLE_ORDERS} gotowa\")\n",
    "\n",
    "# COPY INTO z SELECT - dodaj audit columns\n",
    "result = spark.sql(f\"\"\"\n",
    "COPY INTO {TABLE_ORDERS}\n",
    "FROM (\n",
    "  SELECT \n",
    "    order_id,\n",
    "    customer_id,\n",
    "    product_id,\n",
    "    store_id,\n",
    "    TO_TIMESTAMP(order_datetime) as order_datetime,\n",
    "    CAST(quantity AS INT) as quantity,\n",
    "    CAST(unit_price AS DECIMAL(10,2)) as unit_price,\n",
    "    CAST(discount_percent AS INT) as discount_percent,\n",
    "    CAST(total_amount AS DECIMAL(10,2)) as total_amount,\n",
    "    payment_method,\n",
    "    current_timestamp() as _ingestion_timestamp,\n",
    "    _metadata.file_path as _source_file\n",
    "  FROM '{ORDERS_JSON}'\n",
    ")\n",
    "FILEFORMAT = JSON\n",
    "\"\"\")\n",
    "\n",
    "display(result)\n",
    "\n",
    "count = spark.table(TABLE_ORDERS).count()\n",
    "print(f\"\\n‚úì Za≈Çadowano {count} zam√≥wie≈Ñ\")\n",
    "\n",
    "# Poka≈º audit columns\n",
    "print(\"\\n=== Dane z audit columns ===\")\n",
    "display(spark.table(TABLE_ORDERS).select(\n",
    "    \"order_id\", \"customer_id\", \"total_amount\", \n",
    "    \"_ingestion_timestamp\", \"_source_file\"\n",
    ").limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8344704-0be6-43d7-80a4-39d7d5fe21a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Audit Columns:**\n",
    "\n",
    "Kolumny `_ingestion_timestamp` i `_source_file` sƒÖ kluczowe dla:\n",
    "- Traceability: SkƒÖd pochodzƒÖ dane?\n",
    "- Debugging: Kiedy zosta≈Çy za≈Çadowane?\n",
    "- Data lineage: Pe≈Çna historia pochodzenia danych"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05257fa3-95bd-4c80-a583-ea37bfdb061e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 3: COPY INTO - Parquet (Products)\n",
    "\n",
    "**Cel:** Najszybsze ≈Çadowanie z Parquet (columnar format).\n",
    "\n",
    "**Schema products.parquet:**\n",
    "- product_id, product_name, subcategory_code\n",
    "- brand, unit_cost, list_price\n",
    "- weight_kg, status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b4f97469-1453-417c-a3fe-7bba82d57cdf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 3.1: COPY INTO z Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71aa348d-0aa3-4cfc-bbc1-16c2fc12965d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 3.1 - COPY INTO from Parquet\n",
    "\n",
    "TABLE_PRODUCTS = f\"{BRONZE_SCHEMA}.products_batch\"\n",
    "\n",
    "# Utw√≥rz tabelƒô ze schematem zgodnym z products.parquet\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE_PRODUCTS} (\n",
    "  product_id STRING,\n",
    "  product_name STRING,\n",
    "  subcategory_code STRING,\n",
    "  brand STRING,\n",
    "  unit_cost DECIMAL(10,2),\n",
    "  list_price DECIMAL(10,2),\n",
    "  weight_kg DECIMAL(10,2),\n",
    "  status STRING,\n",
    "  _ingestion_timestamp TIMESTAMP\n",
    ") USING DELTA\n",
    "COMMENT 'Products data - Bronze layer'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úì Tabela {TABLE_PRODUCTS} gotowa\")\n",
    "\n",
    "# COPY INTO z Parquet - najszybszy format!\n",
    "result = spark.sql(f\"\"\"\n",
    "COPY INTO {TABLE_PRODUCTS}\n",
    "FROM (\n",
    "  SELECT \n",
    "    product_id,\n",
    "    product_name,\n",
    "    subcategory_code,\n",
    "    brand,\n",
    "    CAST(unit_cost AS DECIMAL(10,2)) as unit_cost,\n",
    "    CAST(list_price AS DECIMAL(10,2)) as list_price,\n",
    "    CAST(weight_kg AS DECIMAL(10,2)) as weight_kg,\n",
    "    status,\n",
    "    current_timestamp() as _ingestion_timestamp\n",
    "  FROM '{PRODUCTS_PARQUET}'\n",
    ")\n",
    "FILEFORMAT = PARQUET\n",
    "\"\"\")\n",
    "\n",
    "display(result)\n",
    "\n",
    "count = spark.table(TABLE_PRODUCTS).count()\n",
    "print(f\"\\n‚úì Za≈Çadowano {count} produkt√≥w\")\n",
    "\n",
    "print(\"\\n=== Przyk≈Çadowe produkty ===\")\n",
    "display(spark.table(TABLE_PRODUCTS).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fb049fa9-b1e7-4255-a022-8ec9a981a469",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Performance Parquet vs CSV/JSON:**\n",
    "\n",
    "Dla 100GB danych (typowy scenariusz):\n",
    "- **CSV**: ~5 min read time\n",
    "- **JSON**: ~3 min read time  \n",
    "- **Parquet**: ~30 sec read time ‚ö°\n",
    "\n",
    "**üí° Best Practice:** Konwertuj CSV/JSON ‚Üí Parquet w Bronze layer!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "49fe7841-1d3a-439c-a6fb-49f006134232",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 4: Schema Management\n",
    "\n",
    "**Wprowadzenie:**\n",
    "\n",
    "Dwa podej≈õcia do schema:\n",
    "\n",
    "**1. Schema Inference (automatyczne):**\n",
    "- ‚úÖ Szybkie dla prototyping\n",
    "- ‚ùå Mo≈ºe byƒá nieprecyzyjne\n",
    "- ‚ùå Skanuje dane (wolniejsze)\n",
    "\n",
    "**2. Explicit Schema (zdefiniowany):**\n",
    "- ‚úÖ Precyzyjne typy danych\n",
    "- ‚úÖ Walidacja podczas read\n",
    "- ‚úÖ Dokumentacja w kodzie\n",
    "- ‚úÖ **ZALECANE dla production**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1951e3b-d8f5-4c62-ae44-3a9e3850c553",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 4.1: Schema Inference vs Explicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "844e42d3-caf5-478e-8fa5-890d7d4a0c52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 4.1 - Schema Inference vs Enforcement\n",
    "\n",
    "print(\"=== 1. Schema Inference ===\\n\")\n",
    "\n",
    "# Automatyczne wykrywanie schema\n",
    "df_inferred = spark.read.csv(CUSTOMERS_CSV, header=True, inferSchema=True)\n",
    "\n",
    "print(\"Schema z inference:\")\n",
    "df_inferred.printSchema()\n",
    "print(f\"Liczba rekord√≥w: {df_inferred.count()}\")\n",
    "\n",
    "display(df_inferred.limit(3))\n",
    "\n",
    "print(\"\\n=== 2. Explicit Schema ===\\n\")\n",
    "\n",
    "# Zdefiniowany schema (ZALECANE!)\n",
    "schema_explicit = StructType([\n",
    "  StructField(\"customer_id\", StringType(), False),\n",
    "  StructField(\"first_name\", StringType(), True),\n",
    "  StructField(\"last_name\", StringType(), True),\n",
    "  StructField(\"email\", StringType(), True),\n",
    "  StructField(\"phone\", StringType(), True),\n",
    "  StructField(\"city\", StringType(), True),\n",
    "  StructField(\"state\", StringType(), True),\n",
    "  StructField(\"country\", StringType(), True),\n",
    "  StructField(\"registration_date\", DateType(), True),\n",
    "  StructField(\"customer_segment\", StringType(), True)\n",
    "])\n",
    "\n",
    "df_explicit = spark.read.schema(schema_explicit).csv(CUSTOMERS_CSV, header=True)\n",
    "\n",
    "print(\"Schema explicit:\")\n",
    "df_explicit.printSchema()\n",
    "print(f\"Liczba rekord√≥w: {df_explicit.count()}\")\n",
    "\n",
    "display(df_explicit.limit(3))\n",
    "\n",
    "print(\"\\nüí° Production: Zawsze u≈ºywaj explicit schema!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a7bc3d2-5566-4588-bceb-34d98866bc40",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 5: Error Handling\n",
    "\n",
    "**Strategie obs≈Çugi b≈Çƒôd√≥w:**\n",
    "\n",
    "**Parse Modes:**\n",
    "- `PERMISSIVE` (default): Parsuje co siƒô da, b≈Çƒôdy ‚Üí _corrupt_record\n",
    "- `DROPMALFORMED`: Usuwa b≈Çƒôdne rekordy (ostro≈ºnie!)\n",
    "- `FAILFAST`: Zatrzymuje na pierwszym b≈Çƒôdzie\n",
    "\n",
    "**badRecordsPath:**\n",
    "- Zapisuje niepoprawne rekordy do folderu\n",
    "- Umo≈ºliwia analizƒô post-factum\n",
    "- **Rekomendowane dla production**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a1e1d3f-c299-4ba5-9428-9204fadd1d0e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 5.1: Error Handling z badRecordsPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5f804da3-5855-45a3-bb89-ac8d70dd03da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 5.1 - Error Handling z badRecordsPath\n",
    "\n",
    "BAD_RECORDS_PATH = f\"/tmp/{raw_user}/bad_records\"\n",
    "\n",
    "# Wyczy≈õƒá folder (dla demo)\n",
    "try:\n",
    "    dbutils.fs.rm(BAD_RECORDS_PATH, True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"Bad records path: {BAD_RECORDS_PATH}\")\n",
    "\n",
    "# Utw√≥rz tabelƒô z _corrupt_record column\n",
    "TABLE_ERRORS = f\"{BRONZE_SCHEMA}.customers_with_validation\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE_ERRORS} (\n",
    "  customer_id STRING,\n",
    "  first_name STRING,\n",
    "  last_name STRING,\n",
    "  email STRING,\n",
    "  phone STRING,\n",
    "  city STRING,\n",
    "  state STRING,\n",
    "  country STRING,\n",
    "  registration_date STRING,\n",
    "  customer_segment STRING,\n",
    "  _corrupt_record STRING,\n",
    "  _ingestion_timestamp TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Wczytaj z error handling\n",
    "df_with_errors = (\n",
    "    spark.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "    .option(\"badRecordsPath\", BAD_RECORDS_PATH)\n",
    "    .load(CUSTOMERS_CSV)\n",
    "    .withColumn(\"_ingestion_timestamp\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "df_with_errors.write.mode(\"overwrite\").saveAsTable(TABLE_ERRORS)\n",
    "\n",
    "print(f\"‚úì Dane za≈Çadowane do {TABLE_ERRORS}\")\n",
    "\n",
    "# Analiza b≈Çƒôdnych rekord√≥w\n",
    "print(\"\\n=== Statystyki ===\")\n",
    "total = spark.table(TABLE_ERRORS).count()\n",
    "corrupt = spark.table(TABLE_ERRORS).filter(F.col(\"_corrupt_record\").isNotNull()).count()\n",
    "valid = total - corrupt\n",
    "\n",
    "print(f\"≈ÅƒÖcznie: {total}\")\n",
    "print(f\"Poprawnych: {valid}\")\n",
    "print(f\"B≈Çƒôdnych: {corrupt}\")\n",
    "\n",
    "if corrupt > 0:\n",
    "    print(\"\\n‚ö†Ô∏è B≈Çƒôdne rekordy:\")\n",
    "    display(spark.table(TABLE_ERRORS).filter(F.col(\"_corrupt_record\").isNotNull()))\n",
    "else:\n",
    "    print(\"\\n‚úÖ Wszystkie rekordy poprawne!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dde20c8e-c2a0-467f-a89b-0596ea387adb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 6: CTAS (CREATE TABLE AS SELECT)\n",
    "\n",
    "**Wprowadzenie:**\n",
    "\n",
    "CTAS tworzy tabelƒô z zapytania SELECT:\n",
    "- **NIE** jest idempotentne (ka≈ºde uruchomienie tworzy/nadpisuje)\n",
    "- Idealne do transformacji i agregacji\n",
    "- Szybkie wykonanie (parallel processing)\n",
    "\n",
    "**Kiedy u≈ºywaƒá CTAS:**\n",
    "1. Jednorazowe ≈Çadowanie historyczne\n",
    "2. Transformacje Bronze ‚Üí Silver/Gold\n",
    "3. Agregacje (summary tables)\n",
    "4. Format conversion (CSV ‚Üí Delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9a3f17c-9d4f-4aaf-94d8-1cdad169cf8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 6.1: CTAS dla agregacji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65f08354-972b-401f-bc95-d73f0c85f8d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 6.1 - CTAS dla agregacji\n",
    "\n",
    "AGG_TABLE = f\"{SILVER_SCHEMA}.customer_segment_summary\"\n",
    "\n",
    "print(f\"=== Tworzenie tabeli: {AGG_TABLE} ===\\n\")\n",
    "\n",
    "# CTAS z agregacjƒÖ\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {AGG_TABLE}\n",
    "USING DELTA\n",
    "COMMENT 'Customer segmentation summary'\n",
    "AS\n",
    "SELECT \n",
    "  customer_segment,\n",
    "  COUNT(*) as customer_count,\n",
    "  COUNT(DISTINCT state) as states_count,\n",
    "  COUNT(DISTINCT country) as countries_count,\n",
    "  MIN(registration_date) as first_registration,\n",
    "  MAX(registration_date) as last_registration,\n",
    "  current_timestamp() as snapshot_timestamp\n",
    "FROM {TABLE_CUSTOMERS}\n",
    "WHERE customer_segment IS NOT NULL\n",
    "GROUP BY customer_segment\n",
    "ORDER BY customer_count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úì Tabela {AGG_TABLE} utworzona\\n\")\n",
    "\n",
    "print(\"=== Summary po segmentach ===\")\n",
    "display(spark.table(AGG_TABLE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45fc62ee-2f31-48eb-a703-2fea5cc6a7d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 6.2: CTAS Bronze ‚Üí Silver (data quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efeb8409-5bd2-494b-9b79-edb460cee95b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 6.2 - CTAS Bronze ‚Üí Silver transformation\n",
    "\n",
    "SILVER_CUSTOMERS = f\"{SILVER_SCHEMA}.customers_clean\"\n",
    "\n",
    "print(f\"=== Transformacja Bronze ‚Üí Silver: {SILVER_CUSTOMERS} ===\\n\")\n",
    "\n",
    "# CTAS z data quality improvements\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {SILVER_CUSTOMERS}\n",
    "USING DELTA\n",
    "COMMENT 'Cleaned customers - Silver layer'\n",
    "AS\n",
    "SELECT \n",
    "  customer_id,\n",
    "  TRIM(UPPER(first_name)) as first_name,\n",
    "  TRIM(UPPER(last_name)) as last_name,\n",
    "  CONCAT(TRIM(first_name), ' ', TRIM(last_name)) as full_name,\n",
    "  LOWER(TRIM(email)) as email,\n",
    "  phone,\n",
    "  UPPER(city) as city,\n",
    "  UPPER(state) as state,\n",
    "  UPPER(country) as country,\n",
    "  registration_date,\n",
    "  customer_segment,\n",
    "  DATEDIFF(CURRENT_DATE(), registration_date) as days_since_registration,\n",
    "  CASE \n",
    "    WHEN customer_segment = 'Premium' THEN 'High Value'\n",
    "    WHEN customer_segment = 'Basic' THEN 'Standard Value'\n",
    "    ELSE 'Unknown'\n",
    "  END as value_tier,\n",
    "  current_timestamp() as processed_timestamp\n",
    "FROM {TABLE_CUSTOMERS}\n",
    "WHERE \n",
    "  customer_id IS NOT NULL\n",
    "  AND email IS NOT NULL\n",
    "  AND email LIKE '%@%'\n",
    "  AND registration_date IS NOT NULL\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úì Silver table utworzona\\n\")\n",
    "\n",
    "# Statystyki\n",
    "bronze_count = spark.table(TABLE_CUSTOMERS).count()\n",
    "silver_count = spark.table(SILVER_CUSTOMERS).count()\n",
    "filtered = bronze_count - silver_count\n",
    "\n",
    "print(f\"=== Statystyki transformacji ===\")\n",
    "print(f\"Bronze: {bronze_count}\")\n",
    "print(f\"Silver: {silver_count}\")\n",
    "print(f\"Filtered out: {filtered}\")\n",
    "if bronze_count > 0:\n",
    "    print(f\"Quality rate: {(silver_count/bronze_count*100):.2f}%\")\n",
    "\n",
    "print(\"\\n=== Przyk≈Çadowe dane Silver ===\")\n",
    "display(spark.table(SILVER_CUSTOMERS).limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3b96bf1-dc49-40c4-b916-f8d398915d90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Sekcja 7: Best Practices\n",
    "\n",
    "### 7.1 File Size Optimization\n",
    "\n",
    "**Idealne rozmiary:**\n",
    "- Minimum: 128 MB per file\n",
    "- Optimum: 256 MB - 1 GB per file\n",
    "- Maximum: < 1 GB per file\n",
    "\n",
    "**Problem ma≈Çych plik√≥w:**\n",
    "```python\n",
    "# BAD: TysiƒÖce ma≈Çych plik√≥w\n",
    "for file in files:\n",
    "    spark.read.csv(file).write.mode(\"append\")\n",
    "\n",
    "# GOOD: Batch processing + coalesce\n",
    "df.coalesce(10).write.mode(\"append\").saveAsTable(\"table\")\n",
    "```\n",
    "\n",
    "### 7.2 Idempotency Patterns\n",
    "\n",
    "**Pattern 1: COPY INTO (Recommended)**\n",
    "```sql\n",
    "COPY INTO table FROM 'path/*.parquet'  -- Automatyczna idempotency\n",
    "```\n",
    "\n",
    "**Pattern 2: MERGE z watermark**\n",
    "```sql\n",
    "MERGE INTO target USING source\n",
    "ON target.id = source.id AND source.date >= '2024-01-01'\n",
    "```\n",
    "\n",
    "**Pattern 3: Overwrite partition**\n",
    "```sql\n",
    "INSERT OVERWRITE TABLE target PARTITION (date = '2024-01-15')\n",
    "SELECT * FROM source WHERE date = '2024-01-15'\n",
    "```\n",
    "\n",
    "### 7.3 Audit Columns (ObowiƒÖzkowe!)\n",
    "\n",
    "```python\n",
    ".withColumn(\"_ingestion_timestamp\", F.current_timestamp())\n",
    ".withColumn(\"_source_file\", F.input_file_name())\n",
    ".withColumn(\"_job_id\", F.lit(dbutils.notebook.entry_point.getDbutils().notebook().getContext().jobId().get()))\n",
    "```\n",
    "\n",
    "### 7.4 Quick Reference Card\n",
    "\n",
    "| Scenario | Recommended Approach |\n",
    "|----------|---------------------|\n",
    "| Incremental loads (daily/hourly) | COPY INTO |\n",
    "| One-time historical load | CTAS |\n",
    "| SaaS integration | Lakeflow Connect |\n",
    "| High-frequency (< 1h) | Streaming (Auto Loader) |\n",
    "| Transformations | CTAS (Bronze ‚Üí Silver) |\n",
    "| Upserts | MERGE INTO |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8676a216-06ac-43a6-95a9-355b3f4bcd77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Podsumowanie\n",
    "\n",
    "### Co osiƒÖgnƒôli≈õmy:\n",
    "\n",
    "‚úÖ **COPY INTO - Idempotent Loading**\n",
    "- Automatic file tracking\n",
    "- No duplicates na retry\n",
    "- R√≥≈ºne formaty: CSV, JSON, Parquet\n",
    "\n",
    "‚úÖ **Schema Management**\n",
    "- Inference vs Explicit schema\n",
    "- Explicit schema dla production\n",
    "\n",
    "‚úÖ **Error Handling**\n",
    "- badRecordsPath dla quarantine\n",
    "- PERMISSIVE mode\n",
    "- Analiza corrupt records\n",
    "\n",
    "‚úÖ **CTAS Transformations**\n",
    "- Agregacje (segment summary)\n",
    "- Bronze ‚Üí Silver (data quality)\n",
    "- Fast parallel processing\n",
    "\n",
    "‚úÖ **Best Practices**\n",
    "- File size optimization\n",
    "- Idempotency patterns\n",
    "- Audit columns\n",
    "- Performance tuning\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "\n",
    "üí° **1. COPY INTO > INSERT**\n",
    "- Zawsze preferuj COPY INTO dla batch loads\n",
    "\n",
    "üí° **2. Explicit Schema > Inference**\n",
    "- Production pipelines wymagajƒÖ zdefiniowanego schema\n",
    "\n",
    "üí° **3. Always Error Handling**\n",
    "- badRecordsPath + PERMISSIVE mode + monitoring\n",
    "\n",
    "üí° **4. Audit Everything**\n",
    "- _ingestion_timestamp, _source_file, _job_id\n",
    "\n",
    "üí° **5. Optimize Performance**\n",
    "- Parquet > CSV/JSON\n",
    "- Files: 256MB - 1GB\n",
    "- Coalesce ma≈Çe pliki\n",
    "\n",
    "### Nastƒôpne kroki:\n",
    "\n",
    "üìö **Kolejny Notebook:** `03_streaming_data_ingestion.ipynb`\n",
    "- Auto Loader (cloudFiles)\n",
    "- Structured Streaming\n",
    "- Incremental processing\n",
    "\n",
    "üõ†Ô∏è **Warsztat:** `02_ingestion_pipeline_workshop.ipynb`\n",
    "- Hands-on: End-to-end pipeline\n",
    "- Real-world scenarios\n",
    "- Error handling & monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c7b59b3-50ac-40de-a0a3-eaf637a7b175",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "## Cleanup (Opcjonalnie)\n",
    "\n",
    "**Uwaga:** Wykonaj cleanup tylko je≈õli nie potrzebujesz ju≈º tych tabel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4fd57bb-5b3d-44ef-a0db-a26563adb21e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup - usu≈Ñ tabele demo\n",
    "# UWAGA: Odkomentuj tylko je≈õli jeste≈õ pewien!\n",
    "\n",
    "CLEANUP_ENABLED = False  # Zmie≈Ñ na True aby w≈ÇƒÖczyƒá\n",
    "\n",
    "if CLEANUP_ENABLED:\n",
    "    tables_to_drop = [\n",
    "        f\"{CATALOG}.{BRONZE_SCHEMA}.customers_batch\",\n",
    "        f\"{CATALOG}.{BRONZE_SCHEMA}.orders_batch\",\n",
    "        f\"{CATALOG}.{BRONZE_SCHEMA}.products_batch\",\n",
    "        f\"{CATALOG}.{BRONZE_SCHEMA}.customers_with_validation\",\n",
    "        f\"{CATALOG}.{SILVER_SCHEMA}.customer_segment_summary\",\n",
    "        f\"{CATALOG}.{SILVER_SCHEMA}.customers_clean\"\n",
    "    ]\n",
    "    \n",
    "    for table in tables_to_drop:\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n",
    "            print(f\"‚úÖ Usuniƒôto: {table}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è B≈ÇƒÖd: {str(e)}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Cleanup zako≈Ñczony!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cleanup WY≈ÅƒÑCZONY\")\n",
    "    print(\"Zmie≈Ñ CLEANUP_ENABLED = True aby usunƒÖƒá tabele\")\n",
    "    print(\"\\nüí° Zalecane: Zostaw dane dla kolejnych notebook√≥w!\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02_batch_data_ingestion",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
