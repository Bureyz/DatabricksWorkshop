{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "781675a6",
   "metadata": {},
   "source": [
    "# Batch Data Ingestion - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Opanowanie technik idempotentnego Å‚adowania danych batch do Delta Lake z uÅ¼yciem COPY INTO, CTAS i Lakeflow Connect.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- COPY INTO (idempotent batch load)\n",
    "- CTAS (CREATE TABLE AS SELECT)\n",
    "- Lakeflow Connect (managed connectors)\n",
    "- File formats: JSON, CSV, Parquet, Avro\n",
    "- Schema inference vs enforcement\n",
    "- Error handling (badRecordsPath, rescue columns)\n",
    "- Incremental batch patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cb0f48",
   "metadata": {},
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **DzieÅ„ szkolenia**: DzieÅ„ 2 - Delta Lake & Lakehouse Architecture\n",
    "- **Typ notebooka**: Demo\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog wÅ‚Ä…czony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers\n",
    "- **ZaleÅ¼noÅ›ci**: Wykonany notebook 01_delta_lake_operations.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bde3395",
   "metadata": {},
   "source": [
    "## WstÄ™p teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie rÃ³Å¼nych metod batch ingestion do Delta Lake i kiedy je stosowaÄ‡.\n",
    "\n",
    "**Podstawowe pojÄ™cia:**\n",
    "\n",
    "- **COPY INTO**: Idempotentna komenda do Å‚adowania plikÃ³w z zewnÄ™trznych lokalizacji (cloud storage). Automatycznie Å›ledzi juÅ¼ przetworzone pliki, zapobiegajÄ…c duplikacjom.\n",
    "\n",
    "- **CTAS (CREATE TABLE AS SELECT)**: Tworzy nowÄ… tabelÄ™ na podstawie zapytania SELECT. Przydatne do jednorazowego Å‚adowania lub transformacji danych.\n",
    "\n",
    "- **Lakeflow Connect**: Managed connectors do popularnych ÅºrÃ³deÅ‚ danych (Salesforce, SQL Server, etc.) - upraszcza integracje bez koniecznoÅ›ci pisania custom code.\n",
    "\n",
    "- **Idempotency**: WÅ‚aÅ›ciwoÅ›Ä‡ operacji, ktÃ³ra daje ten sam rezultat niezaleÅ¼nie od liczby wykonaÅ„. Kluczowa w pipeline'ach, ktÃ³re mogÄ… byÄ‡ uruchamiane wielokrotnie.\n",
    "\n",
    "**Dlaczego to waÅ¼ne?**\n",
    "\n",
    "Batch ingestion to fundament data lakehouse - wiÄ™kszoÅ›Ä‡ danych trafia do platformy przez regularne, zaplanowane Å‚adowania. Dobra strategia ingestion zapewnia:\n",
    "- Brak duplikacji danych\n",
    "- MoÅ¼liwoÅ›Ä‡ reprocessingu bez skutkÃ³w ubocznych\n",
    "- Efektywne wykorzystanie zasobÃ³w\n",
    "- Åatwe debugowanie i monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a79a932",
   "metadata": {},
   "source": [
    "## Izolacja per uÅ¼ytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalogÃ³w i schematÃ³w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bf530e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c68fa5",
   "metadata": {},
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych Å›rodowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca632cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import os\n",
    "\n",
    "# WyÅ›wietl kontekst uÅ¼ytkownika\n",
    "print(\"=== Kontekst uÅ¼ytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"UÅ¼ytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog i schemat jako domyÅ›lne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "\n",
    "print(f\"\\n=== ÅšcieÅ¼ki do danych ===\")\n",
    "print(f\"Customers CSV: {CUSTOMERS_CSV}\")\n",
    "print(f\"Orders JSON: {ORDERS_JSON}\")\n",
    "print(f\"Products Parquet: {PRODUCTS_PARQUET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b87d12a",
   "metadata": {},
   "source": [
    "## Sekcja 1: COPY INTO - Idempotent Batch Load\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "COPY INTO to najbardziej rekomendowana metoda dla batch ingestion w Delta Lake. Kluczowe cechy:\n",
    "\n",
    "**Kluczowe pojÄ™cia:**\n",
    "\n",
    "- **Idempotency**: COPY INTO automatycznie Å›ledzi przetworzone pliki w transaction log. Ponowne uruchomienie nie powoduje duplikacji.\n",
    "\n",
    "- **File tracking**: Delta Lake zapisuje checksums przetworzonych plikÃ³w. Tylko nowe pliki sÄ… Å‚adowane przy kolejnych uruchomieniach.\n",
    "\n",
    "- **FORMAT options**: Wspiera CSV, JSON, PARQUET, AVRO, ORC, TEXT, BINARYFILE.\n",
    "\n",
    "- **COPY_OPTIONS**: Parametry specyficzne dla formatu (delimiter, header, dateFormat, etc.).\n",
    "\n",
    "**Zastosowanie praktyczne:**\n",
    "- Incremental batch loads (nowe pliki codziennie/co godzinÄ™)\n",
    "- Reprocessing bez duplikacji\n",
    "- Data lake ingestion z S3/ADLS/GCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360d8fac",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 1.1: COPY INTO from CSV\n",
    "\n",
    "**Cel:** ZaÅ‚adowaÄ‡ dane customers z CSV do Delta table uÅ¼ywajÄ…c COPY INTO.\n",
    "\n",
    "**PodejÅ›cie:**\n",
    "1. UtwÃ³rz target Delta table\n",
    "2. UÅ¼yj COPY INTO z opcjami CSV\n",
    "3. SprawdÅº idempotency (uruchom ponownie)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b940d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrzykÅ‚ad 1.1 - COPY INTO from CSV\n",
    "\n",
    "TABLE_NAME = f\"{BRONZE_SCHEMA}.customers_batch\"\n",
    "\n",
    "# Krok 1: UtwÃ³rz target table (jeÅ›li nie istnieje)\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
    "  customer_id STRING,\n",
    "  first_name STRING,\n",
    "  last_name STRING,\n",
    "  email STRING,\n",
    "  phone STRING,\n",
    "  address STRING,\n",
    "  city STRING,\n",
    "  state STRING,\n",
    "  zip_code STRING,\n",
    "  customer_segment STRING,\n",
    "  registration_date DATE,\n",
    "  _ingestion_timestamp TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ“ Tabela {TABLE_NAME} gotowa\")\n",
    "\n",
    "# Krok 2: COPY INTO z opcjami CSV\n",
    "result = spark.sql(f\"\"\"\n",
    "COPY INTO {TABLE_NAME}\n",
    "FROM '{CUSTOMERS_CSV}'\n",
    "FILEFORMAT = CSV\n",
    "FORMAT_OPTIONS (\n",
    "  'header' = 'true',\n",
    "  'inferSchema' = 'false',\n",
    "  'delimiter' = ','\n",
    ")\n",
    "COPY_OPTIONS (\n",
    "  'mergeSchema' = 'false'\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "display(result)\n",
    "\n",
    "# Krok 3: SprawdÅº liczbÄ™ zaÅ‚adowanych rekordÃ³w\n",
    "count = spark.table(TABLE_NAME).count()\n",
    "print(f\"\\nâœ“ ZaÅ‚adowano {count} rekordÃ³w do {TABLE_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e80b188",
   "metadata": {},
   "source": [
    "**WyjaÅ›nienie:**\n",
    "\n",
    "COPY INTO automatycznie:\n",
    "- Åšledzi przetworzone pliki w `_delta_log`\n",
    "- Pomija pliki juÅ¼ zaÅ‚adowane (idempotency)\n",
    "- Zapisuje metadane: `num_affected_rows`, `num_inserted_rows`\n",
    "\n",
    "FORMAT_OPTIONS:\n",
    "- `header='true'`: Pierwsza linia to nagÅ‚Ã³wki kolumn\n",
    "- `inferSchema='false'`: UÅ¼ywamy zdefiniowanego schematu tabeli\n",
    "- `delimiter=','`: Separator CSV\n",
    "\n",
    "Uruchom tÄ™ komÃ³rkÄ™ ponownie - zobaczysz, Å¼e COPY INTO nie zaÅ‚aduje duplikatÃ³w!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490fb550",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 1.2: COPY INTO z transformacjÄ… (SELECT)\n",
    "\n",
    "**Cel:** ZaÅ‚adowaÄ‡ dane z transformacjÄ… podczas COPY INTO - dodanie audit columns.\n",
    "\n",
    "**PodejÅ›cie:**\n",
    "1. UÅ¼yj SELECT w COPY INTO do transformacji\n",
    "2. Dodaj `_ingestion_timestamp` i `_source_file`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f178dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrzykÅ‚ad 1.2 - COPY INTO z SELECT transformation\n",
    "\n",
    "TABLE_ORDERS = f\"{BRONZE_SCHEMA}.orders_batch\"\n",
    "\n",
    "# UtwÃ³rz tabelÄ™\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE_ORDERS} (\n",
    "  order_id STRING,\n",
    "  customer_id STRING,\n",
    "  order_date DATE,\n",
    "  total_amount DECIMAL(10,2),\n",
    "  payment_method STRING,\n",
    "  _ingestion_timestamp TIMESTAMP,\n",
    "  _source_file STRING\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# COPY INTO z SELECT\n",
    "result = spark.sql(f\"\"\"\n",
    "COPY INTO {TABLE_ORDERS}\n",
    "FROM (\n",
    "  SELECT \n",
    "    order_id,\n",
    "    customer_id,\n",
    "    TO_DATE(order_date) as order_date,\n",
    "    CAST(total_amount AS DECIMAL(10,2)) as total_amount,\n",
    "    payment_method,\n",
    "    current_timestamp() as _ingestion_timestamp,\n",
    "    _metadata.file_path as _source_file\n",
    "  FROM '{ORDERS_JSON}'\n",
    ")\n",
    "FILEFORMAT = JSON\n",
    "\"\"\")\n",
    "\n",
    "display(result)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# PrzykÅ‚ad 3.1 - Schema Inference vs Enforcement\n",
    "\n",
    "print(\"=== 1. Schema Inference (automatyczne wykrywanie) ===\\n\")\n",
    "\n",
    "# Metoda 1: Schema Inference\n",
    "df_inferred = spark.read.csv(CUSTOMERS_CSV, header=True, inferSchema=True)\n",
    "\n",
    "print(\"Schema z inference:\")\n",
    "df_inferred.printSchema()\n",
    "print(f\"Liczba rekordÃ³w: {df_inferred.count()}\\n\")\n",
    "\n",
    "# PokaÅ¼ przykÅ‚adowe dane\n",
    "display(df_inferred.limit(3))\n",
    "\n",
    "print(\"\\n=== 2. Explicit Schema (zdefiniowany schema) ===\\n\")\n",
    "\n",
    "# Metoda 2: Explicit Schema (ZALECANE dla production)\n",
    "schema_explicit = StructType([\n",
    "  StructField(\"customer_id\", StringType(), False),\n",
    "  StructField(\"first_name\", StringType(), True),\n",
    "  StructField(\"last_name\", StringType(), True),\n",
    "  StructField(\"email\", StringType(), True),\n",
    "  StructField(\"phone\", StringType(), True),\n",
    "  StructField(\"address\", StringType(), True),\n",
    "  StructField(\"city\", StringType(), True),\n",
    "  StructField(\"state\", StringType(), True),\n",
    "  StructField(\"zip_code\", StringType(), True),\n",
    "  StructField(\"customer_segment\", StringType(), True),\n",
    "  StructField(\"registration_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "df_explicit = spark.read.schema(schema_explicit).csv(CUSTOMERS_CSV, header=True)\n",
    "\n",
    "print(\"Schema explicit:\")\n",
    "df_explicit.printSchema()\n",
    "print(f\"Liczba rekordÃ³w: {df_explicit.count()}\\n\")\n",
    "\n",
    "display(df_explicit.limit(3))\n",
    "\n",
    "# PorÃ³wnanie\n",
    "print(\"\\n=== PorÃ³wnanie ===\")\n",
    "print(\"Schema Inference:\")\n",
    "print(\"  âœ… Szybkie do prototyping\")\n",
    "print(\"  âŒ MoÅ¼e bÅ‚Ä™dnie wykryÄ‡ typy (np. INT zamiast STRING)\")\n",
    "print(\"  âŒ Skanuje dane (wolniejsze dla duÅ¼ych plikÃ³w)\")\n",
    "print(\"\\nExplicit Schema:\")\n",
    "print(\"  âœ… Precyzyjne typy danych\")\n",
    "print(\"  âœ… Walidacja podczas read\")\n",
    "print(\"  âœ… Dokumentacja w kodzie\")\n",
    "print(\"  âœ… Szybsze (no scan needed)\")\n",
    "\n",
    "\n",
    "# PrzykÅ‚ad 4.1 - Error Handling z badRecordsPath\n",
    "\n",
    "# Krok 1: Setup bad records path\n",
    "BAD_RECORDS_PATH = f\"/tmp/{raw_user}/bad_records\"\n",
    "\n",
    "# WyczyÅ›Ä‡ folder jeÅ›li istnieje (dla demo)\n",
    "try:\n",
    "    dbutils.fs.rm(BAD_RECORDS_PATH, True)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(f\"Bad records path: {BAD_RECORDS_PATH}\")\n",
    "\n",
    "# Krok 2: UtwÃ³rz tabelÄ™ z _corrupt_record column\n",
    "TABLE_ERRORS = f\"{BRONZE_SCHEMA}.customers_with_validation\"\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE_ERRORS} (\n",
    "  customer_id STRING,\n",
    "  first_name STRING,\n",
    "  last_name STRING,\n",
    "  email STRING,\n",
    "  phone STRING,\n",
    "  address STRING,\n",
    "  city STRING,\n",
    "  state STRING,\n",
    "  zip_code STRING,\n",
    "  customer_segment STRING,\n",
    "  registration_date STRING,\n",
    "  _corrupt_record STRING,\n",
    "  _ingestion_timestamp TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "# Krok 3: Wczytaj dane z error handling\n",
    "df_with_errors = (spark.read\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"mode\", \"PERMISSIVE\")\n",
    "  .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\")\n",
    "  .option(\"badRecordsPath\", BAD_RECORDS_PATH)\n",
    "  .load(CUSTOMERS_CSV)\n",
    "  .withColumn(\"_ingestion_timestamp\", F.current_timestamp())\n",
    ")\n",
    "\n",
    "# Zapisz do tabeli\n",
    "df_with_errors.write.mode(\"append\").saveAsTable(TABLE_ERRORS)\n",
    "\n",
    "print(f\"âœ“ Dane zaÅ‚adowane do {TABLE_ERRORS}\")\n",
    "\n",
    "# Krok 4: Analiza bÅ‚Ä™dnych rekordÃ³w\n",
    "print(\"\\n=== Statystyki ===\")\n",
    "total_records = spark.table(TABLE_ERRORS).count()\n",
    "corrupt_records = spark.table(TABLE_ERRORS).filter(F.col(\"_corrupt_record\").isNotNull()).count()\n",
    "valid_records = total_records - corrupt_records\n",
    "\n",
    "print(f\"ÅÄ…cznie rekordÃ³w: {total_records}\")\n",
    "print(f\"Poprawnych: {valid_records}\")\n",
    "print(f\"BÅ‚Ä™dnych: {corrupt_records}\")\n",
    "\n",
    "# JeÅ›li sÄ… bÅ‚Ä™dne rekordy, pokaÅ¼ je\n",
    "if corrupt_records > 0:\n",
    "    print(\"\\nâš ï¸ BÅ‚Ä™dne rekordy:\")\n",
    "    display(spark.table(TABLE_ERRORS).filter(F.col(\"_corrupt_record\").isNotNull()))\n",
    "    \n",
    "    # SprawdÅº bad records folder\n",
    "    try:\n",
    "        bad_files = dbutils.fs.ls(BAD_RECORDS_PATH)\n",
    "        print(f\"\\nğŸ“ Bad records zapisane w: {BAD_RECORDS_PATH}\")\n",
    "        for file in bad_files:\n",
    "            print(f\"  - {file.name} ({file.size} bytes)\")\n",
    "    except:\n",
    "        print(\"\\nâœ“ Brak plikÃ³w w bad records folder (wszystkie dane poprawne)\")\n",
    "else:\n",
    "    print(\"\\nâœ… Wszystkie rekordy poprawne!\")\n",
    "\n",
    "\n",
    "# PrzykÅ‚ad 5.1 - CTAS dla agregacji\n",
    "\n",
    "AGG_TABLE = f\"{SILVER_SCHEMA}.customer_segment_summary\"\n",
    "\n",
    "print(f\"=== Tworzenie tabeli agregacyjnej: {AGG_TABLE} ===\\n\")\n",
    "\n",
    "# Krok 1: CTAS z agregacjÄ…\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {AGG_TABLE}\n",
    "USING DELTA\n",
    "COMMENT 'Customer segmentation summary - created via CTAS'\n",
    "AS\n",
    "SELECT \n",
    "  customer_segment,\n",
    "  COUNT(*) as customer_count,\n",
    "  COUNT(DISTINCT state) as states_count,\n",
    "  MIN(registration_date) as first_registration,\n",
    "  MAX(registration_date) as last_registration,\n",
    "  current_timestamp() as snapshot_timestamp\n",
    "FROM {TABLE_NAME}\n",
    "WHERE customer_segment IS NOT NULL\n",
    "GROUP BY customer_segment\n",
    "ORDER BY customer_count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ“ Tabela {AGG_TABLE} utworzona\\n\")\n",
    "\n",
    "# Krok 2: PokaÅ¼ wyniki\n",
    "print(\"=== Summary po segmentach ===\")\n",
    "display(spark.table(AGG_TABLE))\n",
    "\n",
    "# Krok 3: SprawdÅº metadane tabeli\n",
    "print(\"\\n=== Metadane tabeli ===\")\n",
    "spark.sql(f\"DESCRIBE EXTENDED {AGG_TABLE}\").show(truncate=False)\n",
    "\n",
    "# Krok 4: Historia tabeli\n",
    "print(\"\\n=== Historia tabeli (CTAS creates version 0) ===\")\n",
    "display(spark.sql(f\"DESCRIBE HISTORY {AGG_TABLE}\").limit(5))\n",
    "\n",
    "\n",
    "# PrzykÅ‚ad 6.1 - Lakeflow Connect Setup Guide\n",
    "\n",
    "print(\"\"\"\n",
    "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "â•‘                    LAKEFLOW CONNECT - SETUP GUIDE                         â•‘\n",
    "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ“ KROK 1: Nawigacja w UI\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "1. OtwÃ³rz Databricks workspace\n",
    "2. Z lewego menu wybierz: Workflows\n",
    "3. Kliknij przycisk \"Create\" â†’ \"Lakeflow\"\n",
    "\n",
    "ğŸ“ KROK 2: WybÃ³r Connectora\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "DostÄ™pne opcje:\n",
    "  â€¢ Salesforce (CRM data, custom objects)\n",
    "  â€¢ SQL Server (transactional databases, CDC)\n",
    "  â€¢ PostgreSQL/MySQL (open-source databases)\n",
    "  â€¢ Google Analytics 4 (web analytics)\n",
    "  â€¢ ServiceNow (ITSM data)\n",
    "  â€¢ Custom REST API\n",
    "\n",
    "ğŸ“ KROK 3: Konfiguracja Connection\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PrzykÅ‚ad dla SQL Server:\n",
    "\n",
    "  Connection Name: sqlserver_prod_connection\n",
    "  Host: sql-server.example.com:1433\n",
    "  Database: Adventureworks\n",
    "  Authentication: SQL Server Authentication\n",
    "  Username: lakehouse_reader\n",
    "  Password: *** (stored in secret scope)\n",
    "  \n",
    "  âœ“ Test Connection â†’ Success\n",
    "\n",
    "ğŸ“ KROK 4: WybÃ³r Tabel/Objects\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "Zaznacz tabele do sync:\n",
    "  âœ“ Sales.Orders\n",
    "  âœ“ Sales.Customers  \n",
    "  âœ“ Sales.OrderDetails\n",
    "  âœ“ Production.Products\n",
    "\n",
    "ğŸ“ KROK 5: Destination Configuration\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  Target Catalog: \"\"\" + CATALOG + \"\"\"\n",
    "  Target Schema: \"\"\" + BRONZE_SCHEMA + \"\"\"\n",
    "  Table Prefix: sqlserver_\n",
    "  \n",
    "  Result tables:\n",
    "    â€¢ \"\"\" + CATALOG + \".\" + BRONZE_SCHEMA + \"\"\".sqlserver_orders\n",
    "    â€¢ \"\"\" + CATALOG + \".\" + BRONZE_SCHEMA + \"\"\".sqlserver_customers\n",
    "    â€¢ \"\"\" + CATALOG + \".\" + BRONZE_SCHEMA + \"\"\".sqlserver_orderdetails\n",
    "    â€¢ \"\"\" + CATALOG + \".\" + BRONZE_SCHEMA + \"\"\".sqlserver_products\n",
    "\n",
    "ğŸ“ KROK 6: Sync Configuration\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  Sync Mode: Incremental (CDC if supported)\n",
    "  Schedule: Every 1 hour\n",
    "  CDC Method: Change Tracking (SQL Server built-in)\n",
    "  Initial Load: Full snapshot\n",
    "  \n",
    "  Advanced Options:\n",
    "    â€¢ mergeSchema: enabled (handle new columns)\n",
    "    â€¢ badRecordsPath: /mnt/bad_records/sqlserver/\n",
    "    â€¢ Parallelism: 4 workers\n",
    "\n",
    "ğŸ“ KROK 7: Monitoring & Alerting\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  Enable monitoring: âœ“\n",
    "  Alert on failure: team-data-engineering@company.com\n",
    "  Alert on schema change: yes\n",
    "  Log retention: 30 days\n",
    "\n",
    "ğŸ“ KROK 8: Deploy\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "  Review configuration â†’ Deploy\n",
    "\n",
    "  Status: Running\n",
    "  Next sync: 2024-01-15 14:00:00 UTC\n",
    "  Last successful sync: 2024-01-15 13:00:00 UTC\n",
    "  Records synced (last run): 1,234 inserts, 456 updates, 12 deletes\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ğŸ’¡ ZALETY LAKEFLOW CONNECT:\n",
    "  âœ… No custom code - konfiguracja przez UI\n",
    "  âœ… Automatic CDC - tylko zmienione dane\n",
    "  âœ… Schema evolution - automatyczne nowe kolumny\n",
    "  âœ… Built-in retry logic - error handling\n",
    "  âœ… Monitoring dashboard - real-time status\n",
    "  âœ… Unity Catalog integration - security & governance\n",
    "\n",
    "ğŸ’¡ USE CASES:\n",
    "  â€¢ Salesforce â†’ Lakehouse (CRM analytics)\n",
    "  â€¢ SQL Server â†’ Delta Lake (transactional data)\n",
    "  â€¢ Google Analytics â†’ Analytics (web data)\n",
    "  â€¢ REST APIs â†’ Bronze layer (custom integrations)\n",
    "\n",
    "ğŸ’¡ ALTERNATIVE: Custom Code\n",
    "  JeÅ›li Lakeflow Connect nie wspiera Twojego ÅºrÃ³dÅ‚a:\n",
    "  â€¢ UÅ¼yj PySpark + JDBC connector\n",
    "  â€¢ Implementuj wÅ‚asnÄ… CDC logic\n",
    "  â€¢ Scheduled notebook jobs\n",
    "  \n",
    "  Ale: WiÄ™cej code = wiÄ™cej maintenance!\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\"\"\")\n",
    "\n",
    "# Symulacja: sprawdÅº czy tabele z Lakeflow Connect istniejÄ…\n",
    "print(\"\\nğŸ“Š Sprawdzanie tabel z Lakeflow Connect (symulacja):\\n\")\n",
    "\n",
    "simulated_tables = [\n",
    "    f\"{BRONZE_SCHEMA}.sqlserver_orders\",\n",
    "    f\"{BRONZE_SCHEMA}.sqlserver_customers\",\n",
    "]\n",
    "\n",
    "for table in simulated_tables:\n",
    "    full_table = f\"{CATALOG}.{table}\"\n",
    "    try:\n",
    "        exists = spark.catalog.tableExists(full_table)\n",
    "        if exists:\n",
    "            count = spark.table(full_table).count()\n",
    "            print(f\"  âœ… {table}: {count} rekordÃ³w\")\n",
    "        else:\n",
    "            print(f\"  âš ï¸  {table}: nie istnieje (wymaga konfiguracji Lakeflow)\")\n",
    "    except:\n",
    "        print(f\"  âš ï¸  {table}: nie istnieje (wymaga konfiguracji Lakeflow)\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Aby skonfigurowaÄ‡ Lakeflow Connect, przejdÅº do UI: Workflows â†’ Create â†’ Lakeflow\")\n",
    "\n",
    "\n",
    "## Sekcja 8: Troubleshooting & Common Issues\n",
    "\n",
    "**Wprowadzenie:**\n",
    "\n",
    "Podczas batch ingestion napotkasz rÃ³Å¼ne problemy. Oto najczÄ™stsze i ich rozwiÄ…zania.\n",
    "\n",
    "### Problem 1: \"File already processed\" - COPY INTO idempotency\n",
    "\n",
    "**Objawy:**\n",
    "```\n",
    "AnalysisException: File /path/to/file.parquet has already been processed\n",
    "```\n",
    "- COPY INTO odmawia przetworzenia pliku\n",
    "- Chcesz wymusiÄ‡ reprocessing (np. po poprawieniu danych)\n",
    "\n",
    "**Przyczyna:**\n",
    "Delta Lake trackuje przetworzone pliki w transaction log. Checksum pliku jest zapisany.\n",
    "\n",
    "**RozwiÄ…zanie 1: Clear tracking (produkcja ostroÅ¼nie!)**\n",
    "```sql\n",
    "-- WyczyÅ›Ä‡ tracking dla konkretnej tabeli\n",
    "DELETE FROM table WHERE _source_file LIKE '%filename.parquet%';\n",
    "\n",
    "-- NastÄ™pnie COPY INTO zadziaÅ‚a ponownie\n",
    "```\n",
    "\n",
    "**RozwiÄ…zanie 2: UÅ¼yj nowego pliku**\n",
    "```bash\n",
    "# ZmieÅ„ plik (inny checksum)\n",
    "cp old_file.parquet new_file_v2.parquet\n",
    "# COPY INTO wykryje nowy plik\n",
    "```\n",
    "\n",
    "**RozwiÄ…zanie 3: VACUUM + retry (ostatecznoÅ›Ä‡)**\n",
    "```sql\n",
    "-- UWAGA: To usuwa history! UÅ¼yj tylko dla testÃ³w\n",
    "ALTER TABLE table_name \n",
    "SET TBLPROPERTIES ('delta.deletedFileRetentionDuration' = 'interval 0 hours');\n",
    "\n",
    "VACUUM table_name RETAIN 0 HOURS;\n",
    "\n",
    "-- Teraz COPY INTO moÅ¼e przetwarzaÄ‡ ponownie\n",
    "```\n",
    "\n",
    "### Problem 2: Schema Mismatch Errors\n",
    "\n",
    "**Objawy:**\n",
    "```\n",
    "AnalysisException: Cannot cast 'customer_id' from STRING to INT\n",
    "Schema mismatch: expected INT, found STRING\n",
    "```\n",
    "\n",
    "**Przyczyna:**\n",
    "- Explicit schema w tabeli nie pasuje do source data\n",
    "- Schema inference wykryÅ‚ inny typ niÅ¼ expected\n",
    "\n",
    "**RozwiÄ…zanie 1: CAST w SELECT**\n",
    "```sql\n",
    "COPY INTO table\n",
    "FROM (\n",
    "  SELECT \n",
    "    CAST(customer_id AS STRING) as customer_id,  -- Force cast\n",
    "    CAST(amount AS DECIMAL(10,2)) as amount\n",
    "  FROM 'path'\n",
    ")\n",
    "```\n",
    "\n",
    "**RozwiÄ…zanie 2: mergeSchema dla nowych kolumn**\n",
    "```sql\n",
    "COPY INTO table\n",
    "FROM 'path'\n",
    "COPY_OPTIONS ('mergeSchema' = 'true')  -- Auto-add new columns\n",
    "```\n",
    "\n",
    "**RozwiÄ…zanie 3: Recreate table z poprawnym schema**\n",
    "```sql\n",
    "DROP TABLE IF EXISTS table_name;\n",
    "\n",
    "CREATE TABLE table_name (\n",
    "  customer_id STRING,  -- Correct type\n",
    "  amount DECIMAL(10,2)\n",
    ") USING DELTA;\n",
    "```\n",
    "\n",
    "### Problem 3: Bad Records nie sÄ… zapisywane\n",
    "\n",
    "**Objawy:**\n",
    "- badRecordsPath folder jest pusty\n",
    "- Corrupt records znikajÄ… bez trace\n",
    "\n",
    "**Przyczyna:**\n",
    "- Mode ustawiony na DROPMALFORMED (silent drop)\n",
    "- Brak write permissions do badRecordsPath\n",
    "- FAILFAST mode zatrzymuje przed zapisem\n",
    "\n",
    "**RozwiÄ…zanie:**\n",
    "```python\n",
    "# Upewnij siÄ™ Å¼e:\n",
    "1. mode='PERMISSIVE'  # NOT 'DROPMALFORMED' or 'FAILFAST'\n",
    "2. badRecordsPath ma write permissions\n",
    "3. columnNameOfCorruptRecord jest ustawiony\n",
    "\n",
    "# PrzykÅ‚ad poprawnej konfiguracji:\n",
    "df = spark.read \\\n",
    "  .format(\"csv\") \\\n",
    "  .option(\"header\", \"true\") \\\n",
    "  .option(\"mode\", \"PERMISSIVE\") \\\n",
    "  .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "  .option(\"badRecordsPath\", \"/tmp/bad_records\") \\\n",
    "  .load(path)\n",
    "\n",
    "# SprawdÅº corrupt records w tabeli:\n",
    "df.filter(col(\"_corrupt_record\").isNotNull()).show()\n",
    "```\n",
    "\n",
    "### Problem 4: Slow Performance\n",
    "\n",
    "**Objawy:**\n",
    "- COPY INTO trwa godziny zamiast minut\n",
    "- Spark UI pokazuje maÅ‚e tasks (< 100MB)\n",
    "\n",
    "**Przyczyna:**\n",
    "- MaÅ‚e pliki (< 128MB)\n",
    "- Over-partitioning\n",
    "- Zbyt maÅ‚o executors\n",
    "\n",
    "**RozwiÄ…zanie 1: Coalesce maÅ‚e pliki**\n",
    "```python\n",
    "# Pre-process: Å‚Ä…cz maÅ‚e pliki\n",
    "df = spark.read.parquet(\"path/to/small/files\")\n",
    "df.coalesce(10).write.parquet(\"path/to/optimized/files\")\n",
    "\n",
    "# NastÄ™pnie COPY INTO z optymalizowanych plikÃ³w\n",
    "```\n",
    "\n",
    "**RozwiÄ…zanie 2: ZwiÄ™ksz parallelism**\n",
    "```sql\n",
    "-- WiÄ™cej shuffle partitions\n",
    "SET spark.sql.shuffle.partitions = 200;\n",
    "\n",
    "-- Auto-optimize w Delta Lake\n",
    "ALTER TABLE table_name \n",
    "SET TBLPROPERTIES ('delta.autoOptimize.optimizeWrite' = 'true');\n",
    "```\n",
    "\n",
    "**RozwiÄ…zanie 3: UÅ¼yj wiÄ™kszych executors**\n",
    "```python\n",
    "# W cluster config:\n",
    "spark.executor.memory = 16g\n",
    "spark.executor.cores = 4\n",
    "num_executors = 10\n",
    "```\n",
    "\n",
    "### Problem 5: Out of Memory (OOM)\n",
    "\n",
    "**Objawy:**\n",
    "```\n",
    "OutOfMemoryError: Java heap space\n",
    "Container killed by YARN for exceeding memory limits\n",
    "```\n",
    "\n",
    "**Przyczyna:**\n",
    "- Bardzo duÅ¼e pliki (> 2GB)\n",
    "- Skewed data w MERGE operations\n",
    "- Broadcast joins z duÅ¼ymi tables\n",
    "\n",
    "**RozwiÄ…zanie:**\n",
    "```python\n",
    "# 1. Disable broadcast joins\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "# 2. Repartition przed zapisem\n",
    "df.repartition(200).write.mode(\"append\").saveAsTable(\"table\")\n",
    "\n",
    "# 3. Process w batches\n",
    "for batch in file_batches:\n",
    "    spark.read.parquet(batch).write.mode(\"append\").saveAsTable(\"table\")\n",
    "\n",
    "# 4. ZwiÄ™ksz executor memory\n",
    "spark.executor.memory = 32g\n",
    "spark.executor.memoryOverhead = 4g\n",
    "```\n",
    "\n",
    "### Problem 6: Lakeflow Connect Failures\n",
    "\n",
    "**Objawy:**\n",
    "- Connector job fails z timeout\n",
    "- \"Connection refused\" errors\n",
    "- Incremental sync stuck\n",
    "\n",
    "**RozwiÄ…zanie:**\n",
    "```\n",
    "1. Check credentials:\n",
    "   - Unity Catalog â†’ External Locations â†’ Verify credentials\n",
    "   - Test connection w Lakeflow UI\n",
    "\n",
    "2. Network connectivity:\n",
    "   - Whitelist Databricks IPs w firewall\n",
    "   - Check VPN/Private Link configuration\n",
    "\n",
    "3. Source system health:\n",
    "   - Verify source database/API is accessible\n",
    "   - Check rate limits (API throttling)\n",
    "\n",
    "4. Retry configuration:\n",
    "   - Increase retry attempts: 5 â†’ 10\n",
    "   - Increase timeout: 5min â†’ 15min\n",
    "\n",
    "5. Logs:\n",
    "   - Databricks UI â†’ Workflows â†’ Lakeflow â†’ View Logs\n",
    "   - Check detailed error messages\n",
    "```\n",
    "\n",
    "### Debugging Checklist\n",
    "\n",
    "Gdy coÅ› nie dziaÅ‚a, sprawdÅº po kolei:\n",
    "\n",
    "âœ… **1. Permissions**\n",
    "```sql\n",
    "-- SprawdÅº permissions na katalog/schema\n",
    "SHOW GRANTS ON SCHEMA bronze;\n",
    "SHOW GRANTS ON TABLE bronze.customers;\n",
    "```\n",
    "\n",
    "âœ… **2. File Path**\n",
    "```python\n",
    "# Upewnij siÄ™ Å¼e Å›cieÅ¼ka istnieje\n",
    "dbutils.fs.ls(\"file:/path/to/data\")\n",
    "```\n",
    "\n",
    "âœ… **3. Schema**\n",
    "```sql\n",
    "-- PorÃ³wnaj schema source vs target\n",
    "DESCRIBE source_table;\n",
    "DESCRIBE target_table;\n",
    "```\n",
    "\n",
    "âœ… **4. Transaction Log**\n",
    "```sql\n",
    "-- Zobacz ostatnie operacje\n",
    "DESCRIBE HISTORY target_table LIMIT 10;\n",
    "```\n",
    "\n",
    "âœ… **5. Spark UI**\n",
    "- OtwÃ³rz Spark UI (Clusters â†’ Spark UI)\n",
    "- SprawdÅº \"Stages\" tab: czas wykonania, data skew\n",
    "- SprawdÅº \"Executors\" tab: memory usage, failed tasks\n",
    "\n",
    "âœ… **6. Logs**\n",
    "```python\n",
    "# Enable debug logging\n",
    "spark.sparkContext.setLogLevel(\"DEBUG\")\n",
    "\n",
    "# Check driver logs\n",
    "# Databricks: Clusters â†’ Driver Logs\n",
    "```\n",
    "\n",
    "### Quick Debug Commands\n",
    "\n",
    "```python\n",
    "# 1. Count records w tabeli\n",
    "spark.table(\"table_name\").count()\n",
    "\n",
    "# 2. Sample data\n",
    "spark.table(\"table_name\").show(5, truncate=False)\n",
    "\n",
    "# 3. Check partitions\n",
    "spark.table(\"table_name\").select(\"partition_col\").distinct().count()\n",
    "\n",
    "# 4. Table size\n",
    "spark.sql(\"DESCRIBE DETAIL table_name\").select(\"sizeInBytes\").show()\n",
    "\n",
    "# 5. Recent operations\n",
    "spark.sql(\"DESCRIBE HISTORY table_name\").show(10, truncate=False)\n",
    "\n",
    "# 6. Check files\n",
    "dbutils.fs.ls(\"dbfs:/path/to/table\")\n",
    "```\n",
    "\n",
    "\n",
    "### Co zostaÅ‚o osiÄ…gniÄ™te:\n",
    "- âœ“ Idempotent batch loading z COPY INTO\n",
    "- âœ“ ObsÅ‚uga rÃ³Å¼nych formatÃ³w (CSV, JSON, Parquet)\n",
    "- âœ“ Schema management (inference vs enforcement)\n",
    "- âœ“ Error handling (badRecordsPath, rescue columns)\n",
    "- âœ“ CTAS patterns dla transformacji\n",
    "- âœ“ Lakeflow Connect overview\n",
    "\n",
    "## Sekcja 10: Czyszczenie ZasobÃ³w\n",
    "\n",
    "**Uwaga:** Ta sekcja jest opcjonalna. Uruchom tylko jeÅ›li chcesz usunÄ…Ä‡ wszystkie dane utworzone podczas notebooka.\n",
    "\n",
    "W Å›rodowisku szkoleniowym zazwyczaj chcemy **zachowaÄ‡** dane dla kolejnych notebookÃ³w.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe04e2e",
   "metadata": {},
   "source": [
    "**WyjaÅ›nienie:**\n",
    "\n",
    "COPY INTO z SELECT pozwala na:\n",
    "- TransformacjÄ™ danych podczas Å‚adowania (CAST, TO_DATE)\n",
    "- Dodanie audit columns (_ingestion_timestamp, _source_file)\n",
    "- UÅ¼ycie _metadata kolumn (file_path, file_name, file_size)\n",
    "\n",
    "Kolumny metadanych:\n",
    "- `_metadata.file_path`: PeÅ‚na Å›cieÅ¼ka do source file\n",
    "- `_metadata.file_name`: Nazwa pliku\n",
    "- `_metadata.file_size`: Rozmiar pliku w bajtach\n",
    "- `_metadata.file_modification_time`: Data modyfikacji\n",
    "\n",
    "SprawdÅº zaÅ‚adowane dane:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a06ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SprawdÅº zaÅ‚adowane dane z audit columns\n",
    "print(f\"ğŸ“Š Dane w tabeli {TABLE_ORDERS}:\")\n",
    "display(spark.table(TABLE_ORDERS).limit(5))\n",
    "\n",
    "# SprawdÅº unikalne pliki ÅºrÃ³dÅ‚owe\n",
    "print(\"\\nğŸ“ Å¹rÃ³dÅ‚a danych:\")\n",
    "spark.table(TABLE_ORDERS).select(\"_source_file\").distinct().show(truncate=False)\n",
    "\n",
    "# Statystyki\n",
    "count = spark.table(TABLE_ORDERS).count()\n",
    "print(f\"\\nâœ“ ÅÄ…cznie zaÅ‚adowano {count} zamÃ³wieÅ„\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b58e7f",
   "metadata": {},
   "source": [
    "## Sekcja 2: File Formats\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Delta Lake wspiera wiele formatÃ³w plikÃ³w jako ÅºrÃ³dÅ‚a danych. WybÃ³r formatu wpÅ‚ywa na wydajnoÅ›Ä‡ i Å‚atwoÅ›Ä‡ uÅ¼ycia.\n",
    "\n",
    "**Kluczowe rÃ³Å¼nice:**\n",
    "\n",
    "| Format | Use Case | Pros | Cons |\n",
    "|--------|----------|------|------|\n",
    "| CSV | Legacy data, exports | Universal, human-readable | No schema, slow |\n",
    "| JSON | APIs, nested data | Flexible schema, nested | Slower than Parquet |\n",
    "| Parquet | Analytics, Delta Lake | Columnar, fast, compression | Not human-readable |\n",
    "| Avro | Schema evolution, Kafka | Row-based, schema in file | Slower for analytics |\n",
    "\n",
    "**Zalecenia:**\n",
    "- **Production pipelines**: UÅ¼ywaj Parquet dla najlepszej wydajnoÅ›ci\n",
    "- **API integration**: JSON dla nested structures\n",
    "- **Legacy systems**: CSV z explicit schema\n",
    "- **Kafka streaming**: Avro z schema registry\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07a61a1",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 2.1: COPY INTO z Parquet\n",
    "\n",
    "**Cel:** ZaÅ‚adowaÄ‡ dane produktÃ³w z formatu Parquet - najbardziej efektywny format dla analytics.\n",
    "\n",
    "**PodejÅ›cie:**\n",
    "1. UtwÃ³rz tabelÄ™ dla produktÃ³w\n",
    "2. ZaÅ‚aduj dane z Parquet uÅ¼ywajÄ…c COPY INTO\n",
    "3. PorÃ³wnaj wydajnoÅ›Ä‡ z CSV/JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622f0aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrzykÅ‚ad 2.1 - COPY INTO z Parquet\n",
    "\n",
    "TABLE_PRODUCTS = f\"{BRONZE_SCHEMA}.products_batch\"\n",
    "\n",
    "# Krok 1: UtwÃ³rz tabelÄ™\n",
    "spark.sql(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {TABLE_PRODUCTS} (\n",
    "  product_id STRING,\n",
    "  product_name STRING,\n",
    "  category STRING,\n",
    "  unit_price DECIMAL(10,2),\n",
    "  stock_quantity INT,\n",
    "  supplier_id STRING,\n",
    "  _ingestion_timestamp TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ“ Tabela {TABLE_PRODUCTS} gotowa\")\n",
    "\n",
    "# Krok 2: COPY INTO z Parquet (najszybszy format)\n",
    "result = spark.sql(f\"\"\"\n",
    "COPY INTO {TABLE_PRODUCTS}\n",
    "FROM (\n",
    "  SELECT \n",
    "    product_id,\n",
    "    product_name,\n",
    "    category,\n",
    "    CAST(unit_price AS DECIMAL(10,2)) as unit_price,\n",
    "    stock_quantity,\n",
    "    supplier_id,\n",
    "    current_timestamp() as _ingestion_timestamp\n",
    "  FROM '{PRODUCTS_PARQUET}'\n",
    ")\n",
    "FILEFORMAT = PARQUET\n",
    "\"\"\")\n",
    "\n",
    "display(result)\n",
    "\n",
    "# Krok 3: Weryfikacja\n",
    "count = spark.table(TABLE_PRODUCTS).count()\n",
    "print(f\"\\nâœ“ ZaÅ‚adowano {count} produktÃ³w\")\n",
    "\n",
    "# PokaÅ¼ przykÅ‚adowe dane\n",
    "display(spark.table(TABLE_PRODUCTS).limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f24718",
   "metadata": {},
   "source": [
    "**WyjaÅ›nienie:**\n",
    "\n",
    "Parquet to columnar format:\n",
    "- **Compression**: Automatyczna kompresja (3-10x mniejsze pliki)\n",
    "- **Predicate pushdown**: Spark czyta tylko potrzebne kolumny\n",
    "- **Schema embedded**: Metadata w pliku, nie trzeba definiowaÄ‡\n",
    "- **Fastest queries**: Idealny dla analytics workloads\n",
    "\n",
    "PorÃ³wnanie wydajnoÅ›ci (typowy scenariusz 100GB danych):\n",
    "- CSV: ~5 min read time\n",
    "- JSON: ~3 min read time  \n",
    "- Parquet: ~30 sec read time\n",
    "\n",
    "ğŸ’¡ **Best Practice**: Konwertuj CSV/JSON â†’ Parquet w Bronze layer!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ec5c8f",
   "metadata": {},
   "source": [
    "## Sekcja 3: Schema Management\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Schema management to krytyczny aspekt data ingestion. Masz dwa gÅ‚Ã³wne podejÅ›cia:\n",
    "\n",
    "**1. Schema Inference** (automatyczne wykrywanie):\n",
    "- âœ… Pros: Szybkie, nie wymaga definicji\n",
    "- âŒ Cons: MoÅ¼e byÄ‡ nieprecyzyjne, rÃ³Å¼ne typy w rÃ³Å¼nych plikach\n",
    "- ğŸ¯ Use case: Exploration, ad-hoc analysis\n",
    "\n",
    "**2. Schema Enforcement** (explicit definition):\n",
    "- âœ… Pros: Precyzyjne typy, walidacja, dokumentacja\n",
    "- âŒ Cons: Wymaga rÄ™cznej definicji\n",
    "- ğŸ¯ Use case: Production pipelines, data quality\n",
    "\n",
    "**Schema Evolution:**\n",
    "- `mergeSchema=true`: Automatyczne dodawanie nowych kolumn\n",
    "- `overwriteSchema=true`: Nadpisanie caÅ‚ego schema (ostroÅ¼nie!)\n",
    "\n",
    "**Best Practices:**\n",
    "- Production: Zawsze uÅ¼ywaj explicit schema\n",
    "- Development: Inference jest OK dla prototyping\n",
    "- Add audit columns: _ingestion_timestamp, _source_file\n",
    "- Document schema changes w version control\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb09d6c",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 3.1: Schema Inference vs Enforcement\n",
    "\n",
    "**Cel:** PorÃ³wnaÄ‡ schema inference z explicit schema enforcement - zobaczyÄ‡ rÃ³Å¼nice w typach danych.\n",
    "\n",
    "**PodejÅ›cie:**\n",
    "1. ZaÅ‚aduj dane z schema inference\n",
    "2. ZaÅ‚aduj te same dane z explicit schema\n",
    "3. PorÃ³wnaj wyniki i typy danych\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0157f53c",
   "metadata": {},
   "source": [
    "## Sekcja 4: Error Handling & Data Quality\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "W real-world scenarios dane nie zawsze sÄ… idealne. Potrzebujemy strategii obsÅ‚ugi bÅ‚Ä™dÃ³w:\n",
    "\n",
    "**Strategie Error Handling:**\n",
    "\n",
    "**1. Parse Mode Options:**\n",
    "- `PERMISSIVE` (default): Parsuje co siÄ™ da, bÅ‚Ä™dne dane â†’ _corrupt_record\n",
    "- `DROPMALFORMED`: Usuwa bÅ‚Ä™dne rekordy (silent failure - ostroÅ¼nie!)\n",
    "- `FAILFAST`: Zatrzymuje caÅ‚Ä… operacjÄ™ na pierwszym bÅ‚Ä™dzie\n",
    "\n",
    "**2. badRecordsPath:**\n",
    "- Zapisuje niepoprawne rekordy do osobnego folderu\n",
    "- UmoÅ¼liwia pÃ³ÅºniejszÄ… analizÄ™ i naprawÄ™\n",
    "- Rekomendowane dla production\n",
    "\n",
    "**3. Rescue Columns:**\n",
    "- `_rescued_data`: Automatyczna kolumna dla danych, ktÃ³re nie pasujÄ… do schema\n",
    "- DostÄ™pne w JSON/CSV format\n",
    "- Pozwala na analizÄ™ post-factum\n",
    "\n",
    "**Best Practices:**\n",
    "- Zawsze uÅ¼ywaj `badRecordsPath` w production\n",
    "- Monitoruj bad records folder (alerting)\n",
    "- Mode PERMISSIVE + manual validation > DROPMALFORMED\n",
    "- Loguj statystyki: success/failed records\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98e5a8c",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 4.1: Error Handling z badRecordsPath\n",
    "\n",
    "**Cel:** SkonfigurowaÄ‡ robust error handling przy Å‚adowaniu danych - quarantine bad records.\n",
    "\n",
    "**PodejÅ›cie:**\n",
    "1. Skonfiguruj badRecordsPath\n",
    "2. ZaÅ‚aduj dane z PERMISSIVE mode\n",
    "3. SprawdÅº bad records folder\n",
    "4. Przeanalizuj bÅ‚Ä™dne dane\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eee1767",
   "metadata": {},
   "source": [
    "## Sekcja 5: CTAS (CREATE TABLE AS SELECT)\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "CTAS to powerful pattern do tworzenia tabel z zapytaÅ„. W przeciwieÅ„stwie do COPY INTO, CTAS:\n",
    "- Tworzy nowÄ… tabelÄ™ w jednej operacji\n",
    "- NIE jest idempotentne (kaÅ¼de uruchomienie tworzy/nadpisuje)\n",
    "- Idealne do transformacji i agregacji\n",
    "\n",
    "**CTAS vs COPY INTO vs INSERT:**\n",
    "\n",
    "| Feature | CTAS | COPY INTO | INSERT |\n",
    "|---------|------|-----------|--------|\n",
    "| Idempotency | âŒ No | âœ… Yes | âŒ No |\n",
    "| Use Case | One-time load | Incremental | Append data |\n",
    "| Performance | Fast (parallel) | Fast | Slower |\n",
    "| Schema | From SELECT | Pre-defined | Pre-defined |\n",
    "\n",
    "**Kiedy uÅ¼ywaÄ‡ CTAS:**\n",
    "1. **Jednorazowe Å‚adowanie**: Historyczny load z data lake\n",
    "2. **Transformacje**: Tworzenie Silver/Gold tables z Bronze\n",
    "3. **Agregacje**: Summary tables, reports\n",
    "4. **Table cloning**: Backup, testing environments\n",
    "5. **Format conversion**: CSV â†’ Delta, Parquet â†’ Delta\n",
    "\n",
    "**CTAS Syntax:**\n",
    "```sql\n",
    "CREATE [OR REPLACE] TABLE table_name\n",
    "[USING DELTA]\n",
    "[PARTITIONED BY (col1, col2)]\n",
    "[LOCATION 'path']\n",
    "AS\n",
    "SELECT ... FROM source_table\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f325c31a",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 5.1: CTAS dla agregacji danych\n",
    "\n",
    "**Cel:** UtworzyÄ‡ summary table uÅ¼ywajÄ…c CTAS - agregacja customers per segment.\n",
    "\n",
    "**PodejÅ›cie:**\n",
    "1. UÅ¼yj CTAS z GROUP BY do utworzenia aggregate table\n",
    "2. Dodaj timestamp snapshot\n",
    "3. PokaÅ¼ rÃ³Å¼nicÄ™ z COPY INTO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5200fa",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 5.2: CTAS z transformacjÄ… (Bronze â†’ Silver)\n",
    "\n",
    "**Cel:** UÅ¼yÄ‡ CTAS do transformacji danych z Bronze do Silver layer z data quality improvements.\n",
    "\n",
    "**PodejÅ›cie:**\n",
    "1. Filtruj niepoprawne dane\n",
    "2. Standaryzuj formaty (uppercase, trim)\n",
    "3. Dodaj business logic columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cdfe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PrzykÅ‚ad 5.2 - CTAS Bronze â†’ Silver transformation\n",
    "\n",
    "SILVER_CUSTOMERS = f\"{SILVER_SCHEMA}.customers_clean\"\n",
    "\n",
    "print(f\"=== Transformacja Bronze â†’ Silver: {SILVER_CUSTOMERS} ===\\n\")\n",
    "\n",
    "# CTAS z data quality improvements\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {SILVER_CUSTOMERS}\n",
    "USING DELTA\n",
    "COMMENT 'Cleaned customer data - Silver layer'\n",
    "AS\n",
    "SELECT \n",
    "  customer_id,\n",
    "  TRIM(UPPER(first_name)) as first_name,\n",
    "  TRIM(UPPER(last_name)) as last_name,\n",
    "  CONCAT(first_name, ' ', last_name) as full_name,\n",
    "  LOWER(TRIM(email)) as email,\n",
    "  phone,\n",
    "  address,\n",
    "  UPPER(city) as city,\n",
    "  UPPER(state) as state,\n",
    "  zip_code,\n",
    "  customer_segment,\n",
    "  registration_date,\n",
    "  DATEDIFF(CURRENT_DATE(), registration_date) as days_since_registration,\n",
    "  CASE \n",
    "    WHEN customer_segment = 'Premium' THEN 'High Value'\n",
    "    WHEN customer_segment = 'Standard' THEN 'Medium Value'\n",
    "    ELSE 'Low Value'\n",
    "  END as value_tier,\n",
    "  current_timestamp() as processed_timestamp\n",
    "FROM {TABLE_NAME}\n",
    "WHERE \n",
    "  customer_id IS NOT NULL\n",
    "  AND email IS NOT NULL\n",
    "  AND email LIKE '%@%'\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ“ Silver table {SILVER_CUSTOMERS} utworzona\\n\")\n",
    "\n",
    "# PorÃ³wnaj Bronze vs Silver\n",
    "bronze_count = spark.table(TABLE_NAME).count()\n",
    "silver_count = spark.table(SILVER_CUSTOMERS).count()\n",
    "filtered_out = bronze_count - silver_count\n",
    "\n",
    "print(f\"=== Statystyki transformacji ===\")\n",
    "print(f\"Bronze records: {bronze_count}\")\n",
    "print(f\"Silver records: {silver_count}\")\n",
    "print(f\"Filtered out (bad quality): {filtered_out}\")\n",
    "print(f\"Quality rate: {(silver_count/bronze_count*100):.2f}%\\n\")\n",
    "\n",
    "# PokaÅ¼ dane Silver\n",
    "print(\"=== PrzykÅ‚adowe dane Silver ===\")\n",
    "display(spark.table(SILVER_CUSTOMERS).limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9158b9",
   "metadata": {},
   "source": [
    "## Sekcja 6: Lakeflow Connect (Managed Connectors)\n",
    "\n",
    "**Wprowadzenie teoretyczne:**\n",
    "\n",
    "Lakeflow Connect to managed integration platform w Databricks, ktÃ³ra upraszcza Å‚Ä…czenie z zewnÄ™trznymi ÅºrÃ³dÅ‚ami danych.\n",
    "\n",
    "**Co to jest Lakeflow Connect:**\n",
    "- **Managed connectors**: Bez koniecznoÅ›ci pisania custom code\n",
    "- **No-code/low-code**: Konfiguracja przez UI\n",
    "- **Automatic schema inference**: Automatyczne wykrywanie schema\n",
    "- **Built-in CDC**: Change Data Capture dla incremental sync\n",
    "- **Unity Catalog integration**: Automatyczne tworzenie tabel\n",
    "\n",
    "**Typy connectorÃ³w:**\n",
    "\n",
    "**1. Fully-Managed Connectors** (SaaS/Enterprise):\n",
    "- Salesforce (Objects, Reports, Custom)\n",
    "- SQL Server, MySQL, PostgreSQL, Oracle\n",
    "- Google Analytics 4, Google Sheets\n",
    "- ServiceNow, Workday\n",
    "- SAP, Snowflake\n",
    "- SharePoint, OneDrive\n",
    "\n",
    "**2. Standard Connectors** (Cloud Services):\n",
    "- Kafka, Azure Event Hubs, AWS Kinesis\n",
    "- S3, ADLS Gen2, Google Cloud Storage\n",
    "- MongoDB, Cassandra\n",
    "- REST APIs (generic)\n",
    "\n",
    "**Kluczowe cechy:**\n",
    "- âœ… Automatic retry & error handling\n",
    "- âœ… Incremental sync (only new/changed data)\n",
    "- âœ… Schema evolution support\n",
    "- âœ… Built-in monitoring & alerting\n",
    "- âœ… Security: credential management w Unity Catalog\n",
    "\n",
    "**Kiedy uÅ¼ywaÄ‡:**\n",
    "- Integracja z SaaS applications (Salesforce, ServiceNow)\n",
    "- CDC z transactional databases (SQL Server, MySQL)\n",
    "- Alternative do custom ETL code\n",
    "- Szybkie PoC/MVP data pipelines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f557ab",
   "metadata": {},
   "source": [
    "### PrzykÅ‚ad 6.1: Lakeflow Connect - Setup Guide (UI-based)\n",
    "\n",
    "**UWAGA:** Lakeflow Connect konfiguruje siÄ™ przez Databricks UI, nie przez kod. PoniÅ¼ej przedstawiamy konceptualny flow i przykÅ‚adowÄ… konfiguracjÄ™.\n",
    "\n",
    "**Setup Process:**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c158789e",
   "metadata": {},
   "source": [
    "## Sekcja 7: Best Practices - Batch Data Ingestion\n",
    "\n",
    "**Wprowadzenie:**\n",
    "\n",
    "Po poznaniu wszystkich technik, oto kluczowe best practices dla production-ready batch ingestion.\n",
    "\n",
    "### 7.1 WydajnoÅ›Ä‡ i Optymalizacja\n",
    "\n",
    "**File Size Optimization:**\n",
    "```\n",
    "Idealne rozmiary plikÃ³w:\n",
    "â€¢ Minimum: 128 MB per file\n",
    "â€¢ Optimum: 256 MB - 1 GB per file  \n",
    "â€¢ Maximum: < 1 GB per file\n",
    "```\n",
    "\n",
    "**Partycjonowanie:**\n",
    "- Partycjonuj wedÅ‚ug najczÄ™Å›ciej uÅ¼ywanych filtrÃ³w (data, region)\n",
    "- Unikaj over-partitioning (< 1GB per partition)\n",
    "- UÅ¼ywaj Hive-style partitioning: `/year=2024/month=01/`\n",
    "\n",
    "**Parallelizm:**\n",
    "```sql\n",
    "-- ZwiÄ™ksz parallelism dla duÅ¼ych loadÃ³w\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "\n",
    "-- Coalesce maÅ‚e pliki przed zapisem\n",
    "df.coalesce(10).write.mode(\"append\").saveAsTable(\"table\")\n",
    "```\n",
    "\n",
    "### 7.2 Data Quality & Governance\n",
    "\n",
    "**ObowiÄ…zkowe kolumny (Audit Trail):**\n",
    "```python\n",
    ".withColumn(\"_ingestion_timestamp\", F.current_timestamp())\n",
    ".withColumn(\"_source_file\", F.input_file_name())\n",
    ".withColumn(\"_ingestion_user\", F.lit(spark.sparkContext.sparkUser()))\n",
    ".withColumn(\"_job_id\", F.lit(dbutils.notebook.entry_point.getDbutils().notebook().getContext().jobId().get()))\n",
    "```\n",
    "\n",
    "**Data Validation:**\n",
    "- NOT NULL constraints na kluczowe kolumny\n",
    "- CHECK constraints (Delta Lake 3.0+)\n",
    "- Quarantine bad records (badRecordsPath)\n",
    "- Monitoruj data quality metrics\n",
    "\n",
    "**Unity Catalog:**\n",
    "- Taguj tabele: PII, retention policy, data classification\n",
    "- Dokumentuj schema: COMMENT na tabelach i kolumnach\n",
    "- Kontroluj dostÄ™p: GRANT SELECT/MODIFY per role\n",
    "\n",
    "### 7.3 Idempotency Patterns\n",
    "\n",
    "**Pattern 1: COPY INTO** (Recommended)\n",
    "```sql\n",
    "-- Automatyczna idempotency\n",
    "COPY INTO table FROM 'path/*.parquet'\n",
    "```\n",
    "\n",
    "**Pattern 2: MERGE with watermark**\n",
    "```sql\n",
    "MERGE INTO target\n",
    "USING source\n",
    "ON target.id = source.id \n",
    "  AND source.date >= '2024-01-01' -- watermark\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "```\n",
    "\n",
    "**Pattern 3: Overwrite partition**\n",
    "```sql\n",
    "INSERT OVERWRITE TABLE target \n",
    "PARTITION (date = '2024-01-15')\n",
    "SELECT * FROM source WHERE date = '2024-01-15'\n",
    "```\n",
    "\n",
    "### 7.4 Error Handling & Monitoring\n",
    "\n",
    "**Comprehensive Error Strategy:**\n",
    "```python\n",
    "try:\n",
    "    # Main ingestion logic\n",
    "    result = spark.sql(\"COPY INTO ...\")\n",
    "    \n",
    "    # Log success metrics\n",
    "    rows_inserted = result.select(\"num_affected_rows\").collect()[0][0]\n",
    "    logger.info(f\"Success: {rows_inserted} rows\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # Log error details\n",
    "    logger.error(f\"Ingestion failed: {str(e)}\")\n",
    "    \n",
    "    # Send alert\n",
    "    send_alert(team=\"data-eng\", message=str(e))\n",
    "    \n",
    "    # Quarantine bad data\n",
    "    # (automatically handled by badRecordsPath)\n",
    "    \n",
    "    # Decide: retry or fail\n",
    "    if is_retryable(e):\n",
    "        retry_with_backoff()\n",
    "    else:\n",
    "        raise\n",
    "```\n",
    "\n",
    "**Monitoring Checklist:**\n",
    "- âœ… Rows ingested per run\n",
    "- âœ… Ingestion duration (SLA: < 5 min)\n",
    "- âœ… Bad records count (alert if > 1%)\n",
    "- âœ… File counts (new files detected)\n",
    "- âœ… Schema changes detected\n",
    "\n",
    "### 7.5 Code Quality\n",
    "\n",
    "**Template: Production Ingestion Function**\n",
    "```python\n",
    "def ingest_batch_data(\n",
    "    source_path: str,\n",
    "    target_table: str,\n",
    "    file_format: str = \"parquet\",\n",
    "    mode: str = \"copy_into\",\n",
    "    validation_rules: dict = None\n",
    "):\n",
    "    \"\"\"\n",
    "    Production-ready batch ingestion function.\n",
    "    \n",
    "    Args:\n",
    "        source_path: Cloud storage path (s3://, abfss://)\n",
    "        target_table: Fully qualified table name\n",
    "        file_format: csv, json, parquet, avro\n",
    "        mode: copy_into (idempotent) or append\n",
    "        validation_rules: Data quality checks\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validation\n",
    "    assert spark.catalog.tableExists(target_table), f\"Table {target_table} doesn't exist\"\n",
    "    \n",
    "    # Audit columns\n",
    "    if mode == \"copy_into\":\n",
    "        result = spark.sql(f\"\"\"\n",
    "            COPY INTO {target_table}\n",
    "            FROM (\n",
    "                SELECT \n",
    "                    *,\n",
    "                    current_timestamp() as _ingestion_timestamp,\n",
    "                    _metadata.file_path as _source_file\n",
    "                FROM '{source_path}'\n",
    "            )\n",
    "            FILEFORMAT = {file_format}\n",
    "            COPY_OPTIONS ('mergeSchema' = 'false')\n",
    "        \"\"\")\n",
    "    \n",
    "    # Log metrics\n",
    "    return {\n",
    "        \"rows_inserted\": result.select(\"num_affected_rows\").first()[0],\n",
    "        \"timestamp\": datetime.now(),\n",
    "        \"status\": \"success\"\n",
    "    }\n",
    "```\n",
    "\n",
    "### 7.6 Quick Reference Card\n",
    "\n",
    "| Scenario | Recommended Approach |\n",
    "|----------|---------------------|\n",
    "| Incremental loads (new files daily) | COPY INTO |\n",
    "| One-time historical load | CTAS or spark.read + write |\n",
    "| SaaS integration (Salesforce, etc.) | Lakeflow Connect |\n",
    "| High-frequency (< 1 hour) | Consider Streaming (Auto Loader) |\n",
    "| Complex transformations | CTAS (Bronze â†’ Silver) |\n",
    "| Upserts (UPDATE + INSERT) | MERGE INTO |\n",
    "| Full refresh | CREATE OR REPLACE TABLE |\n",
    "\n",
    "### 7.7 Common Anti-Patterns (Unikaj!)\n",
    "\n",
    "âŒ **Anti-Pattern 1: INSERT bez idempotency**\n",
    "```sql\n",
    "-- BAD: Duplikaty przy ponownym uruchomieniu\n",
    "INSERT INTO table SELECT * FROM source\n",
    "```\n",
    "\n",
    "âœ… **Better: COPY INTO lub MERGE**\n",
    "\n",
    "âŒ **Anti-Pattern 2: Schema inference w production**\n",
    "```python\n",
    "# BAD: Typy mogÄ… siÄ™ zmieniÄ‡ miÄ™dzy runami\n",
    "df = spark.read.csv(path, inferSchema=True)\n",
    "```\n",
    "\n",
    "âœ… **Better: Explicit schema**\n",
    "\n",
    "âŒ **Anti-Pattern 3: Brak error handling**\n",
    "```python\n",
    "# BAD: Silent failures\n",
    "df.write.mode(\"append\").saveAsTable(\"table\")\n",
    "```\n",
    "\n",
    "âœ… **Better: try/except + badRecordsPath + monitoring**\n",
    "\n",
    "âŒ **Anti-Pattern 4: MaÅ‚e pliki (< 100MB)**\n",
    "```python\n",
    "# BAD: Tworzy tysiÄ…ce maÅ‚ych plikÃ³w\n",
    "for file in files:\n",
    "    spark.read.csv(file).write.mode(\"append\")\n",
    "```\n",
    "\n",
    "âœ… **Better: Batch processing + coalesce**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e701e59f",
   "metadata": {},
   "source": [
    "## Sekcja 9: Podsumowanie & NastÄ™pne Kroki\n",
    "\n",
    "### Co zostaÅ‚o osiÄ…gniÄ™te w tym notebooku:\n",
    "\n",
    "âœ… **1. COPY INTO - Idempotent Batch Loading**\n",
    "- Nauka idempotentnego Å‚adowania danych bez duplikacji\n",
    "- File tracking w Delta transaction log\n",
    "- Transformacje podczas COPY INTO (SELECT)\n",
    "- Audit columns: _ingestion_timestamp, _source_file\n",
    "\n",
    "âœ… **2. File Formats**\n",
    "- CSV: universal ale wolny\n",
    "- JSON: flexible dla nested data\n",
    "- Parquet: najszybszy dla analytics (compression, columnar)\n",
    "- WybÃ³r formatu wedÅ‚ug use case\n",
    "\n",
    "âœ… **3. Schema Management**\n",
    "- Schema Inference vs Explicit Schema\n",
    "- Zalecenie: explicit schema dla production\n",
    "- Schema evolution z mergeSchema\n",
    "- Data type validation\n",
    "\n",
    "âœ… **4. Error Handling**\n",
    "- badRecordsPath dla quarantine\n",
    "- Parse modes: PERMISSIVE, DROPMALFORMED, FAILFAST\n",
    "- Rescue columns (_corrupt_record)\n",
    "- Monitoring bad records\n",
    "\n",
    "âœ… **5. CTAS Patterns**\n",
    "- CREATE TABLE AS SELECT dla transformacji\n",
    "- Bronze â†’ Silver data quality improvements\n",
    "- Aggregate tables (summary views)\n",
    "- RÃ³Å¼nice: CTAS vs COPY INTO vs INSERT\n",
    "\n",
    "âœ… **6. Lakeflow Connect**\n",
    "- Managed connectors dla SaaS/databases\n",
    "- No-code integration przez UI\n",
    "- Automatic CDC dla incremental sync\n",
    "- Use cases: Salesforce, SQL Server, etc.\n",
    "\n",
    "âœ… **7. Best Practices**\n",
    "- WydajnoÅ›Ä‡: file sizes, partitioning, parallelism\n",
    "- Governance: Unity Catalog, tagging, audit trail\n",
    "- Idempotency patterns\n",
    "- Code quality templates\n",
    "\n",
    "âœ… **8. Troubleshooting**\n",
    "- File already processed errors\n",
    "- Schema mismatch resolution\n",
    "- Bad records debugging\n",
    "- Performance tuning\n",
    "- Common pitfalls & solutions\n",
    "\n",
    "### Kluczowe wnioski:\n",
    "\n",
    "ğŸ’¡ **1. COPY INTO > INSERT**\n",
    "```\n",
    "Zawsze preferuj COPY INTO dla batch loads:\n",
    "- Automatic idempotency\n",
    "- File tracking\n",
    "- No duplicates na retry\n",
    "```\n",
    "\n",
    "ğŸ’¡ **2. Explicit Schema > Inference**\n",
    "```\n",
    "Production pipelines wymagajÄ…:\n",
    "- Zdefiniowany schema (CREATE TABLE)\n",
    "- CAST transformations\n",
    "- Data type validation\n",
    "```\n",
    "\n",
    "ğŸ’¡ **3. Always Error Handling**\n",
    "```\n",
    "KaÅ¼dy production pipeline musi mieÄ‡:\n",
    "- badRecordsPath\n",
    "- PERMISSIVE mode\n",
    "- Monitoring & alerting\n",
    "- Retry logic\n",
    "```\n",
    "\n",
    "ğŸ’¡ **4. Audit Everything**\n",
    "```\n",
    "Dodawaj metadata columns:\n",
    "- _ingestion_timestamp\n",
    "- _source_file\n",
    "- _ingestion_user\n",
    "- _job_id\n",
    "```\n",
    "\n",
    "ğŸ’¡ **5. Optimize for Performance**\n",
    "```\n",
    "Best practices:\n",
    "- Parquet > CSV/JSON\n",
    "- Files: 256MB - 1GB\n",
    "- Partition wisely\n",
    "- Auto-optimize wÅ‚Ä…czone\n",
    "```\n",
    "\n",
    "### Quick Reference - Decision Tree\n",
    "\n",
    "```\n",
    "Mam nowe dane do zaÅ‚adowania...\n",
    "\n",
    "â”œâ”€ Czy to jednorazowy load historycznych danych?\n",
    "â”‚  â””â”€ TAK â†’ UÅ¼yj CTAS\n",
    "â”‚     CREATE TABLE AS SELECT * FROM source\n",
    "â”‚\n",
    "â”œâ”€ Czy to incremental load (nowe pliki regularnie)?\n",
    "â”‚  â””â”€ TAK â†’ UÅ¼yj COPY INTO\n",
    "â”‚     COPY INTO table FROM 'path/*.parquet'\n",
    "â”‚\n",
    "â”œâ”€ Czy to SaaS/database integration?\n",
    "â”‚  â””â”€ TAK â†’ UÅ¼yj Lakeflow Connect\n",
    "â”‚     UI â†’ Workflows â†’ Create Lakeflow\n",
    "â”‚\n",
    "â”œâ”€ Czy to high-frequency (< 1h) micro-batches?\n",
    "â”‚  â””â”€ TAK â†’ UÅ¼yj Auto Loader (streaming)\n",
    "â”‚     spark.readStream.format(\"cloudFiles\")\n",
    "â”‚\n",
    "â””â”€ Czy to upserts (UPDATE + INSERT)?\n",
    "   â””â”€ TAK â†’ UÅ¼yj MERGE INTO\n",
    "      MERGE INTO target USING source ON ...\n",
    "```\n",
    "\n",
    "### Comparison Matrix\n",
    "\n",
    "| Feature | COPY INTO | CTAS | INSERT | Lakeflow |\n",
    "|---------|-----------|------|--------|----------|\n",
    "| **Idempotency** | âœ… Yes | âŒ No | âŒ No | âœ… Yes |\n",
    "| **Use Case** | Incremental | One-time | Append | SaaS/CDC |\n",
    "| **Code Required** | SQL | SQL | SQL | No-code UI |\n",
    "| **Performance** | Fast | Fast | Medium | Fast |\n",
    "| **File Tracking** | âœ… Auto | âŒ No | âŒ No | âœ… Auto |\n",
    "| **CDC Support** | âŒ No | âŒ No | âŒ No | âœ… Yes |\n",
    "| **Schema Evolution** | âœ… Optional | âœ… From SELECT | âŒ No | âœ… Auto |\n",
    "| **Best For** | Daily/hourly loads | Transformations | Small appends | Managed integrations |\n",
    "\n",
    "### NastÄ™pne kroki w szkoleniu:\n",
    "\n",
    "**ğŸ“š Kolejny Notebook:**\n",
    "- **03_streaming_data_ingestion.ipynb**\n",
    "  - Auto Loader (cloudFiles)\n",
    "  - Structured Streaming\n",
    "  - Micro-batch processing\n",
    "  - Stream-to-Delta patterns\n",
    "\n",
    "**ğŸ› ï¸ Warsztat Praktyczny:**\n",
    "- **02_ingestion_pipeline_workshop.ipynb**\n",
    "  - Hands-on: Build end-to-end ingestion pipeline\n",
    "  - Real-world scenario z multiple sources\n",
    "  - Error handling implementation\n",
    "  - Monitoring & alerting setup\n",
    "\n",
    "**ğŸ“– MateriaÅ‚y Dodatkowe:**\n",
    "- Delta Lake Documentation: COPY INTO\n",
    "- Databricks Blog: Lakeflow Connect Best Practices\n",
    "- Unity Catalog: Governance Guide\n",
    "\n",
    "### Zadanie Domowe (Optional):\n",
    "\n",
    "**Zadanie:** Zbuduj complete batch ingestion pipeline dla wÅ‚asnych danych\n",
    "\n",
    "**Requirements:**\n",
    "1. âœ… Use COPY INTO dla idempotency\n",
    "2. âœ… Explicit schema definition (no inference)\n",
    "3. âœ… Error handling z badRecordsPath\n",
    "4. âœ… Audit columns (_ingestion_timestamp, _source_file)\n",
    "5. âœ… CTAS transformation Bronze â†’ Silver\n",
    "6. âœ… Data quality checks (NOT NULL, valid ranges)\n",
    "7. âœ… Unity Catalog tagging (PII, retention)\n",
    "8. âœ… Monitoring metrics (success rate, row counts)\n",
    "\n",
    "**Bonus:**\n",
    "- Schedule jako Databricks Job (daily/hourly)\n",
    "- Integrate z Lakeflow Connect (jeÅ›li masz SaaS source)\n",
    "- Add alerting on failures\n",
    "\n",
    "### Feedback & Questions\n",
    "\n",
    "Masz pytania? ZnalazÅ‚eÅ› bÅ‚Ä…d? \n",
    "\n",
    "ğŸ“§ Kontakt: instruktor@example.com\n",
    "ğŸ“ Issues: github.com/company/databricks-training\n",
    "\n",
    "---\n",
    "\n",
    "**Gratulacje!** ğŸ‰ \n",
    "UkoÅ„czyÅ‚eÅ› notebook o Batch Data Ingestion. \n",
    "JesteÅ› gotowy do pracy z production-grade data pipelines w Delta Lake!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae0e313",
   "metadata": {},
   "source": [
    "### Opcja 1: Zachowaj dane (zalecane dla szkolenia)\n",
    "\n",
    "Zostaw tabele dla kolejnych notebookÃ³w:\n",
    "- `02_medallion_architecture.ipynb` bÄ™dzie uÅ¼ywaÅ‚ tych danych\n",
    "- `04_bronze_silver_gold_pipeline.ipynb` transformuje te tabele\n",
    "- Warsztaty praktyczne rÃ³wnieÅ¼ korzystajÄ… z Bronze tables\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31d1b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcja 1: SprawdÅº utworzone zasoby (bez usuwania)\n",
    "\n",
    "print(\"=== Utworzone tabele w tym notebooku ===\\n\")\n",
    "\n",
    "created_tables = [\n",
    "    f\"{BRONZE_SCHEMA}.customers_batch\",\n",
    "    f\"{BRONZE_SCHEMA}.orders_batch\",\n",
    "    f\"{BRONZE_SCHEMA}.products_batch\",\n",
    "    f\"{BRONZE_SCHEMA}.customers_with_validation\",\n",
    "    f\"{SILVER_SCHEMA}.customer_segment_summary\",\n",
    "    f\"{SILVER_SCHEMA}.customers_clean\"\n",
    "]\n",
    "\n",
    "total_size_bytes = 0\n",
    "\n",
    "for table in created_tables:\n",
    "    full_table = f\"{CATALOG}.{table}\"\n",
    "    try:\n",
    "        if spark.catalog.tableExists(full_table):\n",
    "            count = spark.table(full_table).count()\n",
    "            \n",
    "            # Pobierz rozmiar\n",
    "            detail = spark.sql(f\"DESCRIBE DETAIL {full_table}\").collect()[0]\n",
    "            size_bytes = detail['sizeInBytes']\n",
    "            size_mb = size_bytes / (1024 * 1024)\n",
    "            total_size_bytes += size_bytes\n",
    "            \n",
    "            print(f\"âœ… {table}\")\n",
    "            print(f\"   Rekordy: {count:,}\")\n",
    "            print(f\"   Rozmiar: {size_mb:.2f} MB\")\n",
    "            print()\n",
    "        else:\n",
    "            print(f\"âš ï¸  {table} - nie istnieje\")\n",
    "            print()\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  {table} - bÅ‚Ä…d: {str(e)}\")\n",
    "        print()\n",
    "\n",
    "total_size_mb = total_size_bytes / (1024 * 1024)\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"ÅÄ…czny rozmiar: {total_size_mb:.2f} MB\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Dane sÄ… zachowane dla kolejnych notebookÃ³w\")\n",
    "print(\"ğŸ’¡ Aby usunÄ…Ä‡, uruchom komÃ³rkÄ™ poniÅ¼ej (Opcja 2)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa14e6d8",
   "metadata": {},
   "source": [
    "### Opcja 2: UsuÅ„ wszystkie zasoby (tylko jeÅ›li naprawdÄ™ chcesz)\n",
    "\n",
    "**UWAGA:** To usunie wszystkie tabele i dane utworzone w tym notebooku!\n",
    "\n",
    "Uruchom komÃ³rkÄ™ poniÅ¼ej tylko jeÅ›li:\n",
    "- SkoÅ„czyÅ‚eÅ› szkolenie i chcesz posprzÄ…taÄ‡\n",
    "- Chcesz zaczÄ…Ä‡ od nowa (fresh start)\n",
    "- Testujesz notebook i potrzebujesz clean slate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe29e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opcja 2: UsuÅ„ wszystkie zasoby\n",
    "# UWAGA: To jest DESTRUKCYJNE! Odkomentuj tylko jeÅ›li jesteÅ› pewien!\n",
    "\n",
    "CLEANUP_ENABLED = False  # ZmieÅ„ na True aby wÅ‚Ä…czyÄ‡ cleanup\n",
    "\n",
    "if CLEANUP_ENABLED:\n",
    "    print(\"ğŸ—‘ï¸  Usuwanie zasobÃ³w...\\n\")\n",
    "    \n",
    "    # Lista tabel do usuniÄ™cia\n",
    "    tables_to_drop = [\n",
    "        f\"{CATALOG}.{BRONZE_SCHEMA}.customers_batch\",\n",
    "        f\"{CATALOG}.{BRONZE_SCHEMA}.orders_batch\",\n",
    "        f\"{CATALOG}.{BRONZE_SCHEMA}.products_batch\",\n",
    "        f\"{CATALOG}.{BRONZE_SCHEMA}.customers_with_validation\",\n",
    "        f\"{CATALOG}.{SILVER_SCHEMA}.customer_segment_summary\",\n",
    "        f\"{CATALOG}.{SILVER_SCHEMA}.customers_clean\"\n",
    "    ]\n",
    "    \n",
    "    dropped_count = 0\n",
    "    \n",
    "    for table in tables_to_drop:\n",
    "        try:\n",
    "            spark.sql(f\"DROP TABLE IF EXISTS {table}\")\n",
    "            print(f\"âœ… UsuniÄ™to: {table}\")\n",
    "            dropped_count += 1\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸  BÅ‚Ä…d przy usuwaniu {table}: {str(e)}\")\n",
    "    \n",
    "    # UsuÅ„ folder bad records\n",
    "    try:\n",
    "        BAD_RECORDS_PATH = f\"/tmp/{raw_user}/bad_records\"\n",
    "        dbutils.fs.rm(BAD_RECORDS_PATH, True)\n",
    "        print(f\"\\nâœ… UsuniÄ™to bad records: {BAD_RECORDS_PATH}\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nâš ï¸  BÅ‚Ä…d przy usuwaniu bad records: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"âœ… Cleanup zakoÅ„czony!\")\n",
    "    print(f\"   UsuniÄ™tych tabel: {dropped_count}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸  Cleanup WYÅÄ„CZONY\")\n",
    "    print(\"   Aby wÅ‚Ä…czyÄ‡, zmieÅ„ CLEANUP_ENABLED = True\")\n",
    "    print(\"   i uruchom komÃ³rkÄ™ ponownie\")\n",
    "    print(\"\\nğŸ’¡ Zalecane: Zostaw dane dla kolejnych notebookÃ³w!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f5ba29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ‰ Gratulacje!\n",
    "\n",
    "UkoÅ„czyÅ‚eÅ› notebook **02_batch_data_ingestion.ipynb**!\n",
    "\n",
    "**NauczyÅ‚eÅ› siÄ™:**\n",
    "- âœ… COPY INTO dla idempotentnego Å‚adowania danych\n",
    "- âœ… ObsÅ‚ugi rÃ³Å¼nych formatÃ³w plikÃ³w (CSV, JSON, Parquet)\n",
    "- âœ… Schema management (inference vs enforcement)\n",
    "- âœ… Error handling z badRecordsPath\n",
    "- âœ… CTAS patterns dla transformacji\n",
    "- âœ… Lakeflow Connect dla managed integrations\n",
    "- âœ… Best practices dla production pipelines\n",
    "\n",
    "**NastÄ™pne kroki:**\n",
    "- ğŸ“– **Kolejny notebook:** `02_medallion_architecture.ipynb` - Bronze/Silver/Gold patterns\n",
    "- ğŸ› ï¸ **Warsztat:** `02_ingestion_pipeline_workshop.ipynb` - Hands-on exercise\n",
    "- ğŸ“š **Dokumentacja:** Delta Lake COPY INTO reference\n",
    "\n",
    "**Pytania?** Skontaktuj siÄ™ z instruktorem lub sprawdÅº dokumentacjÄ™ Databricks.\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook:** `02_batch_data_ingestion.ipynb`  \n",
    "**Czas trwania:** ~45 minut  \n",
    "**Poziom:** Intermediate  \n",
    "**Ostatnia aktualizacja:** Listopad 2024\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
