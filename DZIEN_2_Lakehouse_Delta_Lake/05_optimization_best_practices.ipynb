{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23cffde0-db08-4560-8121-7f59b784a13f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Optymalizacja i najlepsze praktyki - Demo\n",
    "\n",
    "**Cel szkoleniowy:** Opanowanie technik optymalizacji performance'u zapyta≈Ñ i tabel Delta w Databricks.\n",
    "\n",
    "**Zakres tematyczny:**\n",
    "- Optymalizacja zapyta≈Ñ: predicate pushdown, file pruning, column pruning\n",
    "- Analiza planu fizycznego (explain())\n",
    "- Optymalizacja tabel: partitioning, small files problem\n",
    "- Auto optimize / auto compaction\n",
    "- Dob√≥r rozmiaru plik√≥w i strategii ZORDER\n",
    "- Monitoring i diagnoza performance issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0dfc4662-9aaa-4a92-8d26-2ebb05cfa80c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Kontekst i wymagania\n",
    "\n",
    "- **Dzie≈Ñ szkolenia**: Dzie≈Ñ 2 - Lakehouse & Delta Lake\n",
    "- **Typ notebooka**: Demo + Workshop\n",
    "- **Wymagania techniczne**:\n",
    "  - Databricks Runtime 13.0+ (zalecane: 14.3 LTS)\n",
    "  - Unity Catalog w≈ÇƒÖczony\n",
    "  - Uprawnienia: CREATE TABLE, CREATE SCHEMA, SELECT, MODIFY\n",
    "  - Klaster: Standard z minimum 2 workers (zalecane: i3.xlarge lub wy≈ºej)\n",
    "- **Zale≈ºno≈õci**: Wykonany notebook 01_delta_lake_operations.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9200a175-242b-4dc7-acd4-288781f4e3df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Wstƒôp teoretyczny\n",
    "\n",
    "**Cel sekcji:** Zrozumienie kluczowych mechanizm√≥w optymalizacji w Databricks i Delta Lake.\n",
    "\n",
    "**Typy optymalizacji:**\n",
    "\n",
    "**1. Optymalizacja zapyta≈Ñ (Query Optimization):**\n",
    "- **Predicate Pushdown**: Przeniesienie filtr√≥w jak najni≈ºej w planie wykonania\n",
    "- **Column Pruning**: Odczyt tylko wymaganych kolumn (kolumnowy format Parquet)\n",
    "- **File Pruning**: Ominiƒôcie plik√≥w nieistotnych dla zapytania\n",
    "- **Join Optimization**: Broadcast joins, bucket joins, sortmerge joins\n",
    "\n",
    "**2. Optymalizacja tabel (Table Optimization):**\n",
    "- **Partitioning**: Fizyczne rozdzielenie danych wed≈Çug kluczy\n",
    "- **Z-Ordering**: Klasterowanie danych w plikach wed≈Çug wybranych kolumn\n",
    "- **Compaction (OPTIMIZE)**: ≈ÅƒÖczenie ma≈Çych plik√≥w w wiƒôksze\n",
    "- **Auto Compaction**: Automatyczne ≈ÇƒÖczenie podczas zapisu\n",
    "\n",
    "**3. Small Files Problem:**\n",
    "Problem wydajno≈õci wywo≈Çany przez zbyt wiele ma≈Çych plik√≥w w tabeli. Spark preferuje pliki 128MB-1GB dla optymalnej wydajno≈õci."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00ee9b43-acbf-4216-a499-8bea3d48739b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Izolacja per u≈ºytkownik\n",
    "\n",
    "Uruchom skrypt inicjalizacyjny dla per-user izolacji katalog√≥w i schemat√≥w:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a250c520-ae2e-4004-b2d6-f2994e411e6c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../00_setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6163cff-16e5-44b6-9cce-a3bf932fe679",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Konfiguracja\n",
    "\n",
    "Import bibliotek i ustawienie zmiennych ≈õrodowiskowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a0cc155-45ac-4679-bf40-0efeb89e7b09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lit, count, avg, sum, max, min\n",
    "from pyspark.sql.types import *\n",
    "import time\n",
    "\n",
    "# Wy≈õwietl kontekst u≈ºytkownika\n",
    "print(\"=== Kontekst u≈ºytkownika ===\")\n",
    "print(f\"Katalog: {CATALOG}\")\n",
    "print(f\"Schema Bronze: {BRONZE_SCHEMA}\")\n",
    "print(f\"Schema Silver: {SILVER_SCHEMA}\")\n",
    "print(f\"Schema Gold: {GOLD_SCHEMA}\")\n",
    "print(f\"U≈ºytkownik: {raw_user}\")\n",
    "\n",
    "# Ustaw katalog i schemat jako domy≈õlne\n",
    "spark.sql(f\"USE CATALOG {CATALOG}\")\n",
    "spark.sql(f\"USE SCHEMA {BRONZE_SCHEMA}\")\n",
    "\n",
    "# Nazwy tabel (za≈Ço≈ºenie: notebook 01_delta_lake_operations zosta≈Ç wykonany)\n",
    "CUSTOMERS_DELTA = f\"{BRONZE_SCHEMA}.customers_delta\"\n",
    "ORDERS_DELTA = f\"{BRONZE_SCHEMA}.orders_delta\"\n",
    "\n",
    "print(f\"\\n=== Tabele do optymalizacji ===\")\n",
    "print(f\"Customers: {CUSTOMERS_DELTA}\")\n",
    "print(f\"Orders: {ORDERS_DELTA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bd499c1-678e-463f-9081-7ef9b14301c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 1: Analiza planu fizycznego (explain())\n",
    "\n",
    "**Cel sekcji:** Naucz siƒô analizowaƒá plany wykonania zapyta≈Ñ, aby zidentyfikowaƒá bottlenecki wydajno≈õciowe.\n",
    "\n",
    "**Teoria:**\n",
    "Plan fizyczny to szczeg√≥≈Çowa mapa tego, jak Spark wykonuje zapytanie:\n",
    "- **Stages**: Logiczne etapy przetwarzania\n",
    "- **Tasks**: Jednostki pracy wykonywane na partycjach\n",
    "- **Shuffles**: Wymiana danych miƒôdzy executor'ami\n",
    "- **Pushdowns**: Optymalizacje przeniesione do ≈∫r√≥d≈Ça danych\n",
    "\n",
    "**Rodzaje explain():**\n",
    "- `explain()` - podstawowy plan\n",
    "- `explain(True)` - pe≈Çny plan z detalami\n",
    "- `explain('extended')` - rozszerzony plan\n",
    "- `explain('cost')` - plan z kosztem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "918895ad-596e-4f57-bdb7-05d587e80952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 1.1: Analiza planu prostego zapytania"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6531acdf-98d5-45d5-a312-5f369f2fa2ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.1 - Analiza planu prostego zapytania\n",
    "\n",
    "# U≈ºyj tabeli customers_delta utworzonej w 01_delta_lake_operations.ipynb\n",
    "# CUSTOMERS_DELTA jest ju≈º zdefiniowany w sekcji Konfiguracja\n",
    "\n",
    "print(\"=== Podstawowy plan zapytania ===\")\n",
    "simple_query = spark.sql(f\"\"\"\n",
    "    SELECT customer_id, first_name, last_name, customer_segment, city\n",
    "    FROM {CUSTOMERS_DELTA}\n",
    "    WHERE customer_segment = 'Premium'\n",
    "    ORDER BY customer_id DESC\n",
    "    LIMIT 10\n",
    "\"\"\")\n",
    "\n",
    "# Wy≈õwietl podstawowy plan\n",
    "simple_query.explain()\n",
    "\n",
    "print(\"\\n=== Rozszerzony plan zapytania ===\")\n",
    "# Wy≈õwietl rozszerzony plan\n",
    "simple_query.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d9af332-8c17-4825-9496-b9029a9421ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 1.2: Predicate Pushdown w praktyce\n",
    "\n",
    "**Teoria:** Predicate pushdown to optymalizacja, gdzie filtry (warunki WHERE) sƒÖ \"przepychane\" jak najni≈ºej w planie wykonania, najlepiej do poziomu czytania plik√≥w. Dziƒôki temu czytamy tylko dane, kt√≥re spe≈ÇniajƒÖ warunki."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73394fb4-ff75-4f2d-aae5-1f68db484ec6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 1.2 - Predicate Pushdown\n",
    "\n",
    "# U≈ºyj tabeli orders_delta utworzonej w 01_delta_lake_operations.ipynb\n",
    "# ORDERS_DELTA jest ju≈º zdefiniowany w sekcji Konfiguracja\n",
    "\n",
    "print(\"=== Zapytanie z filtrami (predicate pushdown) ===\")\n",
    "filtered_query = spark.sql(f\"\"\"\n",
    "    SELECT order_id, customer_id, total_amount, order_date\n",
    "    FROM {ORDERS_DELTA}\n",
    "    WHERE total_amount > 100 \n",
    "    AND order_date >= '2024-01-01'\n",
    "\"\"\")\n",
    "\n",
    "# Sprawd≈∫ plan - poszukaj \"PushedFilters\" w planie\n",
    "filtered_query.explain(True)\n",
    "\n",
    "print(\"\\nüí° Znajd≈∫ w planie:\")\n",
    "print(\"- 'PushedFilters' - filtry przepchniƒôte do poziomu czytania\")\n",
    "print(\"- 'ReadSchema' - tylko wybrane kolumny (column pruning)\")\n",
    "print(\"- 'PartitionFilters' - filtry na partycjach\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a51281f-4f44-4ea6-82f2-996f7edaee07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 2: Strategia partycjonowania\n",
    "\n",
    "**Cel sekcji:** Nauka wyboru optymalnych kluczy partycjonowania dla najlepszej wydajno≈õci.\n",
    "\n",
    "**Teoria partycjonowania:**\n",
    "- **Partitioning**: Fizyczne rozdzielenie tabeli na katalogi wed≈Çug warto≈õci kolumn\n",
    "- **Partition Pruning**: Spark pomija ca≈Çe partycje, kt√≥re nie sƒÖ potrzebne dla zapytania\n",
    "- **Idealne partycje**: 1-10GB danych na partycjƒô, nie wiƒôcej ni≈º 10,000 partycji\n",
    "\n",
    "**Najlepsze praktyki:**\n",
    "- Partycjonuj wed≈Çug kolumn czƒôsto u≈ºywanych w filtrach\n",
    "- Unikaj partycjonowania wed≈Çug kolumn o wysokiej kardinalno≈õci\n",
    "- Preferuj kolumny z naturalnƒÖ hierarchiƒÖ czasowƒÖ (rok/miesiƒÖc/dzie≈Ñ)\n",
    "- Unikaj zbyt wielu ma≈Çych partycji (small files problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0665d14-7df4-4304-b215-ad5531b8a72a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 2.1: Tworzenie tabeli partycjonowanej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00e19ec-cf42-486f-be1f-0082e1e602d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 2.1 - Tworzenie tabeli partycjonowanej\n",
    "\n",
    "# Utw√≥rz tabelƒô partycjonowanƒÖ wed≈Çug roku i miesiƒÖca\n",
    "ORDERS_PARTITIONED = f\"{BRONZE_SCHEMA}.orders_partitioned\"\n",
    "\n",
    "print(\"=== Tworzenie tabeli partycjonowanej ===\")\n",
    "\n",
    "# Dodaj kolumny do partycjonowania\n",
    "orders_with_partitions = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        *,\n",
    "        YEAR(order_date) as year,\n",
    "        MONTH(order_date) as month\n",
    "    FROM {ORDERS_DELTA}\n",
    "\"\"\")\n",
    "\n",
    "# Zapisz jako tabelƒô partycjonowanƒÖ\n",
    "orders_with_partitions.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .saveAsTable(ORDERS_PARTITIONED)\n",
    "\n",
    "print(f\"‚úì Tabela {ORDERS_PARTITIONED} utworzona z partycjonowaniem wed≈Çug roku i miesiƒÖca\")\n",
    "\n",
    "# Sprawd≈∫ strukturƒô partycji\n",
    "spark.sql(f\"DESCRIBE DETAIL {ORDERS_PARTITIONED}\").select(\"name\", \"location\", \"partitionColumns\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26ef8762-1a93-41d5-83af-0545643857ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 2.2: Partition Pruning w dzia≈Çaniu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e30be36f-2a13-44f8-85a8-05c76566b45a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 2.2 - Partition Pruning\n",
    "\n",
    "print(\"=== Zapytanie wykorzystujƒÖce partition pruning ===\")\n",
    "\n",
    "# Zapytanie kt√≥ry wykorzystuje partycje (rok/miesiƒÖc)\n",
    "efficient_query = spark.sql(f\"\"\"\n",
    "    SELECT order_id, customer_id, total_amount, order_date\n",
    "    FROM {ORDERS_PARTITIONED}\n",
    "    WHERE year = 2024 AND month = 1\n",
    "\"\"\")\n",
    "\n",
    "# Sprawd≈∫ plan - poszukaj \"PartitionFilters\"\n",
    "efficient_query.explain(True)\n",
    "\n",
    "print(\"\\n=== Por√≥wnanie: zapytanie BEZ partition pruning ===\")\n",
    "\n",
    "# Zapytanie kt√≥re nie wykorzystuje partycji (nie filtruje po roku/miesiƒÖcu)\n",
    "inefficient_query = spark.sql(f\"\"\"\n",
    "    SELECT order_id, customer_id, total_amount, order_date\n",
    "    FROM {ORDERS_PARTITIONED}\n",
    "    WHERE customer_id = 1\n",
    "\"\"\")\n",
    "\n",
    "inefficient_query.explain(True)\n",
    "\n",
    "print(\"\\nüí° Por√≥wnaj plany:\")\n",
    "print(\"- Pierwszy: partition pruning aktywny (PartitionFilters)\")\n",
    "print(\"- Drugi: brak partition pruning - czyta wszystkie partycje\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8db85bc0-8aa8-4630-9e18-69e5530fea96",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 3: Small Files Problem\n",
    "\n",
    "**Cel sekcji:** Zrozumienie i rozwiƒÖzanie problemu ma≈Çych plik√≥w w Delta Lake.\n",
    "\n",
    "**Co to jest Small Files Problem?**\n",
    "- Gdy tabela ma zbyt wiele ma≈Çych plik√≥w (< 128MB ka≈ºdy)\n",
    "- Spark preferuje pliki 128MB-1GB dla optymalnej wydajno≈õci\n",
    "- Ma≈Çe pliki powodujƒÖ overhead w metadanych i zmniejszajƒÖ throughput\n",
    "\n",
    "**Przyczyny ma≈Çych plik√≥w:**\n",
    "- Czƒôste zapisy INSERT w ma≈Çych batch'ach\n",
    "- Wysokie partycjonowanie z ma≈ÇƒÖ ilo≈õciƒÖ danych na partycjƒô\n",
    "- Streaming z kr√≥tkimi trigger intervals\n",
    "\n",
    "**RozwiƒÖzania:**\n",
    "- **OPTIMIZE** - ≈ÇƒÖczy ma≈Çe pliki w wiƒôksze\n",
    "- **Auto Compaction** - automatyczne ≈ÇƒÖczenie podczas zapisu\n",
    "- **Repartition** przed zapisem\n",
    "- **Coalesce** dla zmniejszenia liczby partycji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "965ad450-5f1a-4e33-ac31-ba922037d071",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 3.1: Symulacja Small Files Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "22a608c1-9ec7-43a3-9b0b-abfd0658aebd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 3.1 - Symulacja Small Files Problem\n",
    "\n",
    "SMALL_FILES_TABLE = f\"{BRONZE_SCHEMA}.small_files_demo\"\n",
    "\n",
    "print(\"=== Tworzenie tabeli z ma≈Çymi plikami ===\")\n",
    "\n",
    "# Symuluj wiele ma≈Çych zapis√≥w (ka≈ºdy tworzy osobny plik)\n",
    "for i in range(5):\n",
    "    small_batch = spark.range(i*100, (i+1)*100).select(\n",
    "        col(\"id\"),\n",
    "        (col(\"id\") * 2).alias(\"value\"),\n",
    "        lit(f\"batch_{i}\").alias(\"batch_name\")\n",
    "    )\n",
    "    \n",
    "    small_batch.write \\\n",
    "        .format(\"delta\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .saveAsTable(SMALL_FILES_TABLE)\n",
    "    \n",
    "    print(f\"‚úì Zapisano batch {i}\")\n",
    "\n",
    "print(\"\\n=== Sprawdzenie liczby plik√≥w ===\")\n",
    "detail = spark.sql(f\"DESCRIBE DETAIL {SMALL_FILES_TABLE}\").collect()[0]\n",
    "print(f\"Liczba plik√≥w: {detail['numFiles']}\")\n",
    "print(f\"Rozmiar tabeli: {detail['sizeInBytes']} bytes\")\n",
    "print(f\"≈öredni rozmiar pliku: {detail['sizeInBytes'] / detail['numFiles']:.0f} bytes\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Problem: zbyt wiele ma≈Çych plik√≥w!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d568f02-9857-4fa2-bbdd-ef467fa13477",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 3.2: RozwiƒÖzanie - OPTIMIZE i Auto Compaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8d3c587-4607-41fa-b0db-3e1ab0774a60",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 3.2 - RozwiƒÖzanie Small Files Problem\n",
    "\n",
    "print(\"=== RozwiƒÖzanie: OPTIMIZE ===\")\n",
    "\n",
    "# Wykonaj OPTIMIZE na tabeli z ma≈Çymi plikami\n",
    "spark.sql(f\"OPTIMIZE {SMALL_FILES_TABLE}\")\n",
    "\n",
    "# Sprawd≈∫ stan po OPTIMIZE\n",
    "detail_after = spark.sql(f\"DESCRIBE DETAIL {SMALL_FILES_TABLE}\").collect()[0]\n",
    "print(f\"\\nPo OPTIMIZE:\")\n",
    "print(f\"Liczba plik√≥w: {detail_after['numFiles']}\")\n",
    "print(f\"Rozmiar tabeli: {detail_after['sizeInBytes']} bytes\")\n",
    "print(f\"≈öredni rozmiar pliku: {detail_after['sizeInBytes'] / detail_after['numFiles']:.0f} bytes\")\n",
    "\n",
    "print(\"\\n=== Konfiguracja Auto Compaction ===\")\n",
    "\n",
    "# W≈ÇƒÖcz auto compaction dla przysz≈Çych zapis√≥w\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {SMALL_FILES_TABLE}\n",
    "    SET TBLPROPERTIES (\n",
    "        'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "        'delta.autoOptimize.autoCompact' = 'true'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Auto Compaction w≈ÇƒÖczony\")\n",
    "\n",
    "# Sprawd≈∫ w≈Ça≈õciwo≈õci tabeli\n",
    "properties = spark.sql(f\"SHOW TBLPROPERTIES {SMALL_FILES_TABLE}\").collect()\n",
    "auto_props = [p for p in properties if 'autoOptimize' in p['key']]\n",
    "print(\"\\nW≈ÇƒÖczone w≈Ça≈õciwo≈õci Auto Compaction:\")\n",
    "for prop in auto_props:\n",
    "    print(f\"- {prop['key']}: {prop['value']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38c75d4e-fc27-4cfc-b053-d93b48b5baef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Sekcja 4: ZORDER BY - Advanced Clustering\n",
    "\n",
    "**Cel sekcji:** Nauka wykorzystania ZORDER BY do optymalizacji zapyta≈Ñ z filtrami i joinami.\n",
    "\n",
    "**Co to jest ZORDER BY?**\n",
    "- Multi-dimensional clustering algorithm w Delta Lake\n",
    "- Organizuje dane w plikach wed≈Çug warto≈õci wybranych kolumn\n",
    "- Poprawia data skipping - pomijanie niepotrzebnych plik√≥w podczas czytania\n",
    "- Szczeg√≥lnie skuteczny dla kolumn czƒôsto u≈ºywanych w filtrach WHERE i JOIN\n",
    "\n",
    "**Kiedy u≈ºywaƒá ZORDER:**\n",
    "- Kolumny czƒôsto filtrowane w zapytaniach\n",
    "- Kolumny u≈ºywane w JOIN operations\n",
    "- High-cardinality columns (wiele unikalnych warto≈õci)\n",
    "- Maksymalnie 3-4 kolumny (wiƒôcej = diminishing returns)\n",
    "\n",
    "**ZORDER vs Partitioning:**\n",
    "- Partitioning: fizyczne rozdzielenie na katalogi\n",
    "- ZORDER: logiczne uporzƒÖdkowanie w plikach (zachowuje pojedynczƒÖ strukturƒô folder√≥w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da4d32df-ebbe-427d-ba8c-720d1006449c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 4.1: ZORDER BY dla czƒôsto filtrowanych kolumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c5e7c6-4377-40d4-9e18-46ff53c6a9e3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 4.1 - ZORDER BY\n",
    "\n",
    "print(\"=== Wykonanie ZORDER BY ===\")\n",
    "\n",
    "# Za≈Ç√≥≈ºmy, ≈ºe czƒôsto filtrujemy podle customer_id i order_date\n",
    "# Wykonaj ZORDER BY na tych kolumnach\n",
    "spark.sql(f\"\"\"\n",
    "    OPTIMIZE {ORDERS_DELTA}\n",
    "    ZORDER BY (customer_id, order_date)\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì ZORDER BY wykonany na kolumnach: customer_id, order_date\")\n",
    "\n",
    "# Sprawd≈∫ historiƒô optymalizacji\n",
    "print(\"\\n=== Historia operacji OPTIMIZE ===\")\n",
    "history = spark.sql(f\"DESCRIBE HISTORY {ORDERS_DELTA}\")\n",
    "optimize_operations = history.filter(col(\"operation\") == \"OPTIMIZE\")\n",
    "\n",
    "display(\n",
    "    optimize_operations.select(\n",
    "        \"version\", \"timestamp\", \"operation\", \n",
    "        \"operationParameters\", \"operationMetrics\"\n",
    "    ).orderBy(\"version\", ascending=False).limit(3)\n",
    ")\n",
    "\n",
    "print(\"\\nüí° Sprawd≈∫ 'operationParameters' - powinno zawieraƒá informacjƒô o ZORDER\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc6cc295-74eb-4af8-b88c-07402191fa62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Przyk≈Çad 4.2: Pomiar skuteczno≈õci ZORDER - Data Skipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc56cf1a-23cd-4ede-aa44-64333b3af744",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Przyk≈Çad 4.2 - Pomiar skuteczno≈õci ZORDER\n",
    "\n",
    "import time\n",
    "\n",
    "print(\"=== Test skuteczno≈õci ZORDER BY ===\")\n",
    "\n",
    "# Zapytanie wykorzystujƒÖce kolumny z ZORDER\n",
    "test_query = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*), AVG(total_amount)\n",
    "    FROM {ORDERS_DELTA}\n",
    "    WHERE customer_id BETWEEN 100 AND 200\n",
    "    AND order_date >= '2024-01-01'\n",
    "\"\"\")\n",
    "\n",
    "# Pomiar czasu wykonania\n",
    "start_time = time.time()\n",
    "result = test_query.collect()\n",
    "end_time = time.time()\n",
    "\n",
    "print(f\"Wynik: {result[0]}\")\n",
    "print(f\"Czas wykonania: {end_time - start_time:.2f} sekund\")\n",
    "\n",
    "# Sprawd≈∫ plan zapytania - data skipping\n",
    "print(\"\\n=== Plan zapytania (sprawd≈∫ data skipping) ===\")\n",
    "test_query.explain(True)\n",
    "\n",
    "print(\"\\nüí° W planie szukaj:\")\n",
    "print(\"- 'numFilesTotal' vs 'numFilesSelected' - ile plik√≥w pominiƒôto\")\n",
    "print(\"- 'metadata time' - czas parsowania metadanych\")\n",
    "print(\"- 'files pruned' - data skipping statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1ca93de-4605-4f85-8401-a40985237e12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Best Practices - Przewodnik optymalizacji\n",
    "\n",
    "### üéØ Strategia optymalizacji (w kolejno≈õci priorytet√≥w):\n",
    "\n",
    "**1. Analiza workload:**\n",
    "- Zidentyfikuj najczƒô≈õciej wykonywane zapytania\n",
    "- Znajd≈∫ kolumny najczƒô≈õciej u≈ºywane w filtrach WHERE\n",
    "- Sprawd≈∫ kt√≥re zapytania zajmujƒÖ najwiƒôcej czasu\n",
    "\n",
    "**2. Optymalizacja zapyta≈Ñ:**\n",
    "- U≈ºywaj filtr√≥w WHERE jak najwcze≈õniej w zapytaniu\n",
    "- Wybieraj tylko potrzebne kolumny (SELECT specific columns, nie *)\n",
    "- Preferuj JOIN na zindeksowanych kolumnach\n",
    "- U≈ºywaj LIMIT gdy mo≈ºliwe\n",
    "\n",
    "**3. Optymalizacja tabel:**\n",
    "- **Partitioning**: Tylko dla du≈ºych tabel (>1TB) i czƒôsto filtrowanych kolumn\n",
    "- **ZORDER BY**: Dla 2-4 najczƒô≈õciej filtrowanych kolumn\n",
    "- **OPTIMIZE**: Regularnie (np. daily) dla aktywnych tabel\n",
    "- **Auto Compaction**: W≈ÇƒÖcz dla tabel z czƒôstymi zapisami\n",
    "\n",
    "**4. Monitoring i maintenance:**\n",
    "- Regularnie sprawdzaj `DESCRIBE DETAIL` - liczba plik√≥w, rozmiar\n",
    "- Uruchamiaj `VACUUM` co najmniej raz w tygodniu\n",
    "- Monitoruj Spark UI dla d≈Çugo dzia≈ÇajƒÖcych zapyta≈Ñ\n",
    "- U≈ºywaj `explain()` do analizy problemowych zapyta≈Ñ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59f7365e-c56b-49d8-8b5c-f76a13678b8e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Troubleshooting - Diagnoza problem√≥w wydajno≈õciowych\n",
    "\n",
    "### üîç Najczƒôstsze problemy i rozwiƒÖzania:\n",
    "\n",
    "**Problem 1: Zapytanie dzia≈Ça bardzo wolno**\n",
    "```python\n",
    "# Diagnoza:\n",
    "df.explain(True)  # Sprawd≈∫ plan wykonania\n",
    "```\n",
    "**Mo≈ºliwe przyczyny:**\n",
    "- Brak filtr√≥w - czyta ca≈ÇƒÖ tabelƒô\n",
    "- Shuffle operations - du≈ºo ruchu sieciowego  \n",
    "- Skewed data - nier√≥wnomierne roz≈Ço≈ºenie danych\n",
    "- Small files - zbyt wiele ma≈Çych plik√≥w\n",
    "\n",
    "**Problem 2: \"OutOfMemoryError\" podczas JOIN**\n",
    "**RozwiƒÖzanie:**\n",
    "```python\n",
    "# Zwiƒôksz partycje przed JOIN\n",
    "df1 = df1.repartition(200, \"join_key\")\n",
    "df2 = df2.repartition(200, \"join_key\")\n",
    "\n",
    "# Lub u≈ºyj broadcast join dla ma≈Çych tabel\n",
    "from pyspark.sql.functions import broadcast\n",
    "result = large_df.join(broadcast(small_df), \"key\")\n",
    "```\n",
    "\n",
    "**Problem 3: D≈Çugie czasy zapisu do Delta**\n",
    "**RozwiƒÖzanie:**\n",
    "- W≈ÇƒÖcz Auto Compaction\n",
    "- U≈ºyj `coalesce()` przed zapisem\n",
    "- Avoid zbyt wysokie partycjonowanie\n",
    "\n",
    "**Problem 4: OPTIMIZE nie poprawia wydajno≈õci**\n",
    "**Przyczyna:** ZORDER BY jest potrzebny dla specific query patterns\n",
    "```python\n",
    "# Zamiast samego OPTIMIZE:\n",
    "OPTIMIZE table_name\n",
    "\n",
    "# U≈ºyj OPTIMIZE z ZORDER:\n",
    "OPTIMIZE table_name ZORDER BY (frequently_filtered_columns)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6ca92322-c50a-41aa-8a24-007833c526ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Podsumowanie\n",
    "\n",
    "### ‚úÖ W tym notebooku nauczyli≈õmy siƒô:\n",
    "\n",
    "**Analiza wydajno≈õci:**\n",
    "- Czytanie i interpretacja plan√≥w fizycznych z `explain()`\n",
    "- Identyfikacja bottleneck√≥w: shuffles, skewed data, small files\n",
    "- Wykorzystanie predicate pushdown i column pruning\n",
    "\n",
    "**Optymalizacja tabel:**\n",
    "- Strategia partycjonowania wed≈Çug czƒôsto filtrowanych kolumn\n",
    "- ZORDER BY dla multi-dimensional clustering\n",
    "- RozwiƒÖzywanie Small Files Problem przez OPTIMIZE\n",
    "- Auto Compaction dla automatycznej optymalizacji\n",
    "\n",
    "**Monitoring i maintenance:**\n",
    "- Regularne wykonywanie DESCRIBE DETAIL\n",
    "- Strategia VACUUM dla storage management\n",
    "- Pomiar skuteczno≈õci optymalizacji\n",
    "\n",
    "### üéØ Kluczowe wnioski:\n",
    "\n",
    "1. **Analiza przed optymalizacjƒÖ**: Zawsze najpierw zdiagnozuj problem przez `explain()`\n",
    "2. **Partitioning ‚â† ZORDER**: R√≥≈ºne techniki dla r√≥≈ºnych przypadk√≥w u≈ºycia\n",
    "3. **Small files = performance killer**: Regularne OPTIMIZE jest konieczne\n",
    "4. **ZORDER BY**: Maksymalnie 3-4 kolumny, wybieraj najczƒô≈õciej filtrowane\n",
    "5. **Auto Compaction**: Must-have dla production workloads\n",
    "\n",
    "### üìà Metryki do monitorowania:\n",
    "\n",
    "| Metryka | Dobra warto≈õƒá | Akcja je≈õli przekroczona |\n",
    "|---------|---------------|--------------------------|\n",
    "| Liczba plik√≥w | < 1000/TB | OPTIMIZE |\n",
    "| ≈öredni rozmiar pliku | 128MB-1GB | OPTIMIZE + Auto Compaction |\n",
    "| Czas zapytania | Baseline +20% | Analiza explain(), ZORDER |\n",
    "| Skipped files ratio | >80% | ZORDER BY na filtrowanych kolumnach |\n",
    "\n",
    "### üöÄ Nastƒôpne kroki:\n",
    "- **02_medallion_architecture.ipynb**: Architektura Bronze/Silver/Gold\n",
    "- **03_batch_streaming_load.ipynb**: Copy Into, Auto Loader, Streaming\n",
    "- **04_bronze_silver_gold_pipeline.ipynb**: End-to-end data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d22f78-2445-487b-931a-e6eb94f7209c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Opcjonalnie usu≈Ñ tabele demo utworzone podczas ƒáwicze≈Ñ:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5b1ec66-98c3-49a5-a03a-8a180a2958a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup - usu≈Ñ tabele demo\n",
    "\n",
    "# Odkomentuj poni≈ºsze linie aby usunƒÖƒá tabele demo:\n",
    "\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {ORDERS_PARTITIONED}\")\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {SMALL_FILES_TABLE}\")\n",
    "\n",
    "# print(\"‚úì Tabele demo usuniƒôte\")\n",
    "\n",
    "print(\"Cleanup wy≈ÇƒÖczony (odkomentuj kod aby usunƒÖƒá tabele demo)\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05_optimization_best_practices",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
